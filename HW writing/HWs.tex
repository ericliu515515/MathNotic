\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}

\title{\Huge{HWs}}
\author{\huge{Eric Liu}}
\date{}
\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{General Analysis HW}
\section{HW1}
\begin{question}{}{}
Show $\R^n$ is complete.
\end{question}
\begin{proof}
Let $\textbf{x}_k$ be an arbitrary Cauchy sequence in $\R^n$. We are required to show  $\textbf{x}_k$ converge in $\R^n$. For each $k$, denote  $\textbf{x}_k$ by $(x_{(1,k)},\dots ,x_{(n,k)})$. We claim that for each $i \in \set{1,\dots ,n}$
\begin{align*}
x_{(i,k)}\text{ is a Cauchy sequence }
\end{align*}
Fix $i$ and $\epsilon >0$. To show $x_{(i,k)}$ is a Cauchy sequence, we are required to find $N\inn$ such that for all $r,m\geq N$ we have 
\begin{align*}
\abso{x_{(i,r)}-x_{(i,m)}}\leq \epsilon 
\end{align*}
Because $\textbf{x}_k$ is a Cauchy sequence in $\R^n$, we know there exists  $N\inn$ such that for all $r,m\geq N$, we have 
\begin{align*}
\abso{\textbf{x}_r-\textbf{x}_m}< \epsilon 
\end{align*}
Fix such $N$ and arbitrary $r,m\geq M$. Observe 
\begin{align*}
\abso{x_{(i,r)}-x_{(i,m)}}\leq \sqrt{\sum_{j=1}^n \abso{x_{(j,r)}-x_{(j,m)}}^2}= \abso{\textbf{x}_r - \textbf{x}_m}< \epsilon 
\end{align*}
We have proved that for each $i\in \set{1,\dots ,n}$, the real sequence $x_{(i,k)}$ is Cauchy. We now claim that for each $i \in \set{1,\dots, n}$, we have 
\begin{align*}
  \limsup_{r\to\infty} x_{(i,r)}\inr \text{ and }\lim_{k\to \infty}x_{(i,k)}= \limsup_{r\to\infty} x_{(i,r)}
\end{align*}
Again fix $i$. Because $x_{(i,k)}$ is a Cauchy sequence, we know there exists some $N$ such that for all $r,m\geq N$, we have 
 \begin{align*}
\abso{x_{(i,r)}-x_{(i,m)}}<1
\end{align*}
This implies that for all $r\geq N$, we have 
\begin{align}
x_{(i,r)}< x_{(i,N)}+1 \label{ran1}
\end{align}
\myref{Equation}{ran1} then tell us 
\begin{align*}
x_{(i,N)}+1\text{ is an upper bound of }\set{x_{(i,r)}:r\geq N}
\end{align*}
Then by definition of $\sup $, we have
\begin{align*}
\sup \set{x_{(i,r)}:r\geq N} \leq x_{(i,N)}+1 \inr
\end{align*}
This then implies $\limsup_{r\to\infty} x_{(i,r)}\inr$. We now prove 
\begin{align}
\label{ran2}
\lim_{k\to \infty}x_{(i,k)}= \limsup_{r\to\infty} x_{(i,r)}
\end{align}
Fix $\epsilon >0$. We are required to find $N$ such that
\begin{align*}
\forall k\geq N, \abso{x_{(i,k)}-\limsup_{r\to\infty}x_{(i,r)}} \leq \epsilon 
\end{align*}
Because $\set{x_{(i,k)}}_{k\inn}$ is a Cauchy sequence, we can let $N_0$ satisfy 
\begin{align*}
\forall k,m\geq N_0, \abso{x_{(i,k)}-x_{(i,m)}}<\frac{\epsilon}{2}
\end{align*}
Because $\sup \set{x_{(i,k)}:k\geq N'}\searrow \limsup_{r\to\infty} x_{(i,r)}$ as $N'\to \infty$, we know there exists $N_1>N_0$ such that 
 \begin{align*}
\limsup_{r\to\infty} x_{(i,r)}-\frac{\epsilon}{2} <\sup \set{x_{(i,k)}:k\geq N_0}\leq \limsup_{r\to\infty}  x_{(i,r)} + \frac{\epsilon}{2} 
\end{align*}
Then because $\limsup_{r\to\infty} x_{(i,r)}-\frac{\epsilon}{2}$ is strictly smaller than the smallest upper bound of $\set{x_{(i,k)}:k\geq N_1}$, we see $\limsup_{n\to\infty} x_{(i,r)}-\frac{\epsilon}{2}$ is not an upper bound of $\set{x_{(i,k)}:k\geq N_1}$. This implies the existence of some $N$ such that  $N\geq N_1$ and
\begin{align*}
\limsup_{r\to\infty} x_{(i,r)}-\frac{\epsilon}{2}< x_{(i,N)}\leq \limsup_{r\to\infty}x_{(i,r)} + \frac{\epsilon}{2}
\end{align*}
Now observe that for all $k\geq N$, because $N\geq N_1\geq N_0$ 
\begin{align*}
  \limsup_{r\to\infty} x_{(i,r)} - \epsilon <x_{(i,N)}-\frac{\epsilon}{2}<x_{(i,k)}< x_{(i,N)}+\frac{\epsilon}{2} \leq \limsup_{r\to\infty} x_{(i,r)} + \epsilon   
\end{align*}
This implies for all $k\geq N$, we have 
\begin{align*}
\abso{x_{(i,k)}-\limsup_{r\to\infty} x_{(i,r)}}\leq \epsilon 
\end{align*}
We have just proved \myref{Equation}{ran2}. Lastly, to close out the proof, we show 
\begin{align}
\label{ran3}
\lim_{k\to \infty}\textbf{x}_k = \Big(\lim_{k\to \infty}x_{(1,k)},\dots ,\lim_{k\to \infty}x_{(n,k)}\Big)
\end{align}
Fix $\epsilon >0$. For each $i \in \set{1,\dots ,n}$, let $N_i$ satisfy 
 \begin{align*}
  \forall r\geq N_i ,\abso{x_{(i,r)}- \lim_{k\to \infty}x_{(i,k)}}\leq  \frac{\epsilon }{\sqrt{n} }
\end{align*}
Observe that for all $r\geq \max_{i\in \set{1,\dots,n}} N_i$, we have 
\begin{align*}
\abso{\textbf{x}_r - \Big(\lim_{k\to \infty}x_{(1,k)},\dots ,\lim_{k\to \infty}x_{(n,k)} \Big)}&=\sqrt{\sum_{i=1}^n \abso{x_{(i,r)}- \lim_{k\to \infty}x_{(i,k)}}^2} \\
&\leq \sqrt{\sum_{i=1}^n \frac{\epsilon^2}{n}}=\epsilon 
\end{align*}
We have proved \myref{Equation}{ran3}.











\end{proof}
\begin{question}{}{}
Show $\Q$ is dense in $\R$. 
\end{question}
\begin{proof}
Fix $x\inr$ and $\epsilon >0$. To show $\Q$ is dense in  $\R$, we have to find $q\inq$ such that $\abso{x-q}<\epsilon $.\\

Let $m\inn$ satisfy $\frac{1}{m}<\epsilon $. Let $n$ be the largest integer such that $n\leq mx$. Because $n$ is the largest integer such that $n\leq mx$, we know $mx-n<1$, otherwise we can deduce $n+1\leq mx$, which is impossible, since $n+1$ is an integer and $n$ is the largest integer such that $n\leq mx$. We now see that 
\begin{align*}
\frac{n}{m}\inq \text{ and }\abso{x-\frac{n}{m}}=\frac{mx-n}{m}< \frac{1}{m}<\epsilon 
\end{align*}



\end{proof}
\begin{theorem}
\textbf{(Distance Formula)} Given two subsets $A,B$ of a metric space, we have 
\begin{align*}
d(A,B)= \inf_{b \in B}d(A,b)
\end{align*}
\end{theorem}
\begin{proof}
Fix arbitrary $b \in B$. It is clear that 
\begin{align*}
d(A,B)\leq d(A,b)
\end{align*}
It then follows $d(A,B)\leq \inf_{b \in B}d(A,b)$. Fix arbitrary $a \in A$ and $b_0 \in B$. Observe that 
\begin{align*}
d(a,b_0)\geq  d(A,b_0) \geq \inf_{b \in B}d(A,b)
\end{align*}
It then follows $\inf_{b \in B}d(A,b)\leq d(A,B)$. 
\end{proof}
\begin{question}{}{}
Let $E_1,E_2$ be non-empty sets in  $\R^n$ with $E_1$ closed and  $E_2$ compact. Show that there are points  $x_1\in E_1$ and $x_2 \in E_2$ such that 
\begin{align*}
d(E_1,E_2)=\abso{x_1-x_2}
\end{align*}
Deduce that $d(E_1,E_2)$ is positive if such $E_1,E_2$ are disjoint. 
\end{question}

\begin{proof}
Because 
\begin{enumerate}[label=(\alph*)]
  \item $f(x)\triangleq d (E_1,x)$ is a continuous function on $\R^n$. 
  \item $E_2$ is compact. 
\end{enumerate}
It now follows by EVT there exists some  $x_2 \in E_2$ such that 
\begin{align*}
d(E_1,x_2)=\min_{x \in E_2} d(E_1,x)=\inf_{x \in E_2}d(E_1,x)=d(E_1,E_2)
\end{align*}
where the last equality is proved above. We can now reduce the problem into finding $x_1$ in $E_1$ such that  
\begin{align*}
  \vi{d(x_1,x_2)=d(E_1,x_2)}
\end{align*}
For each $n\inn$, let $t_n$ satisfy 
 \begin{align*}
t_n \in E_1\text{ and }d(t_n,x_2)< d(E_1,x_2) + \frac{1}{n}
\end{align*}
Clearly, $t_n$ is a bounded sequence. Then by Bolzano-Weierstrass Theorem, there exists a convergence subsequence $t_{n_k}$. Now, because $E_1$ is closed, we know 
\begin{align*}
x_1\triangleq \lim_{k\to \infty}t_{n_k} \in E_1
\end{align*}
It then follows from the function $f(x)\triangleq d(x,x_2)$ being continuous on $\R^n$ such that 
 \begin{align*}
d(x_1,x_2)=\lim_{k\to \infty}d(t_{n,k},x_2)=d(E_1,x_2)
\end{align*}
\end{proof}
\begin{question}{}{}
Prove that the distance between two nonempty, compact, disjoint sets in $\R^n$ is positive. 
\end{question}
\begin{proof}
The proof follows from the result in last question while acknowledging compact is closed. 
\end{proof}
\begin{question}{}{}
Prove that if $f$ is continuous on $[a,b]$, then $f$ is Riemann-integrable on$[a,b]$. 
\end{question}
\begin{proof}
Let $\overline{\int_a^b}fdx$ and $\underline{\int_a^b}fdx$ respectively denote the upper and lower Darboux sums. We prove that 
\begin{align*}
\overline{\int_a^b}fdx=\underline{\int_a^b}fdx
\end{align*}
Fix $\epsilon $. We reduce the problem into proving the existence of some partition $\set{a=x_0,x_1,\dots ,x_n=b}$ such that  
\begin{align*}
  \sum_{i=1}^n \Big[M_i-m_i \Big](x_i-x_{i-1}) \leq \epsilon 
\end{align*}
where 
\begin{align*}
M_i\triangleq \sup_{t \in [x_{i-1},x_i]}f(t)\text{ and }m_i \triangleq \inf_{t \in [x_{i-1},x_i]}f(t)
\end{align*}
Because $f$ is continuous on the compact interval $[a,b]$, we know $f$ is uniformly continuous on $[a,b]$. Let $\delta$ satisfy 
\begin{align*}
\abso{x-y}< \delta \text{ and }x,y \in [a,b] \implies \abso{f(x)-f(y)}< \frac{\epsilon }{b-a}
\end{align*}
Let $n$ satisfy $\frac{b-a }{n}<\delta$. We claim the partition 
\begin{align*}
\set{a=x_0,x_1,\dots ,x_n=b}\text{ where }x_i\triangleq a+\frac{i(b-a)}{n}\text{ suffices }
\end{align*}
Now, by EVT, we know that for each $i$, there exists some  $t_{i,M},t_{i,m} \in [x_{i-1},x_i]$ such that 
\begin{align*}
f(t_{i,m})=m_i\text{ and }f(t_{i,M})=M_i
\end{align*}
Then because 
\begin{align*}
\abso{t_{i,m}-t_{i,M}}\leq x_i-x_{i-1}\leq \frac{b-a}{n}<\delta 
\end{align*}
We know $M_i-m_i< \frac{\epsilon}{b-a}$. This now give us 
\begin{align*}
  \sum_{i=1}^n \Big[M_i-m_i \Big](x_i-x_{i-1})&< \sum_{i=1}^n \frac{\epsilon }{(b-a)}(x_i-x_{i-1})\\
  &= \frac{\epsilon }{b-a}\sum_{i=1}^n (x_i-x_{i-1})\\
  &=\frac{\epsilon}{b-a}(b-a)=\epsilon 
\end{align*}



\end{proof}
\begin{question}{}{}
Find $\limsup_{n\to\infty} E_n$ and $\liminf_{n\to\infty} E_n$ where 
\begin{align*}
E_n\triangleq \begin{cases}
  [\frac{-1}{n},1]& \text{ if $n$ is odd }\\
  [-1,\frac{1}{n}]& \text{ if $n$ is even }
\end{cases}
\end{align*}
\end{question}
\begin{proof}
Fix arbitrary $n\inn$. Let $p,q\geq n$ respectively be odd and even. We see 
\begin{align*}
[0,1]\subseteq E_p \text{ and }[-1,0]\subseteq E_q
\end{align*}
This now implies 
\begin{align*}
[-1,1]\subseteq \bigcup_{k\geq n}E_k 
\end{align*}
Then because $n$ is arbitrary, it follows 
 \begin{align*}
\limsup_{n\to\infty} E_n=\bigcap_{n=1}^{\infty}\bigcup_{k\geq n} E_k = [-1,1]
\end{align*}
Again, fix arbitrary $n\inn$ and $\epsilon >0$. Let $p,q$ respectively be even and odd integers greater than  $\max \set{n,\frac{1}{\epsilon }}$. We now see 
\begin{align*}
\epsilon \not\in[-1 , \frac{1}{p}]=E_p\text{ and }-\epsilon  \not\in  [\frac{-1}{q},1]=E_q
\end{align*}
Because $\epsilon $ is arbitrary and clearly $0 \in E_k$  for all $k$,  we now see 
\begin{align*}
\bigcap_{k\geq n} E_k= \set{0}
\end{align*}
Then because $n$ is arbitrary, we see 
\begin{align*}
\liminf_{n\to\infty} E_n = \bigcup_{n=1}^{\infty}\bigcap_{k\geq n}E_k=\set{0}
\end{align*}


\end{proof}
\begin{question}{}{}
Show that 
\begin{align*}
  (\limsup_{n\to\infty} E_n)^c =\liminf_{n\to\infty} (E_n)^c
\end{align*}
and 
\begin{align*}
E_n\searrow E\text{ or }E_n\nearrow E \implies \limsup_{n\to\infty} E_n=\liminf_{n\to\infty} E_n=E
\end{align*}
\end{question}
\begin{proof}
Fix arbitrary $x\in (\limsup_{n\to\infty} E_n)^c$. We can deduce 
\begin{align*}
\exists n, x \not\in \bigcup_{k\geq n}E_k
\end{align*}
This implies 
\begin{align*}
\exists n, x\in \bigcap_{k\geq  n}E_k^c
\end{align*}
Then we see 
\begin{align*}
x \in \bigcup_{n=1}^{\infty} \bigcap_{k\geq n} E_k^c= \liminf_{n\to\infty} E_n^c
\end{align*}
We have proved $(\limsup_{n\to\infty} E_n)^c \subseteq \liminf_{n\to\infty} E_n^c$. We now prove the converse. Fix arbitrary $x\in \liminf_{n\to\infty} E_n^c$. We can deduce
\begin{align*}
\exists  n, x\in \bigcap_{k\geq n }E_k^c
\end{align*}
This implies 
\begin{align*}
\exists n, x\not\in \bigcup_{k\geq n}E_k
\end{align*}
Then we see 
\begin{align*}
x \not\in \bigcap_{n=1}^{\infty} \bigcup_{k\geq n }E_k = \limsup_{n\to\infty} E_n
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(Equivalent Definition for Limit Superior)}
If we let $E$ be the set of subsequential limits of $a_n$
 \begin{align*}
E\triangleq \set{L\in\overline{\R}:L=\lim_{k\to \infty}a_{n_k}\text{ for some }n_k}
\end{align*}
The set $E$ is non-empty and 
\begin{align*}
\max E=\limsup_{n\to\infty} a_n
\end{align*}
\end{theorem}
\begin{proof}
Let $n_1\triangleq 1$. Recursively, because
\begin{align*}
\sup_{j\geq n_k}a_k\geq \limsup_{n\to\infty} a_n>\limsup_{n\to\infty} a_n - \frac{1}{k}\text{ for each }k
\end{align*}
We can let $n_{k+1}$ be the smallest number such that 
\begin{align*}
a_{n_{k+1}}>\limsup_{n\to\infty} a_n - \frac{1}{k}
\end{align*}
It is straightforward to check $a_{n_k}\to \limsup_{n\to\infty} a_n$ as $k\to \infty$. Note that no subsequence can converge to $\limsup_{n\to\infty} a_n+\epsilon $ because there exists $N$ such that  $\sup_{k\geq N}a_k<\limsup_{n\to\infty} a_n+\epsilon $. 
\end{proof}
\begin{question}{}{}
Show that 
\begin{align*}
\limsup_{n\to\infty} (-a_n)=- \liminf_{n\to\infty}  a_n
\end{align*}
\end{question}
\begin{proof}
Note that $-a_{n_k}$ converge if and only if $a_{n_k}$ converge. Then if we respectively define $E$ and $E^-$ to be the set of subsequential limits of  $a_n$ and  $-a_n$, we see 
 \begin{align*}
E^-= \set{-L\inr : L \in E }
\end{align*}
We now see 
\begin{align*}
\limsup_{n\to\infty} (-a_n)=\max E^-= -\min E =- \liminf_{n\to\infty} a_n
\end{align*}

\end{proof}
\begin{question}{}{}
Show that 
\begin{align}
\label{pb}
\limsup_{n\to\infty} (a_n+b_n)\leq \limsup_{n\to\infty} a_n+\limsup_{n\to\infty}  b_n
\end{align}
\end{question}
\begin{proof}
Fix arbitrary $\epsilon $. Let $N_a,N_b$ respectively satisfy 
 \begin{align*}
 \sup_{n\geq N_a}a_n\leq \limsup_{n\to\infty} a_n + \frac{\epsilon}{2}\text{ and }\sup_{n \geq N_b}b_n \leq \limsup_{n\to\infty} b_n + \frac{\epsilon}{2}
\end{align*}
Let $N\triangleq \max \set{N_a,N_b}$. We now see that 
\begin{align*}
\limsup_{n\to\infty} (a_n+b_n)\leq \sup_{n \geq N} (a_n+b_n)\leq \limsup_{n\to\infty} a_n + \limsup_{n\to\infty} b_n + \epsilon 
\end{align*}
The result then follows from $\epsilon $ being arbitrary.  
\end{proof}
\begin{question}{}{}
\begin{align}
\label{pc}
a_n,b_n\text{ is bounded non-negative }\implies \limsup_{n\to\infty} (a_nb_n)\leq (\limsup_{n\to\infty} a_n)(\limsup_{n\to\infty} b_n)
\end{align}
\end{question}
\begin{proof}
There are three cases we should consider 
\begin{enumerate}[label=(\alph*)]
  \item Both  $\limsup_{n\to\infty} a_n$ and $\limsup_{n\to\infty} b_n$ equal $0$. 
  \item Between $\limsup_{n\to\infty} a_n$ and $\limsup_{n\to\infty} b_n$ , only one of them equals $0$.  
  \item Neither  $\limsup_{n\to\infty} a_n$ nor $\limsup_{n\to\infty} b_n$ equals to $0$. 
\end{enumerate}
In the first case, because $a_n,b_n$ are both non-negative, we can deduce 
\begin{align*}
\lim_{n\to \infty}a_n=\lim_{n\to \infty}b_n=0
\end{align*}
which implies 
\begin{align*}
\limsup_{n\to\infty} (a_nb_n)=\lim_{n\to \infty}a_nb_n=0=\lim_{n\to \infty}a_n \lim_{n\to \infty}b_n
\end{align*}
For second case, WOLG, suppose $\limsup_{n\to\infty} a_n=0$. Fix arbitrary $\epsilon $. We can let $N$ satisfy 
 \begin{align*}
\sup_{n\geq N} a_n < \frac{\epsilon }{\sup_{n\inn}b_n}
\end{align*}
Since for all $n \geq N$, we have 
\begin{align*}
a_nb_n \leq \frac{b_n\epsilon }{\sup_{k\inn}b_k}\leq \epsilon 
\end{align*}

We now see 
\begin{align*}
\limsup_{n\to\infty} (a_nb_n)\leq \sup_{n\geq N}a_nb_n \leq \epsilon 
\end{align*}
The result 
\begin{align*}
\limsup_{n\to\infty} a_nb_n=0=\limsup_{n\to\infty} a_n \limsup_{n\to\infty} b_n
\end{align*}
then follows from $\epsilon $ being arbitrary. \\

Lastly, for the last case, let $N_a,N_b$ respectively satisfy 
 \begin{align*}
\sup_{n\geq N_a} a_n\leq \limsup_{n\to\infty} a_n \sqrt{1+\epsilon } \text{ and }\sup_{n \geq  N_b}b_n\leq \limsup_{n\to\infty} b_n \sqrt{1+\epsilon }  
\end{align*}
Let $N\triangleq \max \set{N_a,N_b}$, because for each  $n\geq N$, we have 
\begin{align*}
a_nb_n\leq (\sup_{k\geq N_a} a_k)(\sup_{k\geq N_b}b_k) \leq (1+\epsilon  )(\limsup_{n\to\infty} a_n)(\limsup_{n\to\infty} b_n)
\end{align*}
It then follows that 
\begin{align*}
\limsup_{n\to\infty} (a_nb_n)\leq \sup_{n\geq N}(a_nb_n)\leq (1+\epsilon )(\limsup_{n\to\infty} a_n)(\limsup_{n\to\infty}  b_n)
\end{align*}
The result then follows from $\epsilon $ being arbitrary.
\end{proof}

\begin{question}{}{}
Show that if either $a_n$ or $b_n$ converge, the equalities in \myref{Equation}{pb} and  \myref{Equation}{pc} both hold true. 
\end{question}
\begin{proof}
WOLG, suppose $\lim_{n\to \infty}a_n=L\inr$. We then see 
\begin{align*}
  (a_{n_k}+b_{n_k})\text{ converge }\iff b_{n,k}\text{ converge }
\end{align*}
Let $E_{a,b}$ and $E_b$ respectively be the set of subsequential limits of $(a_n+b_n)$ and $b_n$. We now have 
\begin{align*}
E_{a,b}=\set{L+L_b \inr : L_b \in E_b}
\end{align*}
This give us 
\begin{align*}
\limsup_{n\to\infty} (a_n+b_n)=\max E_{a,b}= L+ \max E_b= \limsup_{n\to\infty} a_n +\limsup_{n\to\infty} b_n
\end{align*}
Now, additionally, suppose $a_n,b_n$ are both bounded and nonnegative. Again because 
\begin{align*}
a_{n_k}b_{n,k}\text{ converge }\iff b_{n,k}\text{ converge }
\end{align*}
We see 
\begin{align*}
E_{a,b}=\set{L(L_b)\inr: L_b \in E_b}
\end{align*}
This give us 
\begin{align*}
\limsup_{n\to\infty} (a_nb_n)=\max E_{a,b}=L \max E_b= (\limsup_{n\to\infty} a_n)(\limsup_{n\to\infty} b_n)
\end{align*}

\end{proof}
\begin{question}{}{}
Give example for which inequality in \myref{Equation}{pb} and \myref{Equation}{pc} are not equalities. 
\end{question}
\begin{proof}
If 
\begin{align*}
a_n\triangleq \begin{cases}
  1& \text{ if $n$ is odd }\\
  -1& \text{ if $n$ is even }
\end{cases}\text{ and }b_n\triangleq \begin{cases}
  -1& \text{ if $n$ is odd }\\
  1& \text{ if $n$ is even }
\end{cases}
\end{align*}
we have 
\begin{align*}
\limsup_{n\to\infty} (a_n+b_n)=0<2=\limsup_{n\to\infty} a_n+\limsup_{n\to\infty} b_n
\end{align*}
Let $L>1$ and 
\begin{align*}
a_n\triangleq \begin{cases}
  L-\frac{1}{k}& \text{ if $n=2k-1$ }\\
  (L-\frac{1}{k})^{-1}& \text{ if $n=2k$ }
\end{cases} \text{ and }b_n\triangleq \begin{cases}
(L-\frac{1}{k})^{-1}& \text{ if $n=2k-1$ }\\
(L-\frac{1}{k})& \text{ if $n=2k$ }
\end{cases}
\end{align*}
We have 
\begin{align*}
\limsup_{n\to\infty} a_nb_n=1< L^2= \limsup_{n\to\infty} a_n \limsup_{n\to\infty}  b_n
\end{align*}
\end{proof}
\begin{question}{}{}
Give an example of a decreasing sequence of nonempty closed sets in $\R^n$ whose intersection is empty. 
\end{question}
\begin{proof}
\begin{align*}
F_n\triangleq  [n,\infty)\text{ suffices }
\end{align*}
\end{proof}
\begin{question}{}{}
Given an example of two disjoint, nonempty closed sets in $E_1$ and  $E_2$ in  $\R^n$ for which  $d(E_1,E_2)=0$.
\end{question}
\begin{proof}
Let 
\begin{align*}
E_1\triangleq  \set{n-\frac{1}{n}\inr: n\inn \text{ and }n\geq 2 }\text{ and }E_2\triangleq \set{n-\frac{1}{2n}\inr: n\inn\text{ and }n\geq 2}
\end{align*}
To see $E_1\cap E_2=\varnothing$, suppose $n-\frac{1}{n}=k-\frac{1}{2k}$ where $n,k$ are two natural numbers greater than $2$. We then see $\frac{1}{n}- \frac{1}{2k}=n-k$, which is impossible, since 
\begin{align*}
\abso{\frac{1}{n}-\frac{1}{2k}}< \max \set{\frac{1}{2k},\frac{1}{n}}<1
\end{align*}
The fact $E_1,E_2$ are closed follows from both of them being totally disconnected. Now observe that for all $\epsilon $, there exists large enough $n$ such that 
\begin{align*}
  (n+\frac{1}{n})-(n+\frac{1}{2n})<\frac{1}{n}< \epsilon 
\end{align*}
This implies $d(E_1,E_2)=0$.
\end{proof}
\begin{question}{}{}
If $f$ is defined and uniformly continuous on $E$, show there is a function  $\overline{f}$ defined and continuous on $\overline{E}$ such that $\overline{f}=f$ on $E$.
\end{question}
\begin{proof}
Define $\overline{f}$ on $E$ by $\overline{f}=f$. For each $x \in \overline{E}\setminus E$, associate $x$ with a sequence $t_{n,x}$ in $E$ converging to  $x$.  We now claim that for each $x\in \overline{E}\setminus E$ the limit 
\begin{align*}
\lim_{n\to \infty}f(t_{n,x})\text{ converge in $\R$ }
\end{align*}
Fix $\epsilon $. Because $f$ is uniformly continuous on $E$, we know there exists  $\delta$ such that 
\begin{align*}
a,b \in E \text{ and } \abso{a-b}\leq \delta \implies \abso{f(a)-f(b)}< \epsilon 
\end{align*}
Because $t_{n,x}$ converge, we know $t_{n,x}$ is Cauchy, then we know there exists $N$ such that $\abso{t_{n,x}-t_{m,x}}<\delta$ for all $n,m>N$, we then see that for all $n,m>N$, we have 
 \begin{align*}
\abso{f(t_{n,x})-f(t_{m,x})}<\epsilon 
\end{align*}
This implies $\set{f(t_{n,x})}_{n\inn}$ is a Cauchy sequence in $\R$, thus converge in $\R$.  \\

Define 
\begin{align*}
\overline{f}(x)\triangleq \lim_{n\to \infty}f(t_{n,x})\text{ for all }x\in \overline{E}\setminus E
\end{align*}
We are required to show $\overline{f}$ is also continuous on $\overline{E}\setminus E$. Fix $\epsilon \text{ and }x \in \overline{E}\setminus E$. Let $\delta$ satisfy 
\begin{align*}
a,b \in E\text{ and }\abso{a-b}\leq \delta \implies \abso{f(a)-f(b)}<\frac{\epsilon}{3}
\end{align*}
We claim 
\begin{align*}
\sup_{t \in B_{\frac{\delta}{2}}(x)\cap \overline{E}} \abso{\overline{f}(t)-\overline{f}(x)}\leq \epsilon 
\end{align*}
Fix $t\in B_{\frac{\delta}{2}}(x)\cap \overline{E}$. There are two possibilities 
\begin{enumerate}[label=(\alph*)]
  \item $t\in E$
  \item $t \in \overline{E}\setminus E$
\end{enumerate}
If $t\in E$, let $n$ satisfy 
 \begin{align*}
\abso{f(t_{n,x})-\overline{f}(x)}<\frac{\epsilon}{3}\text{ and }\abso{t_{n,x}-x}< \frac{\delta}{2}
\end{align*}
Because 
\begin{align*}
\abso{t_{n,x}-t}\leq \abso{t_{n,x}-x}+ \abso{t-x}<\delta
\end{align*}
we can deduce $\abso{f(t_{n,x})-f(t)}<\frac{\epsilon}{3}$. This now give us 
\begin{align*}
\abso{f(t)-\overline{f}(x)}\leq \abso{f(t_{n,x})-f(t)}+ \abso{f(t_{n,x})-\overline{f}(x)}<\epsilon 
\end{align*}
If $t \in \overline{E}\setminus E$. Write $y=t$ and let  $t_{n,y}$ be the associated sequence in $E$. Because $y \in B_{\frac{\delta}{2}}(x)$, we know there exists $t_{n,y}$ such that 
\begin{align*}
t_{n,y} \in B_{\frac{\delta}{2}}(x)\text{ and }\abso{f(t_{n,y})-\overline{f}(y)}< \frac{\epsilon}{3}
\end{align*}
Again, let $m$ satisfy 
\begin{align*}
t_{m,x} \in B_{\frac{\delta}{2}}(x)\text{ and }\abso{f(t_{m,x})-\overline{f}(x)}<\frac{\epsilon}{3}
\end{align*}
We know $\abso{t_{n,y}-t_{m,x}}\leq \delta $ because they both belong to $B_{\frac{\delta}{2}}(x)$. We can now deduce 
\begin{align*}
\abso{\overline{f}(y)-\overline{f}(x)}= \abso{\overline{f}(y)-f(t_{n,y})}+\abso{f(t_{n,y})-f(t_{m,x})}+ \abso{f(t_{m,x})-\overline{f}(x)}<\epsilon 
\end{align*}
which finish the proof.
\end{proof}
\begin{question}{}{}
If $f$ is defined and uniformly continuous on a bounded set $E$, show that $f$ is bounded on $E$.
\end{question}
\begin{proof}
By last question, we can extend $f$ to a continuous $\overline{f}$ onto $\overline{E}$. Now because $\overline{E}$ is compact and $\abso{\overline{f}}$ is continuous on $\overline{E}$, by EVT, there exists  $a\in \overline{E}$ such that 
\begin{align*}
\sup_{x\in E}\abso{f(x)}\leq \max_{x\in \overline{E}}\abso{f(x)}=f(a)
\end{align*}
\end{proof}
\section{HW2}
\begin{question}{}{}
Construct a two-dimensional Cantor set in the unit square $[0,1]^2$ as follows. Subdivide the square into nine equal parts and keep only the four closed corner squares, removing the remaining region (which form a cross). Then repeat this process in a suitably scaled version for the remaining squares, ad infinitum. Show that the resulting set is perfect, has plane measure zero, and equals $\mathcal{C}\times \mathcal{C}$. 
\end{question}
\begin{proof}
Let $\mathcal{C}'_n \subseteq \R^2$ be the result after the $n$th stage of removal, and let $\mathcal{C}_n \subseteq \R$ be the result after the $n$th stage of removal in the construction of the classical ternary Cantor set. It is clear that 
\begin{align*}
\mathcal{C}'_n= \mathcal{C}_n \times \mathcal{C}_n\text{ for all $n$ }
\end{align*}
It then follows 
\begin{align*}
\bigcap_n \mathcal{C}'_n= \bigcap_n \mathcal{C}_n \times \mathcal{C}_n = \mathcal{C}\times \mathcal{C}
\end{align*}
The fact that $\mathcal{C}\times \mathcal{C}$ has plane measure zero follows from \myref{Lemma}{zme}. Fix  $(a,b)\in \mathcal{C}\times \mathcal{C}$. Because $\mathcal{C}$ is perfect, there exists some $b'\in \mathcal{C}$ such that 
\begin{align*}
0<\abso{b'-b}< \epsilon 
\end{align*}
To see that $\mathcal{C}'$ is perfect, one see that  
\begin{align*}
  (a,b)\neq (a,b')\text{ and }  (a,b')\in \mathcal{C}'\times \mathcal{C}'\text{ and }\abso{(a,b)-(a,b')}=\abso{b'-b}<\epsilon 
\end{align*}

\end{proof}
\begin{question}{}{}
Construct a subset of $[0,1]$ in the same manner as the Cantor set, except that at the $k$th stage, each interval removed has length  $\delta 3^{-k},0<\delta <1$. Show that the resulting set is perfect, has measure $1-\delta$, and contains no intervals.
\end{question}
\begin{proof}
Let $\mathcal{C}'_n \subseteq \R$ be the result after the $n$th stage of removal according to the description.  Clearly, each $\mathcal{C}'_n$ has $2^n$ amount of connected component, we then can compute the length of $\mathcal{C}'\triangleq \bigcap \mathcal{C}'_n$ to be 
\begin{align*}
1-\sum_{k=1}^{\infty} 2^{k-1}\delta 3^{-k}= 1- \frac{\frac{\delta}{3}}{1-\frac{2}{3}}=1-\delta
\end{align*}
Since each $\mathcal{C}'_n$ has $2^n$ amount of connected component of equal length and $\mathcal{C}'_n \subseteq [0,1]$, we know the length of each connected component of $\mathcal{C}'_n$ must not be greater than $\frac{1}{2^n}$. It then follows that no interval $[a,a+h]$ can be contained by all $\mathcal{C}'_n$ because if $[a,a+h]$ is a subset of some connected component of $\mathcal{C}'_k$ of some $k$, then the measure $h=\abso{[a,a+h]}$ must be smaller than $\frac{1}{2^k}$, which is false when $k$ is large enough. 
\end{proof}
\begin{question}{}{}
If $E_k$ is a sequence of sets with  $\sum \abso{E_k}_e < \infty$, show that $\limsup_{n\to\infty} E_n$ has measure zero.
\end{question}
\begin{proof}
Note that  
\begin{align*}
\sum_{k=N}^{\infty} \abso{E_k}_e = \Big(\sum_{k=1}^{\infty} \abso{E_k}_e - \sum_{k=1}^{N-1} \abso{E_k}_e \Big) \to 0\text{ as }N \to \infty
\end{align*}
and note for all $N$ we have 
\begin{align*}
\bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} E_k \subseteq \bigcup_{k=N}^{\infty} E_k
\end{align*}
Then for arbitrary $\epsilon $, if we let $N$ satisfy $\sum_{k=N}^{\infty} \abso{E_k}_e < \epsilon $, we see that 
\begin{align*}
\abso{\limsup_{n\to\infty} E_n}_e= \abso{\bigcap_{n=1}^{\infty}\bigcup_{k=n}^{\infty}E_k}_e\leq \abso{\bigcup_{k=N}^{\infty}E_k}_e\leq \sum_{k=N}^{\infty} \abso{E_k}_e < \epsilon 
\end{align*}

\end{proof}
\begin{question}{}{}
If $E_1,E_2$ are measurable, show that 
\begin{align*}
\abso{E_1\cup E_2}+ \abso{E_1 \cap E_2}=\abso{E_1}+\abso{E_2}
\end{align*}
\end{question}
\begin{proof}
Observe the following expression of each set in disjoint union
\begin{enumerate}[label=(\alph*)]
  \item $E_1=(E_1\setminus E_2)\sqcup (E_1\cap E_2)$ 
  \item $E_2=(E_2\setminus E_1)\sqcup (E_1\cap E_2)$ 
  \item $E_1\cup E_2=(E_1\setminus E_2)\sqcup (E_1\cap E_2)\sqcup (E_2\setminus E_1)$
\end{enumerate}
It now follows 
\begin{align*}
\abso{E_1\cup E_2}+ \abso{E_1 \cap E_2}&= \abso{E_1\setminus E_2}+\abso{E_1\cap E_2}+\abso{E_1\cap E_2}+\abso{E_2\setminus E_1}\\
&=\abso{E_1}+ \abso{E_2}
\end{align*}
\end{proof}
\begin{lemma}
\label{zme}
Given two subsets $Z_1,Z_2$ of  $\R$,  if $\abso{Z_1}=0$, then $\abso{Z_1 \times Z_2}=0$. 
\end{lemma}
\begin{proof}  
Let $A_n\triangleq [n,n+1)$. Because 
\begin{align*}
Z_1 \times Z_2 = \bigsqcup _{n\inz} Z_1 \times (A_n \cap  Z_2)
\end{align*}
To show $\abso{Z_1 \times Z_2}=0$, we only have to $\abso{Z_1 \times (A_n \cap Z_2)}=0$ for all $n\inz$. In other words, we can WOLG suppose $Z_2$ is bounded.\\ 

Now, fix $\epsilon $. We are required to find an countable closed cube cover $Q_n\times C_n$ for $Z_1 \times Z_2$ such that $\sum_n \abso{Q_n\times C_n}< \epsilon $. Let $C_n=C$ for all $n$ where  $C$ is a compact interval containing $Z_2$, and let  $Q_n$ be a countable compact interval cover for  $Z_1$ such that $\sum \abso{Q_n}< \frac{\epsilon }{\abso{C}}$. It then follows $\sum_n \abso{Q_n\times C_n}= \sum_n \abso{Q_n}\abso{C}< \epsilon $. 
\end{proof}
\begin{theorem}
\label{PoFM}
\textbf{(Product of Finite Measure Set)} If $E_1\text{ and }E_2$ are measurable subset of $\R $ and $\abso{E_1},\abso{E_2}<\infty$, then $E_1\times E_2$ is measurable in $\R^2$ and 
 \begin{align*}
\abso{E_1\times E_2} = \abso{E_1} \abso{E_2}
\end{align*}
\end{theorem}
\begin{proof}
Write $E_1\triangleq H_1\sqcup Z_1$ and $E_2 \triangleq H_2 \sqcup Z_2$ where $H_1,H_2\in F_\sigma$ and $\abso{H_1}=\abso{E_1}$ and $\abso{H_2}=\abso{E_2}$. Now observe 
\begin{align*}
E_1 \times E_2 = (H_1 \times H_2)\cup   (Z_1 \times E_2) \cup   (E_1 \times Z_2)
\end{align*}
Note that if we write $H_1=\bigcap F_{1,n}$ and $H_2 = \bigcap F_{2,n}$, we see $H_1 \times H_2 = \bigcap F_{1,n}\times F_{2,n} \in F_{\sigma}$ in $\R^2$, it now follows from \myref{Lemma}{zme} that $E_1 \times E_2$ is measurable.  \\









Now, let $S_n$ be a decreasing sequence of open set containing  $E_1$ such that  $\abso{S_n\setminus E_1}< \frac{1}{n}$, and let $T_n$ be a decreasing sequence of open set containing $E_2$ such that $\abso{T_n \setminus E_2}< \frac{1}{n}$. In other words, 
\begin{align*}
E_1 = S \setminus  Z_1 \text{ and }E_2 = T \setminus  Z_2 \text{ where }\begin{cases}
  S\triangleq \bigcap S_n\\
  T\triangleq \bigcap T_n \\
  \abso{Z_1}=\abso{Z_2}=0
\end{cases}
\end{align*}
We now have 
\begin{align*}
S \times T= (E_1 \times E_2) \cup (Z_1 \times  E_2)  \cup  (E_1\times Z_2)
\end{align*}
This then implies $\abso{S\times T}=\abso{S\times T}_e\leq \abso{E_1\times E_2}_e+ \abso{Z_1 \times E_2}_e + \abso{E_1 \times Z_2}_e= \abso{E_1 \times E_2}$, where the last inequality follows from \myref{Lemma}{zme}. The reverse inequality is clear, since $E_1 \times E_2 \subseteq S \times T$. We have proved $\abso{E_1 \times E_2}=\abso{S \times T}$. \\






Now, for each $n$, write 
 \begin{align*}
S_n= \bigcup_{k \inn} I_{k,S_n} \text{ and } T_n= \bigcup_{k\inn} I_{k,T_n}
\end{align*}
where $(I_{k,S_n})_k$ and $(I_{k,T_n})_k$ are non-overlapping compact interval. It then follows that 
\begin{align*}
  \abso{S_n \times T_n} =  \sum_{i,j} \abso{I_{i,S_n} \times I_{j,T_n}}= \sum_{i,j} \abso{I_{i,S_n}}\times \abso{I_{j,T_n}}= \sum_i \abso{I_{i,S_n}} \sum_j \abso{I_{j,T_n}}= \abso{S_n} \abso{T_n}
\end{align*}
Write $S\triangleq \bigcap_{n\inn} S_n$ and $T\triangleq \bigcap_{n\inn}T_n$. Because
\begin{enumerate}[label=(\alph*)]
  \item Each  $S_n \times T_n$ is open.   
  \item $\abso{S_n\times T_n}= \abso{S_n}\abso{T_n}$ is bounded ($\because \abso{S_n}\searrow \abso{E_1}<\infty$). 
  \item $S_n \times T_n \searrow S \times T$
\end{enumerate}
We can now deduce 
\begin{align*}
\abso{E_1 \times E_2}= \abso{S \times T}&= \lim_{n\to \infty} \abso{S_n \times T_n} \\
&=\lim_{n\to \infty} \abso{S_n} \abso{T_n} \\
&=\abso{E_1} \abso{E_2}
\end{align*}



\end{proof}
\begin{question}{}{}
If $E_1$ and  $E_2$ are measurable subset of  $\R$, then $E_1\times E_2$ is a measurable subset of $\R^2$ and  $\abso{E_1\times E_2}=\abso{E_1}\abso{E_2}$
\end{question}
\begin{proof}
Define 
\begin{align*}
A_n\triangleq \set{x\inr: n\leq x< n+1}
\end{align*}
Because 
\begin{align*}
E_1= \bigcup_{n\inz} E_1 \cap A_n\text{ and }E_2=\bigcup_{k\inz} E_2 \cap A_k
\end{align*}
We can deduce
\begin{align*}
E_1 \times E_2 = \bigcup_{n,k\inz} (E_1 \cap A_n)\times (E_2 \cap A_k)
\end{align*}
Note that \myref{Theorem}{PoFM} tell us $(E_1 \cap A_n) \times (E_2 \cap A_k)$ is measurable, which implies $E_1\times E_2$ is measurable. \myref{Theorem}{PoFM} also tell us  $\abso{(E_1\cap A_n)\times (E_2 \cap A_k)}= \abso{E_1 \cap A_n}\abso{E_2 \cap A_k}$, which allow us to deduce 
\begin{align*}
\abso{E_1 \times E_2}= \sum_{n,k\inz} \abso{(E_1 \cap A_n ) \times (E_2 \cap A_k)}&= \sum_{n,k\inz} \abso{E_1 \cap A_n} \abso{E_2 \cap A_k} \\
&=\sum_{n\inz} \abso{E_1 \cap A_n} \sum_{k\inz} \abso{E_2 \cap A_k}= \abso{E_1} \abso{E_2}
\end{align*}
\end{proof}
\begin{question}{}{}
Give an example that shows that the image of a measurable set under a continuous transformation may not be measurable. See also Exercise $10$ of Chapter 7. 
\end{question}
\begin{proof}
Consider the Cantor-Lebesgue function denoted by $f:[0,1]\rightarrow [0,1]$ and denote the classical ternary Cantor set by $\mathcal{C}$. Let  $V$ be a Vitali set contained by  $[0,1]$. Because $f(\mathcal{C})=[0,1]$, we know there exists $E\subseteq \mathcal{C}$ such that $f(E)=V$. Such $E$ is measurable since $\abso{E}_e\leq \abso{\mathcal{C}}=0$, yet its continuous image $V=f(E)$ is by definition non-measurable. 
\end{proof}
\begin{question}{}{}
Show that there exists disjoint $E_1,E_2,\dots $ such that $\abso{\bigcup E_k}_e< \sum \abso{E_k}_e$ with strict inequality. 
\end{question}
\begin{proof}
Let  $V$ be a Vitali Set contained by $[0,1]$. Enumerate $[0,1]\cap \Q$ by $x_n$. Define 
 \begin{align*}
E_n\triangleq \set{v+x_n\inr: v\in V}\text{ for all }n
\end{align*}
The sequence $E_n$ is disjoint, since if  $p \in E_n \cap E_m$, then there exists some pair $v_n,v_m$ belong to  $V$ such that 
 \begin{align}
\label{vnxn}
v_n+x_n=p=v_m+x_m
\end{align}
which is impossible, since \myref{Equation}{vnxn} implies that $v_n\neq v_m$ and $v_n,v_m$ are of difference of a rational number.  \\

Now, note that for arbitrary $n$ and  $v\in V$, because $v \in V \subseteq [0,1]$ and $x_n \in [0,1]$, we have $v+x_n\in [0,2]$. This implies 
\begin{align*}
\bigsqcup _n E_n \subseteq [0,2]\text{ and }\abso{\bigsqcup _n E_n}_e \leq 2
\end{align*}
Because $V$ is non-measurable by definition, we know $\abso{V}_e>0$, and since outer measure is translation invariant, we can now deduce 
\begin{align*}
\sum_n \abso{E_n}_e= \sum_n \abso{V}_e =\infty >2 \geq \abso{\bigsqcup_n E_n}_e
\end{align*}
\end{proof}
\begin{question}{}{}
Show that there exists decreasing sequence $E_k$ of sets such that 
\begin{enumerate}[label=(\alph*)]
  \item $E_k\searrow E$.
  \item $\abso{E_k}_e<\infty$.
  \item $\lim_{k\to \infty}\abso{E_k}_e> \abso{E}_e$
\end{enumerate}
\end{question}
\begin{proof}
Let $V$ be a Vitali Set contained by  $[0,1]$. Enumerate $[0,1]\cap \Q$ by $x_n$. Define 
\begin{align*}
V+x_n \triangleq \set{v+x_n\inr: v\in V}
\end{align*}
In last question, we have proved that $V+x_n$ is pairwise disjoint. Define for all $n\inn$
\begin{align*}
E_n\triangleq \bigsqcup_{k\geq n}V+x_k  
\end{align*}
Observe that 
\begin{align*}
E_k \searrow \bigcap E_n =\varnothing 
\end{align*}
which implies $\abso{\bigcap E_n}_e=0$, but 
\begin{align*}
\lim_{n\to \infty} \abso{E_n}_e =\lim_{n\to \infty} \abso{\bigsqcup_{k\geq n}V+x_k}\geq \lim_{n\to \infty} \abso{V+x_n}=\abso{V}>0
\end{align*}
\end{proof}
\begin{question}{}{}
Let $Z$ be a subset of $\R$ with measure zero. Show that the set $\set{x^2:x\in Z}$ also has measure zero.
\end{question}
\begin{proof}
Fix $Z_n\triangleq Z\cap [-n,n]$. Since 
\begin{align*}
\abso{\set{x^2:x \in Z}}\leq \sum_{n=1}^{\infty} \abso{\set{x^2:x\in Z_n}}_e
\end{align*}
We only have to prove 
\begin{align*}
  \abso{\set{x^2 : x \in Z_n}}_e =0\text{ for all }n
\end{align*}
Fix $\epsilon ,n$. Let $I_k$ be a compact interval cover of  $Z_n$ such that $\sum \abso{I_k} < \frac{\epsilon }{2n}$. We shall suppose $I_k \subseteq [-n,n]$, since if not, we can just let $I_k'\triangleq I_k \cap [-n,n]$.\\

Now, define 
\begin{align*}
I_k^2\triangleq \set{x^2 : x\in I_k}
\end{align*}
Clearly, $I_k^2$ are all compact intervals, and if we write  $I_k \triangleq [a_k,b_k]$, we have the following inequalities 
\begin{align*}
\begin{cases}
  0 \leq a_k\leq b_k \implies  \abso{I_k^2}=b_k^2-a_k^2= \abso{I_k}(b_k+a_k)\leq 2n \abso{I_k}  \\
  a_k\leq  0 \leq b_k \implies  \abso{I_k^2} = \max \set{a_k^2,b_k^2}\leq (b_k-a_k)^2= \abso{I_k}(b_k-a_k)\leq 2n \abso{I_k} \\
  a_k \leq b_k \leq 0\implies \abso{I_k^2}= a_k^2 - b_k^2 = \abso{I_k}(-a_k-b_k)\leq 2n\abso{I_k}
\end{cases}
\end{align*}
Note that $\set{I_k^2}_{k\inn}$ is a compact interval cover  of $\set{x^2:x\in Z_n}$, we now see 
\begin{align*}
\abso{\set{x^2:x\in Z_n}}_e \leq \sum_k \abso{I_k^2} \leq 2n \sum \abso{I_k}< \epsilon 
\end{align*}
\end{proof}
\section{Brunn-Minkowski Inequality}
\begin{abstract}
This HW assignment require us to prove Brunn-Minkowski Inequality.   
\end{abstract}
\begin{mdframed}
We first introduce some notation. Given two sets $A,B\subseteq \R^d$ and a point $p\inr^d$, we write 
\begin{align*}
A+p\triangleq \set{a+p\inr^d: a \in A}
\end{align*}
and write 
\begin{align*}
A+B \triangleq \set{a+b \inr^d: a\in A\text{ and }b \in  B}
\end{align*}
Note that elementary set theory tell us 
\begin{align}
\label{trans}
  (A+p)+(B+q)=(A+B)+(p+q)
\end{align}

\end{mdframed}
\begin{theorem}
\label{Brunn-Minkowski Inequality for Bricks}
\textbf{(Brunn-Minkowski Inequality for Bricks)} Suppose $A,B$ are two  \textbf{bricks}, i.e., $A$ is of the form $\prod_{j=1}^d[x_j,y_j]$ and so is $B$, then we have 
\begin{align*}
\abso{A}^{\frac{1}{d}}+ \abso{B}^{\frac{1}{d}}\leq \abso{A+B}^{\frac{1}{d}}
\end{align*}
\end{theorem}
\begin{proof}
Because Lebesgue measure is translation invariant, by  \myref{Equation}{trans}, we can WOLG suppose 
\begin{align*}
A=\prod_{j=1}^d [0,a_j]\text{ and }B=\prod_{j=1}^d [0,b_j]
\end{align*}
It is clear that 
\begin{align*}
A+B= \prod_{j=1}^d [0,a_j+b_j]
\end{align*}
Now, by direct computation, we know that 
\begin{align*}
\abso{A+B}=\prod_{j=1}^d (a_j+b_j)\text{ and }\abso{A}=\prod_{j=1}^d a_j\text{ and }\abso{B}=\prod_{j=1}^d b_j
\end{align*}
Note that by AM-GM inequality, we have 
\begin{align*}
\frac{\abso{A}^{\frac{1}{d}}}{\abso{A+B}^{\frac{1}{d}}}=\Big(\prod_{j=1}^n \frac{a_j}{a_j+b_j}\Big)^{\frac{1}{n}}\leq \frac{1}{d}\sum_{j=1}^d \frac{a_j}{a_j+b_j}
\end{align*}
Similarly by AM-GM inequality, we have
\begin{align*}
\frac{\abso{B}^{\frac{1}{d}}}{\abso{A+B}^{\frac{1}{d}}}=\Big(\prod_{j=1}^n \frac{b_j}{a_j+b_j}\Big)^{\frac{1}{n}}\leq \frac{1}{d}\sum_{j=1}^d \frac{b_j}{a_j+b_j}
\end{align*}
Adding two inequalities together, now have 
\begin{align*}
\frac{\abso{A}^{\frac{1}{d}}+\abso{B}^{\frac{1}{d}}}{\abso{A+B}^{\frac{1}{d}}}\leq \frac{1}{d}\sum_{j=1}^d \frac{a_j+b_j}{a_j+b_j}=1
\end{align*}
The result then follows from multiplying both side with $\abso{A+B}^{\frac{1}{d}}$. 
\end{proof}
\begin{theorem}
\textbf{(Brunn-Minkowski Inequality for finite union of non-overlapping of bricks)} Suppose $A$ is a union of a finite collection of non-overlapping brick and so is $B$. We have 
 \begin{align*}
\abso{A}^{\frac{1}{d}}+ \abso{B}^{\frac{1}{d}}\leq \abso{A+B}^{\frac{1}{d}}
\end{align*}
\end{theorem}
\begin{proof}
  We prove by induction on the sum $k$ of the amount of bricks consisting $A$ and the amount of bricks consisting $B$. The base case $k=2$ have been proved by \myref{Theorem}{Brunn-Minkowski Inequality for Bricks}. Suppose the proposition hold true when $k\leq r$. Let $k=r+1$. Because the bricks consisting of $A$ are non-overlapping, by a translation  (and renaming axis if necessary), we can suppose 
\begin{center}
   \begin{minipage}{0.9\linewidth}  
      $A^+\triangleq A\cap \set{\textbf{x}\inr^d: x_1\geq 0}$ is a union of a collection of non-overlapping bricks with amount at least one fewer than $A$. 
   \end{minipage}
\end{center}
This is true since if we write $A=A_1\cup \cdots \cup A_m$, then by translation and remaining axis, we can suppose $A_1$ lies in only one of the closed subspaces $\set{\textbf{x}\inr^d:x_1\geq 0},\set{\textbf{x}\inr^d:x_1\leq 0}$ and $A_2$ lies in another, and since for all $n\geq 3$, $A_n \cap \set{\textbf{x}\inr^d: x_1\geq 0}$ is either also a brick or empty. With similar reason, we now see 
\begin{center}
   \begin{minipage}{0.9\linewidth}  
      $A^-\triangleq A\cap \set{\textbf{x}\inr^d: x_1\leq 0}$ is a union of a collection of non-overlapping bricks with amount at least one fewer than $A$. 
   \end{minipage}
\end{center}
Now, note that $h:\R\rightarrow \R$ defined by
\begin{align*}
h(t)\triangleq  \abso{\Big(B+(t,0,\dots ,0) \Big)\cap \set{\textbf{x}\inr^d: x_1\geq 0}}
\end{align*}
is clearly a continuous function. ($h$ can be expressed as a finite sum of continuous function $\sum_{k=1}h_k^p$ if $B$ consist of $p$-amount of bricks)\\

We then can translate $B$ to let $B$ satisfy 
 \begin{align}
  \label{rai}
\frac{\abso{A^+}}{\abso{A}}= \frac{\abso{B^+}}{\abso{B}}\text{ where }B^+ \triangleq B \cap \set{\textbf{x}\inr^d: x_1\geq 0}
\end{align}
Define $B^-\triangleq B\cap \set{\textbf{x}\inr^d:x_1\leq 0}$. With similar reason stated above, we know $B^+$ and  $B^-$ are both union of collection of non-overlapping bricks with amount not greater than  $B$. \\

At this point, one should note that the sum of the amount of bricks consisting $A^+$ (resp. $A^-$) and the amount bricks consisting $B^+$ (resp. $B^-$) is at least one fewer than the sum $r+1$ of the sum of the amount of bricks consisting $A$ and the amount bricks consisting $B$. Then because the proposition hold true when $k\leq r$, we now have
\begin{align*}
\abso{A^+}^{\frac{1}{d}}+ \abso{B^+}^{\frac{1}{d}}\leq \abso{A^++B^+}^{\frac{1}{d}}\text{ and }\abso{A^-}^{\frac{1}{d}}+ \abso{B^-}^{\frac{1}{d}}\leq \abso{A^-+B^-}^{\frac{1}{d}}
\end{align*}
Note that for each $\textbf{x}$ in the interior of $A^++B^+$, we must have  $x_1>0$, and for each  $\textbf{y}$ in the interior of $A^-+B^-$, we must have  $y_1<0$. This implies that  $(A^++B^+)$ and $(A^-+B^-)$ are non-overlapping. Now, because 
\begin{align*}
A+B =(A^++B^+)\cup (A^-+B^-)
\end{align*}
if we define $\rho \triangleq \frac{\abso{A^+}}{\abso{A}}$, from \myref{Equation}{rai} we can finally deduce 
\begin{align*}
\abso{A+B}&= \abso{A^++B^+}+ \abso{A^-+B^-}\\
&\geq \Big(\abso{A^+}^{\frac{1}{d}} + \abso{B^+}^{\frac{1}{d}} \Big)^{d} + \Big(\abso{A^-}^{\frac{1}{d}}+\abso{B^-}^{\frac{1}{d}}  \Big)^d \\ 
(\because \frac{\abso{A^-}}{\abso{A}}=\frac{\abso{B^-}}{\abso{B}}=1-\rho)\hspace{0.5cm}&=\Big((\rho \abso{A})^{\frac{1}{d}}+ (\rho \abso{B})^{\frac{1}{d}} \Big)^d + \Big(((1-\rho)\abso{A})^{\frac{1}{d}}+ ((1-\rho)\abso{B})^{\frac{1}{d}} \Big)^d \\
&=\Big(\rho^{\frac{1}{d}} (\abso{A}^{\frac{1}{d}}+\abso{B}^{\frac{1}{d}}) \Big)^d + \Big((1-\rho)^{\frac{1}{d}}(\abso{A}^{\frac{1}{d}}+\abso{B}^{\frac{1}{d}}) \Big)^{d}\\
&=(\rho + 1-\rho)(\abso{A}^{\frac{1}{d}}+ \abso{B}^{\frac{1}{d}})^d\\
&=(\abso{A}^{\frac{1}{d}}+\abso{B}^{\frac{1}{d}})^d
\end{align*}
This then give us the desired 
\begin{align*}
\abso{A+B}^{\frac{1}{d}}\geq \abso{A}^{\frac{1}{d}}+ \abso{B}^{\frac{1}{d}}
\end{align*}














\end{proof}
\chapter{Complex Analysis HW}
\section{HW1}
\begin{theorem}
\begin{align*}
  (1+i)^n, \frac{(1+i)^n}{n},\frac{n!}{(1+i)^n}\text{ all diverge as }n\to \infty
\end{align*}
\begin{proof}
Note that 
\begin{align*}
\abso{(1+i)^n}=2^{\frac{n}{2}}\to \infty\text{ as }n\to \infty
\end{align*}
This implies $(1+i)$ is unbounded, thus diverge.\\

Note that 
\begin{align*}
\abso{\frac{(1+i)^n}{n}}=\frac{(\sqrt{2})^n}{n}
\end{align*}
Observe 
\begin{align*}
\frac{(\sqrt{2})^n}{n}= \frac{[(\sqrt{2}-1)+1]^n}{n}&=\frac{\sum_{k=0}^n \binom{n}{k}(\sqrt{2}-1)^k}{n}\\
&\geq \frac{\binom{n}{2}(\sqrt{2}-1 )^2}{n}=(n-1) [\frac{(\sqrt{2}-1 )^2}{2}]\to \infty\text{ as }n\to \infty
\end{align*}
This implies $\frac{(1+i)^n}{n}$ is unbounded, thus diverge. \\

Note that 
\begin{align*}
\abso{\frac{n!}{(1+i)^n}}= \frac{n!}{(\sqrt{2})^n}
\end{align*}
Note that for all $k\geq 8$, we have 
\begin{align*}
\frac{k}{\sqrt{2}}\geq \frac{\sqrt{8}}{\sqrt{2}}=2
\end{align*}
This implies 
\begin{align*}
  \frac{n!}{(\sqrt{2})^n}=\prod_{k=1}^n \frac{k}{\sqrt{2}}= \frac{7!}{(\sqrt{2})^7}\prod_{k=8}^n \frac{k}{\sqrt{2}}\geq \frac{7!}{(\sqrt{2})^7}\prod_{k=8}^n 2\geq \frac{7!2^{n-8+1}}{(\sqrt{2})^7}\to \infty
\end{align*}
which implies $\frac{n!}{(1+i)^n}$ is unbounded, thus diverge.
\end{proof}
\end{theorem}
\begin{theorem}
  \begin{align*}
  n!z^n\text{ converge }\iff z=0
  \end{align*}
\end{theorem}
\begin{proof}
If $z=0$, then  $n!z^n=0$ for all  $n$, which implies  $n!z^n\to 0$. Now, suppose $z\neq 0$. Let $M\inn$ satisfy $\abso{z}>\frac{1}{M}$. Observe 
\begin{align*}
\abso{n!z^n}=\abso{\prod_{k=1}^n kz}=\abso{\prod_{k=1}^{2M-1} kz } \abso{\prod_{k=2M}^{n} kz }\geq \abso{\prod_{k=1}^{2M-1}kz}\abso{\prod_{k=2M}^n 2Mz}\geq \abso{\prod_{k=1}^{2M-1}kz}2^{n-2M+1}\to \infty
\end{align*}
This implies $n!z^n$ is unbounded, thus diverge.
\end{proof}
\begin{theorem}
  \begin{align*}
  u_n\to u\implies v_n\triangleq \sum_{k=1}^n \frac{u_k}{n}\to u
  \end{align*}
\end{theorem}
\begin{proof}
Because 
\begin{align*}
\sum_{k=1}^n \frac{u_k}{n}=\sum_{k\leq \sqrt{n}}\frac{u_k}{n}+ \sum_{\sqrt{n}<k\leq n}\frac{u_k}{n}
\end{align*}
It suffices to prove 
\begin{align*}
\vi{\sum_{k\leq \sqrt{n} }\frac{u_k}{n}\to 0}\text{ and }\blue{\sum_{\sqrt{n}<k\leq n}\frac{u_k}{n}\to u}\text{ as }n\to \infty
\end{align*}
Because $u_n$ converge, we can let  $M$ bound  $\abso{u_n}$. Observe 
\begin{align*}
\abso{\sum_{k\leq \sqrt{n} } \frac{u_k}{n}}\leq \sum_{k\leq \sqrt{n}} \abso{\frac{u_k}{n}}\leq \sum_{k\leq \sqrt{n}} \frac{M}{n}\leq \frac{M\sqrt{n} }{n}=\frac{M}{\sqrt{n}}\to 0\text{ as }n\to 0\vdone
\end{align*}
Because 
\begin{align*}
\sum_{\sqrt{n}<k\leq n} \frac{u_k}{n}= \frac{n-\lceil \sqrt{n}\rceil +1 }{n}\sum_{\sqrt{n}<k\leq n} \frac{u_k}{n-\lceil \sqrt{n}  \rceil+1}
\end{align*}
and 
\begin{align*}
  \lim_{n\to \infty} \frac{n- \lceil \sqrt{n}\rceil +1 }{n}=1
\end{align*}
We can reduce the problem into proving 
\begin{align*}
  \blue{\lim_{n\to \infty}\sum_{\sqrt{n}<k\leq n} \frac{u_k}{n- \lceil \sqrt{n}\rceil +1 }=u}
\end{align*}
Fix $\epsilon $. Let $N$ satisfy that for all  $n\geq N$, we have $\abso{u_n-u}<\epsilon $. Then for all $n \geq N^2$, we have 
\begin{align*}
  \abso{\Big(\sum_{\sqrt{n}<k \leq n } \frac{u_k}{n-\lceil \sqrt{n}\rceil +1 } \Big)- u}  &=\abso{\sum_{\sqrt{n}<k\leq n } \frac{u_k-u}{n-\lceil \sqrt{n}\rceil +1 } }\\
  &\leq \sum_{\sqrt{n}<k\leq n} \frac{\abso{u_k-u}}{n-\lceil \sqrt{n}\rceil +1  }\\
  &\leq \sum_{\sqrt{n}<k\leq n} \frac{\epsilon }{n-\lceil \sqrt{n}\rceil +1  }=\epsilon \bdone
\end{align*}
\end{proof}
\section{Exercise 1}
\begin{mdframed}

Let $R$ be a complex algebra with $1_A$ and $a \in R$. Given a complex polynomial 
\[ 
f(Z) = a_0 + a_1 Z + \cdots + a_n Z^n,
\]
we define the evaluation of $f$ at $a$ by
\[
f(a) = a_0 1_A + a_1 a + \cdots + a_n a^n.
\]
\end{mdframed}
\begin{question}{}{}
Let $R = \mathbb{C}$ and $a = 1 + i$. Given $f(Z) = Z^3$. Evaluate $f(a)$.
\end{question}
\begin{proof}
   $f(a)=(1+i)^3=2i(1+i)=-2+2i$ 
\end{proof}
\begin{question}{}{}
Let $R = M_{2 \times 2}(\mathbb{C})$ be the algebra of $2 \times 2$ complex matrices. Take 
    \[
    a = \begin{bmatrix} 1 & -1 \\ 1 & 1 \end{bmatrix}
    \]
    and $g(Z) = 3 + 2Z$. Evaluate $g(a)$.
    
\end{question}
\begin{proof}

  \begin{align*}
  g(a)=\begin{bmatrix}
    3 & 0\\
    0 & 3 
  \end{bmatrix} + \begin{bmatrix}
    2 & -2 \\
    2 & 2
  \end{bmatrix}= \begin{bmatrix}
    5 & -2 \\
    2 & 5
  \end{bmatrix}
  \end{align*}
\end{proof}
\begin{question}{}{}
Let $R$ be the algebra of complex valued periodic functions of period $2\pi$, i.e., $a \in R$ is a continuous function $a : \mathbb{R} \to \mathbb{C}$ so that $a(x + 2\pi) = a(x)$. Let $e(x) = \cos x + i \sin x$ and 
    \[
    h(Z) = 1 + Z + Z^2 + \cdots + Z^9.
    \]
    Find $h(e)$.
\end{question}
\begin{proof}
Note that 
\begin{align*}
  (\cos x+ i \sin x)(\cos y+i \sin y)&= (\cos x \cos y- \sin x \sin y)+i (\sin x \cos y + \cos x \sin y)\\
  &=\cos (x+y)+ i \sin (x+y)
\end{align*}
This give us 
\begin{align*}
h(e)=\sum_{k=0}^{9} \cos (kx)+ i \sin (kx) 
\end{align*}
\end{proof}
\section{HW2}
\begin{theorem}
\label{Root Test is Stronger Than Ratio Test}
\textbf{(Root Test is Stronger Than Ratio Test)}
\begin{align*}
\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}\leq \liminf_{n\to\infty} \sqrt[n]{\abso{z_n}} \leq \limsup_{n\to\infty} \sqrt[n]{\abso{z_n}} \leq \limsup_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $ and WOLG suppose $\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}>0$. We prove 
\begin{align*}
  \vi{\liminf_{n\to\infty} \sqrt[n]{\abso{z_n}}\geq \liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}-\epsilon  }
\end{align*}
Let $\alpha \inr $ satisfy 
\begin{align*}
\liminf_{n\to\infty}  \abso{\frac{z_{n+1}}{z_n}}-\epsilon <\alpha < \liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}
\end{align*}
Let $N$ satisfy  
\begin{align*}
\text{ For all }n\geq N,\abso{\frac{z_{n+1}}{z_n}}>\alpha  
\end{align*}
We then see 
\begin{align*}
  \sqrt[N+n]{\abso{z_{N+n}}}\geq \sqrt[N+n]{\abso{z_N}\alpha^n}=\alpha \Big(\frac{\abso{z_N}^{\frac{1}{N+n}}}{\alpha ^{\frac{N}{N+n}}} \Big)\to \alpha \text{ as }n\to \infty \vdone
\end{align*}
The proof for the other side is similar.
\end{proof}
\begin{question}{}{}
Find the radius of convergence of the following series:
\begin{enumerate}[label=(\alph*)]
  \item $\sum \frac{z^n}{n}$.
  \item $\sum \frac{z^n}{n!}$.
  \item $\sum n!z^n$.
  \item $\sum n^kz^n$ where $k$ is a positive integer. 
  \item $\sum z^{n!}$. 
\end{enumerate}
\end{question}
\begin{proof}
We know 
\begin{align}
\label{n1n}
n^{\frac{1}{n}}=e^{\frac{\ln n}{n}}\to 1\text{ as }n \to \infty
\end{align}
\myref{Equation}{n1n} implies  $n^{\frac{-1}{n}}\to 1$ as $n\to \infty$ and that $\sum \frac{z^n}{n}$ has radius of convergence $1$. \myref{Equation}{n1n} also implies $n^{\frac{k}{n}}\to 1$ and $\sum n^kz^n$ has radius of convergence $1$. \\

We know 
\begin{align*}
\lim_{n\to \infty} \frac{(n+1)!}{n!}=\infty
\end{align*}
Then  \myref{Theorem}{Root Test is Stronger Than Ratio Test} tell us 
\begin{align}
\label{j0z}
\lim_{n\to \infty} (n!)^{\frac{1}{n}}=\infty 
\end{align}
which implies that $\sum n!z^n$ has radius of convergence $0$ and  $\sum \frac{z^n}{n!}$ has radius of convergence $\infty$. Note that 
\begin{align*}
\sum z^{n!}=\sum a_nz^n\text{ where }a_n=\begin{cases}
  1& \text{ if $n=k!$ for some  $k$ }\\
  0& \text{ otherwise }
\end{cases}
\end{align*}
It is then clear that 
\begin{align*}
\limsup_{n\to\infty} (a_n)^{\frac{1}{n}}=1
\end{align*}
and the radius is $1$. 
\end{proof}
\begin{question}{}{}
The $0$th order Bessel function  $J_0(z)$ is defined by the power series 
\begin{align*}
J_0(z)=\sum_{n=0}^{\infty} (-1)^n \frac{1}{(n!)^2} \frac{z^n}{2n}
\end{align*}
Find its radius of convergence.
\end{question}
\begin{proof}
\myref{Equation}{n1n} and \myref{Equation}{j0z} tell us 
\begin{align*}
\lim_{n\to \infty} (2n(n!)^2)^{\frac{1}{n}}=\infty 
\end{align*}
which implies that the radius of convergence of $J_0(z)$ is $\infty$. 
\end{proof}
\begin{theorem}
\label{Abel's Test for Power Series}
\textbf{(Abel's Test for Power Series)} Suppose $a_n\to0$ monotonically and $\sum a_nz^n$ has radius of convergence $R$ . 
\begin{align*}
  \text{ The power series $\sum a_nz^n$ at least converge on $\overline{D_R(0)}\setminus \set{R}$ }
\end{align*}
\end{theorem}
\begin{proof}
Note that 
\begin{align*}
\sum \frac{a_n}{R^n}z^n\text{ has radius of convergence }R
\end{align*}
Fix $z\in \overline{D_R(0)}\setminus \set{R}$. Note that 
\begin{align*}
\abso{\sum_{n=0}\frac{z^n}{R^n}}= \abso{\frac{1-(\frac{z}{R})^{N+1}}{1-\frac{z}{R}}} \leq \frac{2}{\abso{1-\frac{z}{R}}}\text{ for all }N
\end{align*}
It then follows from  \customref{Dirichlet's Test}{Dirichlet's Test} that $\sum a_n (\frac{z}{R})^n$ converge.
\end{proof}
\begin{question}{}{}
Suppose that $\sum a_nz^n$ has radius of convergence $R$ and let $C$ be the circle $\set{z\inc:\abso{z}=R}$. Prove or disprove 
\begin{enumerate}[label=(\alph*)]
  \item If $\sum a_nz^n$ converge at every point on $C$, except possibly one, then it converges absolutely every where on  $C$
\end{enumerate}
\end{question}
\begin{proof}
  Consider $a_n\triangleq \frac{1}{n}$ for all $n\inn$ and $a_0\triangleq 1$. Then $\sum a_nz^n$ has convergence radius $1$.  Since $a_n\searrow 0$, it follows from  \myref{Theorem}{Abel's Test for Power Series}, $\sum a_nz^n$ converge everywhere on $C\setminus \set{1}$. Observe that when  $z=1$, the series is just harmonic series, which diverge.  
\end{proof}
\begin{question}{}{}
If $\sum a_nz^n$ has radius of convergence $R$, find the radius of convergence of 
\begin{enumerate}[label=(\alph*)]
  \item $\sum n^3 a_nz^n$. 
  \item $\sum a_nz^{3n}$.
  \item $\sum a_n^3 z^n$
\end{enumerate}
\end{question}
\begin{proof}
Since $(n^3)^{\frac{1}{n}}\to 1$, we know $\sum n^3a_nz^n$ also had radius of convergence $R$. We claim that the series $\sum a_nz^{3n}$ has convergence radius $R^{\frac{1}{3}}$. If $\abso{z}<R^{\frac{1}{3}}$, then $\abso{z^3}< R$ and thus 
\begin{align*}
\sum a_n(z^3)^n\text{ converge }
\end{align*}
and if $\abso{z}>R^{\frac{1}{3}}$, then $\abso{z^3}>R$ and 
\begin{align*}
\sum a_n(z^3)^n\text{ diverge }
\end{align*}
We have proved that $\sum  a_nz^{3n}$ has convergence radius $R^{\frac{1}{3}}$. \\

Note that given a sub-sequence $\abso{a_{n_k}}^{\frac{1}{n_k}}$, 
\begin{align*}
  \abso{a_{n_k}}^{\frac{1}{n_k}}\text{ converge in extended reals if and only if }\abso{a_{n_k}}^{\frac{3}{n_k}}\text{ converge in extended reals }
\end{align*}
and if the former converge to $L$, then the latter converge to  $L^3$. It now follows that 
 \begin{align*}
   \limsup_{n\to\infty} \abso{a_n^3}= (\limsup_{n\to\infty} \abso{a_n})^3 = \frac{1}{R^3}
\end{align*}
It now follows that $\sum a_n^3 z^n$ has convergence radius $R^3$. 
\end{proof}
\begin{theorem}
\label{Summation by Part}
\textbf{(Summation by Part)} 
\begin{align*}
  f_ng_n-f_mg_m&=\sum_{k=m}^{n-1}f_k \Delta g_k + g_k \Delta f_k+ \Delta f_k \Delta g_k \\
  &=\sum_{k=m}^{n-1}f_k \Delta g_k + g_{k+1}\Delta f_k
\end{align*}
\end{theorem}
\begin{proof}
The proof follows induction which is based on 
\begin{align*}
f_mg_m+ f_m\Delta g_m+ g_m \Delta f_m +\Delta f_m \Delta g_m=f_{m+1}g_{m+1}
\end{align*} 
\end{proof}
\begin{question}{}{}
Prove that, for $z\neq 1$
\begin{align*}
\sum_{n=1}^k \frac{z^n}{n}= \frac{z}{1-z}\Big( \sum_{n=1}^{k-1}\frac{1}{n(n+1)} - \sum_{n=1}^{k-1} \frac{z^n}{n(n+1)} + \frac{1-z^k}{k} \Big)
\end{align*}
Show that the series $\sum \frac{z^n}{n}$ and $\sum \frac{z^n}{n(n+1)}$ have radius of convergence $1$; that the latter series converge everywhere on  $\abso{z}=1$, while the former converges everywhere on $\abso{z}=1$ except $z=1$. 
\end{question}
\begin{proof}
We prove by induction. The base case $k=1$ is trivial. Suppose the equality hold when  $k=m$. The difference of the left hand side is clearly  $\frac{z^{m+1}}{m+1}$, and the difference of the right hand side is 
\begin{align*}
&\frac{z}{1-z}(\frac{1}{m(m+1)}-\frac{z^m}{m(m+1)}+ \frac{1-z^{m+1}}{m+1}- \frac{1-z^m}{m})\\
&= \frac{z}{1-z} \cdot \frac{1-z^m + m- mz^{m+1}+(m+1)z^m-(m+1)}{m(m+1)} \\
&=\frac{z}{1-z}\cdot \frac{-mz^{m+1}+mz^{m}}{m(m+1)}= \frac{z(z^m-z^{m+1})}{(1-z )(m+1)}=\frac{z^{m+1}}{m+1}
\end{align*}
The fact that both series have radius of convergence $1$ follows from  $n^{\frac{1}{n}}\to 1$. Both of them converge on $\overline{D_1(0)}\setminus \set{1}$ by \myref{Theorem}{Abel's Test for Power Series}. The former clearly diverge at $z=1$, since it would be a harmonic series, and the latter converge at $z=1$ by comparison test with  $\frac{1}{n(n+1)}\leq \frac{1}{n^2}$ for all $n\inn$. 
\end{proof}
\begin{question}{}{}
Suppose that the power series $\sum a_nz^n$ has a recurring sequence of coefficients; that is $a_{n+k}=a_n$ for some fixed positive integer $k$ and all $n$. Prove that the series converge for  $\abso{z}<1$ to a rational function $ \frac{p(z)}{q(z)}$ where $p,q$ are polynomials, and the roots of  $q$ are all on the unit circle. What happens if $a_{n+k}=\frac{a_n}{k}$ instead? 
\end{question}
\begin{proof}
Let 
\begin{align*}
L^-\triangleq \min \set{\abso{a_0},\dots ,\abso{a_{k-1}}}\text{ and }L^+\triangleq \max \set{\abso{a_0},\dots ,\abso{a_{k-1}}}
\end{align*}
We see that 
\begin{align*}
1=\liminf_{n\to\infty} (L^-)^{\frac{1}{n}}\leq \lim_{n\to \infty} \abso{a_n}^{\frac{1}{n}}\leq \limsup_{n\to\infty} (L^+)^{\frac{1}{n}}=1
\end{align*}
It then follows that $\sum a_nz^n$ has convergence radius $1$. Now observe that for $\abso{z}<1$, we have 
\begin{align*}
z^k \sum_{n=0}^{\infty}a_nz^n=\sum_{n=k}^{\infty}a_nz^n=\sum_{n=0}^{\infty}a_nz^n - \sum_{n=0}^{k-1}a_nz^n
\end{align*}
This now implies 
\begin{align*}
\sum_{n=0}^{\infty}a_nz^n= \frac{\sum_{n=0}^{k-1}a_nz^n}{1-z^k}
\end{align*}
Since $q(z)=1-z^k$, clearly the roots are all on the unit circle. Suppose now $b_n\triangleq a_n$ for all $n<k$ and  $b_{n+k}\triangleq \frac{b_n}{k}$ for all $n\geq k$. We then have 
\begin{align*}
  b_n= \frac{a_n}{k^{q(n)}}\text{ where }q\text{ is the largest integer such that }qk\leq n
\end{align*}
Note that $n-q(n)$ is always smaller than $k$. It then follows that 
 \begin{align*}
   (k^{q(n)})^{\frac{1}{n}}= k^{\frac{q(n)}{n}}= k \cdot k^{\frac{q(n)-n}{n}}\to k
\end{align*}
We then see that 
\begin{align*}
\limsup_{n\to\infty} \abso{b_n}^{\frac{1}{n}}= \frac{\limsup_{n\to\infty} \abso{a_n}^{\frac{1}{n}}}{k}=\frac{1}{k}
\end{align*}
It then follows that $\sum b_nz^n$ has convergence radius $k$. Now observe that for $\abso{z}<k$, we have 
\begin{align*}
z^k \sum_{n=0}^{\infty}b_nz^n = \sum_{n=0}^{\infty}b_nz^{n+k}=\frac{1}{k}\sum_{n=k}^{\infty} b_nz^{n}=\frac{1}{k}\Big(\sum_{n=0}^{\infty}b_nz^n - \sum_{n=0}^{k-1}b_nz^n \Big)
\end{align*}
This now implies 
\begin{align*}
\sum_{n=0}^{\infty}b_nz_n= \frac{\sum_{n=0}^{k-1}b_nz^n}{k(\frac{1}{k}-z^k)}
\end{align*}
\end{proof}
\section{Exercises 2}
\begin{mdframed}
Let $(M,d)$ be a metric space, $x \in M $ and $F$ a subset of $M$. 
\end{mdframed}
\begin{question}{}{}
Prove that the following statements are equivalent
\begin{enumerate}[label=(\alph*)]
  \item There exists a sequence $\set{x_n}$ in $F$ with $x_n\neq x$ so that $\lim_{n\to \infty}x_n=x$. 
  \item For any $\epsilon $, the intersection of $B'_\epsilon  (x)\triangleq \set{y\in M:0<d(x,y)<\epsilon }$ and $F$ are non-empty. 
\end{enumerate}
\end{question}
\begin{proof}
If (a) is true, then for all $\epsilon $ there exists some $x_n\in F$ such that $d(x_n,x)<\epsilon $. Because $x_n\neq x$, we know that $0<d(x_n,x)$. This now implies $x_n \in B'_\epsilon (x)\cap F$. \\

If (b) is true, then for all $n$, we simply select a point in  $x_n \in B'_{\frac{1}{n}} (x)\cap F$. After such selection, we see that $x_n\neq x$ and for all $\epsilon $, if $n>\frac{1}{\epsilon }$, then $x_n\in B'_\epsilon (x)\cap F$. 
\end{proof}
\begin{question}{}{}
Prove that the following statements are equivalent 
\begin{enumerate}[label=(\alph*)]
  \item $F$ contain all its limit point.
  \item $U=M\setminus F$ is open.
\end{enumerate}
\end{question}
\begin{proof}
If (a) is true, then for all $p\in  U$, we know that $p$ is not a limit point of $F$, then from the first question, we know that there exists  $\epsilon $ such that $B'_\epsilon (x)\cap F=\varnothing$. Because $x\in U=M\setminus F$ also does not belong $x$, we also know that  $B_\epsilon (x)\cap F=\varnothing$. This then implies that $B_\epsilon (x)\subseteq U$, since $U=M\setminus F$. We have proved that $U$ is open.   \\

If (b) is true, then for arbitrary $p\not\in F$, we know there exists some $\epsilon $ such that $B_\epsilon (x)$ is disjoint with $F$. Because  $B'_\epsilon (x)$ is a subset of $B_\epsilon (x)$, we can deduce that $B_\epsilon (x)\cap F=\varnothing$, which from the first question implies that $p$ is not a limit point of $F$. Because $p$ is arbitrary selected from $M\setminus F$, we have proved that none of the points in $M \setminus F$ is a limit point of $F$. This implies that if  $F$ has any limit point, then $F$ must contain that limit point.
\end{proof}
\begin{question}{}{}
Prove the following statements 
\begin{enumerate}[label=(\alph*)]
  \item $M$ and $\varnothing$ are closed. 
  \item The intersection of any family of closed subsets of $M$ is closed.  
  \item The union of finitely many closed subsets of $M$ is closed.
\end{enumerate}
\end{question}
\begin{proof}
It is clear that $M$ is open and trivially true that $\varnothing$ is open. It then follows from the second question that $M$ and  $\varnothing$ are both closed. \\

Let $(F_\alpha )$ be a collection of closed subsets of $M$. Arbitrary select a limit point $x$ of  $\bigcap  F_\alpha $. Let $\set{x_n}$ be a sequence in $\bigcap F_\alpha $ with $x_n\neq x$ so that $\lim_{n\to \infty}x_n=x$. Arbitrary select $\beta $. Note that $\set{x_n}$ is also a sequence in $F_\beta  $ that converge to  $x$ with $x_n\neq x$. This now implies that $x$ is a limit point of $F_\beta  $. Then because $F_\beta  $ is closed, we see that $x\in F_\beta$. Now, since  $\beta $ is arbitrary selected, we see $x\in \bigcap_\alpha F_\alpha $. Because $x$ is arbitrary, we have proved $\bigcap F_\alpha $ contained all its limit points.\\

Let $\set{F_1,\dots ,F_N}$ be a collection of closed subsets of $M$.  Let $x$ be an arbitrary limit point of $\bigcup_{n=1}^N F_n$. Let $\set{x_n}$ be a sequence in $\bigcup_{n=1}^N F_n$ with $x_n\neq x$ converging to $x$. It is clear that there must exists some $j \in \set{1,\dots ,N}$  such that $F_j$ contain infinite terms of $\set{x_n}$, i.e., there exists a subsequence $x_{n_k}$ such that $x_{n_k} \in F_j$ for all $k$. Because  $\lim_{k\to \infty}x_{n_k}=\lim_{n\to \infty}x_n=x$, we now see that $x$ is a limit point of  $F_j$. It then follows from  $F_j$ being closed that $x\in F_j \subseteq\bigcup_{n=1}^N F_n$. Because $x$ is arbitrary, we have proved that $\bigcup_{n=1}^N F_n$ is closed.
\end{proof}
\chapter{PDE intro HW}
\section{1.2 First Order Linear Equations}
\begin{mdframed}
\textbf{(Principle of Geometric Method)} Given a first order homogeneous linear PDE with the form  
\begin{align*}
u_x+g(x,y)u_y=0
\end{align*}
We know if a curve $\gamma (x)= (x,y)$ satisfy 
\begin{align*}
\gamma '(x)= c_x(1,g(x,y))\text{ for some $c_x$ }
\end{align*} 
Then 
\begin{align*}
  (u\circ \gamma )'(x)=0\text{ for all $x$ }
\end{align*}
Since 
\begin{align*}
\gamma '(x)=(1, \frac{dy}{dx})
\end{align*}
To find $\gamma $, we only wish to solve 
\begin{align*}
\frac{dy}{dx}=g(x,y)
\end{align*}
\end{mdframed}
\begin{question}{}{}
Solve 
\begin{align*}
  (1+x^2)u_x+u_y=0
\end{align*}
\end{question}
\begin{proof}
The ODE 
\begin{align*}
\frac{dy}{dx}=\frac{1}{1+x^2}
\end{align*}
has general solution $y=\arctan x + C$, so 
\begin{align*}
u(x,y)=f(y-\arctan x)
\end{align*}
\end{proof}
\begin{question}{}{}
Solve 
\begin{align*}
\begin{cases}
  yu_x+xu_y=0\\
  u(0,y)=e^{-y^2}
\end{cases}
\end{align*}
In which region of the $xy$ plane is the solution uniquely determined? 
\end{question}
\begin{proof}
We are required to solve the ODE 
\begin{align*}
\frac{dy}{dx}= \frac{x}{y}
\end{align*}
Separating the variables and integrate 
\begin{align*}
\int yy' dx= \int xdx  \implies \frac{y^2}{2}= \frac{x^2}{2}+C
\end{align*}
This now implies 
\begin{align*}
u(x,y)=f(y^2-x^2)
\end{align*}
Initial Condition then give us 
\begin{align*}
u=e^{x^2-y^2}
\end{align*}

\end{proof}
\begin{question}{}{}
Solve the equation 
\begin{align*}
u_x+u_y=1
\end{align*}
\end{question}
\begin{proof}
Clearly $u=\frac{x}{2}+\frac{y}{2}$ is a solution. We now solve the PDE 
\begin{align*}
v_x+v_y=0
\end{align*}
Solving the ODE 
\begin{align*}
\frac{dy}{dx}=1
\end{align*}
we have 
\begin{align*}
y=x+C
\end{align*}
Thus the general solution is 
\begin{align*}
u=\frac{x}{2}+\frac{y}{2}+ f(y-x)
\end{align*}


\end{proof}
\begin{question}{}{}
Solve 
\begin{align*}
\begin{cases}
  u_x+u_y+u = e^{x+2y}\\
  u(x,0)=0
\end{cases}
\end{align*}
\end{question}
\begin{proof}
Let $\gamma (x)=x+C$, we have 
\begin{align*}
  (u\circ \gamma )'+(u\circ \gamma )= e^{3x+2C}
\end{align*}
We now solve the ODE 
\begin{align*}
y'+y=e^{3x+2C}
\end{align*}
The particular solution is clearly 
\begin{align*}
y=\frac{1}{4}e^{3x+2C}
\end{align*}
Thus the general solution is 
\begin{align*}
y=\frac{e^{3x+2C}}{4}+ \tilde{C}e^{-x} 
\end{align*}
We then now know 
\begin{align}
\label{uo}
  (u\circ \gamma )(x)= \frac{e^{3x+2C}}{4}+\tilde{C}e^{-x} 
\end{align}
In other words, 
\begin{align*}
u(x,x+C)= \frac{e^{3x+2C}}{4}+ \tilde{C}e^{-x} 
\end{align*}
Putting in the initial conditions, we have
\begin{align*}
0=u(-C,0)= \frac{e^{-C}}{4}+ \tilde{C}e^C 
\end{align*}
This implies 
\begin{align*}
\tilde{C}=\frac{-e^{-2C}}{4} 
\end{align*}
Putting the back into \myref{Equation}{uo}, we have 
\begin{align*}
u(x,x+C)= \frac{e^{3x+2C}- e^{-x-2C}}{4}
\end{align*}
So 
\begin{align*}
u(x,y)=\frac{e^{3x+2(y-x)}-e^{-x-2(y-x)}}{4}
\end{align*}
\end{proof}
\begin{question}{}{}
Use the coordinate method to solve the equation 
\begin{align*}
u_x+2u_y+(2x-y)u=2x^2 + 3xy -2y^2
\end{align*}
\end{question}
\begin{proof}
Let 
\begin{align*}
\begin{cases}
  \xi\triangleq 2x-y \\
  \eta  \triangleq x+2y 
\end{cases}
\end{align*}
We see 
\begin{align*}
\begin{cases}
  u_{\xi}=\frac{2}{5}u_x -\frac{1}{5}u_y\\
  u_{\eta }= \frac{1}{5}u_x + \frac{2}{5}u_y
\end{cases}
\end{align*}
We then can rewrite 
\begin{align}
\label{5ua}
5 u_\eta + \xi u= \xi \eta 
\end{align}
which clearly have particular solution 
\begin{align*}
u=\eta - \frac{5}{\xi} 
\end{align*}
To solve the linear homogeneous PDE 
\begin{align*}
5u_{\eta}+ \xi u =0
\end{align*}
Observe that for all fixed $\xi$, the PDE is just an ODE whose solution is exactly $u=C_\xi e^{\frac{-\xi \eta }{5}}$. We now know the general solution for \myref{PDE}{5ua} is exactly 
\begin{align*}
u=\eta - \frac{5}{\xi} +f(\xi) e^{\frac{-\xi \eta }{5}} 
\end{align*}
Then 
\begin{align*}
u= x+2y - \frac{5}{2x-y}+ e^{\frac{-(2x-y)(x+2y)}{5}} f(2x-y)
\end{align*}

\end{proof}
\section{1.4 Initial and Boundary Condition}
\begin{question}{}{}
Find a solution of 
\begin{align*}
\begin{cases}
  u_t=u_{xx}\\
  u(x,0)=x^2
\end{cases}
\end{align*}
\end{question}
\begin{proof}
Clearly $u=x^2+2t$ suffices.
\end{proof}
\section{1.5 Well Posed Problems}
\begin{question}{}{}
Consider the ODE
\begin{align*}
\begin{cases}
  u''+u=0\\
  u(0)=0\text{ and }u(L)=0
\end{cases} 
\end{align*}
Is the solution unique? Does the answer depend on $L$? 
\end{question}
\begin{proof}
We know the general solution space is exactly spanned by $\cos x$ and $\sin x$. Because 
\begin{enumerate}[label=(\alph*)]
  \item $u(0)=0$.
  \item $\sin 0=0$
  \item $\cos 0=1$
\end{enumerate}
we know the solution of our original ODE must be of the form 
\begin{align*}
u(x)=C \sin x
\end{align*}
This implies that the solution is unique if and only if $2\pi \not\equiv L$ (mod $2\pi$)
\end{proof}
\begin{question}{}{}
Consider the ODE 
\begin{align*}
\begin{cases}
  u''+u'=f \\
  u'(0)=u(0)=\frac{1}{2}(u'(l)+u(l))
\end{cases}
\end{align*}
where $f$ is given. 
\begin{enumerate}[label=(\alph*)]
  \item Is the solution unique? 
  \item Does a solution necessarily exist, or is there a condition that $f$ must satisfy for existence? 
\end{enumerate}
\end{question}
\begin{proof}
The solution space of linear homogeneous ODE $u''+u'=0$  is spanned by $e^{-x}$ and constant. If we add in the initial condition  $u'(0)=u(0)$, then the solution space become the subspace spanned by $e^{-x}-2$. One can check that if $u \in\operatorname{span}(e^{-x}-2)$, then 
\begin{align*}
u(0)=\frac{1}{2}(u'(l)+u(l))\text{ for all $l\inr$ }
\end{align*}
We now know the solution of the original ODE is not unique, since any solution added by $e^{-x}-2$ is again a solution. \\

Integrating both side on $[0,l]$, we see that given the boundary conditions, $f$ must satisfy 
\begin{align*}
\int_0^l f(x)dx&=\int_0^l u''+u'dx\\
&=u(l)+u'(l)-u(0)-u'(0)=0
\end{align*}
\end{proof}
\begin{question}{}{}
Consider the Neumann problem 
\begin{align*}
\Delta u=f(x,y,z)\text{ in }D\text{ and } \frac{\partial u}{\partial n}=0\text{ on }\operatorname{bdy}D
\end{align*}
\begin{enumerate}[label=(\alph*)]
  \item What can we add to any solution to get another solution? 
  \item Use the divergence Theorem and the PDE to show that 
  \begin{align*}
  \iiint_D f(x,y,z)dxdydz=0
  \end{align*}
  is a necessary condition for the Neumann problem to have a solution. 
\end{enumerate}
\end{question}
\begin{proof}
Clearly, constants suffices, and observe 
\begin{align*}
\iiint_D fdxdydx=\iiint_D \Delta u dxdydz= \iiint_D \nabla \cdot (\nabla u)dxdydz = \iint_{\operatorname{bdy}D} \nabla u \cdot \textbf{n}dS= 0
\end{align*}
\end{proof}
\begin{question}{}{}
Consider the equation 
\begin{align*}
u_x+yu_y=0 
\end{align*}
with the boundary condition $u(x,0)=\phi (x)$. 
\begin{enumerate}[label=(\alph*)]
  \item $\phi (x)=x \implies $ no solution exists
  \item $\phi (x)=1 \implies $ multiple solutions exist.
\end{enumerate}
\end{question}
\begin{proof}
Using the geometric method, we see the characteristic curve is exactly $y=\tilde{C}e^x$. Thus the general solution is of the form 
\begin{align*}
u(x,y)=f(e^{-x}y)
\end{align*}
The boundary condition implies 
\begin{align*}
\phi (x)=u(x,0)=f(0)
\end{align*}
The result then follows. 
\end{proof}
\section{1.6 Types of Second-Order Equations} 
\begin{mdframed}
Consider the constant coefficient Linear PDE 
\begin{align*}
u_{xx}+2a_{12}u_{xy}+a_{22}u_{yy}+a_1u_x+a_2u_y+a_0u=0 
\end{align*}
Ignoring the term with order less than $2$, we have 
\begin{align*}
u_{xx}+2a_{12}u_{xy}+a_{22}u_{yy}=0 
\end{align*}
Or equivalently 
\begin{align*}
  (\partial_x + a_{12}\partial_y)^2 u + ( a_{22}-a_{12}^2)\partial_{yy} u =0 
\end{align*}
Now, if we set  
\begin{align*}
x\triangleq \xi \text{ and }y\triangleq a_{12}\xi + (\abso{a_{22}-a_{12}^2})^{\frac{1}{2}}\eta 
\end{align*}
we see that 
\begin{align*}
\begin{cases}
  a_{22}-a_{12}^2>0 \implies u_{\xi \xi}+u_{\eta \eta  }=0 \hspace{0.5cm}(\text{Elliptic})\\
  a_{22}-a_{12}^2=0 \implies u_{\xi \xi}=0\hspace{0.5cm}(\text{Parabolic})\\
  a_{22}-a_{12}^2 < 0 \implies u_{\xi \xi}-u_{\eta \eta  }=0\hspace{0.5cm}(\text{Hyperbolic})
\end{cases}
\end{align*}
More generally, if we are given 
\begin{align*}
a_{11}u_{xx}+2a_{12}u_{xy}+a_{22}u_{yy}=0
\end{align*}
then the discriminant is exactly 
\begin{align*}
a_{11}a_{22}-a_{12}^2
\end{align*}
\end{mdframed}
\begin{question}{}{}
What is the type of each of the following equations. 
\begin{enumerate}[label=(\alph*)]
  \item $u_{xx}-u_{xy}+u_{yy}+\cdots +u =0$. 
  \item $9u_{xx}+6u_{xy}+u_{yy}+u_x=0$
\end{enumerate}
\end{question}
\begin{proof}
The discriminant for (a) and (b) are respectively  $\frac{3}{4}$ are $0$, thus elliptic and parabolic. 
\end{proof}
\begin{question}{}{}
Find the regions in the $xy$ plane where the equation 
 \begin{align*}
   (1+x)u_{xx}+2xyu_{xy}-y^2u_{yy}=0
\end{align*}
is elliptic, hyperbolic, or parabolic. Sketch them. 
\end{question}
\begin{proof}
The discriminant is exactly 
\begin{align*}
  (xy)^2- (1+x)(-y^2)&=x^2y^2+xy^2+y^2\\
  &=y^2 (x^2+x+1)\\
  &=y^2[(x+\frac{1}{2})^2+ \frac{3}{4}]
\end{align*}
It then follows that the equation is parabolic if and only if $y=0$, and hyperbolic if and only if  $y\neq 0$. 

\end{proof}
\begin{question}{}{}
Reduce the elliptic equation 
\begin{align*}
u_{xx}+3u_{yy}-2u_x+24u_y+5u=0
\end{align*}
to the form 
\begin{align*}
v_{xx}+v_{yy}+cv=0
\end{align*}
by a change of dependent variable 
\begin{align*}
u\triangleq ve^{\alpha x+\beta  y}
\end{align*}
then a change of scale 
\begin{align*}
y'=\gamma y
\end{align*}
\end{question}
\begin{proof}
The original elliptic equation can be written in the form 
\begin{align*}
e^{\alpha x+ \beta  y}\Big[ v_{xx}+2\alpha v_x+ \alpha^2 v +3 (v_{yy}+2\beta  v_y + \beta ^2 v) -2(v_x + \alpha v)+24 (v_y+\beta v)+5v \Big]=0
\end{align*}
which implies 
\begin{align*}
v_{xx}+3v_{yy}+(2\alpha -2)v_x+ (6 \beta  +24)v_y + (\alpha ^2 +3\beta ^2 -2\alpha +24 \beta  +5)v=0 
\end{align*}
Letting $\alpha \triangleq 1$ and $\beta  \triangleq -4$, we see 
\begin{align*}
v_{xx}+3v_{yy}+ \tilde{c}v =0 
\end{align*}
Letting $y\triangleq \sqrt{3}y' $, we then see 
\begin{align*}
v_{xx}+v_{y'y'}+\tilde{c}v=0 
\end{align*}
\begin{question}{}{}
Consider the equation $3u_y+u_{xy}=0$.
\begin{enumerate}[label=(\alph*)]
  \item What is its type? 
  \item Find the general solution. (Hint: Substitute $v=u_y$).
  \item With the auxiliary conditions $u(x,0)=e^{-3x}$ and $u_{y}(x,0)=0$, does a solution exist? Is it unique? 
\end{enumerate}
\end{question}
\begin{proof}
Since the discriminant is exactly $\frac{-1}{4}$, the type is hyperbolic. Letting $v\triangleq u_y$, we see 
\begin{align*}
3v+v_x=0
\end{align*}
This PDE on each horizontal line is an ODE and thus have the solution 
\begin{align*}
u_y=v=f(y)e^{-3x}
\end{align*}
Then $u$ must be of the form 
\begin{align*}
u=F(y)e^{-3x}+ g(x)
\end{align*}
Applying the initial condition $u_y(x,0)=0$, we see 
\begin{align*}
f(0)e^{-3x}=u_y(x,0)=0
\end{align*}
which implies $f(0)=0$. Now apply another initial condition $u(x,0)=e^{-3x}$. 
\begin{align*}
F(0)e^{-3x}+g(x)=u(x,0)=e^{-3x}
\end{align*}
We then see the solutions exist and are not unique, since 
\begin{align*}
\begin{cases}
  F(y)=1\\
  g(x)=0
\end{cases}\text{ and }\begin{cases}
  F(y)=y^2\\
  g(x)=e^{-3x}
\end{cases}
\end{align*}
are both solutions. 
\end{proof}
\section{2.1 The Wave Equation}
\begin{abstract}
In this section, $c\inr^*$. 
\end{abstract}
\begin{theorem}
\textbf{(General Solution of The Wave Equation)} The general solution of the wave equation 
\begin{align*}
u_{tt}=c^2u_{xx}
\end{align*}
is of the form 
\begin{align*}
u=f(x+ct)+g(x-ct)
\end{align*}
\end{theorem}
\begin{proof}
Observe that 
\begin{align*}
0=u_{tt}-c^2u_{xx}=(\partial_t + c\partial_x)(\partial_t - c\partial_x)u
\end{align*}
If we let $v=u_t-cu_x$, then we must have $v_t+cv_x=0$. We know the general solution of $v$ is $v=g(x-ct)$. We have reduce the problem into solving  
\begin{align}
\label{ime}
u_t-cu_x=g(x-ct)
\end{align}
Now observe that for all $w:\R\rightarrow \R$
\begin{align*}
  (\partial_t - c \partial_x)\big(w(x-ct) \big)=2cw'(x-ct)
\end{align*}
We then see the particular solution for \myref{Equation}{ime} is 
\begin{align*}
u= \frac{1}{2c}G(x-ct)
\end{align*}
and the general solution is then 
\begin{align*}
u=f(x+ct)+\frac{1}{2c}G(x-ct)
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(IVP for The Wave Equation)} The Initial Value Problem 
\begin{align*}
\begin{cases}
  u_{tt}=c^2u_{xx} \\
  u(x,0)=\phi (x)\\
  u_t(x,0)=\psi (x)
\end{cases}
\end{align*}
has exactly one solution 
\begin{align*}
u(x,t)=\frac{1}{2}[ \phi (x+ct)+\phi (x-ct)]+ \frac{1}{2c}\int_{x-ct}^{x+ct}\psi (s)ds
\end{align*}
\end{theorem}
\begin{proof}
Write $u(x,t)=f(x+ct)+g(x-ct)$. By initial condition, we know 
\begin{align*}
f(x)+g(x)=\phi (x)\text{ and }f'(x)-g'(x)= \frac{\psi (x)}{c}
\end{align*}
Differentiating the former, we also have 
\begin{align*}
f'(x)+g'(x)=\phi' (x)
\end{align*}
This then give us 
\begin{align*}
f'(x)= \frac{\phi '(x)}{2} + \frac{\psi (x)}{2c}\text{ and }g'(x)= \frac{\phi '(x)}{2}- \frac{\psi (x)}{2c}
\end{align*}
It now follows that 
\begin{align*}
f(s)=\frac{\phi (s)}{2}+ \frac{1}{2c}\int_0^s \psi (x)dx +A \text{ and }g(s)= \frac{\phi (s)}{2} - \frac{1}{2c} \int_0^s \psi (x)dx + B
\end{align*}
Note that since $f(x)+g(x)=\phi (x)$, we know $B=-A$. \\

We now have 
\begin{align*}
u(x,t)&=f(x+ct)+g(x-ct)\\
&=\frac{\phi (x+ct)+\phi (x-ct)}{2}+ \frac{1}{2c} \int^{x+ct}_{x-ct}\psi (x)dx
\end{align*}
\end{proof}
\begin{question}{}{}
Solve 
\begin{align*}
\begin{cases}
  u_{tt}=c^2u_{xx} \\
  u(x,0)=e^x \\
  u_t(x,0)= \sin x 
\end{cases}
\end{align*}
\end{question}
\begin{proof}
Let 
\begin{align*}
u\triangleq f(x+ct)+g(x-ct)
\end{align*}
Plugging the initial conditions, we know 
\begin{align*}
\begin{cases}
 f(x)+g(x)=e^x \\
 f'(x)-g'(x)= \frac{\sin x}{c} 
\end{cases}
\end{align*}
This give us 
\begin{align*}
f'(x)= \frac{e^x+ \frac{\sin x}{c}}{2}\text{ and }g'(x)= \frac{e^x - \frac{\sin x}{c}}{2}
\end{align*}
Then by FTC, we have
\begin{align*}
\begin{cases}
  f(x)= \frac{e^x-\frac{\cos x}{c}}{2}+f(0)- \frac{1}{2} +\frac{1}{2c} \\
  g(x)= \frac{e^x + \frac{\cos x}{c}}{2} +g(0) - \frac{1}{2}- \frac{1}{2c}
\end{cases}
\end{align*}
Note that $f(0)+g(0)=e^0=1$, which cancel the constant terms in $u$, i.e.
\begin{align*}
u&=f(x+ct)+g(x-ct)\\
&=\frac{e^{x+ct}+ e^{x-ct} + \frac{-\cos (x+ct)+ \cos (x+ct)}{c}}{2}
\end{align*}
\end{proof}
\begin{question}{}{}
If both $\phi$ and $\psi$ are odd functions of $x$, show that the solution of  $u(x,t)$ of the wave equation is also odd in $x$ for all $t$. 
\end{question}
\begin{proof}
Suppose 
\begin{align*}
u=f(x+ct)+g(x-ct)
\end{align*}
We have  
\begin{align*}
\begin{cases}
  f+g=\phi \\
  f'-g'= \frac{\psi }{c}
\end{cases}
\end{align*}
This give us 
\begin{align*}
f'= \frac{\phi ' + \frac{\psi}{c}}{2}\text{ and }g'= \frac{\phi' - \frac{\psi}{c}}{2}
\end{align*}
and 
\begin{align*}
\begin{cases}
f(x)= \frac{1}{2}\int_0^x (\phi' + \frac{\psi}{c})ds +f(0)\\
g(x)= \frac{1}{2}\int_0^x (\phi' - \frac{\psi}{c})ds +g(0)
\end{cases}
\end{align*}
that is 
\begin{align*}
\begin{cases}
  f(x)=f(0)+ \frac{1}{2}\Big[\phi (x)-\phi (0) \Big]+ \frac{1}{2c}\int_0^x \psi (s)ds \\
  g(x)=g(0)+ \frac{1}{2}\Big[ \phi (x)- \phi (0) \Big]- \frac{1}{2c}\int_0^x \psi (s)ds
\end{cases}
\end{align*}
Noting $f+g=\phi$, we now have 
\begin{align*}
u(x,t)&=f(x+ct)+g(x-ct) \\
&=\frac{\phi (x+ct)+ \phi (x-ct)}{2}+ \frac{1}{2c}\int^{x+ct}_{x-ct}\psi (s)ds
\end{align*}
and 
\begin{align*}
u(-x,t)&=\frac{\phi (-x+ct)+ \phi (-x-ct)}{2} + \frac{1}{2c}\int^{-x+ct}_{-x-ct}\psi (s)ds \\
&= \frac{-\phi (x-ct)-\phi (x+ct)}{2} -\frac{1}{2c}\int_{x+ct}^{x-ct}\psi (-u)du \hspace{0.5cm}(\because u=-s)\\
&=\frac{-\phi (x-ct)-\phi (x+ct)}{2} +\frac{1}{2c}\int_{x+ct}^{x-ct}\psi (u)du \hspace{0.5cm}(\because \psi\text{ is odd }) \\
&=\frac{-\phi (x-ct)-\phi (x+ct)}{2} -\frac{1}{2c}\int^{x+ct}_{x-ct}\psi (u)du = -u(x,t)
\end{align*}
\end{proof}
\begin{question}{}{}
A spherical wave is a solution of the three-dimensional wave equation of the form $u(r,t)$, where $r$ is the distance to the origin (the spherical coordinate). The wave equation takes the form 
\begin{align*}
u_{tt}=c^2 \Big(u_{rr}+ \frac{2}{r}u_r \Big)
\end{align*}
\begin{enumerate}[label=(\alph*)]
  \item Change variables $v=ru$ to get the equation for $v:v_{tt}=c^2v_{rr}$. 
  \item Solve for $v$ and thereby solve the spherical wave equation. 
  \item Solve it with the initial condition $u(r,0)=\phi (r),u_t(r,0)= \psi (r)$, taking both $\phi (r)$ and $\psi (r)$ to be even functions of $r$. 
\end{enumerate}
\end{question}
\begin{proof}
If we let $v=ru$, then 
 \begin{align*}
v_{tt}=ru_{tt}\text{ and }v_{rr}=ru_{rr}+2u_r
\end{align*}
This then give us 
\begin{align*}
v_{tt}=ru_{tt}=rc^2\Big(u_{rr}+\frac{2}{r}u_r \Big)=c^2(ru_{rr}+2u_r)=c^2v_{rr}
\end{align*}
The general solution of $v$ is 
\begin{align*}
v=f(r+ct)+g(r-ct)
\end{align*}
and thus 
\begin{align*}
u(r,t)= \frac{f(ct+r)+g(r-ct)}{r}
\end{align*}
Plugging the initial conditions, we have 
\begin{align*}
\frac{f(r)+g(r)}{r}=u(r,0)=\phi (r)\text{ and }\frac{c[f'(r)-g'(r)]}{r}=u_t(r,0)=\psi (r)
\end{align*}
In other words, 
\begin{align*}
\begin{cases}
  f(r)+g(r)=r\phi (r)\\
  f'(r)-g'(r)=\frac{r \psi (r)}{c}
\end{cases}
\end{align*}
Differentiating the first equation, we have  
\begin{align*}
  f'(r)+g'(r)= \phi (r)+r\phi'(r)
\end{align*}
We now can solve $f',g'$ 
\begin{align*}
f'(r)= \frac{\phi (r)+r\phi '(r)+ \frac{r\psi (r)}{c}}{2}\text{ and }g'(r)= \frac{\phi (r)+r\phi'(r)- \frac{r\psi (r)}{c}}{2}
\end{align*}
We now have 
\begin{align*}
f(r)&= f(1)+\int_1^{r} f'(s)ds   \\
&=f(1)+ \Big[\frac{s \phi (s)}{2} \Big] \Big|_{s=1}^{r} + \frac{1}{2c}\int_1^r s \psi (s)ds
\end{align*}
and 
\begin{align*}
g(r)&=g(1)+ \int_1^r g'(s)ds \\
&=g(1)+ \Big[ \frac{s \phi (s)}{2} \Big]\Big|_{s=1}^r -\frac{1}{2c}\int_1^r s\psi (s)ds
\end{align*}
Noting that $f(1)+g(1)=1\phi (1)$, we can cancel these terms and get 
\begin{align*}
u(r,t)&= \frac{f(r+ct)+g(r-ct)}{r} \\
&= \frac{(r+ct)\phi (r+ct)+ (r-ct)\phi (r-ct)}{2r}+ \frac{1}{2cr}\int_1^{r+ct}s \phi (s)ds - \frac{1}{2cr}\int_1^{r-ct}s\phi (s)ds\\
&=\frac{(r+ct)\phi (r+ct)+ (r-ct)\phi (r-ct)}{2r}+ \frac{1}{2cr}\int_{r-ct}^{r+ct}s \phi (s)ds 
\end{align*}
\end{proof}
\begin{question}{}{}
Solve 
\begin{align*}
\begin{cases}
u_{xx}+u_{xt}-20u_{tt}=0 \\
u(x,0)=\phi (x) \\
u_t(x,0)= \psi (x)
\end{cases}
\end{align*}
\end{question}
\begin{proof}
The PDE can be write in the form of 
\begin{align*}
  (\partial_x + 5\partial_t) (\partial_x - 4 \partial _t)u=0
\end{align*}
which have the general solution 
\begin{align*}
u(x,t)=f(5x-t)+g(4x+t)
\end{align*}
We now see 
\begin{align*}
f(5x)+g(4x)=\phi (x)\text{ and }-f'(5x)+g'(4x)=\psi (x)
\end{align*}
This then give us 
\begin{align*}
f'(5x)=\frac{(\phi' - 4\psi)(x)}{9}\text{ and }g'(4x)= \frac{(\phi '+5\psi )(x)}{9}
\end{align*}
and thus 
\begin{align*}
f(x)&=f(0)+ \int_0^x f'(s)ds\\
&=f(0)+\int_0^x \frac{(\phi' -4\psi)(\frac{s}{5})}{9}ds \\
&=f(0)+ \frac{5}{9}\Big[\phi (\frac{x}{5})-\phi (0) \Big] - \frac{4}{9}\int_0^x \psi (\frac{s}{5})ds
\end{align*}
and similarly 
\begin{align*}
g(x)&=g(0)+\int_0^x g'(s)ds \\
&=g(0)+ \int_0^x \frac{(\phi ' +5 \psi )(\frac{s}{4})}{9}ds \\
&=g(0)+ \frac{4}{9}\Big[ \phi (\frac{x}{4})- \phi (0) \Big]+\frac{5}{9} \int_0^x \psi (\frac{s}{4})ds
\end{align*}
Noting that $f(0)+g(0)=u(0,0)=\psi (0)$, we now have
\begin{align*}
u(x,t)&=f(5x-t)+g(4x+t) \\
&=\frac{5\phi (\frac{5x-t}{5})+4 \phi (\frac{4x+t}{4})}{9} - \frac{4}{9}\int_0^{5x-t}\psi (\frac{s}{5})ds + \frac{5}{9}\int_0^{4x+t}\psi (\frac{s}{4})ds
\end{align*}
\end{proof}
\begin{question}{}{}
Find the general solution of 
\begin{align*}
3u_{tt}+10u_{xt}+3u_{xx}=\sin (x+t)
\end{align*}
\end{question}
\begin{proof}
Clearly, we can rewrite the equation to be 
\begin{align*}
  (3\partial_x + \partial_t )(\partial_x + 3\partial_t)u=\sin (x+t)
\end{align*}
If we let $v=u_x+3u_t$, then  we have 
\begin{align*}
3v_x+v_t= \sin (x+t)
\end{align*}
Define 
\begin{align*}
\gamma (x)\triangleq  (x, \frac{x}{3}+c)
\end{align*}
We then see 
\begin{align*}
  (v\circ \gamma )'(x)= \frac{\sin (x+ \frac{x}{3}+c)}{3}
\end{align*}
which implies 
\begin{align*}
v(x, \frac{x}{3}+c) = v\circ \gamma  (x)= \frac{\cos (\frac{4x}{3}+c)}{-4}+ C_c
\end{align*}
and thus 
\begin{align*}
v(x,t)&= \frac{\cos (\frac{4x}{3}+ t- \frac{x}{3})}{-4}+f(t-\frac{x}{3})\\
&= \frac{\cos (x+t)}{-4}+f(3t-x)
\end{align*}
It remains to solve 
\begin{align*}
u_x+ 3u_t= \frac{\cos (x+t)}{-4}+ f(3t-x)
\end{align*}
Now define 
\begin{align*}
\gamma  (x)\triangleq  (x,3x+c)
\end{align*}
We then see 
\begin{align*}
  (u\circ \gamma )'(x)= \frac{\cos (4x+c)}{-4} + f(8x+3c)
\end{align*}
which implies 
\begin{align*}
u(x,3x+c)= \frac{\sin (4x+c)}{-16}+\frac{F(8x+3c)}{8}+u(0,c)+f(c)
\end{align*}
and thus 
\begin{align*}
u(x,t)= \frac{\sin (x+t)}{-16}+ \tilde{F}(-x+3t)+ g(t-3x)
\end{align*}
where $g$ is the initial condition. 
\end{proof}
\section{2.2 Causality and Energy}
\begin{question}{}{}
Show that the wave equation has the following invariant properties 
\begin{enumerate}[label=(\alph*)]
  \item Any translate $u(x-y,t)$ where $y$ is fixed, is also a solution. 
  \item Any derivative, say $u_x$, is also a solution. 
  \item The dilated function $u(ax,at)$ is also a solution.
\end{enumerate}
\end{question}
\begin{proof}
The first property follows from direct computation, the second property follows from $0_x=0$ and the third property follows from  observing $v\triangleq u(ax,at)$ satisfy $v_{tt}=a^2u_{tt}=a^2c^2u_{xx}=v_{xx}$. 
\end{proof}
\begin{question}{}{}
If $u(x,t)$ satisfy the wave equation $u_{tt}=u_{xx}$, prove the identity 
\begin{align*}
u(x+h,t+k)+u(x-h,t-k)=u(x+k,t+h)+u(x-k,t-h)
\end{align*}
\end{question}
\begin{proof}
Define $\phi,\psi :\R\rightarrow \R$ by 
\begin{align*}
\phi (x)\triangleq u(x,0)\text{ and }\psi (x)\triangleq u_t(x,0)
\end{align*}
We then know that 
\begin{align*}
u(x,t)&= \frac{\phi (x+t)+ \phi (x-t)}{2}+ \frac{1}{2}\int^{x+t}_{x-t}\psi (s)ds\\
&\triangleq \frac{A(x,t)+B(x,t)+C(x,t)}{2}
\end{align*}
where 
\begin{align*}
\begin{cases}
  A(x,t)\triangleq \phi (x+t)\\
  B(x,t)\triangleq \phi (x-t)\\
  C(x,t)\triangleq \int_{x-t}^{x+t}\psi (s)ds
\end{cases}
\end{align*}
The proof then follows from 
\begin{align*}
  A(x+h,t+k)=A(x+k,t+h)&\text{ and }A(x-h,t-k)=A(x-k,t-h)\\
B(x+h,t+k)=B(x-k,t-h)&\text{ and }B(x-h,t-k)=B(x+k,t+h)\\
C(x+h,t+k)=C(x+k,t+h)&\text{ and }C(x-h,t-k)=C(x-k,t-h)
\end{align*}
\end{proof}
\begin{question}{}{}
Suppose that we have the PDE of damped string 
\begin{align*}
  \begin{cases}    
\rho u_{tt}-T u_{xx}+ru_t=0\text{ where }r>0\\
u(x,0)=0 \text{ if }\abso{x}>N
  \end{cases}
\end{align*}
Show that if we define the energy $E(t)$ of this system by 
\begin{align*}
E(t)=\int_{-\infty}^{\infty} (\rho u_t^2 + Tu_x^2)dx
\end{align*}
Then the energy decrease as time goes. 
\end{question}
\begin{proof}
Because $u$ is smooth, we have 
\begin{align*}
E'(t)&=\int_{-\infty}^{\infty} (\rho u_t^2+Tu_x^2)_tdx\\
&=\int_{-\infty}^{\infty} (2\rho u_tu_{tt}+2Tu_xu_{xt})dx\\
&=\int_{-\infty}^{\infty} [2u_t(Tu_{xx}-ru_t)+2Tu_xu_{xt}]dx\\
&=\int_{-\infty}^{\infty} [2T(u_tu_x)_x-2ru_t^2]dx \\
&=2Tu_tt_x\Big|_{x=0}^{\infty} - \int_{-\infty}^{\infty} 2ru_t^2\\
&=- \int_{-\infty}^{\infty} 2ru_t^2\leq  0
\end{align*}
\end{proof}

\section{2.3 The Diffusion Equation}
\begin{mdframed}
In this section, we shall first set 
\begin{align*}
\Omega \triangleq (0,l)\text{ and }\Gamma \triangleq \Omega \times \set{0}\cup  \partial \Omega \times [0,T]
\end{align*}
and write  
\begin{align*}
\Omega_T\triangleq \Omega \times (0,T)
\end{align*}
We suppose $u:\overline{\Omega_T}\rightarrow \R$ satisfy 
\begin{align*}
  u \in C^2(\Omega \times (0,T])
\end{align*}
If $u$ achieve a maximum on $\Omega \times (0,T]$, then at that point $u$ must have 
\begin{align*}
u_t\geq 0\text{ and }u_{xx}\leq 0
\end{align*}
\end{mdframed}
\begin{theorem}
\textbf{(Weak Maximum Principle)} If 
\begin{align}
\label{utk}
  u_t-ku_{xx}\leq  0\text{ on }\Omega \times (0,T]
\end{align}
then $u$ must achieve its maximum at  $\Gamma $.  
\end{theorem}
\begin{proof}
Because $\Gamma $ is compact, we know there exists a maximum $M$ of $u$ on $\Gamma $. Fix $\epsilon $ and define $v:\overline{\Omega_T}\rightarrow \R$
\begin{align*}
v(x,t)\triangleq u(x,t)+ \epsilon x^2
\end{align*}
Because 
\begin{align*}
u(x,t) \leq \max_{\overline{\Omega_T}}v - \epsilon x^2\text{ for all }(x,t)\in \overline{\Omega_T}
\end{align*}
we can reduce the problem into proving 
\begin{align*}
\vi{\max_{\overline{\Omega_T}}v\leq M+\epsilon l^2\text{ for all }p \in F}
\end{align*}
From \myref{Equation}{utk}, we can deduce
\begin{align*}
v_t-kv_{xx}=u_t-ku_{xx}-2k\epsilon  <0
\end{align*}
Because $v$ is continuous, we know $v$ attain its maximum at some point. Now, with diffusion inequality, we can deduce 
\begin{enumerate}[label=(\alph*)]
  \item The maximum of $v$ must not be in $\Omega_T$, otherwise at that point $v_t=0$ and  $v_{xx}\leq 0$ yield a contradiction. 
  \item The maximum of $v$ must also not be in the top edge $\partial \Omega_T \setminus \Gamma $, otherwise $v_t\geq 0$ and $v_{xx}\leq 0$ yield a contradiction. 
\end{enumerate}
We have proved that $v$ can only attain maximum at some point $(x_0,t_0)\in F_0$, and it follows that 
\begin{align*}
\max_{(x,t) \in F}v(x,t)=v(x_0,t_0)=u(x_0,t_0)+\epsilon  x_0^2 \leq M+ \epsilon l^2  \vdone
\end{align*}
\end{proof}
\begin{corollary}
\textbf{(Weak Minimum Principle)} The minimum of $u$ must also happen on $F_0$.  
\end{corollary}
\begin{mdframed}
Now, consider the Dirichlet's problem 
\begin{align}
\label{Dirichlet's problem}
\begin{cases}
u_t-ku_{xx}=f(x,t)\text{ for }0<x<l\text{ and }t>0 \\
u(x,0)=\phi (x)\text{ for $0\leq x\leq l$ }\\
u(0,t)=g(t)\text{ and }u(l,t)=h(t)\text{ for $t\geq 0$ }
\end{cases}
\end{align}
Note that for all $T$, because the difference $w$ of two solution  $u_1,u_2$ for Dirichlet's function must satisfy 
\begin{align*}
\begin{cases}
  w_t=kw_{xx}\text{ on }\overline{\Omega_T}\setminus  \Gamma \\
  w(x,0)=w(0,t)=0\text{ for any }0\leq x\leq l\text{ and }0\leq t\leq T
\end{cases}
\end{align*}
By minimum and maximum principle we can deduce $w=0$ on  $\Omega$, and thus  $u_1=u_2$ on  $F$. It then follows that  $u_1=u_2$ on  $[0,l]\times [0,\infty)$. 
\end{mdframed}
\begin{theorem}
\textbf{(Uniqueness for the Dirichlet's problem for the diffusion equation with Energy Method)} If $u_1,u_2:[0,l]\times [0,\infty)$ are both solution of the Dirichlet's problem, then  $u_1=u_2$. 
\end{theorem}
\begin{proof}
Define $w:[0,l]\times [0,\infty)\rightarrow \R$ by $w=u_1-u_2$. Multiplying $w$ with $(w_t-kw_{xx})$, we see that for all $x\in (0,l)$ and $t> 0$, 
\begin{align*}
0=(w_t-kw_{xx})w= \Big(\frac{w^2}{2}\Big)_t + (-kw_xw)_x + kw_x^2
\end{align*}
Because $w(0,t)=w(l,t)=0$ for all $t$, it follows that for all $t>0$ 
\begin{align*}
0&=\int_0^l \Big[\Big(\frac{w^2}{2}\Big)_t + (-kw_xw)_x + kw_x^2\Big]dx  \\
 &=\int_0^l \Big[\Big(\frac{w^2}{2}\Big)_t + kw_x^2\Big]dx
\end{align*}
which implies   
\begin{align*}
I'(t)\leq 0\text{ if we define }I:[0,\infty)\rightarrow \R\text{ by }I(t)\triangleq \int_0^l \Big(\frac{w^2}{2} \Big)dx 
\end{align*}
Because $I(0)=0$ by definition and $I(t)$ are integrals of non-negative functions, we can deduce $I$ is identically $0$. The desired result $w(x,t)=0$ for all $x,t \in [0,l]\times [0,\infty)$ then follows. 
\end{proof}
\begin{mdframed}
Now, consider \customref{Dirichlet's problem}{Dirichlet's problem} with different initial conditions $\phi_1,\phi_2: [0,l]\rightarrow \R$, and suppose $u_1,u_2:[0,l]\times [0,\infty)$ are corresponding solutions. The maximum and minimum principle give us a $L^\infty$ estimation for stability 
\begin{align*}
\max_{[0,l]\times [0,\infty)} \abso{u_1-u_2}\leq \max_{[0,l]}\abso{\phi_1-\phi_2}
\end{align*}
While the energy method give us a $L^2$ estimation for  stability: For all $t\geq 0$,  
\begin{align*}
\int_0^l \Big(\frac{w^2(x,t)}{2} \Big)dx =I(t)\leq I(0)=\int_0^l \Big(\frac{w^2(x,0)}{2} \Big)= \int_0^l \frac{(\phi_1-\phi_2)^2}{2}dx
\end{align*}
\end{mdframed}
\begin{question}{}{}
Consider the diffusion equation  
\begin{align*}
\begin{cases}  
u_t=u_{xx}\text{ on }(0,1)\times (0,\infty)\\
u(0,t)=u(1,t)=0 \text{ for all }t>0\\
u(x,0)=1-x^2
\end{cases}
\end{align*}
\begin{enumerate}[label=(\alph*)]
  \item Show that $u(x,t)>0$ for all $(x,t)\in (0,1)\times (0,\infty)$. 
  \item Define $\mu : (0,\infty)\rightarrow \R$ by $\mu(t)\triangleq \max_{x\in [0,1]} u(x,t)$. Show that $\mu$ is a decreasing function. 
\end{enumerate}
\end{question}
\begin{proof}
The proof of (a) follows from a loose application of the strong minimum principle. \\

The proof of (b) follows from noting $v(x,t)\triangleq u(x,t+t_0):[0,1]\times [0,\infty)$ also is a solution of the diffusion equation and application of maximum principle on $v$.
\end{proof}
\begin{question}{}{}
Consider the Dirichlet's problem 
\begin{align*}
\begin{cases}  
u_t=u_{xx}\text{ on }(0,1)\times (0,\infty)\\
u(0,t)=u(1,t)=0 \text{ for all }t\geq 0\\
u(x,0)=4x(1-x)
\end{cases}
\end{align*}
Show that 
\begin{enumerate}[label=(\alph*)]
  \item $0<u(x,t)<1$ for all $t>0$ and  $0<x<1$.  
  \item $u(x,t)=u(1-x,t)$ for all $t\geq 0$ and $0\leq x\leq 1$. 
  \item Use the energy method to show that $\int_0^1 u^2dx$ is a strictly decreasing function of $t$.
\end{enumerate}
\end{question}
\begin{proof}
  (a) follows from application of strong maximum and minimum principle. (b) follows from uniqueness of Dirichlet's problem and the observation that $u(1-x,t)$ is also a solution. \\

The energy method give us 
\begin{align*}
\frac{d}{dt}\int_0^1 \frac{u^2}{4}dx = - \int_0^1 u_x^2dx \leq 0\text{ for all }t>0
\end{align*}
and (c) follows.
\end{proof}
\begin{question}{}{}
Verify that 
\begin{align*}
u=-2xt-x^2\text{ is a solution of }u_t=xu_{xx}
\end{align*}
and find the location of maximum of $t$ in the close rectangle $\set{-2\leq x\leq 2,0\leq t\leq 1}$. 
\end{question}
\begin{proof}
Write 
\begin{align*}
u=-(x+t)^2+t^2
\end{align*}
It follows that the maximum occurs at $t=-x=1$. 
\end{proof}
\begin{question}{}{}
Prove the comparison principle for the diffusion equation: If $u$ and  $v$ are two solutions and 
 \begin{align*}
u\leq v\text{ for }t=0,x=0,x=l
\end{align*}
then 
\begin{align*}
u\leq v\text{ on }[0,l]\times [0,\infty)
\end{align*}
\end{question}
\begin{proof}
This follows from application of the minimum principle on $v-x$. 
\end{proof}
\begin{question}{}{}
Suppose 
\begin{align*}
\begin{cases}
u_t-ku_{xx}=f\\
v_t-kv_{xx}=g
\end{cases}\text{ and }f\leq g
\end{align*}
and suppose 
\begin{align*}
u\leq v\text{ at }x=0,x=l\text{ and }t=0
\end{align*}
Prove that 
\begin{align*}
u\leq v\text{ on }[0,l]\times[0,\infty)
\end{align*}
\end{question}
\begin{proof}
Let $w\triangleq u-v:\overline{\Omega_T}\rightarrow \R$. It is clear that 
\begin{align*}
w_t-kw_{xx}\leq 0\text{ on }\overline{\Omega_T}\setminus \Gamma 
\end{align*}
It then follows that $w$ attain its maximum on $\Gamma $, which must not be greater than $0$. 
\end{proof}
\section{Diffusion on the whole line}
\begin{mdframed}
In this section, we consider diffusion for $\Omega \triangleq \R$ and $u:\Omega \times [0,\infty]\rightarrow \R$ satisfying  
\begin{align*}
u_t=ku_{xx}\text{ on }\Omega \times (0,\infty)
\end{align*}
and initial condition 
\begin{align*}
u(x,0)=\phi (x)
\end{align*}
Let's define $\operatorname{erf}:\R\rightarrow \R$ by 
\begin{align*}
\operatorname{erf}(x)\triangleq  \frac{2}{\sqrt{\pi}}\int_0^x e^{-p^2}dp
\end{align*}
and if a function $S(x,t)$ 
\end{mdframed}
\begin{theorem}
\textbf{(Solution of the Diffusion)} A solution to 
\begin{align*}
u_t=ku_{xx}
\end{align*}
is 
\begin{align*}
u(x,t)\triangleq \frac{\operatorname{erf}(\frac{x}{\sqrt{4kt}})}{2}
\end{align*}
\end{theorem}
\chapter{PDE HW}
\section{PDE HW 1}
\begin{theorem}
\begin{align*}
\text{ Show }u\mapsto u_x+uu_y\text{ is non-linear }
\end{align*}
\end{theorem}
\begin{proof}
See that 
\begin{align}
\label{he1}
2u\mapsto 2u_x+4uu_y\neq 2(u_x+uu_y)
\end{align}
\end{proof}
\begin{theorem}
\begin{align*}
\text{ Solve }(1+x^2)u_x+u_y=0
\end{align*}
\end{theorem}
\begin{proof}
The characteristic curve has the derivative 
\begin{align*}
\frac{dy}{dx}=\frac{1}{1+x^2}
\end{align*}
The solution to this ODE is 
\begin{align*}
y=\arctan x + C
\end{align*}
We now see that the solution to the PDE in \myref{Equation}{he1} is 
\begin{align*}
u=f\big((\arctan x)-y\big)\text{ where }f:\R\rightarrow \R\text{ is an arbitrary smooth function }
\end{align*}
A characteristic curve is as followed.
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering       
\includegraphics[height=8cm,width=15cm]{pdehw1}
   \end{minipage}
\end{center}
\end{proof}
\begin{theorem}
\begin{align}
\label{he2}
\text{ Solve }au_x+b u_y +cu=0
\end{align}
\end{theorem}
\begin{proof}
Fix 
\begin{align*}
\begin{cases}
  x'\triangleq ax+by \\
  y'\triangleq bx-ay
\end{cases}
\end{align*}
This map is clearly a diffeomorphism. Compute 
\begin{align*}
  \begin{cases}
    u_x= \frac{\partial u}{\partial x'}\frac{\partial x'}{\partial x}+\frac{\partial u}{\partial y'}\frac{\partial y'}{\partial x}=au_{x'}+bu_{y'}\\
    u_y=\frac{\partial u}{\partial x'}\frac{\partial x'}{\partial y}+\frac{\partial u}{\partial y'}\frac{\partial y'}{\partial y}=bu_{x'}-au_{y'}
  \end{cases}
\end{align*}
Plugging it back into the PDE in \myref{Equation}{he2}, we have 
\begin{align}
\label{he3}
cu+ (a^2+b^2)u_{x'}=0
\end{align}
If $c=a^2+b^2=0$, then all smooth functions are solution. If $a^2+b^2=0$ but $c\neq 0$, then clearly the only solution is $u=\tilde{0} $. If $a^2+b^2\neq 0$ but $c=0$, then $u_{x'}=\tilde{0}$, which implies $u=f(y')$ where $y'=bx-ay$ and  $f$ can be arbitrary smooth function.\\

Now, suppose $a^2+b^2\neq 0\neq c$, note that the PDE in \myref{Equation}{he3} is just an ODE of the form 
\begin{align*}
y+\frac{a^2+b^2}{c}y'=0
\end{align*}
The general solution to this ODE is 
\begin{align*}
y=Ce^{\frac{-ct}{a^2+b^2}}
\end{align*}
In other words, the general solution of the PDE in \myref{Equation}{he3} is 
\begin{align*}
u=C e^{\frac{-cx'}{a^2+b^2}}=Ce^{\frac{-c(ax+by)}{a^2+b^2}}
\end{align*}


\end{proof}
\section{PDE HW 2}
\begin{question}{}{}
Consider hear flow in a long circular cylinder where the temperature depends only on $t$ and on the distance $r$ to the axis of the cylinder. Here $r=\sqrt{x^2+y^2}$ is the cylindrical coordinate. From the three dimensional hear equation derive the equation $u_t=k (u_{rr}+\frac{u_r}{r})$
\end{question}
\begin{proof}
Write the three dimensional hear equation by 
\begin{align*}
u_t= k \Delta u
\end{align*}
Note that the Laplacian $\Delta u$ when written in cylindrical coordinate is 
\begin{align*}
\Delta u= u_{rr} + \frac{u_r}{r} +\frac{u_{\theta \theta}}{r^2} + u_{zz}
\end{align*}
Because the premise says that $u$ is constant in $z$ and  $\theta$, we know $u_{\theta \theta}=u_{zz}=0$
\begin{align*}
\Delta u = u_{rr}+ \frac{u_r}{r}
\end{align*}
This give us 
\begin{align*}
u_t= k (u_{rr}+ \frac{u_r}{r})
\end{align*}
\end{proof}
\section{PDE HW 3}
\begin{question}{}{}
Find a solution of 
\begin{align*}
\begin{cases}
  u_t=u_{xx}\\
  u(x,0)=x^2
\end{cases}
\end{align*}
\end{question}
\begin{proof}
Clearly $u=x^2+2t$ suffices.
\end{proof}
\begin{question}{}{}
Consider the ODE 
\begin{align*}
\begin{cases}
  u''+u'=f \\
  u'(0)=u(0)=\frac{1}{2}(u'(l)+u(l))
\end{cases}
\end{align*}
where $f$ is given. 
\begin{enumerate}[label=(\alph*)]
  \item Is the solution unique? 
  \item Does a solution necessarily exist, or is there a condition that $f$ must satisfy for existence? 
\end{enumerate}
\end{question}
\begin{proof}
The solution space of linear homogeneous ODE $u''+u'=0$  is spanned by $e^{-x}$ and constant. If we add in the initial condition  $u'(0)=u(0)$, then the solution space become the subspace spanned by $e^{-x}-2$. One can check that if $u \in\operatorname{span}(e^{-x}-2)$, then 
\begin{align*}
u(0)=\frac{1}{2}(u'(l)+u(l))\text{ for all $l\inr$ }
\end{align*}
We now know the solution of the original ODE is not unique, since any solution added by $e^{-x}-2$ is again a solution. \\

Integrating both side on $[0,l]$, we see that given the boundary conditions, $f$ must satisfy 
\begin{align*}
\int_0^l f(x)dx&=\int_0^l u''+u'dx\\
&=u(l)+u'(l)-u(0)-u'(0)=0
\end{align*}
\end{proof}
\begin{question}{}{}
Find the regions in the $xy$ plane where the equation 
 \begin{align*}
   (1+x)u_{xx}+2xyu_{xy}-y^2u_{yy}=0
\end{align*}
is elliptic, hyperbolic, or parabolic. Sketch them. 
\end{question}
\begin{proof}
The discriminant is exactly 
\begin{align*}
  (xy)^2- (1+x)(-y^2)&=x^2y^2+xy^2+y^2\\
  &=y^2 (x^2+x+1)\\
  &=y^2[(x+\frac{1}{2})^2+ \frac{3}{4}]
\end{align*}
It then follows that the equation is parabolic if and only if $y=0$, and hyperbolic if and only if  $y\neq 0$. 

\end{proof}
\section{PDE HW 4}
\begin{question}{}{}
Solve 
\begin{align*}
\begin{cases}
  u_{tt}=c^2u_{xx} \\
  u(x,0)=e^x \\
  u_t(x,0)= \sin x 
\end{cases}
\end{align*}
\end{question}
\begin{proof}
Let 
\begin{align*}
u\triangleq f(x+ct)+g(x-ct)
\end{align*}
Plugging the initial conditions, we know 
\begin{align*}
\begin{cases}
 f(x)+g(x)=e^x \\
 f'(x)-g'(x)= \frac{\sin x}{c} 
\end{cases}
\end{align*}
This give us 
\begin{align*}
f'(x)= \frac{e^x+ \frac{\sin x}{c}}{2}\text{ and }g'(x)= \frac{e^x - \frac{\sin x}{c}}{2}
\end{align*}
Then by FTC, we have
\begin{align*}
\begin{cases}
  f(x)= \frac{e^x-\frac{\cos x}{c}}{2}+f(0)- \frac{1}{2} +\frac{1}{2c} \\
  g(x)= \frac{e^x + \frac{\cos x}{c}}{2} +g(0) - \frac{1}{2}- \frac{1}{2c}
\end{cases}
\end{align*}
Note that $f(0)+g(0)=e^0=1$, which cancel the constant terms in $u$, i.e.
\begin{align*}
u&=f(x+ct)+g(x-ct)\\
&=\frac{e^{x+ct}+ e^{x-ct} + \frac{-\cos (x+ct)+ \cos (x-ct)}{c}}{2}
\end{align*}
\end{proof}
\begin{question}{}{}
Solve 
\begin{align*}
\begin{cases}
u_{xx}+u_{xt}-20u_{tt}=0 \\
u(x,0)=\phi (x) \\
u_t(x,0)= \psi (x)
\end{cases}
\end{align*}
\end{question}
\begin{proof}
The PDE can be write in the form of 
\begin{align*}
  (\partial_x + 5\partial_t) (\partial_x - 4 \partial _t)u=0
\end{align*}
which have the general solution 
\begin{align*}
u(x,t)=f(5x-t)+g(4x+t)
\end{align*}
We now see 
\begin{align*}
f(5x)+g(4x)=\phi (x)\text{ and }-f'(5x)+g'(4x)=\psi (x)
\end{align*}
This then give us 
\begin{align*}
f'(5x)=\frac{(\phi' - 4\psi)(x)}{9}\text{ and }g'(4x)= \frac{(\phi '+5\psi )(x)}{9}
\end{align*}
and thus 
\begin{align*}
f(x)&=f(0)+ \int_0^x f'(s)ds\\
&=f(0)+\int_0^x \frac{(\phi' -4\psi)(\frac{s}{5})}{9}ds \\
&=f(0)+ \frac{5}{9}\Big[\phi (\frac{x}{5})-\phi (0) \Big] - \frac{4}{9}\int_0^x \psi (\frac{s}{5})ds
\end{align*}
and similarly 
\begin{align*}
g(x)&=g(0)+\int_0^x g'(s)ds \\
&=g(0)+ \int_0^x \frac{(\phi ' +5 \psi )(\frac{s}{4})}{9}ds \\
&=g(0)+ \frac{4}{9}\Big[ \phi (\frac{x}{4})- \phi (0) \Big]+\frac{5}{9} \int_0^x \psi (\frac{s}{4})ds
\end{align*}
Noting that $f(0)+g(0)=u(0,0)=\psi (0)$, we now have
\begin{align*}
u(x,t)&=f(5x-t)+g(4x+t) \\
&=\frac{5\phi (\frac{5x-t}{5})+4 \phi (\frac{4x+t}{4})}{9} - \frac{4}{9}\int_0^{5x-t}\psi (\frac{s}{5})ds + \frac{5}{9}\int_0^{4x+t}\psi (\frac{s}{4})ds
\end{align*}
\end{proof}
\chapter{Differential Geometry HW} 
\section{HW1} 
\begin{abstract}
In this HW, we give precise definition to $\P^n$  and $\RP^n$, and we rigorously show
\begin{enumerate}[label=(\alph*)]
  \item $\RP^n$  has a smooth structure. 
  \item $\P^n$ is homeomorphic to $\RP^n$ 
   \item $\P^n$ has a smooth structure.
\end{enumerate}
Note that in this PDF, brown text is always a clickable hyperlink reference. 
\end{abstract}
\begin{mdframed}
Define an equivalence relation on $\R^{n+1}\setminus \set{\textbf{0}}$ by 
\begin{align*}
\textbf{x}\sim \textbf{y}\overset{\triangle}{\iff }\textbf{y}=\ld \textbf{x}\text{ for some $\ld \inr^*$ } 
\end{align*}
Let $\RP ^n\triangleq (\R^{n+1}\setminus \set{\textbf{0}})\setminus \sim $ be the quotient space and let  
\begin{align*}
V_i\triangleq \set{\textbf{x}\inr^{n+1}:\textbf{x}^i \neq 0}\text{ for each }1\leq i\leq n+1
\end{align*}
By definition, it is clear that 
\begin{align*}
\text{ either }\pi^{-1}(\pi (\textbf{x}))\subseteq V_i\text{ or }\pi^{-1}(\pi (\textbf{x}))\subseteq V_i^c
\end{align*}
Then if we define $\phi_i:V_i\rightarrow \R^n$ by
\begin{align*}
\phi_i(\textbf{x})\triangleq \Big(\frac{\textbf{x}^1}{\textbf{x}^i},\dots ,\frac{\textbf{x}^{i-1}}{\textbf{x}^i},\frac{\textbf{x}^{i+1}}{\textbf{x}^i},\dots,\frac{\textbf{x}^{n+1}}{\textbf{x}^i} \Big)
\end{align*}
because $\pi (\textbf{x})=\pi(\textbf{y})\implies \phi_i(\textbf{x})=\phi_i(\textbf{y})$, we can well induce a map 
\begin{align*}
\Phi_i:U_i\triangleq \pi (V_i)\subseteq \R P^n\rightarrow \R^n; \pi (\textbf{x})\mapsto \phi_i(\textbf{x})
\end{align*}
Note that one has the equation 
\begin{align*}
\Phi_i(\pi (\textbf{x}))= \phi_i(\textbf{x})\text{ for all }\textbf{x}\in V_i
\end{align*}
\end{mdframed}
\begin{theorem}
\textbf{(Real Projective Space with a differentiable atlas)} We have
\begin{align*}
  \R P^n\text{ with atlas }\set{(U_i,\Phi_i):1\leq i\leq n+1}\text{ is a differentiable manifold }
\end{align*}
\end{theorem}
\begin{proof}
We are required to prove 
\begin{enumerate}[label=(\alph*)] 
  \item $(U_i,\Phi_i)$ are all charts. 
  \item $\set{(U_i,\Phi_i):1\leq i\leq n+1}$ form a differentiable atlas. 
\item $\R P^n$ is Hausdorff.
\item $\R P^n$ is second-countable. 
\end{enumerate}
 Because $\pi^{-1}(U_i)=V_i$ and $V_i$ is clearly open in $\R^{n+1}\setminus \set{\textbf{0}}$, we know $U_i\subseteq \R P^n$ is open. Note that clearly,   $\Phi_i(U_i)=\R^n$. To show $(U_i,\Phi_i)$ is a chart, it remains to show that $\Phi_i$ is a homeomorphism between $U_i$ and  $\R^n$.  It is straightforward to check $\Phi_i$ is one-to-one on $U_i$. This implies $\Phi_i$ is a bijective between $U_i$ and  $\R^n$. \\


Fix open $E\subseteq \R^n$. We see 
\begin{align*}
  \pi^{-1}(\Phi_i^{-1}(E))=\phi_i^{-1}(E)
\end{align*}
Then because $\phi_i:V_i\rightarrow \R^n$ is clearly continuous, we see $\phi_i^{-1}(E)$ is open in $\R^{n+1}\setminus \set{\textbf{0}}$, and it follows from definition of quotient topology  $\Phi_i^{-1}(E)\subseteq \R P^n$ is open. Then because $U_i$ is open in  $\R P^n$, we see $\Phi_i^{-1}(E)$ is open in $U_i$. We have proved  $\Phi_i:U_i\rightarrow \R^n$ is continuous.\\


Define $\Psi_i: \R^n\rightarrow V_i$ by 
\begin{align*}
\Psi (\textbf{x}^1,\dots ,\textbf{x}^n)= (\textbf{x}^1,\dots ,\textbf{x}^{i-1},1,\textbf{x}^i,\dots ,\textbf{x}^n)
\end{align*}



Observe that for all $\textbf{x}\in \Phi_i(U_i)$, we have
\begin{align*}
\Phi_i^{-1}(\textbf{x})= \pi(\Psi_i (\textbf{x}))
\end{align*}
It then follows from $\Psi_i:\R^n\rightarrow V_i$ and $\pi:\R^{n+1}\setminus \set{\textbf{0}}\rightarrow \R P^n$ are continuous that $\Phi_i^{-1}:\R^n\rightarrow \R P^n$ is continuous.\\

We have proved that $(\Psi_i,U_i)$ are all charts. Now, because $V_i$ clearly cover $\R^{n+1}$, we know $U_i$ also cover  $\R P^n$. We have proved $\set{(U_i,\Phi):1\leq i\leq n+1}$ form an atlas. The fact $\R P^n$ is second-countable follows. \\

Fix $(\textbf{x}_1,\dots ,\textbf{x}_{n})\in \Phi_i(U_i\cap U_j)$. We compute 
\begin{align*}
\Phi_j\circ \Phi_i^{-1} (\textbf{x}^1,\dots ,\textbf{x}^{n} )&=\Phi_j \Big( [(\textbf{x}^1,\dots ,\textbf{x}^{i-1},1,\textbf{x}^i,\textbf{x}^{i+1},\dots, \textbf{x}^n)] \Big) \\
&= \begin{cases}
  (\frac{\textbf{x}^1}{\textbf{x}^j},\dots , \frac{\textbf{x}^{j-1}}{\textbf{x}^j},\frac{\textbf{x}^{j+1}}{\textbf{x}^j},\dots ,\frac{\textbf{x}^{i-1}}{\textbf{x}^j},\frac{1}{\textbf{x}^j},\frac{\textbf{x}^i}{\textbf{x}^j},\dots ,\frac{\textbf{x}^n}{\textbf{x}^j})& \text{ if $j<i$ }\\
  (\frac{\textbf{x}^1}{\textbf{x}^{j-1}},\dots , \frac{\textbf{x}^{i-1}}{\textbf{x}^{j-1}},\frac{1}{\textbf{x}^{j-1}},\frac{\textbf{x}^i}{\textbf{x}^{j-1}},\dots , \frac{\textbf{x}^{j-2}}{\textbf{x}^{j-1}}, \frac{\textbf{x}^j}{\textbf{x}^{j-1}}, \dots,\frac{\textbf{x}^n}{\textbf{x}^{j-1}})& \text{ if $j>i$ }
\end{cases}
\end{align*}
This implies our atlas is indeed differentiable. \\

Before we prove $\RP^n$ is Hausdorff, we first prove that  \olive{$\pi:\R^{n+1}\setminus \set{\textbf{0}}\rightarrow \RP^n$ is an open mapping}. Let $U\subseteq \R^{n+1}\setminus \set{\textbf{0}}$ be open. Observe that 
\begin{align*}
\pi^{-1}(\pi(U))= \set{t\textbf{x}\inr^{n+1}:t\neq 0\text{ and }\textbf{x}\in U}
\end{align*}
Fix $t_0\textbf{x}\in \pi^{-1}(\pi (U))$. Let $B_\epsilon (\textbf{x})\subseteq U$. Observe that
\begin{align*}
B_{\abso{t_0}\epsilon }(t_0\textbf{x})\subseteq t_0B_\epsilon (\textbf{x})\subseteq t_0U\subseteq \pi^{-1}(\pi (U))
\end{align*}
This implies $\pi ^{-1}(\pi (U))$ is open.  $\odone$ \\

Now, \customref{Hausdorff and Quotient}{because $\pi$ is open, to show $\RP^n$ is Hausdorff, we only have to show}
 \begin{align*}
R_\pi \triangleq \set{(\textbf{x},\textbf{y})\in (\R^{n+1}\setminus \set{\textbf{0}})^2:\pi (\textbf{x})= \pi (\textbf{y})}\text{ is closed }
\end{align*}
Define $f:(\R^{n+1}\setminus \set{\textbf{0}})^2\rightarrow \R$ by 
\begin{align*}
f(\textbf{x},\textbf{y})\triangleq \sum_{i\neq j} (\textbf{x}^i\textbf{y}^j-\textbf{x}^j \textbf{y}^i)^2
\end{align*}
Note that $f$ is clearly continuous and $f^{-1}(0)=R_\pi$, which finish the proof.
\end{proof}
\begin{mdframed}
Alternatively, we can characterize $\R P^n$ by identifying the antipodal pints on $S^n\triangleq \set{\textbf{x}\inr^{n+1}:\abso{\textbf{x}}=1}$ as one point 
\begin{align*}
\textbf{x}\sim \textbf{y}\overset{\triangle}{\iff } \textbf{x}=\textbf{y}\text{ or }\textbf{x}=-\textbf{y}
\end{align*}
and let $\P^n\triangleq S^n\setminus \sim $ be the quotient space.
\end{mdframed}
\begin{theorem}
\label{RPnhom}
\textbf{(Equivalent Definitions of Real Projective Space)} 
\begin{align*}
\RP^n\text{ and }\P^n\text{ are homeomorphic }
\end{align*}
\end{theorem}
\begin{proof}
Define $F:\P^n\rightarrow \RP^n$ by 
\begin{align*}
\set{\textbf{x},-\textbf{x}}\mapsto \set{\ld  \textbf{x}:\ld \inr^*}
\end{align*}
It is straightforward to check that $F$ is well-defined and bijective. Define $f:S^n \rightarrow \R P^n$ by 
\begin{align*}
f= \pi \circ \textbf{id}
\end{align*}
where $\textbf{id}:S^n \rightarrow \R^{n+1}\setminus \set{\textbf{0}}$ and $\pi:\R^{n+1}\setminus \set{\textbf{0}}\rightarrow \R P^n$ are continuous. Check that  
\begin{align*}
f=F\circ p
\end{align*}
where $p:S^n\rightarrow \P^n$ is the quotient mapping. It now follows from the \customref{quotient property}{universal property} that $F$ is continuous, and since $\P^n$ is compact and  $\R P^n$ is Hausdorff,  it also follows that  \customref{HbC}{$F$ is a homeomorphism between $\R P^n$ and  $\P^n$}. 
\end{proof}
\begin{mdframed}
Knowing that $F:\P^n\rightarrow \RP^n$ is a homeomorphism and $\RP^n$ is a smooth manifold, we see that  $\P^n$ is Hausdorff and second-countable, and if we define the atlas 
\begin{align*}
  \set{(F^{-1}(U_i), \Phi_i \circ F):1\leq i\leq n+1}
\end{align*}
We see this atlas is indeed smooth, since 
\begin{align*}
  (\Phi_i\circ F)\circ (\Phi_j \circ F)^{-1}=\Phi_i \circ \Phi_j^{-1}
\end{align*}
\end{mdframed}

\section{Appendix}
\begin{theorem}
\label{HbC}
\textbf{(Homeomorphism between Compact Space and Hausdorff Space)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $X$ is compact.  
  \item $Y$ is Hausdorff.  
  \item $f:X\rightarrow Y$ is a continuous bijective function. 
\end{enumerate}
Then 
\begin{align*}
f\text{ is a homeomorphism between }X\text{ and }Y
\end{align*}
\end{theorem}
\begin{proof}
Because closed subset of compact set is compact and continuous function send compact set to compact set, we see for each closed $E\subseteq X$, $f(E)\subseteq Y$ is compact. The result then follows from \customref{Compact Subspace of Hausdorff Space is Closed}{$f(E)\subseteq Y$ being closed since $Y$ is Hausdorff}. 
\end{proof}
\begin{theorem}
\label{Hausdorff and Quotient}
  \textbf{(Hausdorff and Quotient)} If $\pi:X\rightarrow Y$ is an open mapping, and we define
\begin{align*}
R_\pi\triangleq \set{(x,y)\in X^2 : \pi (x)=\pi (y)} 
\end{align*}
Then
\begin{align*}
R_\pi\text{ is closed }\iff Y\text{ is Hausdorff }
\end{align*}
\end{theorem}
\begin{proof}
  Suppose $R_\pi$ is closed. Fix some $x,y$ such that $\pi (x)\neq \pi (y)$. Because $R_\pi$ is closed, we know there exists open neighborhood $U_x,U_y$ such that $U_x \times U_y \subseteq (R_\pi)^c$. It is clear that $\pi (U_x),\pi (U_y)$ are respectively open neighborhood of $\pi (x)$ and $\pi (y)$. To see $\pi (U_x)$ and $\pi (U_y)$ are disjoint, \red{assume} that $\pi (a)\in \pi (U_x)\cap \pi (U_y)$. Let $a_x\in U_x$ and $a_y\in U_y$ satisfy $\pi (a_x)=\pi (a)=\pi (a_y)$, which is impossible because $(a_x,a_y)\in (R_\pi)^c$. \CaC\\


Suppose $Y$ is Hausdorff. Fix some  $x,y$ such that  $\pi (x)\neq \pi (y)$. Let $U_x,U_y$ be open neighborhoods of  $\pi (x),\pi (y)$ separating them. Observe that $(x,y)\in \pi^{-1}(U_x)\times \pi^{-1}(U_y)\subseteq (R_\pi)^c$
\end{proof}

\section{Example: $S^1,\R\setminus \Z$ diffeomorphism } 
\begin{mdframed}
Equip $S^1\triangleq \set{(x,y)\inr^2: \abso{(x,y)}=1}$ with the standard four projection chart smooth atlas  as one of them being
\begin{align*}
V\triangleq  \set{(x,y)\inr^2: y>0}\text{ and }\phi_V: V\rightarrow \R; (x,y)\mapsto  x
\end{align*}
Let $p:\R\rightarrow \R\setminus \Z$  be the quotient map and let 
\begin{align*}
U_0\triangleq p\Big((0,1)\Big)\text{ and }U_1\triangleq p\Big((\frac{-1}{2},\frac{1}{2})\Big)
\end{align*}
which are both open as one can readily check. Define $\phi_0:U_0\rightarrow (0,1)$ by 
\begin{align*}
\phi_0 (p(t))\triangleq t_0\text{ where $t_0\in (0,1)$ and $p(t_0)=p(t)$ }
\end{align*}
and $\phi_1:U_1\rightarrow (-\frac{1}{2},\frac{1}{2})$ by
\begin{align*}
\phi_1(p(t))\triangleq t_0\text{ where }t_0 \in (-\frac{1}{2},\frac{1}{2})\text{ and }p(t_0)=p(t)
\end{align*}
Clearly, the function $G:\R\setminus \Z\rightarrow S^1$ well-defined by $G(p(x))\triangleq (\cos 2\pi x, \sin 2\pi x)$ is a homeomorphism, as one can check that 
\begin{enumerate}[label=(\alph*)]
  \item $G$ is a continuous bijection. (Using Universal property of quotient map)
  \item $\R\setminus \Z$ is compact. (by finite sub-cover definition)
  \item $S^1$ is Hausdorff.
\end{enumerate}
We now compute that $\phi_V\circ G\circ \phi_0^{-1}$ is defined on whole $(0,1)$, and is exactly 
\begin{align*}
\phi_V\circ G\circ \phi_0^{-1}(t)\triangleq \cos 2\pi t\text{ smooth }
\end{align*}
\end{mdframed}
\end{document}
