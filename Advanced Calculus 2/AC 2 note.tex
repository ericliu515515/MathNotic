\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}

\title{\Huge{NCKU 112.2}\\
Advanced Calculus 2}
\author{\huge{Eric Liu}}
\date{}
\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{General Topology}
\section{Equivalent axiomazations}
\begin{mdframed}
In the first lecture of Topology, we learned that a topological space $(X,\tau)$ is a space $X$ with a topology  $\tau \subseteq \power{X}$ such that $\tau$ satisfy the following three axioms.
\begin{align}
\label{1.1}
\begin{cases}
X,\varnothing \in \tau\\
\forall O,Y \in \tau, O \cap Y \in \tau\text{ (Closed under finite intersection) }\\
\forall T \subseteq \power{\tau}, \bigcup T \in \tau\text{ (Closed under arbitrary union) }
\end{cases}
\end{align}
It is only after the explicit listing of the above three of open sets, we then start defining "closed sets", "neighborhoods", "continuous functions", "compact sets" or "connected sets" based on open sets.\\

Although this approach, axiomatization via open sets, is mathematically sufficient, in history, there are other axiomatization proved to be equivalent to the traditional axiomaization via open sets.\\

In this section, we will give other three axiomatizations of topology, via neighborhood systems, via nets and via filters, and show they are equivalent with each other.\\

In this note, a \textit{neighborhood system} is a function $\mathcal{N}:X\rightarrow \power{\power{X}}$. We shall first show that there exists an explicit one-to-one correspondence between collection of topologies on $X$ and collection of neighborhood systems that satisfy the following axioms.  
\end{mdframed}
\begin{axiom}
\label{1.1.1}
\textbf{(Axioms of neighborhood systems)} 
\begin{align*}
\begin{cases}
\forall x\in X, \mathcal{N}(x)\neq \varnothing \text{ (Non empty) }\\
\forall x\in X, \forall M \in \mathcal{N}(x), x \in M\text{ (Around) }\\
\forall x \in X, \forall M\subseteq X, \exists N \in \mathcal{N}(x), N\subseteq M \implies M \in \mathcal{N}(x)\text{(Closed under superset)}\\
\forall x \in X, \forall M,N \in \mathcal{N}(x), M \cap  N \in \mathcal{N}(x)\text{(Closed under finite intersection)}\\
\forall x \in X, \forall M\in \mathcal{N}(x), \exists N \in \mathcal{N}(x), N \subseteq M \text{ and } \forall y \in N, M \in \mathcal{N}(y)\text{(Open neighborhood)}
\end{cases}
\end{align*}
\end{axiom}
\begin{mdframed}
Suppose $C$ is the collection of topologies on $X$ and $D$ is the collection of neighborhood systems that satisfy \myref{Axioms}{1.1.1}. Now we wish to prove that the function $f:C\rightarrow D$
\begin{align}
\label{1.2}
\tau \mapsto  \mathcal{N}_\tau\text{ where }\mathcal{N}_\tau (x)= \set{A \in \power{X}: \exists O \in \tau, x \in O \subseteq A}
\end{align}
is bijective. In order to prove this, we first have to prove that $f$ is well-defined, meaning for each topology $\tau$, the function $\mathcal{N}_\tau$ is indeed a neighborhood system that satisfy the above axioms. This is easy, which we will omit here. It remains that we have to prove two statements: $f$ is one-to-one, and  $f$ is onto.
\end{mdframed}
\begin{theorem}
\label{1.1.2}
\textbf{($f$ in \myref{Equation}{1.2} is one-to-one)} As titled. 
\end{theorem}
\begin{proof}
Suppose $\tau$ and $\tau'$ are two different topologies on $X$. WOLG, suppose $O \in \tau \setminus \tau'$. By definition of $f$ (\myref{Equation}{1.2}), we know 
\begin{align*}
\forall x \in O, O \in \mathcal{N}_\tau (x)
\end{align*}
This means, to prove $\mathcal{N}_\tau \neq \mathcal{N}_{\tau'}$, we only have to prove the existence of some $x \in O$ such that $O \not\in \mathcal{N}_\tau (x)$. \As{$\forall x \in O, O \in \mathcal{N}_{\tau'}(x)$}. By definition of $f$ (\myref{Equation}{1.2}), we can deduce $\forall x \in O, \exists A_x \in \tau', x \in A_x \subseteq O$. It is easy to verify that $O= \bigcup_{x\in O} A_x$. Then by \myref{Axiom}{1.1} of open sets , we $O=\bigcup_{x \in O}A_x \in \tau'\tCaC$ to that $O \in \tau \setminus \tau'$. 
\end{proof}
\begin{mdframed}
To prove $f$ is onto is a little bit complicated. 
\end{mdframed}

\begin{theorem}
\label{1.1.3}
\textbf{($f$ in \myref{Equation}{1.2} is onto)} As titled. 
\end{theorem}
\begin{proof}
Define $g:D\rightarrow C$ by 
\begin{align}
\label{1.3}
\mathcal{N} \mapsto \sigma_\mathcal{N} \text{ where }\sigma_{\mathcal{N}}=\set{O \in \power{X}: \forall  x\in O, O \in \mathcal{N}(x)} 
\end{align}
It is easy to check for each neighborhood system $\mathcal{N}:X \rightarrow \power{\power{X}}$ satisfying \myref{Axioms}{1.1.1}, the collection $\sigma_{\mathcal{N}}$ is a topology.\\

Then, to prove $f$ is onto, it suffice to prove 
\begin{align*}
\forall \mathcal{N} \in D, \mathcal{N}=f(\sigma_\mathcal{N})
\end{align*}
In other words, we wish to prove 
\begin{align*}
  \vi{\forall \mathcal{N} \in D, \forall x \in X, \mathcal{N}(x)=\big(f(\sigma_\mathcal{N}) \big)(x)}
\end{align*}
By definition of $f$ (\myref{Equation}{1.2}), we know 
\begin{align}
\label{1.4}
\big(f(\sigma_\mathcal{N}) \big)(x)=\set{A \in \power{X}:\exists O \in \sigma_\mathcal{N}, x \in O \subseteq A}
\end{align}
Suppose $A \in \big(f(\sigma_\mathcal{N}) \big)(x)$. Let $O \in \sigma_\mathcal{N}$ satisfy $x \in O \subseteq A$. By definition of $g$ (\myref{Equation}{1.3}), we know $O \in \mathcal{N}(x)$. Then because \myref{Axiom}{1.1.1} says that $\mathcal{N}(x)$ is closed under superset, we now see $A \in \mathcal{N}(x)$. We have proved $\big(f(\sigma_\mathcal{N}) \big)(x)\subseteq \mathcal{N}(x)$.\\

Suppose $M \in \mathcal{N}(x)$. By the fifth axiom listed in \myref{Axiom}{1.1.1}, we know there exists some $N \in \mathcal{N}(x)$ such that $N \subseteq M$ and $\forall y \in N,M \in \mathcal{N}(y)$. Notice that the union of such $N$ is still a subset of  $M$ and  $M$ is a neighborhood of each point in the union. This means that there exists a maximum $N$ that satisfy 
\begin{align*}
\begin{cases}
  N \subseteq M\\
  \forall y \in N, M \in \mathcal{N}(y)
\end{cases}
\end{align*}
We now prove  \blue{$N \in \sigma_\mathcal{N}$}. Arbitrarily pick $z \in N$, by definition of $N$ above, we can deduce $M \in \mathcal{N}(z)$. Then by the fifth axiom listed in \myref{Axiom}{1.1.1}, we know there exists some $N' \in \mathcal{N}(z)$ satisfying $N' \subseteq M$ and $\forall y \in N' ,M \in \mathcal{N}(y)$. Because $N$ by definition is the maximum of such neighborhood, we see $N'\subseteq N$. Then because \myref{Axiom}{1.1.1} says that $\mathcal{N}(z)$ is closed under superset, we see $N \in \mathcal{N}(z)$.\\

We have shown $\forall z \in N, N \in \mathcal{N}(z)$. Then by definition of $g$  (\myref{Equation}{1.3}), we see $N \in \sigma_\mathcal{N}\bdone$.\\

Then because $N \subseteq M$ by definition of $N$ and because $N \in \sigma_{\mathcal{N}}$, we see  $M \in \big(f(\sigma_\mathcal{N}) \big)(x)$, according to \myref{Equation}{1.4}. We have proved $\mathcal{N}(x)\subseteq \big(f(\sigma_\mathcal{N}) \big)(x)$$\vdone$
\end{proof}
\begin{mdframed}
Before embarking on the axiomatization via nets, we first have to settle the terminologies. Recall that a set $D$ is \textbf{directed} if there exists an ordering $\leq $ on $D$ such that  $\leq $ is reflexive, transitive, and we have $\forall i,j \in D, \exists k \in D, i\leq k\text{ and }j\leq k$. By a \textbf{net}, we mean a function $w$ whose domain is directed. We say a subset $D'$ of a directed set is  \textbf{cofinal} if $\forall d \in D, \exists d' \in D', d \leq d'$. By a \textbf{subnet} of  $w:D\rightarrow X$, we mean a net  $v:E \rightarrow X$ such that there exists $h:E\rightarrow D$ such that
\begin{align*}
\begin{cases}
\forall e,e'\in E , e\leq e' \implies h(e)\leq h(e')\text{(Monotone)}\\
h[E]\text{ is cofinal }\\
v=w \circ  h
\end{cases}
\end{align*}
One can check that when $w$ is a sequence  $x_n$ and  $v$ is the sub-sequence  $x_{n_k}$, the corresponding $h$ is just $n_k$.\\

By a \textbf{tail} $T_d$ of a directed set $D$, we mean  $T_d=\set{e \in D: d\leq e}$. We say \textbf{$w:D \rightarrow X$ is eventually in $A\subseteq X$} if $\exists d \in D, w[T_d] \subseteq A$. We say $w:D\rightarrow X$\textbf{ is frequently in $A\subseteq X$} if $\forall d\in D,\exists e \in T_d, w(e) \in A$. Given a topological space $(X,\tau)$, we say $w$  \textbf{converge} to a point $x$, if for all neighborhood $O$ around $x$ there exists $d \in D$ such that $w[T_d]\subseteq O$. Notice that if we wish to prove $w \to x$ we only have to verify for all open neighborhoods $O$ around $x$. Also notice that  $w$ can converge to multiple points. A trivial example is when two point are topologically indistinguishable.
\end{mdframed}
\section{Equivalent definitions}
\section{Product topology}
\section{ABOVE ARE FIXED AND SHOULD BE FOCUSED BEFORE ARRANGEMENT OF THE FOLLOWING}
\section{Manifold}
\section{Quotient topology}
\section{Countability axioms}
\section{Separation axioms}
\section{Path connected}
\section{Basic notion on compact}
\section{Baire space}
\section{Homotopy}
\section{Simply connected}
\section{Fundamental group}
\chapter{Metric Space and some Linear Algebra}
\section{Pseudo Metric}
\section{Completion}
\section{Bounded and Totally Bounded}
\section{Compactness}
\section{Holder Continuity}
\section{Limit Interchange}
\begin{mdframed}
Given an arbitrary set $X$ and  a complete metric space  $(\overline{Y},d)$, in \myref{Section}{27CaB}, we have proved that the set of functions with the following properties 
\begin{enumerate}[label=(\alph*)]
  \item boundedness 
  \item unboundedness
\end{enumerate}
are respectively closed under uniform convergence. In next section (\myref{Section}{UCaCH}), we will prove that the following three properties 
\begin{enumerate}[label=(\alph*)]
  \item continuity
  \item uniform continuity
  \item $K$-Lipschitz continuity
\end{enumerate}
are again all closed under uniform convergence, where the proof for continuity is closed under uniform convergence use \myref{Theorem}{COLO} as a lemma.\\

Here, we prove 
\begin{enumerate}[label=(\alph*)]
  \item convergent of sequences 
\end{enumerate}
in, of course, complete metric space, is also closed under uniform convergence.\\


The reason we require the codomain $\overline{Y}$ of sequence to be complete is explained in the last paragraph of \myref{Section}{27CaB}. An example of such beautiful closure is lost if the codmain $(Y,d)$ is not complete is $Y=\R^*$ and  $a_{n,k}=\frac{1}{n}+\frac{1}{k}$. 
\end{mdframed}
\begin{theorem}
\label{COLO}
\textbf{(Change Order of Limit Operations: Part 1)} Given a double sequence $a_{n,k}$ whose codomain is $(Y,d)$. Suppose
\begin{enumerate}[label=(\alph*)]
  \item $a_{n,k}\to a_{\bullet,k}$ uniformly as $n\to \infty$ 
  \item $a_{n,k}\to A_n$ pointwise as  $k\to \infty$.
  \item $A_n \to A$ 
\end{enumerate}
Then we can deduce 
\begin{align*}
\lim_{k\to \infty}a_{\bullet,k}\text{ exists and }\lim_{k\to \infty}a_{\bullet,k}=A
\end{align*}
In other words, we can switch the order of limit operations
\begin{align*}
\lim_{k\to \infty}\lim_{n\to \infty}a_{n,k}=\lim_{n\to \infty}\lim_{k\to \infty}a_{n,k}
\end{align*}
\end{theorem}
\begin{proof}
We wish to prove 
\begin{align*}
\vi{a_{\bullet,k}\to A\text{ as }k\to \infty}
\end{align*}
Fix $\epsilon $. Because $a_{n,k}\to a_{\bullet,k}$ uniformly and $A_n\to A$ as $n\to \infty$, we know there exists $m$ such that 
 \begin{align}
  \label{K1}
d(A_m,A)<\frac{\epsilon}{3}\text{ and }\forall k\inn,d(a_{m,k},a_{\bullet,k})<\frac{\epsilon}{3}
\end{align}
Then because $a_{m,k}\to A_m$ as $k\to \infty$, we know there exists $K$ such that
\begin{align}
\label{K2}
\forall k>K, d(a_{m,k},A_m)<\frac{\epsilon}{3}
\end{align}
We now claim 
\begin{align*}
  \vi{\forall k>K, d(a_{\bullet,k},A)<\epsilon}
\end{align*}
The claim is true since by \myref{Equation}{K1} and \myref{Equation}{K2}, we have 
\begin{align*}
  \forall k>K, d(a_{\bullet,k},A)\leq d(a_{\bullet,k},a_{m,k})+d(a_{m,k},A_m)+d(A_m,A)<\epsilon \vdone
\end{align*}
\end{proof}
\begin{theorem}
\label{COLO2}
\textbf{(Change Order of Limit Operations: Part 2)} Given a double sequence $a_{n,k}$ whose codomain is $(Y,d)$. Suppose 

\begin{enumerate}[label=(\alph*)]
  \item $a_{n,k}\to a_{\bullet,k}$ uniformly as $n\to \infty$ 
  \item $a_{n,k}\to A_n$ pointwise as $k\to \infty$ 
  \item $a_{\bullet,k}\to A$ as $k\to \infty$
\end{enumerate}
Then we can deduce
\begin{align*}
A_n\text{ converge and  }A_n \to A
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish to find $N$ such that 
 \begin{align*}
   \vi{\forall n>N, d(A_n,A)<\epsilon}
\end{align*}
Because  $a_{n,k}\to a_{\bullet,k}$ uniformly as $n \to \infty$, we can let $N$ satisfy 
\begin{align}
\label{K3}
\forall n>N,\forall k\inn, d(a_{n,k},a_{\bullet,k})<\frac{\epsilon}{3}
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Arbitrarily pick $n>N$. Because $a_{\bullet,k} \to A$, and because $a_{n,k} \to A_n$, we know there exists $j$ such that 
 \begin{align}
  \label{K4}
d(a_{\bullet,j},A)<\frac{\epsilon}{3}\text{ and }d(a_{n,j},A_n)<\frac{\epsilon}{3}
\end{align}
From \myref{Equation}{K3} and \myref{Equation}{K4}, we now have
\begin{align*}
d(A_n,A)\leq d(A_n,a_{n,j})+d(a_{n,j},a_{\bullet,j})+d(a_{\bullet,j},A)<\epsilon \vdone
\end{align*}
\end{proof}
\begin{corollary}
\label{COiC}
\textbf{(Change Order of Limit Operation in Complete Metric Space)} Given a sequence of function $f_n:E\rightarrow (Y,d)$ and a function $f:E\rightarrow (Y,d)$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f_n$ converge uniformly
   \item $\lim_{t\to x}f_n(t)$ exists for all $n \inn$
    \item $(Y,d)$ is complete
\end{enumerate}
We have
\begin{align*}
\lim_{n\to \infty }\lim_{t\to x}f_n(t)=\lim_{t\to x}\lim_{n\to \infty}f_n(t)
\end{align*}
\end{corollary}
\begin{proof}
Fix a sequence $t_k$ in $E$ that converge to  $x$. We reduced the problem into proving
\begin{align*}
  \vi{\lim_{n\to \infty}\lim_{k \to \infty}f_n(t_k)=\lim_{k\to \infty}\lim_{n\to \infty}f_n(t_k)}
\end{align*}
Set 
\begin{align}
a_{n,k}\triangleq f_n(t_k)
\end{align}
We then reduced the problem into proving 
\begin{align*}
  \vi{\lim_{n\to \infty}\lim_{k\to \infty}a_{n,k}=\lim_{k\to \infty}\lim_{n\to \infty}a_{n,k}}
\end{align*}
Set 
\begin{align*}
A_n\triangleq \lim_{t \to x}f_n(t)\text{ and }a_{\bullet,k}\triangleq \lim_{n \to \infty}f_n(t_k)
\end{align*}
We now prove 
\begin{align*}
\blue{\text{ $A_n$ converge }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\blue{\text{ to find $N$ such that }d(A_n,A_m)\leq \epsilon \text{ for all $n,m>N$ }}
\end{align*}
Because $a_{n,k}$ uniformly converge (to $a_{\bullet , k}$) as $n \to \infty$ by our setting, we know there exists $N$ such that
\begin{align*}
\forall n,m>N,\forall k\inn, d(a_{n,k},a_{m,k})<\frac{\epsilon}{3}
\end{align*}
We claim 
\begin{align*}
\blue{\text{ such $N$ works }}
\end{align*}
Fix $n,m>N$. Because $a_{n,k}\to A_n$ and $a_{m,k}\to A_m$, we know there exists $j \inn$ such that 
\begin{align*}
d(a_{n,j},A_n)<\frac{\epsilon}{3}\text{ and }d(a_{m,j},A_m)<\frac{\epsilon}{3}
\end{align*}
We now have
\begin{align*}
  d(A_n,A_m)&\leq  d(A_n,a_{n,j})+d(a_{n,j},a_{m,j})+d(a_{m,j},A_m)\\
&< \frac{\epsilon}{3}+\frac{\epsilon}{3}+\frac{\epsilon}{3}=\epsilon \bdone
\end{align*}
Now, because $a_{n,k}\to a_{\bullet ,k}$ uniformly, by \myref{Theorem}{COLO2}, we have 
\begin{align*}
\lim_{n\to \infty}\lim_{k\to \infty}a_{n,k}=\lim_{n\to \infty}A_n=\lim_{k\to \infty}a_{\bullet ,k}=\lim_{k\to \infty}\lim_{n\to \infty}a_{n,k}\vdone
\end{align*}
\end{proof}
\begin{mdframed}
In summary of \myref{Theorem}{COLO} and \myref{Theorem}{COLO2}, given a double sequence $a_{n,k}$ converging both side 
\begin{enumerate}[label=(\alph*)]
  \item $a_{n,k}\to a_{\bullet,k}$ pointwise as $n \to \infty$
  \item $a_{n,k}\to a_{n,\bullet}$ pointwise as $k\to \infty$
\end{enumerate}
As long as 
\begin{enumerate}[label=(\alph*)]
  \item one side of convergence is uniform 
  \item between two sequence $\set{a_{\bullet,k}}_{k\inn}$ and $\set{a_{n,\bullet}}_{n\inn}$, one of them converge, say, to $A$
\end{enumerate}
Then the other sequence also converge, and the limit is also $A$.\\

It is at this point, we shall introduce two other terminologies. Suppose $f_n$ is a sequence of functions from an arbitrary set  $X$ to a metric space  $Y$. We say $f_n$ is  \textbf{pointwise Cauchy} if for all fixed $x \in X$, the sequence $f_n(x)$ is Cauchy. We say $f_n$ is  \textbf{uniformly Cauchy} if for all $\epsilon $, there exists $N\inn$ such that 
\begin{align*}
\forall n,m>N, \forall x\in X, d\big(f_n(x),f_m(x) \big)<\epsilon 
\end{align*}
In last Section (\myref{Section}{27CaB}), we define the \textbf{uniform metric} $d_{\infty}$ on $X^Y$ by 
 \begin{align*}
d_{\infty}(f,g)=\sup_{x\in X}d\big(f(x),g(x) \big)
\end{align*}
and say that $f_n\to f$ uniformly if and only if $f_n\to f$ in $(X^Y,d_{\infty})$. Similar to this clear fact, we have 
\begin{align*}
f_n\text{ is uniformly Cauchy }\iff \text{ $f_n$ is Cauchy in  $(X^Y, d_\infty)$ }
\end{align*}
It should be very easy to verify that if $f_n$ uniformly converge, then $f_n$ is uniformly Cauchy, and just like sequences in metric space, the converse hold true if and only if the space $\big(X^Y,d_\infty \big)$ is complete. In \myref{Theorem}{Tsof}, we give a necessary and sufficient condition for $\big(X^Y,d_{\infty}\big)$ to be complete. 
\end{mdframed}
\begin{theorem}
\label{Tsof}
\textbf{(Space of functions $\big(X^Y,d_{\infty}\big)$ is Complete iff $Y$ is Complete)} Given an arbitrary set $X$ and a metric space  $(Y,d)$, we have 
\begin{align*}
\text{ the extended metric space $\big(X^Y,d_{\infty} \big)$ is complete }\iff Y\text{ is complete }
\end{align*}
\end{theorem}
\begin{proof}
$(\longleftarrow)$\\

Suppose $f_n$ is uniformly Cauchy. We wish 
\begin{align*}
  \vi{\text{ to construct a $f:X\rightarrow Y$ such that }f_n\to f\text{ uniformly }}
\end{align*}
Because $f_n$ is uniformly Cauchy, we know that for all $x \in X$, the sequence $f_n(x)$ is Cauchy in $(Y,d)$. Then because $Y$ is complete, we can define $f:X\rightarrow Y$ by 
 \begin{align*}
f(x)=\lim_{n\to \infty}f_n(x)
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such $f$ works, i.e.  $f_n\to f$ uniformly }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
  \vi{\text{ to find $N\inn$ such that for all $n>N$ and  $x \in X$ we have $d\big(f_n(x),f(x)\big)<\epsilon $ }}
\end{align*}
Because $f_n$ is uniformly Cauchy, we know there exists $N$ such that  
\begin{align}
\label{K5}
\forall n,m>N, \forall x\in X, d\big(f_n(x),f_m(x) \big)<\frac{\epsilon }{2}
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
 \As{there exists $n>N\text{ and }x\in X$ such that  $d\big(f_n(x),f(x) \big)\geq \epsilon $}. Because $f_k(x)\to f(x)$ as $k\to \infty$, we know 
\begin{align}
  \label{K6}
\exists m \inn, d\big(f_m(x),f(x) \big)<\frac{\epsilon}{2}
\end{align}
Then from \myref{Equation}{K5} and \myref{Equation}{K6}, we can deduce 
\begin{align*}
\epsilon \leq d\big(f_n(x),f(x)\big)\leq  d\big(f(x),f_m(x)\big)  + d\big(f_n(x),f_m(x)\big)<\epsilon \tCaC \vdone
 \end{align*}
 $(\longrightarrow)$\\

Let $K$ be the set of constant functions  in $X^Y$. We first prove 
 \begin{align*}
\blue{K\text{ is closed }}
\end{align*}
Arbitrarily pick $f \in K^c$. We wish 
\begin{align*}
\blue{\text{ to find $\epsilon \inr^+$ such that $B_{\epsilon }(f) \in K^c$ }}
\end{align*}
Because $f$ is not a constant function, we know there exists $x_1,x_2 \in X$ such that 
\begin{align*}
d\big(f(x_1),f(x_2) \big)>0
\end{align*}
We claim that 
\begin{align*}
\blue{\epsilon=\frac{d\big(f(x_1),f(x_2) \big)}{3}\text{ works }}
\end{align*}
Arbitrarily pick $g \in B_\epsilon(f)$. We wish 
\begin{align*}
\blue{\text{ to show $g\in K^c$  }}
\end{align*}
Notice the triangle inequality 
\begin{align}
\label{K7}
3\epsilon =d\big(f(x_1),f(x_2) \big)\leq d\big(f(x_1),g(x_1) \big)+d\big(g(x_1),g(x_2) \big)+d\big(g(x_2),f(x_2) \big)
\end{align}
Also, because $g\in B_\epsilon (f)$, we have
\begin{align}
  \label{K8}
\forall x\in X, d\big(f(x),g(x) \big)<\epsilon 
\end{align}
Then by \myref{Equation}{K7} and \myref{Equation}{K8}, we see 
\begin{align*}
d\big(g(x_1),g(x_2) \big)> \epsilon 
\end{align*}
This then implies $g$ is not a constant function.  $\bdone$\\

Now, Because by premise $(X^Y,d_{\infty})$ is complete, and we have proved $K$ is closed in  $(X^Y,d_\infty)$, we know $K$ is complete. Then, we resolve the whole problem into proving 
\begin{align*}
\vi{Y \text{ is isometric to }K}
\end{align*}
Define $\sigma:Y \to K $ by 
\begin{align*}
y \mapsto \tilde{y}\text{ where }\forall x \in X, \tilde{y}(x)=y 
\end{align*}
It is easy to verify $\sigma$ is an isometry. $\vdone$
\end{proof}
\begin{corollary}
\label{SoB}
\textbf{(Space of Bounded functions $\big(B(X,Y),d_{\infty} \big)$ is Complete iff  $Y$ is Complete)} 
\begin{align*}
\big(B(X,Y),d_\infty \big)\text{ is complete }\iff  Y\text{ is complete }
\end{align*}
\end{corollary}
\begin{proof}
$(\longleftarrow)$\\

By \myref{Theorem}{Tsof}, the space $(X^Y, d_{\infty})$ is complete. Then because $B(X,Y)$ is closed in $(X^Y,d_\infty)$, we know $B(X,Y)$ is complete.\\

$(\longrightarrow)$\\

Notice that the set of constant function $K$ is a subset of the galaxy  $B(X,Y)$. The whole proof in \myref{Theorem}{Tsof} works in here too. 
\end{proof}
\begin{mdframed}
Remember in the beginning of this section we say we will prove convergent sequences in $Y$ is closed under uniform convergence if $Y$ is complete. The proof of this result relies on \myref{Theorem}{Tsof}.\\

Now, before we actually prove convergence sequences are closed under uniform convergence if codomain $(Y,d)$ is complete (\myref{Theorem}{CSaC}), we will state and prove Weierstrass M-test  (\myref{Theorem}{WM-t}), which concerns the uniform convergence of series of complex functions. 
\end{mdframed}
\begin{theorem}
\label{WM-t}
\textbf{(Weierstrass M-test)} Given sequences $f_n:X\rightarrow \C$, and suppose 
\begin{align}
\label{K9}
\forall n\inn,\forall x\in X, \abso{f_n(x)}\leq M_n
\end{align}
Then 
\begin{align*}
\sum_{n=1}^\infty M_n\text{ converge }\implies \sum_{n=1}^\infty f_n\text{ uniformly converge }
\end{align*} 
\end{theorem}
\begin{proof}
Because $(\C,\norm{\cdot }_2)$  is complete, by \myref{Corollary}{SoB}, we only wish to prove 
\begin{align*}
\vi{ \bset{\sum_{k=1}^n f_k}_{n\inn}\text{ is uniformly Cauchy }} 
\end{align*}
Fix $\epsilon $. We wish 
 \begin{align*}
   \vi{\text{ to find $N$ such that  }\forall n,m>N, \forall  x\in X, \abso{\sum_{k=n}^m f_k(x)}< \epsilon}
\end{align*}
Because $\sum_{n=1}^\infty M_n$ converge, we know there exists $N$ such that 
\begin{align*}
\forall n,m>N, \sum_{k=n}^m M_k<\epsilon 
\end{align*}
We claim
\begin{align*}
\text{ such $N$ works }
\end{align*}
By \myref{Premise}{K9}, we have 
\begin{align*}
  \forall n,m>N, \forall x \in X, \abso{\sum_{k=n}^m f_k(x)}\leq \sum_{k=n}^m \abso{f_k(x)}\leq \sum_{k=n}^m M_k <\epsilon 
\end{align*}
\end{proof}
\begin{theorem}
\label{CSaC}
\textbf{(Convergent Sequences are Closed under Uniform Convergence if Codomain $\big(Y,d\big)$ is Complete)} Given a complete metric space $\big(Y,d\big)$, let $\mathcal{C}_\N^Y$ be the set of convergent sequences in $Y$.
\begin{align*}
Y\text{ is complete }\implies \text{ $\mathcal{C}_\N^Y$ is closed under uniform convergent }
\end{align*}
\end{theorem}
\begin{proof}
Let $a_{n,k}\to a_{\bullet,k}$ uniformly as $n\to \infty$ where for all $n,k \inn, a_{n,k}\in Y$ and let $A_n= \lim_{k\to \infty}a_{n,k}$ for all $n\inn$.  
\begin{align*}
  \vi{\text{ to prove $a_{\bullet,k}$ converge}}
\end{align*}
By \myref{Theorem}{COLO2}, we can reduce the problem to 
\begin{align*}
\vi{\text{ proving $A_n$ converge }}
\end{align*}
Then because $Y$ is complete, we can then reduce the problem into proving 
\begin{align*}
  \vi{A_n\text{ is Cauchy }}
\end{align*}
Fix $\epsilon $. We wish to find $N$ such that 
 \begin{align*}
   \vi{\forall n,m>N, d(A_n,A_m)<\epsilon }
\end{align*}
Because $a_{n,k}\to a_{\bullet,k}$ uniformly, we can find $N$ such that  
 \begin{align}
\label{K10}
\forall n,m> N, d_\infty(\set{a_{n,k}}_{k\inn},\set{a_{m,k}}_{k\inn})<\frac{\epsilon}{3} 
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Arbitrarily pick $n,m>N$. We wish to prove 
\begin{align*}
\vi{d(A_n,A_m)<\epsilon }
\end{align*}
Because $a_{n,k} \to A_n$ and $a_{m,k}\to A_m$ as $k \to \infty$, we can find $j$ such that  
\begin{align}
\label{K11}
d(a_{n,j},A_n)<\frac{\epsilon}{3}\text{ and }d(a_{m,j},A_m)<\frac{\epsilon}{3}
\end{align}
Then from  \myref{Equation}{K10} and \myref{Equation}{K11}, we can deduce
\begin{align*}
d(A_n,A_m)\leq d(A_n,a_{n,j})+d(a_{n,j},a_{m,j})+d(a_{m,j},A_m)<\epsilon \vdone
\end{align*}
\end{proof}
\section{Closed under Uniform Convergence}
\label{UCaCH}
\begin{mdframed}
The end goal for this section is to prove that the following properties 
\begin{enumerate}[label=(\alph*)]
  \item continuity 
  \item uniform continuity 
  \item $K$-Lipschitz Continuity 
\end{enumerate}
\end{mdframed}
\begin{theorem}
\label{ULT}
\textbf{(Uniform Limit Theorem)} Given a sequence of function $f_n$ from a topological space $(X,\tau)$ to a metric space $(Y,d)$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f_n\to f$ uniformly as $n\to \infty$
  \item $f_n$ is continuous for all  $n\inn$ 
\end{enumerate}
Then $f$ is also continuous. 
\end{theorem}
\begin{proof}
Fix $x \in X$, and let $x_k\to x$. We wish to prove
\begin{align*}
  \vi{f(x_k)\to f(x)}
\end{align*}
Because $f_n\to f$ uniformly as $n\to \infty$, we know 
\begin{align}
\label{282e1}
\bset{f_n(x_k)}_{k\inn}\to \bset{f(x_k)}_{k\inn}\text{ uniformly as }n\to \infty
\end{align}
Also, because for each $n\inn$, the function $f_n$ is continuous at $x$, we know 
\begin{align}
\label{282e2}
\forall n\inn, f_n(x_k)\to f_n(x)\text{ as $k\to \infty$ }
\end{align}
Then because $f_n\to f$ pointwise, we know 
\begin{align}
\label{282e3}
f_n(x)\to f(x)
\end{align}
Now, because \myref{Equation}{282e1},  \myref{Equation}{282e2} and  \myref{Equation}{282e3}, by \myref{Theorem}{COLO}, we have
\begin{align*}
\lim_{k\to \infty}f(x_k)=\lim_{k\to \infty}\lim_{n\to \infty}f_n(x_k)=\lim_{n\to \infty}\lim_{k\to \infty}f_n(x_k)=\lim_{n\to \infty}f_n(x)=f(x)\vdone
\end{align*}
\end{proof}
\begin{mdframed}
Suppose $X$ is a compact Hausdroff space,  with \myref{Theorem}{}, we can now say that the set $\mathcal{C}(X)$ of complex-valued continuous functions on $X$ 
\end{mdframed}
\begin{theorem}
\textbf{(Uniformly Continuous functions are Closed under Uniform Convergence)} Given a sequence of functions $f_n$ from a metric space  $(X,d_X)$ to metric space $(Y,d_Y)$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f_n\to f$ uniformly 
  \item $f_n$ is uniformly continuous for all  $n\inn$
\end{enumerate}
Then $f$ is also uniformly continuous
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find $\delta$ such that $\forall x,y \in X, d_X(x,y)<\delta \implies d_Y\big(f(x),f(y) \big)<\epsilon $}}
\end{align*}
Because $f_n\to f$ uniformly, we know there exists  $m \inn$ such that 
\begin{align}
\label{L1}
\forall x \in X, d_Y\big(f_m(x),f(x) \big)<\frac{\epsilon}{3}
\end{align}
Because $f_m$ is uniformly continuous, we know 
\begin{align}
\label{L2}
\exists \delta, \forall x,y \in X, d_X(x,y)<\delta \implies d_Y\big(f_m(x),f_m(y) \big)<\frac{\epsilon}{3}
\end{align}
We claim 
\begin{align*}
\text{ \vi{such $\delta$ works} }
\end{align*}
Let $x,y \in X$ satisfy $d_X(x,y)<\delta$. We wish 
\begin{align*}
\vi{\text{ to prove }d_Y\big(f(x),f(y) \big)<\epsilon }
\end{align*}
From \myref{Equation}{L1} and \myref{Equation}{L2}, we have 
\begin{align*}
d_Y\big(f(x),f(y) \big)\leq d_Y\big(f(x),f_m(x) \big)+d_Y\big(f_m(x),f_m(y) \big)+d_Y\big(f_m(y),f(y) \big)=\epsilon \vdone
\end{align*}
\end{proof}
\begin{theorem}
\textbf{($K$-Lipschitz functions are Closed under Uniform Convergence)} Given a sequence of functions $f_n$ from metric space $(X,d_X)$ to metric space $(Y,d_Y)$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f_n\to f$ uniformly as $n\to \infty$ 
  \item $f_n$ is $K$-Lipschtize continuous for all $n\inn$
\end{enumerate}
Then $f$ is also $K$-Lipschtize continuous. 
\end{theorem}
\begin{proof}
Arbitrarily pick $x,y \in X$, to show $f$ is $K$-Lipschtize continuous, we wish 
\begin{align*}
\vi{\text{ to show $d_Y\big(f(x),f(y) \big)\leq Kd_X(x,y)$ }}
\end{align*}
Fix $\epsilon $. We reduce the problem into proving 
\begin{align*}
  \vi{d_Y\big(f(x),f(y) \big)<Kd_X(x,y)+\epsilon }
\end{align*}
Because $f_n\to f$ uniformly as $n\to \infty$, we know there exists $m$ such that 
 \begin{align}
\label{L4}
\forall z \in X, d_Y\big(f(z),f_m(z) \big)<\frac{\epsilon}{2}
\end{align}
Because $f_m$ is $K$-Lispchitz continuous, we know 
\begin{align}
\label{L3}
d_Y\big(f_m(x),f_m(y) \big)\leq Kd_X(x,y)
\end{align}
Now, from \myref{Equation}{L3} and \myref{Equation}{L4}, we now see 
\begin{align*}
  d_Y\big(f(x),f(y) \big)&\leq d_Y\big(f(x),f_m(x) \big)+d_Y\big(f_m(x),f_m(y) \big)+d_Y\big(f_m(y),f(y) \big)< Kd_X(x,y)+\epsilon 
\end{align*}
\end{proof}
\begin{mdframed}
An example of sequences of Lipschitz continuous functions with unbounded Lipschitz constant can uniformly converge to a non-Lipschitz continuous function is given below 
\begin{Example}{\textbf{(Lipschitz functions with Unbounded Lipschitz constant Uniformly Converge to a non-Lipschitz function)}}{}
\begin{align*}
X=[0,1]\text{ and }f_n(x)=\sqrt{x+\frac{1}{n}} 
\end{align*}
\end{Example}
\end{mdframed}
\section{Modes of Convergence} 
\label{27CaB}
\begin{mdframed}
This section is the starting point for us to study spaces of function. At first, we will define two modes of convergence for sequence of function and point out some basic properties and the difference between two modes of convergence.\\



Given an arbitrary set $X$ and a metric space  $Y$, we say a sequence of functions $f_n$ from $X$ to  $Y$ \textbf{pointwise converge} to $f$ if for all $\epsilon \text{ and }x$ in $X$, there exists $N$ such that 
\begin{align*}
\forall n>N, f_n(x)\in B_{\epsilon }\big(f(x) \big)
\end{align*}
In other words, for each fixed $x$ in $X$, we have $f_n(x)\to f(x)$.\\

We say $f_n$ \textbf{uniformly converge} to $f$ if for all $\epsilon $ there exists $N$ such that 
\begin{align*}
\forall x\in X, \forall n>N, f_n(x)\in B_\epsilon \big(f(x)\big)
\end{align*}
The difference between pointwise convergence and uniform convergence is that if we require $f_n(x)$ to be $\epsilon $-close to $f(x)$ for all $n>N$, then 
\begin{enumerate}[label=(\bullet)]
  \item $N$ depend on both  $\epsilon $ and $x$ if $f_n\to f$ pointwise
  \item $N$ depend on only  $\epsilon $ if $f_n\to f$ uniformly 
\end{enumerate}
A few properties of sequence of functions similar to that of sequences in metric space is obvious. If $f_n\to f$ pointwise, then all sub-sequences  $f_{n_k}\to f$ pointwise. If $f_n \to f$ uniformly, then all sub-sequences $f_{n_k}\to f$ uniformly. Suppose $Z\subseteq X$. It is clear that if $f_n \to f$ uniformly (resp: pointwise) the restricts $f_n|_Z\to f|_Z$ uniformly (resp: pointwise). Also, if $f_n \to f$ uniformly, then $f_n\to f$ pointwise.\\

Suppose we have a family $\mathcal{F}$ of functions $f:X\rightarrow (Y,d)$. If we define 
\begin{align*}
d_{\infty}(f,g)=\sup_{x \in X}d\big(f(x),g(x) \big)
\end{align*}
instead of a metric, $d_{\infty}$ become an extended metric. If $f$ is bounded and  $g$ is unbouneded, we have  $d_{\infty}(f,g)=\infty$. If $f,g$ are both bounded, then  $d_{\infty}(f,g)\inr^+$. Because of such, for $d_{\infty}$ to be a metric, one but not the only condition is for  $\mathcal{F}$ to be space of bounded functions.\\

Now, regardless of $d_{\infty}$ is an extended metric or not, we have 
\begin{align*}
f_n\to f\text{ uniformly }\iff d_{\infty}(f_n,f)\to 0
\end{align*}
With this in mind, it shall be clear that the uniform limit of bounded (resp: unbounded) functions is always bounded (resp: unbounded).\\ 

Examples for bounded (resp: unbounded) function $f_n$ pointwise converge to unbounded (resp: bounded) function $f$ are as follows.

\begin{Example}{\textbf{(Bounded functions pointwise converge to unbounded function)}}{}
\label{bfpce}
\begin{align*}
  X=(0,1],f_n(x)=\min  \set{n,\frac{1}{x}}
\end{align*}
It is clear that $\forall n\inn,f_n(x) \in [0,n]$, and the limit $f:X\rightarrow \R$ is $f(x)=\frac{1}{x}$
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
\includegraphics[height=9cm,width=15cm]{pwise converge1.png}
   \end{minipage}
\end{center}
\end{Example}
\begin{Example}{\textbf{(Unbounded functions pointwise converge to bounded function)}}{}
\begin{align*}
X=\R,f_n(x)=\frac{1}{n}x
\end{align*}
The limit function is $f(x)=0$
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering  \includegraphics[height=7cm,width=15cm]{pwise converge2.png}
   \end{minipage}
\end{center}
\end{Example}
\end{mdframed} 
\begin{mdframed}
As pointed out earlier, if $f:X\rightarrow (Y,d)$ is bounded and $g:X\rightarrow (Y,d)$ is unbounded, then $d_{\infty}(f,g)=\infty$. This means that if $Y$ is unbounded, the uniform metric $d_{\infty}$ is extended on $X^Y$. For this, it is necessary to develop some basic fact concerning extended metric space.\\

Suppose $(X,d)$ is an extended metric space. If we define $\sim$ on $X$ by $x \sim y\iff  d(x,y)<\infty$, then $\sim $ is an equivalence relation. We say each equivalence class is a \textbf{galaxy} of $(X,d)$. Suppose $T$ is the collection of the galaxies of $(X,d)$. For each $\mathcal{T} \in T$, the space $(\mathcal{T},d)$ is just a metric space.\\

It is easy to see that the way we induce topology from metric space is still valid if the metric is extended. That is 
\begin{align*}
  \tau=\set{Z \in X: \forall z \in Z, \exists \epsilon ,B_\epsilon (z)\subseteq Z}
\end{align*}
is still a topology, even though $d$ is an extended metric on $X$.\\

We can verify that a set $Y$ in $X$ is open if and only if for all  $\mathcal{T} \in T$, the set $Y \cap \mathcal{T}$ is open, and the set $Y$ in $X$ is closed if and only if all convergent sequences $y_n$ in  $Y$ converge to points in $Y$. \\
\end{mdframed}
\begin{mdframed}
Now, suppose we are given an arbitrary set  $X$ and a complete metric space  $(\overline{Y},d)$, and on $X^{\overline{Y}}$, we define the uniform metric  $d_{\infty}$. We say a set $\mathcal{F}\subseteq X^{\overline{Y}}$ of functions is \textbf{closed under uniform convergence} if  for all uniform convergent sequence $f_n \subseteq \mathcal{F}$, the limit function $f$ is also in  $\mathcal{F}$. There are justified reasons for us to give the premise that $\overline{Y}$ is complete prior to the definition of the term \textbf{closed under uniform convergence}. One reason is that by \myref{Theorem}{Tsof}, if $Y$ is not complete, then the extended metric space  $(X^Y,d_{\infty})$ is also not complete, which implies the possibility a Cauchy sequence $f_n$ in  $X^Y$ converge to a function $f \in X^{\overline{Y}} \setminus X^Y$ where $\overline{Y}$ is the completion of  $Y$. For instance, if we let $Y=\R\setminus \set{1}$ where $X=\R$, and let $f_n(x)=\begin{cases}
  0& \text{ if  }x\neq 0\\
  1+\frac{1}{n}& \text{ if $x=0$ }
\end{cases}\in Y$, we see that the set $\mathcal{F}=\set{f_n:n\inn}$ is "closed under uniform convergence" in the context of $X^Y$, but when in fact $f_n$ uniformly converge to  $f(x)=\begin{cases}
  0& \text{ if  }x\neq 0\\
  1& \text{ if $x=0$ }
\end{cases}$ which is not in $\mathcal{F}$. This awkward usage of words can be solved if we define the term  \textbf{closed under uniform convergence} after the premise that $Y$ is complete.\\

Now, given a set of functions $\mathcal{F}\subseteq X^{\overline{Y}}$, one can verify that 
\begin{align*}
  \mathcal{F}\text{ is closed under uniform convergence }&\iff (\mathcal{F},d_{\infty})\text{ is complete }\\
&\iff \mathcal{F}\text{ is closed with respect to $(X^{\overline{Y}},d_{\infty})$ }
\end{align*}
Let $\mathcal{G}$ be a galaxy of $(X^{\overline{Y}},d_{\infty})$. With multiple ways, we can verify that $\mathcal{G}$ is closed with respect to $(X^{\overline{Y}},d_{\infty})$. Then, acknowledging the space of bounded functions $B(X,\overline{Y})$ is a galaxy of $X^{\overline{Y}}$, we see that $B(X,\overline{Y})$ is closed under uniform convergence. The statement that $B(X,\overline{Y})$ is closed under uniform convergence, although already "proved" before as we pointed out the limit of uniform convergent sequence of bounded functions must be bounded, is now in fact actually proved in the sense the term "closed under uniform convergence" is formally given a satisfying definition.
\end{mdframed}
\section{Arzelà–Ascoli Theorem}
\begin{mdframed}
In this section, we will give a complete proof of Arzelà–Ascoli Theorem for functions from arbitrary compact topological space to arbitrary metric space. Note that in Baby Rudin, Arzelà–Ascoli Theorem are given for functions from compact metric space to metric space. Because Arzelà–Ascoli Theorem are concerned with family of equicontinuos functions, it is crucial for us to give a definition to equicontinuity for functions from topological  space to metric space, for the sake of our generalization.\\

Let $X,Y$ be metric space. Let  $Z$ be topological space. Let  $\mathcal{F}_X$ be family of functions from $X$ to  $Y$, and let  $\mathcal{F}_Z$ be family of functions from $Z$ to  $Y$. We say  $\mathcal{F}_Z$ is \textbf{pointwise equicontinuous} if 
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
       For all $\epsilon $ and for all $x$, there exists a neighborhood $U_x$ such that  $d_Y(f(x),f(y))<\epsilon $ for all $y \in U_x$   
   \end{minipage}
\end{center}
We say $\mathcal{F}_X$ is \textbf{equicontinuous} if 
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
       For all $\epsilon $, there exists $\delta$ such that  $d_{Y}(f(x),f(y))<\epsilon $ for all $\delta$-close $x,y \in X$ and all  $f \in \mathcal{F}$. 
   \end{minipage}
\end{center}
It is easy to verify that if $\mathcal{F}_X$ is equicontinuous, then $\mathcal{F}_X$ is pointwise equicontinuous. The converse don't always hold true. Say, $\mathcal{F}=\set{n+x^2}_{n\inn}$, the set $\set{n+x^2}_{n\inn}$ is clearly pointwise equicontinuous on $\R$, and is not equicontinuous on $\R$, since no function $n+x^2$  is uniform continuous on $\R$. However, the same set $\mathcal{F}=\set{n+x^2}$ is equicontinous on compact domain $[a,b]$. This is a general result, as we shall prove below.
\end{mdframed}
\begin{theorem}
\textbf{(Pointwise Equicontinous is Uniform on Compact Domain)} Given two metric space $(X,d_X),(Y,d_Y)$, and a family $\mathcal{F}$ of functions from $X$ to  $Y$ such that 
 \begin{enumerate}[label=(\alph*)]
  \item $X$ is compact 
   \item $\mathcal{F}$ is pointwise equicontinuous
\end{enumerate}
Then 
\begin{align*}
\mathcal{F}\text{ is equicontinuous }
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish to 
\begin{align*}
\vi{\text{ find $\delta$ such that $d_X(x,y)<\delta \implies d_Y(f(x),f(y))\leq \epsilon $ for all $f \in \mathcal{F}$ }}
\end{align*}
Because $\mathcal{F}$ is pointwise equicontinuous, we know for each $x \in X$, there exists $\delta_x$ such that 
\begin{align}
\label{PEe1}
\forall y\in B_{\delta_x}(x), d_Y\big(f(x),f(y) \big)<\frac{\epsilon}{2} \text{ for all $f\in \mathcal{F}$ }
\end{align}
It is clear that $\set{B_{\frac{\delta_x}{2}}(x):x \in X}$ form an open cover of $X$. Then because  $X$ is compact, we know  
 \begin{align*}
\text{ there exists a finite open sub-cover: }\set{B_{\frac{\delta_x}{2}}(x):x \in X_{\text{finite}}}
\end{align*}
We claim 
\begin{align*}
\vi{\delta=\min_{x \in X_{\text{finite}}}\frac{\delta_x}{2}\text{ works }}
\end{align*}
Fix $y,z \in X: d_X(y,z)<\delta$. We have to prove 
\begin{align*}
\vi{d_Y\big(f(y),f(z) \big)< \epsilon }
\end{align*}
We know $y$ must lie in some  $B_\frac{\delta_x}{2}(x)$ for some $x \in X_\text{finite}$. Because $d_X(y,z)<\frac{\delta_x}{2}$, we see that $z$ must lie in  $B_{\delta_x}(x)$. We now know $y,z$ are both in  $B_{\delta_x}(x)$. Then from \myref{(}{PEe1}), we can now deduce 
\begin{align*}
d_Y\big(f(y),f(z) \big)\leq d_Y\big(f(y),f(x) \big)+d_Y\big(f(x),f(z) \big)< \epsilon \vdone
\end{align*}
\end{proof}
\begin{mdframed}
The proof above should be a great example why in the discussion of metric space, instead of using sequential definition of compactness, which leads to the beautiful Bolzano-Weierstrass Theorem, some people prefer the open-cover definitions.\\

Now, we give proof for the Arzelà–Ascoli Theorem. 
\end{mdframed}
\begin{theorem}
\textbf{(Arzelà–Ascoli Theorem)} Given a compact topological space $(X,\tau)$, a metric space $(Y,d_Y)$, and a family $\mathcal{F}\subseteq C\big(X,Y \big)$ of continuous function  
 \begin{align*}
&\mathcal{F}\text{ is pointwise equicontinuous and $\set{f(x):f \in \mathcal{F}}$ has compact closure in $Y$ for all $x \in X$}\\
&\implies \mathcal{F}\text{ has a compact closure in $C(X,Y)$}
\end{align*}
\end{theorem}
\begin{proof}
Fix a sequence $f_n$ in $\mathcal{F}$. We wish to show
\begin{align*}
\vi{f_n\text{ has a sub-sequence $f_{n_k}$ uniformly converge to some }f:X\rightarrow Y}
\end{align*}
First, we prove 
\begin{align*}
\blue{\text{ there exists a countable set $P$ such that $P$ works like a dense set }}
\end{align*}
Because $\mathcal{F}$ is pointwise equicontinuous, we know for all $x \in X$ 
\begin{align*}
\exists U_{x,n}, \forall y \in U_{x,n},\forall f \in \mathcal{F}, d_Y\big(f(x),f(y) \big)<\frac{1}{n} \text{ for each fixed $n\inn$ }
\end{align*}
Now, because $X$ is compact, for each $n\inn$, there exists a finite subset $P_n\subseteq X$ such that $\bset{U_{x,n}:x \in P_n}$ is a cover of $X$. Let $P=\bigcup_{n\inn}P_n$. $\bdone$\\

Now, we wish to 
\begin{align*}
\olive{\text{ construct a sub-sequence $f_{n_k}$ pointwise converge on $P$ }}
\end{align*}

Express $P=\set{p_k}_{k\inn}$. By premise (pointwise image has compact closure), we know there exists a compact set that contain $\set{f_n(p_1)}_{n\inn}$, so by Bolzano-Weierstrass Theorem, there exists a sub-sequence 
\begin{align*}
\bset{f_{g_1(k)}(p_1)}_{k\inn}\text{ converge to some point in $Y$ }
\end{align*}
Now, again by premise and Bolzano-Weierstrass Theorem, there exists a sub-sequence 
\begin{align*}
\bset{f_{g_2\circ g_1(k)}(p_2)}_{k\inn}\text{ converge to some point in $Y$ }
\end{align*}
Repeatedly doing such, we have 
\begin{align*}
\begin{matrix} 
  f_{g_1(1)}(p_1) & f_{g_2\circ g_1 (1)}(p_2) & f_{g_3\circ g_2 \circ  g_1 (1)}(p_3) & \cdots \\
  f_{g_1(2)}(p_1) & f_{g_2\circ g_1 (2)}(p_2) & f_{g_3\circ g_2 \circ  g_1 (2)}(p_3) & \cdots \\
  f_{g_1(3)}(p_1) & f_{g_2\circ g_1 (3)}(p_2) & f_{g_3\circ g_2 \circ  g_1 (3)}(p_3) & \cdots \\
  \vdots &\vdots &\vdots & \ddots\\
  \downarrow & \downarrow &\downarrow &\\
  y_1 & y_2 & y_3 & \cdots 
\end{matrix}
\end{align*}
Now, let 
 \begin{align*}
n_k=g_k\circ \cdots \circ g_1(k)
\end{align*}
Then 
\begin{align*}
n_k\text{ is eventually a sub-sequence of $g_m\circ \cdots \circ g_1(k)$ for all $m$ }
\end{align*}
This then implies 
\begin{align*}
f_{n_k}(p_m)\to y_m\text{ for all $p_m \in P \odone$ }
\end{align*}
Next, we show 
\begin{align*}
\blue{\text{ To prove $f_{n_k}$ uniformly converge on $X$, it suffice to prove $f_{n_k}$ is uniformly Cauchy on $X$. }}
\end{align*}
By premise (pointwise image has compact closure), if $f_{n_k}$ is uniformly Cauchy, then we know $f_{n_k}$ pointwise converge to some $f$.\\

Fix $\epsilon $. We reduced the problem into 
\begin{align*}
\blue{\text{ finding  $N$ such that for all $k>N$, we have  $d_Y\big(f_{n_k}(x), f(x)\big)\leq \epsilon $ for all $x\in X$ }}
\end{align*}
Because $f_{n_k}$ is uniformly Cauchy, we know there exists $N$ such that for all $m,k>M$ $d_Y\big(f_{n_k}(x),f_{n_m}(x) \big)\leq \frac{\epsilon}{2}$ for all $x \in X$. We claim 
\begin{align*}
 \blue{\text{ such $N$ works }}
\end{align*}
Let $k>N$.  \As{$d_Y\big(f_{n_k}(x),f(x) \big)>\epsilon $}. We see that 
 \begin{align*}
   d_Y\big(f(x),f_{n_m}(x) \big)\geq d_Y\big(f(x),f_{n_k}(x)\big)-d_Y\big(f_{n_k}(x),f_{n_m}(x) \big)>\frac{\epsilon}{2}\text{ for all $m>N\tCaC \bdone$} 
\end{align*}
Lastly, we wish to prove 
\begin{align*}
\vi{f_{n_k}\text{ is uniformly Cauchy }}
\end{align*}
Fix $\epsilon $. We wish  
\begin{align*}
\vi{\text{ to find }N\text{ such that }\forall j,k >N,\forall x\in X, d_Y\big(f_{n_j}(x),f_{n_k}(x)\big)\leq \epsilon  }
\end{align*}
Fix $m > \frac{3}{\epsilon }$. Express $P_m=\set{p^m_1,\dots ,p^m_u}$. Because $f_{n_k}(p^m_t)$ converge for each $t \in \set{1,\dots ,u}$, we know 
\begin{align*}
\forall t , \exists N_t, d_Y\big(f_{n_j}(p_t^m),f_{n_k}(p_t^m) \big)<\frac{\epsilon}{3}\text{ for all $j,k>N_t$ }
\end{align*}
We claim 
\begin{align*}
\vi{N=\max_t N_t\text{ works }}
\end{align*}
Fix $j,k>N$ and $x \in X$. We have to show 
\begin{align*}
  \vi{d_Y\big(f_{n_j}(x),f_{n_k}(x) \big)\leq \epsilon}
\end{align*}
Because $\set{U_{p_t^m,m}}$ form an open cover of $X$, we know there exists  $t$ such that $x \in U_{p_t^m,m}$. We can now deduce 
\begin{align*}
d_Y\big(f_{n_j}(x),f_{n_k}(x) \big)\leq d_Y\big(f_{n_j}(x),f_{n_j}(p_t^m) \big)+d_Y\big(f_{n_j}(p_t^m),f_{n_k}(p_t^m) \big)+d_Y\big(f_{n_k}(p_t^m),f_{n_k}(x) \big)<\epsilon 
\end{align*}
$\vdone$\\

\end{proof}
\section{Contraction Mapping Theorem}
\chapter{Calculus}

\section{Equivalent Definitions for Riemann Integral}
\begin{mdframed}
In this section, we will give a principal Theorem (\myref{Theorem}{1}) that can serve as a lemma to prove the equivalency of multiple different definitions of Riemann integral on a compact interval. With this approach, we shall diminish the trouble of getting through miscellaneous minor definitions, where they are all equivalent, with only the difference of taking different tags and partitions of certain pattern, which solely serve as a pedagogical tool to give students a concrete idea of integration.\\

A caveat will be made clear here: this section concern only the proper Riemann Integral. That is, we only consider the integration of a function bounded on a compact interval. For a treatment of inproper integral, see \myref{Section}{s1}.\\

In this section, by a \textbf{partition} $P$ of $[a,b]$, we mean a finite set of values $P=\set{a=x_0 \leq  x_1 \leq  \cdots \leq  x_{n_P-1} \leq x_{n_P} =b}$. We say the partition $P'$ is \textbf{finer} than $P$ if $P\subseteq P'$. Given a partition $P$, we put
\begin{align*}
\begin{cases}  
M_i=\sup_{[x_{i-1},x_i]} f(x)\\
m_i=\inf_{[x_{i-1},x_i]}f(x)
\end{cases}\text{and }\begin{cases}
  U(P,f)=\sum_{i=1}^{n_P} M_i \Delta x_i\\
  L(P,f)=\sum_{i=1}^{n_P}m_i\Delta x_i
\end{cases}\text{ where }\Delta x_i= x_i-x_{i-1}
\end{align*}
We shall write $n$ instead of $n_P$ if no confusion will be made.\\


The word \textbf{norm of partition} $\norm{P}$ is defined by $\max_{1\leq i\leq n} \Delta x_i$. We say $U(P,f)$ is an \textbf{upper sum} of $f$. We say the \textbf{upper integral} $\overline{\int_a^b}fdx$ of $f$ on $[a,b]$ is $\inf_P U(P,f)$ where the infimum run through all partitions $P$ of $[a,b]$. The \textbf{lower integral} $\underline{\int_a^b}fdx$ is defined similarly. We say a function $f$ is \textbf{integrable} on $[a,b]$ if   $\overline{\int_a^b}fdx=\underline{\int_a^b}fdx$.\\

Give close attention to the setting that $P$ is finite. This is crucial for making the integration operation possible, since if  $P$ is countable and we define $U(P,f)$ by taking limits for sums, the order of addition can make a difference if the sum does not converge absolutely. This fact is backed by Riemann Rearrangement Theorem  (\myref{Theorem}{t4}), of which we will later give a proof.
\end{mdframed}
\begin{theorem}
\label{1}
\textbf{(Principal for Proving Equivalency of Definitions for Riemann Integral)} 
\begin{align*}
\int_a^b fdx\inr \iff  \forall \set{P_k} : \norm{P_k}\to 0, U(P_k,f)-L(P_k,f)\to 0
\end{align*}
\end{theorem}
\begin{proof}
From right to left is obvious. We prove only 
\begin{align*}
  \vi{\int_a^b fdx\inr \implies   \forall \set{P_k} : \norm{P_k}\to 0, U(P_k,f)-L(P_k,f)\to 0}
\end{align*}
Fix $\epsilon $. We wish to find a positive number $\beta \inr^+$ such that $\forall P: \norm{P}\leq \beta , U(P,f)-L(P,f)<\epsilon $. Because $\int_a^b fdx \inr$, we can let $W$ be a partition such that 
\begin{align*}
U(W,f)-L(W,f)<\frac{\epsilon}{2}
\end{align*}
Let $W=\set{a=x_0^*,x_1^*,\dots ,x_{n_W}^*=b}$, and let $J=\set{1,\dots ,n_W}$ be the set of indices of $W$. Suppose
\begin{align}
\label{e1}
L= \max_{1\leq j\leq n_W-1} \big(\sup_{[x_{j-1}^*,x_{j+1}^*]} f(x)- \inf_{[x_{j-1}^*,x_{j+1}^*]} f(x) \big)
\end{align}
Notice that if $L=0$, then  $f$ must be constant and the proof become trivial, so we can assume $L>0$. We claim that 
\begin{align*}
L\beta n_W\leq \frac{\epsilon}{2}\text{ and }\beta < \min_{j \in J}\Delta x_j
\end{align*}
suffice so that $\forall P:\norm{P}\leq \beta , U(P,f)-L(P,f)<\epsilon $. Let $C= \min_{j \in J}\Delta x_j$. In other words, we now reduce the problem into proving 
\begin{align*}
  \vi{ \norm{P} \leq \min \set{\frac{\epsilon }{2Ln_W },C} \implies U(P,f)-L(P,f)<\epsilon }
\end{align*}
Let $I=\set{1,\dots , n_P}$ be the set of indices for $P$. Suppose 
\begin{align*}
P=\set{a=x_0,x_1,\dots ,x_{n_P}=b}
\end{align*}
We partition $I$ into 
\begin{align*}
\begin{cases}
  A=\bset{i \in I : \exists j \in J,  [x_{i-1},x_i]\subseteq [x_{j-1}^*,x_j^*]}\\
  B=I \setminus  A
\end{cases}
\end{align*}
We now have 
\begin{align}
\label{e2}
U(P,f)-L(P,f)= \sum_{i \in A} \big(M_i-m_i \big) \Delta x_i +\sum_{i \in B} \big(M_i-m_i \big)\Delta x_i
\end{align}
Because for each $i\in A$, there is a unique corresponding $j \in J$ such that $[x_{i-1},x_i] \subseteq [x_{j-1}^*,x_j^*]$, we have 
\begin{align}
  \label{e3}
\sum_{i \in A} \big( M_i-m_i\big) \Delta x_i \leq \sum_{j \in J} \big(M_j^W-m_j^W \big) \Delta x_j^* = U(W,f)-L(W,f)< \frac{\epsilon}{2}
\end{align}
Because $\norm{P}\leq C=\min_{j \in J} \Delta x_j$, we know for each distinct $i \in B$, there exists a distinct $ j \in J$ such that $[x_{i-1},x_i]\subseteq [x_{j-1}^*, x_{j+1}^*]$, so by definition of $L$  (\myref{Equation}{e1}), we have 
\begin{align}
\label{e4}
\sum_{i \in B} \big(M_i-m_i \big) \Delta x_i \leq L\sum_{i \in B} \Delta x_i \leq L n_W \norm{P} \leq \frac{\epsilon }{2} 
\end{align}
Combining  \myref{Equation}{e2}, \myref{Equation}{e3} and \myref{Equation}{e4}, we now see 
\begin{align*}
U(P,f)-L(P,f) <\epsilon \vdone
\end{align*}
\end{proof}
\begin{mdframed}
Recall that we say a series $\sum_{n=1}^\infty a_n$ \textbf{absolutely converge} if $\sum_{n=1}^\infty \abso{a_n} $ converge. We can show that a series converges if it absolutely converges by proving it is Cauchy. In this section, by a \textbf{permutation on $\N$}, we mean a bijective function $\sigma$ from $\N$ to $\N$. Another two important terminologies are the followings. We say that $\sum_{n=1}^\infty a_n$ \textbf{unconditionally converge} if for all permutation $\sigma:\N\rightarrow \N$, the series $\sum_{n=1}^\infty a_{\sigma(n)}$ converge to the same number. We say $\sum_{n=1}^\infty a_n$ \textbf{conditionally converge} if it converge but not unconditionally.\\

In our treatment, Riemann Rearrangement Theorem will be split into 4 parts. The summary is at \myref{Theorem}{t4}. The first part (\myref{Theorem}{2}) states that the limit an absolutely convergent series remain the same under any permutations. The proof for the first part (\myref{Theorem}{2}) may seem technical, but the essence is quite easy to remember. Just "see" that 
\begin{align*}
  \abso{\sum_{k=1}^\infty a_{\sigma (k)}-L}&\leq \abso{\sum_{i<M}a_i -L + \sum_{i\geq M}a_i}\\
&\leq \abso{\sum_{i<M}a_i-L }+\sum_{i\geq M} \abso{a_i} \to 0 \text{ as $M\to 0$ }
\end{align*}
\end{mdframed}
\begin{theorem}
\label{2}
\textbf{(Riemann Rearrangement Theorem, Part 1)}  
\begin{align*}
\sum_{k=1}^\infty a_k \text{ absolutely converge }\implies  \sum_{k=1}^\infty a_k \text{ unconditionally converge }
\end{align*}
\end{theorem}
\begin{proof}
Suppose $\sum_{k=1}^\infty \abso{x_k}$ converge. Let $\sum_{k=1}^\infty x_k=L$. Fix permutation $\sigma :\N\rightarrow \N$. We wish to prove 
\begin{align*}
  \vi{\sum_{k=1}^\infty x_{\sigma(k)}=L}
\end{align*}
Fix $\epsilon $. We reduce the problem into  
 \begin{align*}
   \vi{\text{ finding $N$ such that }\forall n>N, \abso{\sum_{k=1}^n x_{\sigma (k)}- L} <\epsilon }
\end{align*}
Because both $\sum_{k=1}^\infty x_k$ and $\sum_{k=1}^\infty \abso{x_k}$ converge by premise. We know there exists $M$ such that 
\begin{align}
\label{e5}
  \forall n>M, \abso{\sum_{k=1}^n x_k -L}<\frac{\epsilon}{2}\text{ and } \sum_{k=n}^\infty \abso{x_k}<\frac{\epsilon}{2}
\end{align}
Let 
\begin{align*}
I=\sigma^{-1}\big(\set{1,\dots ,M} \big)\text{ and }N=\max I
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
To prove our claim, fix $n>N$. We wish to show 
 \begin{align*}
   \vi{\abso{\sum_{k=1}^n x_{\sigma (k)}-L}<\epsilon }
\end{align*}
Let $I_n=\set{1,\dots , n}$. Observe that 
\begin{align}
\label{e6}
\abso{\sum_{k=1}^n x_{\sigma(k)}-L}&= \abso{\sum_{k \in I_n \setminus I} x_{\sigma (k)}+ \sum_{k \in I} x_{\sigma (k)} - L }
\end{align}
Notice that $k \not \in I \implies  \sigma (k)>M$. Then by definition of $M$  (\myref{Equation}{e5}), we have
\begin{align}
  \label{e7}
\abso{\sum_{k \in I_n \setminus I} x_{\sigma (k)} }\leq \sum_{k \in I_n \setminus  I} \abso{x_{\sigma(k)}}\leq \sum_{j>M}^\infty \abso{x_j}< \frac{\epsilon}{2}
\end{align}
Notice that $\sigma [I]=\set{1,\dots , M}$. Then also by definition of $M$  (\myref{Equation}{e5}), we have 
\begin{align}
\label{e8}
\abso{\sum_{k \in I}x_{\sigma(k)}- L}= \abso{\sum_{j=1}^M x_j-L}< \frac{\epsilon}{2}
\end{align}
Then by inequalities \myref{Equation}{e6}, \myref{Equation}{e7} and \myref{Equation}{e8}, we now have 
\begin{align*}
\abso{\sum_{k=1}^n a_{\sigma(k)}-L} &= \abso{\sum_{k \in I_n \setminus I}x_{\sigma(k)} + \sum_{k \in I} x_{\sigma(k)}-L}\\
&\leq \abso{\sum_{k \in I_n \setminus I}x_{\sigma(k)}}+\abso{\sum_{k \in I} x_{\sigma(k)}-L} \\
&< \frac{\epsilon }{2}+\frac{\epsilon}{2}=\epsilon \vdone 
\end{align*}
\end{proof}
\begin{mdframed}
The second, third and forth parts (\myref{Theorem}{t1}, \myref{Theorem}{t2} and \myref{Theorem}{t3}), respectively states that if a series converge but not absolutely, then the limit value can be changed to any real number, infinite, negative infinite and even jumping-ly diverges.\\

The detail of the proof is very tedious and cumbersome, while the essence is easy to understand. The only two tools for proving \myref{Theorem}{t1}, \myref{Theorem}{t2} and \myref{Corollary}{t3}, is \myref{Lemma}{l1} and the fact $\sum a_k\to L \implies a_k\to 0$. If any part of the proof can be considered interesting, I believe it lies in that of \myref{Lemma}{l1}, where one split the series $\sum a_k$ into two $\sum a_k^+,\sum a_k^-$, and shows that they must both diverge. 
\end{mdframed}
\begin{lemma}
\label{l1}
\textbf{(Intrinsic Structure of Series that Converge but not Absolutely)} Let $f^+:\N\rightarrow \N$ and $f^-:\N\rightarrow \N$ satisfy that $\set{a_{f^+(n)}}$ contain all and only positive terms of $\set{a_n}$ and $\set{a_{f^-(n)}}$ contain all and only negative terms. If $\sum_{k=1}^\infty a_k$ converge but not absolutely, then for each $\alpha \inr^+$ and $n\inn$, there exists  $u_n>n$ and $t_n>n$ such that 
\begin{align*}
\sum_{n\leq k\leq u_n} a_{f^+(k)}>\alpha \text{ and }\sum_{n\leq k\leq t_n}a_{f^-(k)}<-\alpha 
\end{align*}
\end{lemma}
\begin{proof}
Let $a_n^+=\max \set{0,a_n}$ and $a_n^-=\min \set{0,a_n}$. It is easy to check $\forall n\inn, a_n=a_n^++a_n^-$. Because $\sum_{k=1}^\infty a_k$ converge but not absolutely, we know 
\begin{align}
\label{e9}
\begin{cases}
  \sum_{k=1}^\infty a_k^+\to\infty\\
  \sum_{k=1}^\infty a_k^-\to-\infty
\end{cases}
\end{align}
This is true because if both of them converge then $\sum_{k=1}^\infty \abso{a_k}$ converges and if only one of them converge them $\sum_{k=1}^\infty a_k$ diverges.\\


Because of  \myref{Equation}{e9}, we know 
\begin{align*}
\begin{cases}
  \sum_{k=1}^\infty a_{f^+(k)}\to \infty\\
  \sum_{k=1}^\infty a_{f^-(k)}\to -\infty
\end{cases}
\end{align*}
The result then follow, since 
\begin{align*}
\forall n\inn, \sum_{k\geq n}a_{f^+(k)}\nearrow \infty \text{ and }\sum_{k\geq n}a_{f^-(k)}\searrow -\infty
\end{align*}
\end{proof}
\begin{theorem}
\label{t1}
\textbf{(Riemann Rearrangement Theorem, Part 2)} If $\sum_{k=1}^\infty a_k$ converge but not absolutely, then there exists permutations $\sigma_{\infty}, \sigma_{-\infty}:\N\rightarrow \N$ such that $\sum_{k=1}^\infty a_{\sigma_\infty(k)}\to\infty$ and $\sum_{k=1}^\infty a_{\sigma_{-\infty}(k)}\to-\infty$. 
\end{theorem}
\begin{proof}
We wish
\begin{align*}
\vi{\text{ to construct $\sigma_{\infty}:\N\rightarrow \N$ such that  $\sum_{k=1}^\infty a_{\sigma_{\infty}(k)}\to\infty$}}
\end{align*}
Using \myref{Lemma}{l1}, construct $\sigma_{\infty}:\N\rightarrow \N$ as follows. Let $p_n$ be a sequence of natural number such that for each $n\inn$, $p_{n+1}$ is the smallest natural number such that 
\begin{align}
\label{e10}
\sum_{k=p_n+1}^{p_{n+1}} a_{f^+(k)}> 3\text{ and }p_1=0
\end{align}
Similarly, let $q_n$ be a sequence of natural number such that for each $n\inn,q_{n+1}$ is the smallest natural number such that 
\begin{align}
\label{e11}
\sum_{k=q_n+1}^{q_{n+1}}a_{f^-(k)}<-1\text{ and }q_1=0
\end{align}
Notice that the definition of $p_n$ and  $q_n$ (\myref{Equation}{e10}, \myref{Equation}{e11}) are done recursively. Now, recursively define $\sigma_{\infty}$ to follow the order
\begin{align*}
&f^+(p_1+1),\dots,f^+(p_2),f^-(q_1+1),\dots ,f^-(q_2)\\
  \longrightarrow &  f^+(p_2+1),\dots ,f^+(p_3),f^-(q_2+1),\dots ,f^-(q_3),f^+(p_3+1),\dots 
\end{align*}
If there exists  $k\inn$ such that $a_k=0$, which is not in the range $f^+[\N]\cup f^-[\N]$, we can merge these zero term into our $\sigma_{\infty}$ by putting them in terms of even order. This way, our $\sigma_{\infty}$ then become bijecetive, a permutation.\\

We claim 
\begin{align*}
  \vi{\text{ such }\sigma_{\infty}\text{ works }}
\end{align*}
Recall the definition of $p_n$ (\myref{Equation}{e10}) is that for each  $n\inn,p_{n+1}$ is the smallest natural number such that
\begin{align*}
\sum_{k=p_{n}+1}^{p_{n+1}}a_{f^+(k)}>3 
\end{align*}
Also recall the similarly defined $q_n$. This tell us 
\begin{align*}
\sum_{k=p_n+q_n+1}^{p_{n+1}+q_{n+1}}a_{\sigma_{\infty}(k)}\to 2\text{ as $n\to \infty$ }
\end{align*}
where 
\begin{align*}
\sum_{k=p_n+q_n+1}^{p_{n+1}+q_n}a_{\sigma_\infty(k)}\to 3\text{ and }\sum_{k=p_{n+1}+q_n+1}^{p_{n+1}+q_{n+1}}a_{\sigma_\infty(k)}\to -1\text{ as $n \to \infty$ }
\end{align*}
With this, it is easy to verify $\sum_{k=1}^\infty a_{\sigma_{\infty}(k)} \to \infty\vdone$. The construction of $\sigma_{-\infty}$ and the proof for its validity is done similarly. 
\end{proof}
\begin{theorem}
\label{t2}
\textbf{(Riemann Rearrangement Theorem, Part 3)} If $\sum_{k=1}^\infty a_k$ converges but not absolutely, then for all $[L,M]\subseteq \R$, there exists a permutation $\sigma:\N\rightarrow \N$ such that $\liminf_{n\to\infty} \sum_{k=1}^n a_{\sigma(k)}=L\text{ and }\limsup_{n\to\infty} \sum_{k=1}^n a_{\sigma(k)}=M$.
\end{theorem}
\begin{proof}
We wish 
\begin{align*}
\vi{\text{ to construct a working $\sigma$ }}
\end{align*}
The construction of $\sigma$ is similar to that of $\sigma_{\infty}$ in \myref{Theorem}{t1}. WOLG, let $M>0$. Let $p_1=0$, and let  $p_2$ be the smallest natural number such that 
 \begin{align*}
\sum_{k=1}^{p_2}a_{f^+(k)}>M\text{ and }p_1=0
\end{align*}
Next, define $q_1=0$ and let $q_2$ be the smallest natural number such that 
 \begin{align*}
\sum_{k=1}^{p_2}a_{f^+(k)}+\sum_{k=1}^{q_2} a_{f^-(k)}<L
\end{align*}
Then, let $p_3$ be the smallest natural number such that 
\begin{align*}
\sum_{k=1}^{p_3}a_{f^+(k)}+\sum_{k=1}^{q_2}a_{f^-(k)}>M
\end{align*}
Recursively do such. We get two sequences $\set{p_n},\set{q_n}$ of natural number such that for all $n\inn,p_{n+1}$ is the smallest natural number such that 
\begin{align*}
\sum_{k=1}^{p_{n+1}} a_{f^+(k)} +\sum_{k=1}^{q_n}a_{f^-(k)}>M
\end{align*}
and for all $n\inn,q_{n+1}$ is the smallest natural number such that 
\begin{align*}
\sum_{k=1}^{p_{n+1}}a_{f^+(k)} + \sum_{k=1}^{q_{n+1}} a_{f^-(k)}<L
\end{align*}
Them, recursively define $\sigma$ to follow the order 
\begin{align*}
&f^+(p_1+1),\dots,f^+(p_2),f^-(q_1+1),\dots ,f^-(q_2)\\
  \longrightarrow &  f^+(p_2+1),\dots ,f^+(p_3),f^-(q_2+1),\dots ,f^-(q_3),f^+(p_3+1),\dots 
\end{align*}
Again, merge in the zero terms like in   \myref{Theorem}{t1}. The proof for the claim  \vi{such $\sigma$ works } is easy to verify knowing $a_{\sigma(k)}\to 0\vdone$ 
\end{proof}
\begin{corollary}
\label{t3}
\textbf{(Riemann Rearrangement Theorem, Part 4)} If $\sum_{k=1}^\infty a_k$ converges but not absolutely, then for all $L \inr$, there exists a permutation $\sigma$ such that $\sum_{k=1}^\infty a_{\sigma(k)}=L$
\end{corollary}
\begin{theorem}
\label{t4}
\textbf{(Summary of Riemann Rearrangement Theorem)} If $\sum_{k=1}^\infty a_k$ converge, then 
\begin{align*}
\sum_{k=1}^\infty a_k\text{ absolutely converges }\iff  \sum_{k=1}^\infty a_k\text{ unconditionally converges }
\end{align*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

This is \myref{Theorem}{2}.\\

$(\longleftarrow)$\\

The fact that the contraposition of this  statement is true is implied by any of \myref{Theorem}{t1}, \myref{Theorem}{t2} and \myref{Corollary}{t3}. 
\end{proof}
\section{Product, Quotient and Chain Rule}
\begin{mdframed}
This section concern mostly the computation of actual value of the derivative and integral of function. With this in mind, we first prove the product and quotient rules for derivative of $\R$ to  $\R$ functions taught in most Calculus 1 classes. The proofs for the laws are easy, as it require no ingenious idea but ability to manipulate the limit symbol. However, without philosophical comments, we left an graph for geometric intuition for product rule. There are also graphs for geometric intuition for quotient rule on Internet, but we won't put it here as it require more than subtle work to understand the graph.    
\includegraphics[height=7cm,width=10cm]{product rule.png}
\end{mdframed}
\label{11}
\begin{theorem}
\textbf{(Product Rule and Quotient Rule for Real to Real Function)} Suppose $f$ and $g$ is differentiable at  $x$, and $g'(x)\neq 0$. We have 
\label{3}
\begin{enumerate}[label=(\alph*)]
  \item $(fg)'(x)=f'(x)g(x)+f(x)g'(x)$ (Product Rule)
  \item $(\frac{f}{g})'(x)=\frac{f'(x)g(x)-f(x)g'(x)}{\big(g(x) \big)^2}$ (Quotient Rule)
\end{enumerate}
\end{theorem}
\begin{proof}
Compute 
\begin{align*}
  (fg)'(x)&=\lim_{h\to_0} \frac{f(x+h)g(x+h)-f(x)g(x)}{h}\\
          &=\lim_{h\to 0} \frac{f(x+h)g(x+h)-f(x)g(x+h)+f(x)g(x+h)-f(x)g(x)}{h}\\
          &=\lim_{h\to0} \big(g(x+h) \big) \frac{f(x+h)-f(x)}{h} + \big(f(x) \big) \frac{g(x+h)-g(x)}{h}\\
          &=g(x)f'(x)+f(x)g'(x)
\end{align*}
Compute 
\begin{align*}
  (\frac{f}{g})'(x)&=\lim_{h\to0} \frac{\frac{f(x+h)}{g(x+h)}-\frac{f(x)}{g(x)}}{h}\\
  &=\lim_{h\to0} \frac{f(x+h)g(x)-f(x)g(x+h)}{h g(x+h)g(x)}\\
  &=\lim_{h\to0}\big(\frac{1}{g(x+h)g(x)} \big)\cdot \big(\frac{f(x+h)g(x)-f(x)g(x)+f(x)g(x)-f(x)g(x+h)}{h} \big)\\
  &=\lim_{h\to 0} \Big(\frac{1}{g(x+h)g(x)} \Big) \cdot \Big(\big(g(x) \big) \frac{f(x+h)-f(x)}{h} +\big(f(x) \big) \frac{g(x)-g(x+h)}{h}\Big)\\
  &=\Big(\frac{1}{\big(g(x) \big)^2} \Big)\cdot \Big(\big(g(x)f'(x) \big)-f(x)g'(x) \Big)=\frac{f'(x)g(x)-f(x)g'(x)}{\big(g(x) \big)^2}
\end{align*}
\end{proof}
\begin{mdframed}
  Even a year has past, I can still remember what happened in the first class of Vector Analysis last year. The professor asked: "What is derivative?". A lot of answers emerge, from extremely formal and abstract like $\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$ to those following geometric intuition like tangent line. Everyone gave a correct answer, but none of them philosophically satisfy the requirement of the question from the professor. Then, he stated: "Derivative is exactly linear approximation", and stated on black board the most general definition:


\end{mdframed}
\begin{definition}
\label{4}
  \textbf{(Definition of Differential)} Given two normed space $V,W$ and an open subset $U\subseteq V$,  we say a function $f:U\rightarrow W$ is \textbf{differentiable at $x$} if there exists a bounded linear operator $A:V\rightarrow W$ such that 
\begin{align*}
\lim_{ h\to 0} \frac{\norm{f(x+h)-f(x)-A(h)}_W}{\norm{h}_V}=0
\end{align*}
and we say the bounded linear map $A$ is the \textbf{(total) derivative} of $f$ at  $x$. 
\end{definition}
\begin{mdframed}
If one put the key words "proof for chain rule" in Google search box, just like the situation in my classes, lots of rigorous proof emerge, but none of them is philosophical satisfying. For this reason, I shall give a proof of chain rule for real to real function based on the concept of linear approximation.\\

In Baby Rudin, derivative of a real to real function $f$ is defined by 
\begin{align*}
f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}
\end{align*}
Immediately from this definition, we can derive a linear approximation $P$ of $f$ at $a$ by setting 
\begin{align*}
P(x)=f(a)+f'(a)(x-a)
\end{align*}
Then, we see if we set $R(x)=f(x)-P(x)$ as the error (or the remainder) of the approximation, then trivially we have the behavior
\begin{align*}
R(x)\to 0\text{ as $x\to a$ }
\end{align*}
what behavior of $R(x)$ that give $P$ the name approximation is 
\begin{align*}
\frac{R(x)}{x-a}\to 0\text{ as }x\to a
\end{align*}
The difference between the two behaviors is symbolically apparent, yet without geometric help, it may be difficult to precisely describe how insignificant the first behavior is compared to the second behavior. For this, observe that any function $g$ that converge to  $f(a)$ at $a$ satisfy the first behavior, yet only a few satisfy the second. One can easily verify that the only linear $\R$ to  $\R$ function that satisfy the second behavior is  $P(x)=f(a)+f'(a)(x-a)$. Geometrically, this means that $R(x)=o(f'(x)dx)$ as $x \to a$.
\label{12}
\end{mdframed}
\begin{theorem}
\textbf{(Chain Rule for $\R$ to $\R$ function)} Suppose $g$ is differentiable at  $a$ and  $f$ is differentiable at  $g(a)$. We have 
\begin{align*}
  (f\circ g)'(a)=f'(g(a))g'(a)
\end{align*}
\end{theorem}
\begin{proof}
Define the remainders $R_{f(g(a))}(x)$ and $R_{g(a)}(x)$ by 
\begin{align*}
\begin{cases}
R_{f(g(a))}(x)=f(x)-f(g(a))-f'(g(a))(x-g(a))\\
R_{g(a)}(x)=g(x)-g(a)-g'(a)(x-a)
\end{cases}
\end{align*}
Compute 
\begin{align}
\label{3.13}
  (f\circ g)'(a)&=\lim_{ x\to a}\frac{f(g(x))-f(g(a))}{x-a}\\
  &=\lim_{x\to a}\frac{R_{f(g(a))}(g(x))+f'(g(a))(g(x)-g(a))}{x-a}
\end{align}
Notice that because $x \to a \implies g(x)\to g(a)$, we have 
\begin{align}
\label{3.12}
\lim_{x\to a} \frac{R_{f(g(a))}(g(x))}{x-a}=\lim_{ x\to a} \frac{R_{f(g(a))}(g(x))}{g(x)-g(a)}\cdot \frac{g(x)-g(a)}{x-a}=0\cdot g'(x)=0
\end{align}
Notice that the above deduction (\myref{Equation}{3.12}) is quite informal for two reasons: First, it may happen that $g(x)=g(a)$ locally. Second, for some reader it may require a mini proof to verify that $\frac{R_{f(g(a))}(g(x))}{g(x)-g(a)}\to 0$ as $x\to a$. These two obstacles for advanced readers should be insignificant.\\

Getting back to \myref{Equation}{3.13}, by \myref{Equation}{3.12}, we now see 
\begin{align*}
  (f\circ g)'(a)=\lim_{x\to a}\frac{f'(g(a))(g(x)-g(a))}{x-a}=f'(g(a))g'(a)
\end{align*}
which finish the proof.
\end{proof}
\section{IVT, EVT and MVT}
\begin{mdframed}
  If one wish to understand most of the Theorems after this section, one must first know MVT, for what an important role it cast in the sections after. Logically prior to MVT is IVT. Yet, unlike MVT involve the intrinsic nature of field and limit structure of $\R$. IVT can be considered as purely topological in the sense that its proof can be stated almost in the language of topology. There are only two facts (the first are purely topological and the second is very close to purely topological) one need to know to prove IVT.\\

First, continuous functions map a connected sets to  connected set. Second, a set in $\R$ is connected if and only if it is an interval.\\

Combining the above two facts, we have the following statement:  
\label{13}
\end{mdframed}
\label{5}
\begin{theorem}
\textbf{(Continuous Real to Real Function Maps Interval to Interval)} as titled.
\end{theorem}
\begin{proof}
Consider the fact a continuous function map connected sets to connected sets and the fact a set in $\R$ is connected if and only if it is an interval.
\end{proof}
\begin{mdframed}
\label{14}
Then, given the necessary constraint (the interval considered is compact) to give the conclusion, we have the famous statement:
\label{6}
\end{mdframed}
\begin{theorem}
\textbf{(IVT)} Given a continuous function $f:[a,b]\to \R$, for each $y$ that lies between $f(a)$ and $f(b)$, there exists $x \in [a,b]$ such that $f(x)=y$.
\end{theorem}
\begin{mdframed}
Given the simplicity of the logical deduction, we shall not give a rigorous proof here. However, one can notice that the interval considered in IVT "must be" compact, otherwise the Theorem is invalid. This constraint is in some sense a showcase how the concept of compact really match  the description of "smallness (bounded) and rigidness (closed)". \\
\label{15}

\label{7}
Compared to IVT, another famous MVT is richer in both the results and the proof. Clearly for a logical and economic purpose, we shall first prove the Cauchy MVT.

\includegraphics[height=7cm,width=10cm]{CMVT.png}
\end{mdframed}
\begin{theorem}
\label{CMVT}
\textbf{(Cauchy's MVT)} Given a function $f:[a,b]\to \R$ such that  
\begin{enumerate}[label=(\alph*)]
  \item $f,g$ are  differentiable on $(a,b)$
  \item $f,g$ are continuous on $[a,b]$
\end{enumerate}
There exists $x \in (a,b)$ such that 
 \begin{align*}
   \big[f(b)-f(a) \big]g'(x)=\big[g(b)-g(a) \big]f'(x)
\end{align*}
\end{theorem}
\begin{proof}
We wish to find $x \in (a,b)$ such that  
\begin{align*}
\vi{\big[f(b)-f(a)]g'(x)-[g(b)-g(a)]f'(x)=0}
\end{align*}
Define $h$ on  $(a,b)$ by 
\begin{align*}
h(x)=\big[f(b)-f(a)\big]g'(x)-\big[g(b)-g(a)\big]f'(x)
\end{align*}
We reduced our problem into finding $x \in (a,b)$ such that 
\begin{align*}
\vi{h(x)=0}
\end{align*}
Because $f,g$ are both differentiable on  $(a,b)$, we know there exists an anti-derivative $H$ of  $h$ on  $(a,b)$ such that
\begin{align*}
H(x)=\big[f(b) -f(a)\big]g(x)-\big[g(b)-g(a) \big]f(x)
\end{align*}
We have $h=H'$ on  $(a,b)$. This let us reduce our problem into 
\begin{align*}
  \vi{\text{ finding a local extremum of $H$ on  $(a,b)$ }}
\end{align*}
Because $f,g$ are both continuous  on  $[a,b]$, we know $H$ is continuous on $[a,b]$. Then by EVT, we know 
 \begin{align*}
\exists x \in [a,b] , H(x)=\max_{t \in [a,b]}H(t)\text{ and }\exists y \in [a,b], H(y)=\min_{t \in [a,b]}H(t)
\end{align*}
If such $x,y$ is in $(a,b)$, we are done. If not, says that $x,y$ both are on end points $a$ or  $b$. Compute that 
\begin{align*}
H(a)=f(b)g(a)-g(b)f(a)=H(b)
\end{align*}
We see $H$ is constant on  $[a,b]$. Then all points in $(a,b)$ are extremums. $\vdone$
\end{proof}
\begin{corollary}
\label{MVT}
\textbf{(Lagrange's MVT)} Given a function $f:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable on  $(a,b)$ 
  \item $f$ is continuous on  $[a,b]$
\end{enumerate}
Then there exists $x \in (a,b)$ such that 
\begin{align*}
f'(x)=\frac{f(b)-f(a)}{b-a}
\end{align*}
\end{corollary}
\begin{proof}
Let $g(x)=x$ in Cauchy's MVT (\myref{Theorem}{CMVT}), and we are done.
\end{proof}
\begin{mdframed}
There are two hypotheses in Lagrange's MVT 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable on $(a,b)$ 
  \item $f$ is continuous on $[a,b]$
\end{enumerate}
They are all necessary. The necessity of differentiablity on $(a,b)$ is clear as shown by the canonical example using absolute value. The necessity of continuity on $[a,b]$ can be shown by the example 
\begin{align*}
f(x)=\begin{cases}
  1& \text{ if $a<x\leq b$ }\\
  0& \text{ if  }x=a
\end{cases}
\end{align*}
\end{mdframed}
\begin{theorem}
\textbf{(First Mean Value Theorem for Definite Integral)} Given a function $f:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous on $(a,b)$
\end{enumerate}
There exists $\xi \in (a,b)$ such that 
\begin{align*}
\int_a^b f(x)dx = f(\xi)\cdot (b-a)
\end{align*}
\end{theorem}
\begin{proof}
We wish 
\begin{align*}
\vi{\text{ to find  }\xi \in (a,b)\text{ such that }f(\xi)=\frac{\int_a^b f(x)dx}{b-a}}
\end{align*}
Define $\tilde{f}:[a,b]\rightarrow \R$ on $[a,b]$ by 
\begin{align}
\label{M1}
\tilde{f}(x)=\begin{cases}
  f(x)& \text{ if $x \in (a,b)$ }\\
  \lim_{t\to a}f(t)& \text{ if $x=a$ }\\
  \lim_{t\to b}f(t)& \text{ if $x=b$ }
\end{cases}
\end{align}
Then, because $\int_a^b f(x)dx=\int_a^b \tilde{f}(x)dx $, we reduce our problem into  
\begin{align*}
\vi{\text{ finding $\xi\in(a,b)$ such that $\tilde{f} (\xi)= \frac{\int_a^b \tilde{f}(x)dx }{b-a}$ }}
\end{align*}
Because $\tilde{f} $ is continuous on $[a,b]$ by definition \myref{Equation}{M1}, by EVT, we know we there exists $\alpha,\beta  \in [a,b]$ such that 
\begin{align}
\label{M4}
\tilde{f} (\alpha )=\inf_{x \in [a,b]}\tilde{f}(x)\text{ and } \tilde{f}(\beta ) =\sup_{x \in[a,b]}\tilde{f}(x) 
\end{align}
WOLG, suppose $\alpha \leq \beta $. Deduce 
\begin{align*}
\tilde{f}(\alpha )= \inf_{x \in [a,b]}\tilde{f}(x)\leq \frac{\int_a^b \tilde{f}(x)dx }{b-a} \leq \sup_{x \in [a,b]}\tilde{f}(x) =\tilde{f}(\beta ) 
\end{align*}
by IVT, we then know there exists $\xi\in [\alpha ,\beta ]$ such that 
\begin{align}
\label{M3}
\exists \xi\in [\alpha ,\beta ], \tilde{f}(\xi)=\frac{\int_a^b \tilde{f}(x)dx }{b-a} 
\end{align}
If $a<\alpha $ and $\beta <b$, our proof is done.\\


If not, notice that if $\tilde{f}(\alpha )=\tilde{f}(\beta )$, then by definition of $\alpha ,\beta $ (\myref{Equation}{M4}), the proof is trivial since $\tilde{f}$ is a constant, so we only have to consider when $\tilde{f}(\alpha )<\tilde{f}(\beta )  $, and we wish to show
\begin{align*}
\vi{\xi \text{ can not happen at $a$ nor $b$ }}
\end{align*}
\As{$\xi=a $, WOLG}. Because $\xi \in [\alpha ,\beta ]$, we know $\alpha =a$. Because $\tilde{f}(\beta )>\tilde{f}(\alpha )  $, we can find $\delta $ such that 
\begin{align}
\label{M5}
\inf_{x \in [\beta  -\delta, \beta  ]}\tilde{f}(x)\geq \frac{\tilde{f}(\alpha )+2\tilde{f}(\beta )  }{3}
\end{align}
We then from \myref{Equation}{M3} see that 
\begin{align}
\label{M6}
\int_a^b \tilde{f}(x)dx=\tilde{f}(\xi)(b-a)=\tilde{f}(\alpha ) (b-a)
\end{align}
Also, we see from definition of $\alpha $ (\myref{Equation}{M4}) and \myref{Equation}{M5} that 
\begin{align}
\int_a^b \tilde{f}(x)dx&=\int_a^{\beta -\delta} \tilde{f}(x)dx + \int_{\beta -\delta}^{\beta  } \tilde{f}(x)dx+\int_{\beta }^{b}\tilde{f}(x)dx \\
&\geq (b -\delta -a) \tilde{f}(\alpha ) + \delta \cdot  \big( \frac{\tilde{f}(\alpha )+2\tilde{f}(\beta )  }{3}\big)\\
&> (b -\delta -a) \tilde{f}(\alpha ) + \delta \cdot  \big( \frac{\tilde{f}(\alpha )+\tilde{f}(\beta )  }{2}\big)\\
&=\tilde{f}(\alpha ) \big(b-a-\frac{\delta}{2} \big) + \tilde{f}(\beta ) \cdot \big( \frac{\delta}{2}\big)
 \label{M7}
\end{align}
Now, from \myref{Equation}{M6} and \myref{Equation}{M7}, we can deduce  
\begin{align*}
\tilde{f}(\alpha )(b-a)> \tilde{f}(\alpha )(b-a-\frac{\delta}{2}) +\tilde{f}(\beta ) \cdot \big(\frac{\delta}{2} \big)
\end{align*}
Then we can deduce
\begin{align*}
\tilde{f}(\alpha )\cdot \big(\frac{\delta}{2} \big)>\tilde{f}(\beta )\cdot \big(\frac{\delta}{2} \big) \tCaC \vdone
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(Second Mean Value Theorem for Definite Integral)} Given functions $G,\phi :[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $G$ is monotonic
  \item $\phi$ is Riemann-Integrable
\end{enumerate}
Let $G(a^+)=\lim_{t\to a^+}G(t)$ and $G(b^-)=\lim_{t\to b^-}G(t)$. Then there exists $\xi \in (a,b)$ such that 
\begin{align*}
\int_a^b G(t)\phi (t)dt= G(a^+)\int_a^{\xi} \phi(t)dt+G(b^-)\int_\xi^{b}\phi (t)dt
\end{align*}
\end{theorem}
\begin{proof}
Define $f$ on  $[a,b]$ by 
\begin{align*}
f(x)=G(a^+)\int_a^x \phi(t)dt+G(b^-)\int_x^b \phi(t)dt
\end{align*}
We then reduce the problem into 
\begin{align*}
\vi{\text{ finding }\xi \in (a,b)\text{ such that }\int_a^b G(t)\phi (t)dt=f(\xi)}
\end{align*}
By \myref{Theorem}{FTC1}, we know $f$ is continuous on  $[a,b]$. Then by IVT, we can reduce the problem into 
\begin{align*}
\vi{\text{ finding an interval $[c,d]\subseteq (a,b)$ such that }\int_a^b G(t)\phi (t)}\text{ is between $f(c) $ and $f(d)$ }
\end{align*}


Observe that 
\begin{align*}
f(a)=G(b^-)\int_a^b \phi(t)dt\text{ and }f(b)=G(a^+)\int_a^b \phi(t)dt 
\end{align*}




\end{proof}




\section{Riemann-Stieltjes on Computation}
\begin{theorem}
\label{CoV}
\textbf{(Change of Variable)} Given two functions $g,\beta  :[A,B]\rightarrow \R$, a function $\phi: [A,B]\rightarrow [a,b]$ and two functions $f,\alpha :[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $g = f \circ  \phi $ for all $x \in [a,b]$
  \item $\beta = \alpha  \circ \phi $ for all $x \in [a,b]$ 
  \item $\alpha , \beta  $ increase respectively on $[a,b]$ and $[A,B]$ 
  \item $\phi:[A,B]\rightarrow [a,b]$ is a homeomorphism 
  \item $\int_a^b fd\alpha $ exist
\end{enumerate}
Then 
\begin{align*}
\int_A^B g d\beta  =\int_a^b fd\alpha \text{   (This implies $\int_A^B gd\beta $ exists)}
\end{align*}
\end{theorem}          
\begin{proof}
Fix $\epsilon $. We only wish 
\begin{align*}
  \vi{\text{ to find a partition $Q$ of  $[A,B]$ &such that $U(Q,g,\beta )-L(Q,g,\beta )<\epsilon  $  }}\\
  \vi{\text{ and such that $\int_a^b fd\alpha \in \Big[L(Q,g,\beta ),U(Q,g,\beta ) \Big]$  }}
\end{align*}
Because $\int_a^b fd\alpha $ exists, we know 
\begin{align}
\label{CR1}
\text{ there exists a partition $P$ of  $[a,b]$ such that }U(P,f,\alpha )-L(P,f,\alpha )<\epsilon 
\end{align}
where, of course, $\int_a^b fd\alpha  \in \Big[L(P,f,\alpha ),U(P,f,\alpha )\Big]$.\\

Let $P=\set{a=x_0,x_1,\dots ,x_n=b}$. Because $\phi$ is a homeomorphism, we can let $\phi$ be strictly increasing WOLG.\\ 

Define a partition $Q$ on  $[A,B]$ by 
\begin{align*}
Q=\phi^{-1}[P]=\set{A=\phi^{-1}(x_0),\phi^{-1}(x_1),\dots ,\phi^{-1}(x_n)=B}
\end{align*}
Now, because $\beta =\alpha \circ  \phi$ and $g=f\circ \phi$  for all $x\in [a,b]$ by premise, and because $\phi$ is a homeomorphism, we have 
\begin{align}
  U(Q,g,\beta )&=\sum_{k=1}^n \big[  \sup_{t \in [\phi^{-1}(x_{k-1}),\phi^{-1}(x_k)]} g(t)\big] \big[\beta (\phi^{-1}(x_{k})) - \beta  (\phi^{-1}(x_{k-1}))\big]\notag\\
  &=\sum_{k=1}^n  \big[  \sup_{t \in [\phi^{-1}(x_{k-1}),\phi^{-1}(x_k)]} f \circ  \phi (t)\big] \big[\alpha \circ \phi  \big(\phi^{-1}(x_{k})\big) - \alpha \circ \phi \big(\phi^{-1}(x_{k-1})\big)\big]\notag\\
  &=\sum_{k=1}^n \big[\sup_{t \in [x_{k-1},x_k]}f(t) \big] \big(\alpha (x_k)-\alpha (x_{k-1}) \big)=U(P,f,\alpha )
\label{CR2}
\end{align}
Similarly, we can deduce $L(Q,g,\beta )=L(P,f,\alpha )$. Now, from \myref{Equation}{CR2} and by definition of $P$ (\myref{Equation}{CR1}), we see 
\begin{align*}
&U(Q,g,\beta )-L(Q,g,\beta )=U(P,f,\alpha )-L(P,f,\alpha )<\epsilon \\
  \text{ and }&\int_a^b fd\alpha \in \Big[L(P,f,\alpha ),U(P,f,\alpha ) \Big]=\Big[ L(Q,g,\beta ),U(Q,g,\beta ) \Big]\vdone
\end{align*}



\end{proof}
\begin{theorem}
\label{RRSI}
\textbf{(Reduction of Riemann-Stieltjes Integral: Part 1)} Given two functions $f,\alpha :[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $\alpha $ increase on $[a,b]$ 
  \item $\alpha $ is differentiable on $(a,b)$ 
  \item $\lim_{x\to b^-}\frac{\alpha (x)-\alpha (b)}{x-b}$ exists and $\lim_{x\to a^+}\frac{\alpha (x)-\alpha (a)}{x-a}$ exists
  \item $\alpha '$ is properly Riemann-Integrable on $[a,b]$  
  \item $f$ is bounded on  $[a,b]$
\end{enumerate}
Then 
\begin{align*}
\int_a^b fd\alpha \text{ exists }\iff  \int_a^b f(x)\alpha '(x)dx\text{ exists and they equal to each other if exists}
\end{align*}
\end{theorem}
\begin{proof}
We wish to prove 
\begin{align*}  \vi{\overline{\int_a^b}fd\alpha = \overline{\int_a^b} f(x)\alpha '(x)dx}
\end{align*}
Fix $\epsilon $. We reduce the problem into proving 
\begin{align*}  \vi{\abso{\overline{\int_a^b}fd\alpha -\overline{\int_a^b}f(x)\alpha '(x)dx}<\epsilon }
\end{align*}
Then, because for all partition $P$ of  $[a,b]$, we have 
\begin{align*}  &\abso{\overline{\int_a^b}fd\alpha - \overline{\int_a^b}f(x)\alpha '(x)dx}\\
  &\leq \abso{\overline{\int_a^b} fd\alpha -U(P,f,\alpha )}-\Bigg|U(P,f,\alpha )-U(P,f\alpha' )\Bigg|-\abso{U(P,f\alpha' )- \overline{\int_a^b} f(x)\alpha '(x)dx}
\end{align*}
We only wish 
\begin{align*}
\vi{\text{  to find $P$ such that}}&\vi{\abso{\overline{\int_a^b}fd\alpha - U(P,f,\alpha )}<\frac{\epsilon}{3}}\\
\vi{\text{ and }}&\vi{\Bigg|U(P,f,\alpha )-U(P,f\alpha' )\Bigg|<\frac{\epsilon}{3}\text{ and }\abso{\overline{\int_a^b}f(x)\alpha '(x)dx- U(P,f\alpha ')}<\frac{\epsilon}{3}}
\end{align*} 
Because $f$ is bounded on  $[a,b]$, we can let $M=\sup_{x \in [a,b]} \abso{f(x)}$. Because $\int_a^b \alpha' (x)dx$ exists, we can let $P$ satisfy 
 \begin{align}
\label{CR3}
U(P,\alpha ')-L(P,\alpha ')<\frac{\epsilon }{4M}
\end{align}
By definition of Riemann Upper sum, we can further refine $P$ to let $P$ satisfy 
\begin{align*}
\abso{\overline{\int_a^b}fd\alpha -U(P,f,\alpha )}<\frac{\epsilon}{3}\text{ and }\abso{\overline{\int_a^b}f(x)\alpha '(x)dx- U(P,f\alpha ')}<\frac{\epsilon}{3}
\end{align*}
It is clear that the statement concerning $P$  (\myref{Equation}{CR3}) remain valid after refinement of $P$. Fix such $P$. We now have reduced the problem into proving 
\begin{align*}
  \vi{\abso{U(P,f,\alpha )-U(P,f\alpha ')}<\frac{\epsilon}{3}}
\end{align*}
Express $P$ in the form $P=\set{a=x_0,x_1,\dots ,x_n=b}$. By MVT (\myref{Theorem}{MVT}), we know for all $k \in \set{1,\dots ,n}$ there exists $t_k \in [x_{k-1},x_k]$ such that
\begin{align}
\label{CR4}
\Delta \alpha_k=  \alpha' (t_k)\Delta x_k
\end{align}
Then, because $U(P,\alpha ')-L(P,\alpha )'<\frac{\epsilon}{3M}$ (\myref{Equation}{CR3}), we now see 
\begin{align}
  \label{CR5}
\sum_{k=1}^n \abso{\alpha '(s_k)-\alpha '(t_k)} \Delta x_k<\frac{\epsilon}{3M} \text{ if $s_k \in [x_{k-1},x_k]$ for all $k \in \set{1,\dots , n}$ }
\end{align}
Then from \myref{Equation}{CR4}, definition of $M$ and \myref{Equation}{CR5}, we have 
\begin{align*}
\abso{\sum_{k=1}^n f(s_k)\Delta \alpha_k - \sum_{k=1}^n f(s_k)\alpha '(s_k)\Delta x_k}&=\abso{\sum_{k=1}^n f(s_k)\big(\alpha '(s_k)-\alpha '(t_k) \big)\Delta x_k}\\
&\leq \sum_{k=1}^n \abso{f(s_k)}\cdot \abso{\alpha '(s_k)-\alpha '(t_k)}\Delta x_k\\
&\leq M \sum_{k=1}^n \abso{\alpha '(s_k)-\alpha '(t_k)}\Delta x_k \\
&<\frac{\epsilon}{4}
\end{align*}
Then because $\sum_{k=1}^m f(s_k)\alpha '(s_k)\Delta x_k\leq  U(P,f\alpha ')$, we now have 
\begin{align}
\label{CR6}
\sum_{k=1}^n f(s_k)\Delta \alpha_k < U(P,f\alpha ')+\frac{\epsilon}{4}
\end{align}
Because \myref{Equation}{CR6} hold true for all choices of  $s_k$, we have 
\begin{align*}
U(P,f,\alpha )<  U(P,f\alpha ')+\frac{\epsilon}{3}
\end{align*}
Similarly, we can deduce 
\begin{align*}
U(P,f\alpha ')<U(P,f,\alpha )+\frac{\epsilon}{3}\vdone
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(Substitution Law)} Given a function $\phi: [a,b]\rightarrow [A,B]$ and a function $f:[A,B]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $\phi$ is a homoeomorphism. 
  \item $\phi$ is differentiable on $(a,b)$ 
  \item $\int_a^b \phi' (x)dx$ exists.  
  \item $f$ is integrable on  $[A,B]$
\end{enumerate}
We have 
\begin{align*}
\int_a^b f\big(\phi (x) \big)\phi' (x)dx=\int_A^B f(u)du
\end{align*}
\end{theorem}
\begin{proof}
Because $f\circ \phi$ and  $\phi'$ is integrable on $[a,b]$, by reduction of Riemann-Stieljes Integral (\myref{Tehroem}{RRSI}), we know 
\begin{align*}
\int_a^b \big(f\circ \phi)(x)  \phi'(x)dx= \int_a^b \big(f\circ \phi \big)(x)d \phi
\end{align*}
Let $\alpha (x)=x$. Let $\beta = \alpha \circ  \phi$. Define $g=f \circ  \phi $. By Change of Variable (\myref{Theorem}{CoV}), we now have  
\begin{align*}
\int_a^b \big(f \circ  \phi \big)(x)d \phi =\int_a^b g(x)d\beta = \int_A^B f(x)dx
\end{align*}
\end{proof}
\section{Weierstrass approximation Theorem: $[a,b]\rightarrow \R$}
\begin{theorem}
\label{Bernoulli's Inequality}
\textbf{(Bernoulli's Inequality)} Given $r,x \in\R$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $r\geq 1$ 
  \item $x\geq -1$
\end{enumerate}
Then
\begin{align*}
  (1+x)^r\geq 1+rx
\end{align*}
\end{theorem}
\begin{proof}
Fix $r\geq 1$. We wish 
\begin{align*}
\vi{\text{ to prove }(1+x)^r\geq 1+rx\text{ for all  }x \geq -1}
\end{align*}
Define $f:[-1,\infty)\rightarrow \R$ by 
 \begin{align}
\label{Bere1}
f(x)=(1+x)^r-(1+rx)
\end{align}
We reduced the problem into  
\begin{align*}
\vi{\text{ proving }f(x)\geq 0\text{ for all } x\geq -1}
\end{align*}
Because $r\geq 1$ by premise, by definition of $f(x)$  (\myref{Equation}{Bere1}), we see that 
\begin{align*}
f(0)=0\text{, and }f(-1)=r-1\geq 0
\end{align*}
Notice that by definition of $f$  (\myref{Equation}{Bere1}),  $f(x)$ is clearly differentiable on $(-1,\infty)$.\\

Then, by MVT (\myref{Theorem}{MVT}), to prove $f(x)\geq 0$ on $(-1,\infty)$, we only wish 
\begin{align*}
\vi{\text{ to prove }f'(x)\geq 0\text{ for all }x> 0\text{ and }f'(x)\leq 0\text{ for all $x \in (-1,0)$ }}
\end{align*}
Compute $f'$
 \begin{align*}
f'(x)&=r(1+x)^{r-1}-r\\
&=r\Big((1+x)^{r-1}-1 \Big)
\end{align*}
Because $r\geq 1$, we can deduce 
\begin{align*}
x>0 \implies (1+x)^{r-1}\geq 1 \implies f'(x)=r\Big((1+x)^{r-1}-1 \Big)\geq 0
\end{align*}
and deduce 
\begin{align*}
x \in (-1,0) \implies 1+x \in (0,1) \implies (1+x)^{r-1}\leq 1 \implies f'(x)=r\Big((1+x)^{r-1}-1 \Big)\leq  0
\end{align*}
$\vdone$
\end{proof}
\begin{mdframed}
In this section, notation  $\mathcal{C}\big([a,b] \big)$ means the set of \textbf{real-valued continuous function on $[a,b]$}.
\end{mdframed}
\begin{theorem}
\textbf{(Weierstrass approximation Theorem: $[a,b]\rightarrow \R$)} Let $\R[x]\big|_{[a,b]}$ be the space of polynomials on $[a,b]$ with real coefficient. We have 
\begin{align*}
\text{ $\R[x]\big|_{[a,b]}$ is dense in $\Big(\mathcal{C}\big([a,b] \big),\norm{\cdot}_{\infty} \Big)$ }
\end{align*}
\end{theorem}
\begin{proof}
WOLG, we can let $[a,b]=[0,1]$. The reason we can assume such is explained at last. Now, let $f:[0,1]\rightarrow \R$ be a continuous function. Fix $\epsilon $. We only wish 
\begin{align*}
\vi{\text{ to find $P \in \R[x]\big|_{[0,1]}$ such that $\norm{f-P}_{\infty}<\epsilon $}}
\end{align*}
Define $\tilde{f} \in \mathcal{C}\big([0,1] \big)$ by 
\begin{align}
  \label{tse1}
\tilde{f}(x)= f(x)-f(0)-x \big[f(1)-f(0) \big] 
\end{align}
It is easy to check $\tilde{f}$ is continuous. We first prove that 
\begin{align*}
\blue{\big( \tilde{f}(x)-f(x)\big)\in \R[x]\big|_{[0,1]}  }
\end{align*}
By definition of $\tilde{f} $ (\myref{Equation}{tse1}), we see 
\begin{align*}
\tilde{f}(x)-f(x)=(f(0)-f(1))x- f(0)\in \R[x]\big|_{[0,1]}\bdone
\end{align*}
This reduce our problem into 
\begin{align*}
\vi{\text{ finding }P\in \R[x]\big|_{[0,1]}\text{ such that $\norm{\tilde{f}-P}_{\infty}<\epsilon $ }}
\end{align*}
Notice that by definition of $\tilde{f}$ (\myref{Equation}{tse1}), we have 
\begin{align*}
\tilde{f}(0)=0=\tilde{f}(1) 
\end{align*}
Then, we can expand the definition of $\tilde{f} $  by
\begin{align}
\label{tse2}  
\tilde{f}(x)=\begin{cases}
  \tilde{f} (x)& \text{ if $x\in [0,1]$ }\\
  0& \text{ if $x \not \in [0,1]$ }
\end{cases}
\end{align}
This makes $\tilde{f}$ uniformly continuous on $\R$, since  $\tilde{f}$ is uniformly continuous on $[0,1]$ and $[0,1]^c$. Now, for each $n\inn$, define $Q_n \in \R[x]$ by 
\begin{align}
\label{tse3}
Q_n=c_n(1-x^2)^n\text{ where $c_n$ is chosen to satisfy }\int_{-1}^1 Q_n(x)dx=1
\end{align}
Define $P_n:[0,1]\rightarrow \R$ by 
\begin{align*}
P_n(x)=\int_{-1}^1 \tilde{f} (x+t)Q_n(t)dt
\end{align*}
We now prove 
\begin{align*}
\olive{P_n \in \R[x]\big|_{[0,1]}}
\end{align*}
Because $\tilde{f}(x)=0$ for all $x \not \in (0,1)$ by definition of $\tilde{f} $ (\myref{Equation}{tse2}), we see that 
\begin{align}
\label{tse4}
P_n(x)=\int_{-x}^{1-x}\tilde{f}(x+t)Q_n(t)dt \text{ for all }x \in [0,1]
\end{align}
Fix $x \in [0,1]$. Now, by change of variable, we see 
\begin{align*}
P_n(x)=\int_{-x}^{1-x} \tilde{f}(x+t)Q_n(t)dt=\int_{0}^1 \tilde{f}(u)Q_n(u-x)du  
\end{align*}
Because $Q_n$ is a polynomial by definition (\myref{Equation}{tse3}), we can express $Q_n(u-x)$ by 
\begin{align*}
Q_n(u-x)=\sum_{k=0}^{m} a_k x^k\text{ for some $\set{a_0,\dots ,a_m}$ depending on $u$}
\end{align*}
Then we see 
\begin{align*}
  P_n(x)=\int_0^1 \tilde{f}(u)Q_n(u-x)du= \sum_{k=0}^m x^k \Big( 
\int_0^1 \tilde{f}(u) a_kdu
  \Big)  
\end{align*}
This shows that $P_n \inr[x]\big|_{[0,1]}$

$\odone$\\


Now, because $\tilde{f}$ is uniformly continuous on $\R$, we can fix $\delta<1$ such that 
\begin{align}
\label{tse5}
\forall x,y \inr, \abso{x-y}<\delta \implies \abso{\tilde{f}(x)-\tilde{f}(y)  }<\frac{\epsilon}{2}
\end{align}
By definition of $\tilde{f}$ (\myref{Equation}{tse2}), we know $\tilde{f} $ is a bounded function. Then we can set $M$ by 
\begin{align*}
M=\sup_{x \inr} \abso{f(x)}
\end{align*}
Let $n$ satisfy 
 \begin{align}
  \label{tse5}
4M \sqrt{n} (1-\delta^2)^n < \frac{\epsilon}{2} 
\end{align}
Such $n$ exists, because  $\delta<1 \implies  \sqrt{n}(1-\delta^2)^n \to 0 $. We claim 
\begin{align*}
\vi{\text{ $P_n$ satisfy $\norm{\tilde{f}-P_n}_{\infty}<\epsilon $}}
\end{align*}
We first prove 
\begin{align*}
\blue{c_n< \sqrt{n} }
\end{align*}
By Bernoulli's Inequality (\myref{Theorem}{Bernoulli's Inequality}). Compute 
\begin{align*}
1=\int_{-1}^1 Q_n(x)dx&=  c_n\int_{-1}^1 (1-x^2)^n dx \\
&=2c_n\int_0^1 (1-x^2)^n dx\\
&\geq 2c_n\int_0^{\frac{1}{\sqrt{n} }}(1-x^2)^n dx\\
&\geq 2c_n \int_0^{\frac{1}{\sqrt{n} }} 1-nx^2dx=c_n\big(\frac{4}{3\sqrt{n} } \big)> c_n (\frac{1}{\sqrt{n} })
\end{align*}


This implies 
\begin{align*}
\sqrt{n}>c_n \bdone
\end{align*}
Because $\sqrt{n}>c_n $, by definition of $Q_n$  (\myref{Equation}{tse3}), we have 
\begin{align*}
Q_n(x)<\sqrt{n}(1-x^2)^n \leq \sqrt{n}(1-\delta^2)^n \text{ for all $x$ such that  $\delta \leq \abso{x}\leq 1$ }
\end{align*}
Fix $x \in [0,1]$. Finally, because 
\begin{enumerate}[label=(\alph*)]
  \item $\int_{-1}^1 Q_n(x)dx=1$ by definition of  $Q_n$  (\myref{Equation}{tse3})
  \item $Q_n(x)=c_n(1-x^2)^n\geq 0$ for all $x \in [-1,1]$ 
  \item $\abso{\tilde{f}(x+t)-\tilde{f}(x)}<\frac{\epsilon}{2} $ for all $t$ such that $\abso{t}<\delta $, by definition of $\delta $ (\myref{Equation}{tse5})
  \item $Q_n(x)\leq \sqrt{n}(1-\delta^2)^n $ for all $x$ such that  $\delta\leq \abso{x}\leq 1$
  \item $4M\sqrt{n}(1-\delta^2)^n<\frac{\epsilon}{2} $ by definition of $n$  (\myref{Equation}{tse5})
\end{enumerate}
we have
\begin{align*}
\abso{P_n(x)-\tilde{f}(x)}&=\abso{\int_{-1}^1 \tilde{f}(x+t)Q_n(t)dt- \tilde{f}(x)}\\
&=\abso{\int_{-1}^1 \tilde{f}(x+t)Q_n(t)dt- \tilde{f}(x)\int_{-1}^1 Q_n(t)dt }\\
&=\abso{\int_{-1}^1 \tilde{f}(x+t)Q_n(t)dt -\int_{-1}^1 \tilde{f}(x)Q_n(t)dt}\\
&=\abso{\int_{-1}^1 \big[\tilde{f}(x+t)-\tilde{f}(x)   \big]Q_n(t)dt}\\
&\leq \int_{-1}^1 \abso{\big[ \tilde{f}(x+t)-\tilde{f}(x)\big]Q_n(t)}dt\\
&=\int_{-1}^1 \abso{\tilde{f}(x+t)-\tilde{f}(x)  }Q_n(t)dt\\
&\leq \int_{-1}^{-\delta} 2MQ_n(t)dt +\int_{-\delta}^{\delta} \abso{\tilde{f}(x+t)-\tilde{f}(x)  }Q_n(t)dt+\int_{\delta}^1 2M Q_n(t)dt\\
&\leq 2M\Big(\int_{-1}^{-\delta}Q_n(t)dt+\int_{\delta}^1 Q_n(t)dt  \Big)+ \int_{-\delta}^\delta \big(\frac{\epsilon}{2} \big)Q_n(t)dt\\
&\leq 4M(1-\delta) \sqrt{n}(1-\delta^2)^n+ \frac{\epsilon}{2} \\
&\leq 4M\sqrt{n}(1-\delta^2)^n+\frac{\epsilon}{2} <\epsilon  
\end{align*}
Because $x$ is arbitrarily picked from  $[0,1]$, we now have $\norm{P_n-\tilde{f} }_{\infty}<\epsilon \vdone$\\

Lastly, we show 
\begin{align*}
\olive{\text{ our result can be transplanted to arbitrary $\mathcal{C}\big([a,b] \big)$ }}
\end{align*}
Let $[a,b]$ be arbitrary. Fix $\epsilon $ and $f \in \mathcal{C}\big([a,b] \big)$. We wish 
\begin{align*}
\olive{\text{ to find $P \in \R[x]\big|_{[a,b]}$ such that $\norm{f-P}_\infty\leq \epsilon $ }}
\end{align*}
Define $g:[0,1]\rightarrow \R$ by 
\begin{align}
g(x)\triangleq f(a+(b-a)x)
\end{align}
We know there exists $P_n:[0,1]\rightarrow \R$ such that 
\begin{align*}
\norm{P_n-g}_{\infty}<\epsilon 
\end{align*}
Define $H_n:[a,b]\rightarrow \R$ by 
\begin{align*}
H_n(x)=P_n\big(\frac{x-a}{b-a} \big)
\end{align*}
Because $P_n$ is a real polynomial on  $[0,1]$, we know $H_n$ is a real polynomial on $[a,b]$. We now claim 
\begin{align*}
  \olive{\text{ such $H_n\text{ works }$ }}
\end{align*}
Fix $x \in [a,b]$. Observe 
\begin{align*}
\big|f(x)-H_n(x)\big|&=\abso{f(x)-P_n\big(\frac{x-a}{b-a} \big)}\\
&=\abso{g\big(\frac{x-a}{b-a} \big)-P_n\big(\frac{x-a}{b-a} \big)}< \epsilon \odone
\end{align*}
\end{proof}
\begin{mdframed}
It is at now, we will show that every real-valued continuous functions on $[a,b]$ can be approximated by polynomials with rational coefficient. This fact enable our computer to more easily approximate real-valued continuous function on $[a,b]$.\\

Note that since $\mathcal{C}\big([a,b] \big)$ is a separable metric space, we can show that $\mathcal{C}\big([a,b] \big)$ has cardinality of at most continuum $\mathfrak{c}$ . 
\end{mdframed}
\begin{theorem}
\textbf{(The space $\Q[x]|_{[a,b]}$ is dense in $\Big( \mathcal{C}\big([a,b] \big),\norm{\cdot}_{\infty}\Big)$, thus $\mathcal{C}\big([a,b] \big)$ is separable)} 
\begin{align*}
\Big( \mathcal{C}\big([a,b] \big),\norm{\cdot}_{\infty}\Big)\text{ is separable }
\end{align*}
\end{theorem}
\begin{proof}
Because $\Q[x]\big|_{[a,b]}$ is countable, to show $\mathcal{C}\big([a,b] \big)$ is separable, we only wish to show 
\begin{align*}
\vi{\Q [x]\big|_{[a,b]}\text{ is dense in }\mathcal{C}\big([a,b] \big)}
\end{align*}
Because $\R[x]\big|_{[a,b]}$ is dense in $\mathcal{C}\big([a,b] \big)$, we reduce our problem into proving 
\begin{align*}
\vi{\Q[x]\big|_{[a,b]}\text{ is dense in }\R[x]\big|_{[a,b]}}
\end{align*}
Fix $\epsilon \text{ and } P \in \R[x]\big|_{[a,b]}$. We must
\begin{align*}
\vi{\text{ find $Q \in \Q[x]\big|_{[a,b]}$ such that $\norm{Q-P}_\infty \leq \epsilon $}}
\end{align*}
Express $P(x)=\sum_{k=0}^n r_kx^k$. Let $M> \max \set{\abso{a},\abso{b}}$. Because $\Q$ is dense in $\R$, we know there exists $c_k \inq$ such that  $\abso{c_k-r_k}< \frac{\epsilon }{(n+1)M^n}$. We claim 
\begin{align*}
\vi{Q(x)=\sum_{k=0}^n c_kx^k\text{ works }}
\end{align*}
Fix $x \in [a,b]$. See
\begin{align*}
\abso{P(x)-Q(x)}&=\abso{\sum_{k=0}^n (c_k-r_k)x^k}\\
&\leq \sum_{k=0}^n \abso{c_k-r_k}\cdot \abso{x}^k\\
&\leq \sum_{k=0}^n \abso{c_k-r_k}\cdot M^k\\
&\leq (M^n)\sum_{k=0}^n \abso{c_k-r_k}\\
&< M^n (n+1)\big( \frac{\epsilon }{(n+1)M^n}\big)=\epsilon \vdone
\end{align*}
\end{proof}
\section{The Stone-Weierstrass Theorem}
\begin{mdframed}
Recall that a \textbf{vector space over a field $\F$} is a set $V$ equipped with \textbf{vector addition} $+:V\times V\to V$  and \textbf{scalar multiplication}  such that 
\begin{enumerate}[label=(\alph*)]
  \item $(V,+)$ is an abelian group. 
  \item Scalar multiplication is compatible with field multiplication:   $\Big((ab)v=a(bv) \Big)$
  \item Scalar multiplication is distributive: $\Big((a+b)v=av+bv\text{ and }a(v+w)=av+aw \Big)$
\end{enumerate}

There are many ways to define the term \textbf{algebra over a field $\F$}. One can exhaust all the laws an algebra should obey. In short, an \textbf{algebra over a field $\F$} (or \textbf{$\F$-algebra}) is a set $(A,+,\cdot )$ equipped scalar multiplication over $\F$ such that 
\begin{enumerate}[label=(\alph*)]
  \item Multiplication $\cdot $ is $\underline{\text{distributive}}$ with respect to $+$  
  \item $(A,+)$ and scalar multiplication form a vector space. 
   \item Scalar multiplication and vector multiplication $\cdot $ is compatible: $\Big((av)\cdot (bw)=ab(v\cdot w) \Big)$
\end{enumerate}
Given an arbitrary set $E$ and a field $\F$, let $A$ be the set of all functions from  $E$ to $\F$. The following is a list of some algebra 
\begin{enumerate}[label=(\alph*)]
  \item $(\R^3,\text{cross product})$  over $\R$ 
  \item $(\C,\text{complex multiplication})$ over $\C$ 
  \item $(\Q[x],\text{function multiplication})$ over $\Q$
  \item $(\text{Functions from }E\text{ to }\F,\text{function multiplication})$ over $\F$
  \item $(\text{Continuous functions from $(E,\tau)$ to $\C$ },\text{function multiplication})$ over $\C$  
  \item $(\text{Linear transformation from $V$ to $V$, composition})$ over $\F$ where  $V$ is over $\F$
  \item $\big(M_n(\F),\text{matrix multiplication}\big)$ over $\F$
\end{enumerate}
Note that $B=(\text{continuous functions from }\C\text{ to }\C,\text{composition})$ over $\C$ is not an algebra, even though $B$ is both a vector space and a ring. ($\because$ scalar multiplication and multiplication are not compatible).\\

It is at here we shall introduce some general terminologies. Given an arbitrary set $E$, a field  $\F$ and a point $x \in E$, we say a family $\mathcal{F}$ of functions  from $E$ to  $\F$  \textbf{vanish at $x$} if for all $f \in \mathcal{F}$, we have $f(x)=0$. We say $\mathcal{F}$ \textbf{separate points} in $E$ if for all  $x_2\neq x_1 \in E$, there exists $f\in \mathcal{F}$ such that $f(x_2)\neq f(x_1)$. 
\end{mdframed}

\section{FTC}
\begin{theorem}
  \label{FTC1}
\textbf{(Fundamental Theorem of Calculus: Part 1)} Suppose a function $f:[a,\infty )\rightarrow \R$ satisfy
\begin{align*}
f\text{ is $\underline{\text{proper-Riemann integrable}}$ on $[a,b]$  for all $b>a$}
\end{align*}
If we set $F:[a,\infty) \to \R$ 
\begin{align*}
F(x)=\int_a^x f(t)dt
\end{align*}
Then 
\begin{enumerate}[label=(\alph*)]
  \item $F$ is continuous on $[a,\infty )$
  \item $F$ is differentiable at  $x_0 \in [a,\infty )$  where $F'(x_0)=f(x_0)$ if $f$ is continuous at $x_0$
\end{enumerate}
\end{theorem}
\begin{proof}
Fix $\epsilon $ and $[a,b]$. We only wish  
\begin{align*}
\vi{\text{ to prove }F\text{ is continuous on $[a,b]$ }}
\end{align*}
To prove $F$ is continuous on  $[a,b]$, we only wish 
\begin{align*}
\vi{\text{ to find $\delta$ such that $\forall [x,y]\subseteq [a,b], \abso{x-y}<\delta \implies \abso{F(x)-F(y)}<\epsilon $}}
\end{align*}
Because $f $ is proper-Riemann-Integrable on $[a,b]$, we know $f$ is bounded on  $[a,b]$. Let $M$ be an upper bound of  $\abso{f}$ on $[a,b]$. We claim 
\begin{align*}
\vi{\text{ $\delta=\frac{\epsilon}{M}$ works }} 
\end{align*}
Because $y-x <\delta=\frac{\epsilon}{M}$, we have
\begin{align*}
  \abso{F(x)-F(y)}&=\abso{\int_x^y f(t)dt}\\
  &\leq \int_x^y \abso{f(t)}dt\\
  &\leq (y-x)<\epsilon \vdone
\end{align*}
Now, to prove $F'(x_0)=f(x_0)$, we wish 
\begin{align*}
  \blue{\text{ to prove $\lim_{x\to x_0} \frac{F(x)-F(x_0)}{x-x_0}= f(x_0)$ }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\blue{\text{ to find $\delta$ such that $\abso{x-x_0}<\delta \implies \abso{\frac{F(x)-F(x_0)}{x-x_0}-f(x_0)}<\epsilon $ }}
\end{align*}
Because $f$ is continuous at $x_0$, we know  
\begin{align}
\label{F1}
\exists \delta, \abso{x-x_0}<\delta \implies \abso{f(x)-f(x_0)}<\epsilon 
\end{align}
We claim 
\begin{align*}
\blue{\text{ such $\delta$ in}}\text{ \myref{Equation}{F1} }\blue{\text{works}}
\end{align*}
WOLG, let $x>x_0$. Deduce 
\begin{align*}
\abso{\frac{F(x)-F(x_0)}{x-x_0}-f(x_0)}&=\abso{\frac{\int_{x_0}^x f(t)dt}{x-x_0}-f(x_0)}\\                  &=\abso{\frac{\int_{x_0}^x \big[f(t)-f(x_0) \big]dt}{x-x_0}}\\
&\leq \frac{\int_{x_0}^x \abso{f(t)-f(x_0)}dt}{\abso{x-x_0}}\\
&\leq \frac{\int_{x_0}^x \epsilon dt}{\abso{x-x_0}}=\epsilon \bdone
\end{align*}
\end{proof}
\begin{theorem}
\label{FTC2}
\textbf{(Fundamental Theorem of Calculus: Part 2, Leibniz Rule)} Suppose two functions $f,F:[a,\infty )\rightarrow \R$ satisfy 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is proper Riemann-Integrable on $[a,b]$ for all $b>a$ 
\item $F'(x)=f(x)$ for all $x \in (a, \infty)$ 
\item $F$ is continuous on  $[a,\infty)$
\end{enumerate}
Then for all $b>a$, 
\begin{align*}
\int_a^b f(x)dx=F(b)-F(a)
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to show that $\abso{\Big( F(b)-F(a)\Big)-\int_a^b f(x)dx}<\epsilon $ }}
\end{align*}
Because $f$ is proper Riemann-Integrable on $[a,b]$, we know there exists a partition $P=\set{a=x_0,x_1,\dots , x_n=b}$ of  $[a,b]$ such that 
\begin{align}
\label{FP}
U(P,f)-L(P,f)<\epsilon 
\end{align}
Because $f=F'$ on  $(a,b)$, for each $k \in \set{1,\dots ,n}$, by MVT (\myref{Theorem}{MVT}), we know 
\begin{align*}
\exists t_k \in (x_{k-1},x_k), \frac{F(x_k)-F(x_{k-1})}{x_k-x_{k-1}}=f(t_k)
\end{align*}
This let us deduce
\begin{align*}
F(b)-F(a)=\sum_{k=1}^n F(x_k)-F(x_{k-1})=\sum_{k=1}^n f(t_k) \Delta x_k
\end{align*}
Now, we have 
\begin{align*}
  \int_a^b f(x)dx\text{ and }F(b)-F(a)\text{ are both in }\big[ L(P,f),U(P,f) \big]
\end{align*}
Then by \myref{Equation}{FP}, we can deduce 
\begin{align*}
\abso{F(b)-F(a)- \int_a^b f(x)dx}<\epsilon \vdone
\end{align*}




\end{proof}
\begin{theorem}
\textbf{(Integral By Part)} Given four function $f,g,F,G:[a,b]\rightarrow \R$ such that 

\begin{enumerate}[label=(\alph*)]
  \item $F'(x)=f(x)\text{ and }G'(x)=g(x)$ for all $x\in (a,b)$ 
  \item $f,g$ are properly Riemann-Integrable on  $[a,b]$ 
  \item $F,G$ are continuous on  $[a,b]$
\end{enumerate}
We have
\begin{align}
\label{FI}
\int_a^b F(x)g(x)dx=FG\Big|^b_a-\int_a^b f(x)G(x)dx
\end{align}
\end{theorem}
\begin{proof}
To prove \myref{Equation}{FI}, we only with 
\begin{align*}
\vi{\text{ to prove }\int_a^b F(x)g(x)dx+\int_a^b f(x)G(x)dx=FG\Big|_a^b}
\end{align*}
We can reduce the problem 
\begin{align*}
\vi{\text{ into proving }\int_a^b \big( Fg+fG \big) dx=FG\Big|_a^b}
\end{align*}
Notice that by Chain Rule,  
\begin{align*}
\big(FG \big)'(x)=F(x)g(x)+f(x)G(x)\text{ for all $x \in (a,b)$ }
\end{align*}
Then the result follows from Part 2 of Fundamental Theorem of Calculus (\myref{Theorem}{FTC2}). $\vdone$
\end{proof}
\section{Uniform Convergence on Integration and Differentiation}
\begin{theorem}
\label{RIFac}
\textbf{(Riemann-Integration and Uniform Convergence)} Given a function $\alpha :[a,b]\rightarrow \R$ and a sequence of functions $f_n:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $\alpha $ increase on $[a,b]$ 
  \item $\int_a^b f_nd\alpha $ exists for all $n\inn$ 
  \item $f_n \to f $ uniformly on $[a,b]$ 
\end{enumerate}
Then 
\begin{align*}
  \lim_{n\to \infty}\int_a^b f_n d\alpha \text{ exists and }\int_a^b fd\alpha =\lim_{n\to \infty}\int_a^b f_nd\alpha 
\end{align*}
\end{theorem}
\begin{proof}
We first prove 
\begin{align*}
\vi{\int_a^b fd\alpha \text{ exists }}
\end{align*}
Fix $\epsilon $. We wish to prove 
\begin{align*}
\vi{\overline{\int_a^b}fd\alpha - \underline{\int_a^b}fd\alpha < \epsilon }
\end{align*}
Let $\epsilon _n = \norm{f_n-f}_\infty$. Because $f_n \to f$ uniformly, we know 
\begin{align*}
\text{ there exists $n\inn$ such that $\epsilon _n=\norm{f_n-f}_\infty < \frac{\epsilon }{2\big[\alpha (b)-\alpha (a) \big]}$ }
\end{align*}
Because $\alpha $ increase, by definition of $\epsilon _n$, we see 
\begin{align*}
\int_a^b (f_n-\epsilon _n)d\alpha \leq \underline{\int_a^b}fd\alpha \leq \overline{\int_a^b}fd\alpha \leq \int_a^b (f_n+\epsilon_n) d\alpha  
\end{align*}
Because $\epsilon _n <\frac{\epsilon}{2\big[\alpha (b)-\alpha (a) \big]}$, we now see 
\begin{align*}
  \overline{\int_a^b}fd\alpha -\underline{\int_a^b}fd\alpha &\leq \int_a^b (f_n+\epsilon _n)d\alpha -\int_a^b (f_n-\epsilon _n)d\alpha \\
&=\int_a^b (2\epsilon _n)d\alpha<2 \epsilon_n \cdot \big[\alpha (b)-\alpha (a) \big] = \epsilon \vdone
\end{align*}
We now prove 
\begin{align*}
  \blue{\int_a^b f_n d\alpha \to \int_a^b fd\alpha \text{ as $n \to \infty $ }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\blue{\text{ to find $N$ such that }\forall n>N,\abso{\int_a^b f_nd\alpha-\int_a^b fd\alpha }<\epsilon  }
\end{align*}
Recall the definition $\epsilon_n= \norm{f_n-f}_\infty$. Because $\epsilon _n \to 0$, we know 
\begin{align}
\label{CuU1}
\text{ there exists $N$ such that }\forall n>N, \epsilon_n < \frac{\epsilon }{\alpha (b)-\alpha (a)}
\end{align}
We claim 
\begin{align*}
\blue{\text{ such $N$ works }}
\end{align*}
Fix $n>N$. From \myref{Equation}{CuU1}, we see
 \begin{align*}
  \abso{\int_a^b f_nd\alpha -\int_a^b fd\alpha }&=\abso{\int_a^b (f_n-f)d\alpha }\\
  &\leq \int_a^b \abso{f_n-f}d\alpha \\
  &\leq \int_a^b \epsilon_n d\alpha =\epsilon_n \big[\alpha (b)-\alpha (a) \big]<\epsilon \bdone
\end{align*}


\end{proof}
\begin{mdframed}
Before the next Theorem, let's see three examples why this time we don't (can't) use the hypothesis: $f_n \to f$ uniformly. 
\begin{Example}{\textbf{(Differentiable functions are NOT closed under uniform convergence)}}{}
\begin{align*}
X=[-1,1]\text{ and }f(x)=\abso{x}
\end{align*}
By Weierstrass approximation Theorem, there is a sequence of polynomials (differentiable) that uniformly converge to $f$, which is not differntiable at  $0$. 
\end{Example}
\begin{Example}{\textbf{(Derivative won't necessarily converge to the right place)}}{}
\begin{align*}
X=\R \text{ and }f_n(x)=\frac{\sin nx}{\sqrt{n} }
\end{align*}
Compute 
\begin{align*}
f'(x)=0 \text{ and }f'_n(x)=\sqrt{n} \cos nx
\end{align*}
\end{Example}
\begin{Example}{\textbf{(Derivative won't necessarily converge to the right place)}}{}
\begin{align*}
X=\R \text{ and }f_n(x)=\frac{x}{1+nx^2}
\end{align*}
Compute 
\begin{align*}
f=\tilde{0} \text{ and }f'_n(0)=1
\end{align*}
\end{Example}
Informally speaking, these examples together with the fact integral are closed under uniform convergence (\myref{Theorem}{RIFac}) should give you some ideas that differentiation and integration although are operations inverse to each other, are NOT symmetric. There is a certain hierarchy on continuous functions on a fixed compact interval. Thus, we have the next Theorem in its form. Note that in application, the next Theorem only require us to prove $f'_n$ uniformly converge, and doesn't require us to prove to where does it converge. 
\end{mdframed}
\begin{theorem}
\label{UCaD}
\textbf{(Uniform Convergence and Differentiation)} Given a sequence of function $f_n:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f'_n$ uniformly converge on  $(a,b)$
  \item $f_n$ are continuous on  $[a,b]$
  \item $f_n(x_0)\to L$ for some $x_0 \in [a,b]$
\end{enumerate}
Then 
\begin{enumerate}[label=(\alph*)]
  \item $f_n$ uniformly converge on  $[a,b]$ 
  \item and
\begin{align*}
\Big(\lim_{n\to \infty}f_n \Big)'(x_0)=\lim_{n\to \infty}f_n'(x_0)\text{ on $(a,b)$ }
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\begin{align}
\label{fnun}
\vi{f_n\text{ uniformly converge on $[a,b]$}}
\end{align}
Fix $\epsilon $. We wish  
\begin{align*}
  \vi{\text{ to find $N$ such that $\norm{f_n-f_m}_\infty \leq \epsilon $ for all $n,m>N$}}
\end{align*}
Because $f_n(x_0)$ converge, and $f'_n$ uniformly converge, we know there exists $N$ such that 
 \begin{align}
\label{UCD1}
\begin{cases}
 \abso{f_n(x_0)-f_m(x_0)}<\frac{\epsilon}{2} \\
\norm{f_n'-f_m'}_\infty <\frac{\epsilon }{2(b-a)}
\end{cases}\text{ for all $n,m>N$ }
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Fix $x \in [a,b]$ and $n,m>N$. We need
\begin{align*}
  \vi{\text{ to show }\abso{f_n(x)-f_m(x)}\leq \epsilon}
\end{align*}
We first prove
\begin{align*}
\olive{\abso{f_n(x)-f_m(x)-f_n(x_0)+f_m(x_0)}\leq \frac{\epsilon}{2}}
\end{align*}
Because $(f_n-f_m)'=f_n'-f_m'$, by MVT (\myref{Theorem}{MVT}) and \myref{Equation}{UCD1}, we can deduce 
\begin{align*}
 \abso{f_n(x)-f_m(x)-f_n(x_0)+f_m(x_0)}&=\abso{(f_n-f_m)(x)-(f_n-f_m)(x_0)}\\
 &=\Big|\big[(f_n-f_m)'(t)\big](x-x_0)\Big|\text{ for some $t$ between $x,x_0$ }\\
 &< \frac{\epsilon}{2(b-a)}\cdot \abso{x-x_0}\\
 &\leq \frac{\epsilon }{2(b-a)}\cdot (b-a)=\frac{\epsilon}{2}\hspace{0.3cm}\big(\because x,x_0  \in [a,b]\big)\odone
\end{align*}
Now, by \myref{Equation}{UCD1}, we have 
\begin{align*}
  \abso{f_n(x)-f_m(x)}&\leq \abso{f_n(x)-f_m(x)-f_n(x_0)+f_m(x_0)}+\abso{f_n(x_0)-f_m(x_0)}\\
&<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon \vdone
\end{align*}
We claim 
\begin{align}
\label{UCAC2}
\blue{f(x)\triangleq \lim_{n\to \infty}f_n(x)\text{ satisfy }f'(x)=\lim_{n\to \infty}f'_n(x)\text{ on $(a,b)$ }}
\end{align}
We first show 
\begin{align*}
\olive{f\text{ is differentiable on $(a,b)$ }}
\end{align*}
Fix $x \in (a,b)$. We wish to prove
\begin{align*}
  \olive{{\lim_{t\to x}\frac{f(t)-f(x)}{t-x}\text{ exists }}}
\end{align*}
Define $\phi :[a,b]\setminus x\rightarrow \R$ by 
\begin{align*}
\phi (t)\triangleq \frac{f(t)-f(x)}{t-x}
\end{align*}
We reduce our problem into proving 
\begin{align*}
  \olive{\lim_{t\to x}\phi (t)\text{ exists }}
\end{align*}
Set $\phi_n:[a,b]\setminus x\rightarrow \R$ by 
\begin{align*}
\phi_n(t)\triangleq \frac{f_n(t)-f_n(x)}{t-x}
\end{align*}
We first show  
\begin{align}
\label{UCACu}
  \vi{\phi_n\text{ uniformly converge on }[a,b]\setminus x}
\end{align}
Fix $\epsilon $. We have
\begin{align*}
  \vi{\text{ to find $N$ such that  $\abso{\phi_n(t)-\phi _m(t)}\leq \epsilon $ for all $n,m>N$ and  $t\in [a,b]\setminus x$ }}
\end{align*}
Because $f_n'$ uniformly converge on  $[a,b]$, we know there exists $N$ such that 
 \begin{align}
\label{CUaC1}
\norm{f_n'-f_m'}_\infty\leq \epsilon \text{ for all $n,m>N$ }
\end{align}
We claim 
\begin{align*}
\vi{\text{ such }N\text{ works }}
\end{align*}
Fix $n,m>N$ and $t \in [a,b]\setminus x$. We wish to prove 
\begin{align*}
\vi{\abso{\phi_n(t)-\phi_m(t)}\leq \epsilon }
\end{align*}
Because $(f_n-f_m)'=f_n'-f'_m$, by MVT (\myref{Theorem}{MVT}) and \myref{Equation}{CUaC1}, we can deduce 
\begin{align*}
  \abso{\phi_n(t)-\phi_m(t)}&\leq \abso{\frac{f_n(t)-f_n(x)}{t-x}-\frac{f_m(t)-f_m(x)}{t-x}}\\
                            &=\abso{\frac{\big(f_n-f_m\big)(t)-\big(f_n-f_m\big)(x)}{t-x}}\\
 &=\abso{\big(f'_n-f'_m\big)(t_0)}\text{ for some $t_0$ between $t,x$  }\\
&\leq \epsilon \vdone
\end{align*}
We now show 
\begin{align}
\label{UCACP}
\brown{\phi_n \to \phi\text{ pointwise on $[a,b]\setminus x$}}
\end{align}
Because $f_n \to f$ on $[a,b]$ by definition (\myref{Equation}{UCAC2}), (the convergence is in fact uniform as we have shown. This doesn't matter here tho), for each $t \in [a,b]\setminus x$, we can deduce
\begin{align*}
\lim_{n\to \infty} \phi_n(t)&=\lim_{n\to \infty}\frac{f_n(t)-f_n(x)}{t-x}=\frac{f(t)-f(x)}{t-x}=\phi (t)\bodone
\end{align*}
Now, by \myref{Equation}{UCACu} and \myref{Equation}{UCACP}, we know 
\begin{align*}
\phi_n \to \phi \text{ uniformly on $[a,b]\setminus x$ }
\end{align*}
Notice that because $f_n'(x)$ converge, we know
\begin{align*}
\lim_{n\to \infty}\lim_{t\to x}\phi_n(t)=\lim_{n\to \infty}f'_n(x)\text{ exists }
\end{align*}
Then (Notice that the second equality below hold true because we have known $\lim_{n\to \infty}\lim_{t\to x}\phi_n(t)$ exists), we can finally deduce 
\begin{align*}
\lim_{t\to x}\phi (t)&=\lim_{t\to x}\lim_{n\to \infty}\phi_n(t)\\
&=\lim_{n\to \infty}\lim_{t\to x}\phi_n(t)\\
&=\lim_{n\to \infty}f'_n(x)\text{ exists }\odone
\end{align*}
Now, notice that $f'(x)=\lim_{t\to x}\phi (t)$, so in fact, we have just proved $f'_n \to f'\odone \bdone$



\end{proof}
\begin{mdframed}
As Rudin remarked, a much shorter (and much more intuitive) proof can be given, if we require $f'$ to be continuous on  $[a,b]$. 
\end{mdframed}
\begin{theorem}
\textbf{(Uniform Convergence and Differentiation: Weaker Version)} Given a sequence of function $f_n:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f'_n$ uniformly converge on  $[a,b]$
  \item $f_n(x_0)\to L$ for some $x_0 \in [a,b]$
  \item $f_n$ are of class $C^1$ on $[a,b]$ 
\end{enumerate}

Then 
\begin{enumerate}[label=(\alph*)]
  \item $f_n$ uniformly converge on  $[a,b]$ 
  \item and
\begin{align*}
\frac{d}{dx}\Big(\lim_{n\to \infty}f_n(x) \Big)\Big|_{x=x_0}=\lim_{n\to \infty}f_n'(x_0)\text{ on $(a,b)$ }
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
We claim 
\begin{align*}
  \vi{f(x)=\lim_{n\to \infty}\int_{x_0}^x f'_n(t)dt+L\text{ works }} 
\end{align*}
Note that $\lim_{n\to \infty}\int_{x_0}^x f'_n(t)dt$ exists because $f'_n$ uniformly converge  (\myref{Theorem}{RIFac}).\\

Because $f'_n$ uniformly converge and are continuous on $[a,b]$, by ULT, we know
 \begin{align*}
\int_{x_0}^x \lim_{n\to \infty}f'_n(t)dt+L\text{ exists }
\end{align*}
and know 
\begin{align*}
f(x)=\int_{x_0}^x \lim_{n\to \infty}f'_n(t)dt + L 
\end{align*}
By FTC, we see
 \begin{align*}
f'(x)=\lim_{n\to \infty}f'_n(x)\text{ on }(a,b)
\end{align*}
Such convergence is uniform by premise. To finish the proof, we now only have to prove 
\begin{align*}
\vi{f_n\to f\text{ uniformly on }[a,b]}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find $N$ such that  $\abso{f_n(x)-f(x)}\leq \epsilon $ for all $n>N$ and  $x \in [a,b]$}}
\end{align*}
Because $f'_n \to f'$ uniformly, and $f_n(x_0) \to L=f(x_0)$ (Check $L=f(x_0)$), we know there exists $N$ such that 
 \begin{align*}
\begin{cases}
  \norm{f'_n-f'}_\infty < \frac{\epsilon }{2(b-a)}\\
  \abso{f_n(x_0)-f(x_0)}<\frac{\epsilon}{2}
\end{cases}\text{ for all $n>N$ }
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Fix $n>N$ and  $x\in [a,b]$. Observe 
\begin{align*}
\abso{f(x)-f_n(x)}&= \abso{\int_{x_0}^x \big(f'(t)-f'_n(t) \big)dt+f(x_0)-f_n(x_0)}\\
&\leq \int_{x_0}^x \abso{f'(t)-f'_n(t)}dt+ \abso{f(x_0)-f_n(x_0)}\\
&\leq \frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon \vdone
\end{align*}
\end{proof}
\section{Fundamental Tests*}
\begin{theorem}
\textbf{(LCT)}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Geometric Series)}
\end{theorem}
\begin{theorem}
\textbf{(p-Series)}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Limit Superior Lemma 1)} 
\begin{align*}
d_n \text{ converge }\implies \limsup_{n\to\infty} c_nd_n=\lim_{n\to \infty}d_n\limsup_{n\to\infty} c_n
\end{align*}
\end{theorem}
\begin{theorem}
\textbf{(Limit Superior Lemma 2)} 
\begin{align*}
\limsup_{n\to\infty} \sqrt[n]{c_n} =\limsup_{n\to\infty} \sqrt[n]{c_{n+k}} 
\end{align*}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Ratio Test)}
\end{theorem}
\begin{theorem}
\textbf{(Root Test)}
\end{theorem}
\begin{theorem}
\textbf{(Root Test is Stronger Than Ratio Test)}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Absolute Convergent Series Converge)}
\end{theorem}
\begin{theorem}
\textbf{(Absolute Convergent Series Unconditionally Converge)}
\end{theorem}
\begin{theorem}
\textbf{(Fubini's Theorem for Infinite Series)}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Summation by Part)}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Alternating Series Test)}
\end{theorem}
\begin{theorem}
\textbf{(Abel's Test)}
\end{theorem}
\begin{theorem}
\textbf{(Dirichlet's Test)}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\label{Merten Cau}
\textbf{(Merten's Theorem for Cauchy Product)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $\sum_{n=0}^\infty a_n$ converge absolutely 
  \item $\sum_{n=0}^\infty a_n=A$
  \item $\sum_{n=0}^\infty b_n=B$ 
  \item $c_n=\sum_{k=0}^n a_kb_{n-k}$
\end{enumerate}
Then we have 
\begin{align*}
\sum_{n=0}^{\infty}c_n=AB
\end{align*}
\end{theorem}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Special Sequence)} $(n!)^{\frac{1}{n}}$
\end{theorem}
\section{Analytic Functions}
\begin{mdframed}
In this section, by a  \textbf{real power series}, we mean a pair $(a,c_n)$ where $a \inr$ is called the \textbf{center} of power series, and $c_n\inr$ are the coefficients sequence. By \textbf{radius of convergence}, we mean a unique  $R \in \R^+_0 \cup  \infty$ such that 
 \begin{align*}
\sum_{n=0}^\infty c_n(x-a)^n
\begin{cases}
  \text{ converge absolutely }& \text{ if  }\abso{x-a}<R\\
  \text{ diverge }& \text{ if  }\abso{x-a}>R\\
\end{cases}
\end{align*}
Such $R$ always exist (and is unique, this fact can be checked without computing the actual value of $R$) and is exactly 
\begin{align}
\label{Cauchy-Hadamard}
R=\frac{1}{\limsup_{n\to\infty} \sqrt[n]{c_n} }
\end{align}
This result is called \textbf{Cauchy-Hadamard Theorem}. It can be directly proved by applying Root Test to $\sum c_n(z-a)^n$. For this, we say $(a-R,a+R)$ is the \textbf{interval of convergence}. Note that Cauchy-Hadamard Theorem does not tell us whether a power series converges at points of boundary of disk of convergence. It require extra works to determine if the power series converge at endpoints. Following is an example of such discussion.   
\begin{Example}{\textbf{(Discussion of Convergence on Boundary)}}{}
\begin{align*}
f_q(z)=\sum_{n=0}^\infty n^q z^n \text{ provided $q\inr$ }
\end{align*}
It is clear that  $f_q$ has convergence radius  $1$ for all  $q\inr$. For boundary, we have
\begin{align*}
\begin{cases}
  q<-1 \implies f_q\text{ converge on $S^1$ }\\
  -1\leq q<0 \implies f_q\text{ converge on }S^1\setminus \set{1}\\
  0 \leq q \implies f_q \text{ diverge on }S^1
\end{cases}
\end{align*}
At $z=1$, the discussion is just p-series. On $S^1\setminus \set{1}$, the discussion use Dirichlet's Test, where boundedness of $\sum^n_{k=0} z^k$ is proved by geometric formula.  
\begin{align*}
\abso{\sum_{k=1}^n z^k}&=\abso{\sum_{k=1}^n e^{ik\theta}}=\frac{\abso{e^{i\theta}-e^{i(n+1)\theta}}}{\abso{1-e^{i\theta}}}\leq \frac{2}{\abso{1-e^{i\theta}}}
\end{align*}
\end{Example}
Notice that the fact $\sum c_n(z-a)^n$ absolutely converge in $(a-R,a+R)$ implies the convergence is uniform on all $[a-R+\epsilon ,a+R-\epsilon ]$ by M-Test. However, on $(a-R,a+R)$, the convergence is not always uniform. 
\begin{Example}{\textbf{(Failure of Uniform Convergence on $(a-R,a+R)$)}}{}
\begin{align*}
f(z)=\sum_{n=0}^\infty z^n
\end{align*}
Note $R=1$. Use Geometric Series Formula to show $f(x)=\frac{1}{1-x}$ on $(-1,1)$. It is then clear that $f$ is unbounded on  $(-1,1)$ while all partial sums $\sum_{k=0}^n z^k$ is bounded on $(-1,1)$. 
\end{Example}
We now introduce some terminologies. We say a real function $f$ is \textbf{real analytic at} $a\inr$ if there exists a power series $(a,c_n)$ such that $f$ agrees with $\sum_{n=0}^\infty c_n(z-a)^n$ on $(a-R,a+R)$ for some $R$  (of course, such $R$ must not be strictly greater than the radius of convergence of $(a,c_n)$).\\

It shall be quite clear that if $f,g$ are both analytic at  $a\inr$ with radius $R_f\leq R_g$, then $f+g$ and $fg$ are both analytic at  $a$ with radius at least $R_f$. (the fact $fg$ is analytic at $a$ with radius at least $R_f$ is an immediate consequence of  Merten's Theorem)\\

We now investigate deeper into real analytic functions. We first prove that real analytic functions are smooth, that is,  $C^{\omega}(I)\subseteq C^{\infty}(I)$ on open $I\subseteq \R$. 
\end{mdframed}
\begin{theorem}
\label{AfaS}
\textbf{(Analytic functions are Smooth)} Given a power series $(a,c_n)$ of convergence radius $R$, if we define $f:D_R(a)\rightarrow \C$ by
\begin{align*}
f(z)=\sum _{n=0}^{\infty}c_n(z-a)^n 
\end{align*}
Then 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is of class $C^{\infty}$ on $D_R(a)$ 
  \item $f^{(k)}(z)=\sum_{n=k}^{\infty}\frac{n!}{(n-k)!}c_n(z-a)^{n-k}$
\end{enumerate}
\end{theorem}
\begin{proof}
We prove by induction. Base case $k=0$ is trivial. Fix $k\geq 0$. Suppose we have 
\begin{align*}
f^{(k)}(z)=\sum_{n=k}^{\infty} \frac{n!}{(n-k)!}c_n(z-a)^{n-k}\text{ on $D_R(a)$ }
\end{align*}
We are required to prove 
\begin{align*}
\vi{f^{(k+1)}(z)=\sum_{n=k+1}^\infty \frac{n!}{(n-k-1)!}c_n(z-a)^{n-k-1}\text{ on }D_R(a)}
\end{align*} 
Set $f_m$ 
 \begin{align*}
f_m(z)\triangleq \sum_{n=k}^{k+m} \frac{n!}{(n-k)!}c_n(z-a)^{n-k}
\end{align*}
We have 
\begin{align}
\label{PS1}
f_m \to f^{(k)}\text{ pointwise on $D_R(a)$ and }f'_m (z)= \sum_{n=k+1}^{k+m}\frac{n!}{(n-k-1)!}c_n(z-a)^{n-k-1}
\end{align}
We abstract our problem into proving 
\begin{align*}
  \vi{f'_m \to f^{(k+1)}\text{ pointwise on $D_R(a)$ }}
\end{align*}
Fix $z_0 \in D_R(a)$. We only wish to prove 
\begin{align*}
\vi{(f^{(k)})'(z_0)=\lim_{m\to \infty}f'_m(z_0)}
\end{align*}
Fix $\epsilon $ such that $\abso{z_0-a}< R-\epsilon $. By \myref{Equation}{PS1}, using \myref{Theorem}{UCaD} (Uniform Convergence and Differentiaiton). We only have to prove 
\begin{align*}
  \vi{f'_m\text{ uniformly converge on $\overline{D}_{R-\epsilon }$ }}
\end{align*}
Note that 
\begin{align*}
f'_m(z)=\sum_{n=0}^{m-1} \frac{(n+k+1)!}{n!}c_{n+k+1}(z-a)^n
\end{align*}
so we can compute the radius of convergence for $f'_m$
\begin{align*}
\limsup_{n\to\infty} \sqrt[n]{\frac{(n+k+1)!}{n!}\abso{c_{n+k+1}}}&=\limsup_{n\to\infty} \sqrt[n]{\abso{c_{n+k+1}}} \\
&=\limsup_{n\to\infty} \sqrt[n]{\abso{c_n}}=R 
\end{align*}
Together by Cauchy-Hadamrd (absolute convergent on $a+R-\epsilon $) and M-test show that 
\begin{align*}
\sum_{n=0}^\infty \frac{(n+k+1)!}{n!}c_{n+k+1}(z-a)^n \text{ uniformly converge on $\overline{D}_{R-\epsilon }(a)$ }\vdone
\end{align*}
\end{proof}
\begin{mdframed}
Now by \myref{Theorem}{AfaS}, if we are given a real function $f$ analytic at $a$, the power series representation $(a,c_n):\sum_{n=0}^\infty c_n(z-a)^n=f$ must be unique, since $f$ is proved to be infinitely differentiable at $a$ and proved to satisfy $c_k=\frac{f^{(k)}(a)}{k!}$.\\

Notice that the arguments above are all based on the hypothesis that $f$ is analytic, and that smoothness does not imply analytic. See the following examples.
\end{mdframed}
\begin{mdframed}
\begin{Example}{\textbf{(Smooth but not Analytic Function)}}{}
\begin{align*}
f(x)=\begin{cases}
  e^{\frac{-1}{x^2}}& \text{ if $x\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}
\end{align*}
Use induction to show that 
\begin{align*}
f^{(k)}(x)=P_k(\frac{1}{x})e^{-(\frac{1}{x})^2}\hspace{0.5cm}\exists P_k\inr[x^{-1}],\forall k\inz^+_0,\forall x\inr^*
\end{align*}
and again use induction to show that  
\begin{align*}
f^{(k)}(0)=0\hspace{0.5cm}\forall k\inz_0^+
\end{align*}
The trick to show $f^{(k)}(0)=0$ is let $u=\frac{1}{x}$.\\

Now, with \myref{Theorem}{AfaS}, we see that $f$ is not analytic at $0$. 
\end{Example}
\begin{Example}{\textbf{(Bump Function)}}{}
\begin{align*}
f(x)=\begin{cases}
  e^{\frac{-1}{1-x^2}}& \text{ if $\abso{x}<1$ }\\
  0& \text{ otherwise }
\end{cases}
\end{align*}
Use the same trick (but more advanced) to show $f$ is smooth, and note that $f$ is not analytic at $\pm 1$. 
\end{Example}
\end{mdframed}
\begin{mdframed}
Now, it comes an interesting question. Given a real function $f$ analytic at $a$ with radius  $R$, and suppose $b \in (a-R,a+R)$.
\begin{enumerate}[label=(\alph*)]
  \item Is $f$ also analytic at $b$?
   \item What do we know about the radius of convergence of $f$ at $b$?
   \item Suppose $f$ is indeed analytic at $b$. It is trivial to see that the power series $(a,c_{a;n})$ and $(b,c_{b;n})$ must agree on the common convergence interval, and because $f$ is given, we by \myref{Theorem}{AfaS}, have already known the value of $c_{b;n}$. Can we verify that the power series  $(a,c_{a;n})$ and $(a,c_{b;n})$ do indeed agree with each other on the common convergence interval?
\end{enumerate}
\myref{Theorem}{TT} (Taylor's Theorem) give satisfying answers to these problems. 
\end{mdframed}
\begin{theorem}
\label{TT}
  \textbf{(Taylor's Theorem)} Given a real function $f$ analytic at $a$ with radius $R$, and suppose $b\in (a-R,a+R)$. Then 
\begin{align*}
f(x)=\sum_{k=0}^\infty \frac{f^{(k)}(b)}{k!}(x-b)^k\text{ on $\abso{x-b}<R-\abso{b-a}$ }
\end{align*}
\end{theorem}
\begin{proof}
WOLG, let $a=0$. Suppose $x$ satisfy $\abso{x-b}<R-\abso{b}$. Compute 
 \begin{align*}
f(x)&=\sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}x^k\\
&=\sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}(x-b+b)^k\\
&=\sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}\sum_{n=0}^k \binom{k}{n} (x-b)^n b^{k-n}\\
&=\sum_{k=0}^\infty \sum_{n=0}^k \frac{f^{(k)}(a)}{k!}\binom{k}{n}(x-b)^n b^{k-n}
\end{align*}
Note that 
\begin{align*}
  \sum_{k=0}^\infty \abso{\sum_{n=0}^\infty \frac{f^{(k)}(a)}{k!}\binom{k}{n}(x-b)^n b^{k-n}}&\leq \sum_{k=0}^\infty \sum_{n=0}^\infty \abso{\frac{f^{(k)}(a)}{k!}}\binom{k}{n}\abso{x-b}^n \cdot \abso{b}^{k-n}\\
  &=\sum_{k=0}^{\infty} \abso{\frac{f^{(k)}(a)}{k!}}\sum_{n=0}^{\infty}\binom{k}{n}\abso{x-b}^{n}\cdot \abso{b}^{k-n}\\
  &=\sum_{k=0}^\infty \abso{\frac{f^{(k)}(a)}{k!}} \big(\abso{x-b}+\abso{b} \big)^k
\end{align*}
converge, by Cauchy-Hadamard Theorem and $\abso{x-b}+\abso{b}<R$.\\

Now, using Fubini's Theorem for Infinite Series, we have 
\begin{align*}
\sum_{k=0}^\infty \sum_{n=0}^{k} \frac{f^{(k)}(a)}{k!}\binom{k}{n}(x-b)^{n}b^{k-n}&=\sum_{k=0}^\infty \sum_{n=0}^{\infty} \frac{f^{(k)}(a)}{k!} \binom{k}{n}(x-b)^n b^{k-n}\\
&=\sum_{n=0}^\infty \sum_{k=0}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}(x-b)^n b^{k-n}\\
&=\sum_{n=0}^\infty \sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}(x-b)^n b^{k-n}\\
&=\sum_{n=0}^\infty \Big[ \sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}b^{k-n}\Big] (x-b)^n\\
\end{align*}
We have reduced the problem into proving 
\begin{align*}
\vi{\sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}b^{k-n}=\frac{f^{(n)}(b)}{n!}}
\end{align*}
Using the formula in \myref{Theorem}{AfaS}, because $b$ is in $(a-R,a+R)$, we can compute 
\begin{align*}
f^{(n)}(b)&=\sum_{k=n}^{\infty}\frac{k!}{(k-n)!}c_{a;k}(b)^{k-n}\\
&=\sum_{k=n}^\infty \frac{k!}{(k-n)!} \cdot \frac{f^{(k)}(a)}{k!} \cdot b^{k-n}\\
&=\sum _{k=n}^{\infty}\frac{f^{(k)}(a)}{(k-n)!}b^{k-n}
\end{align*}
This now implies 
\begin{align*}
\frac{f^{(n)}(b)}{n!}&=\sum_{k=n}^\infty \frac{f^{(k)}(a)}{n!(k-n)!}b^{k-n}\\
&=\sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!}\binom{k}{n}b^{k-n}\vdone
\end{align*}

\end{proof}
\section{Abel's Theorem and its application}
\begin{mdframed}
In this section, we use the notation $\mathbb{S}_M(R)$ to denote \textbf{stolz region}
\begin{align*}
\mathbb{S}_M(R)\triangleq \set{z\inc: \frac{\abso{R-z}}{R-\abso{z}}\in (0,M)}
\end{align*}
\end{mdframed}
\begin{theorem}
\textbf{(Abel's Theorem for Power Series)} Given a complex Maclaurin series $f(z)=\sum_{n=0}^{\infty} c_nz^n$ of convergence radius $R$ such that  
\begin{align*}
\sum_{n=0}^{\infty}c_nR^n\text{ converge }
\end{align*}
Then for all $M>1$, we have  
\begin{align*}
f|_{\mathbb{S}_M(R)}(z)\to \sum_{n=0}^{\infty}c_nR^n=f(R)\text{ as }z\to R 
\end{align*}
\end{theorem}
\begin{proof}
We first 
\begin{align*}
\vi{\text{ prove when }R=1}
\end{align*}
Fix $\epsilon $. We wish  
\begin{align*}
\vi{\text{ to find $\delta$ such that }\abso{\sum_{n=0}^{\infty}c_nz^n-c_n}<\epsilon \text{ for all }z \in \mathbb{S}_M(1)\cap D_\delta(1)}
\end{align*}
To use summation by part, we first fix 
\begin{align*}
s_n\triangleq \sum_{k=0}^{n}c_k\text{ and }s\triangleq \lim_{n\to \infty}s_n
\end{align*}
Now Use summation by part 
\begin{align*}
\sum_{n=0}^k c_nz^n&=\sum_{n=0}^{k}(s_n-s_{n-1})z^n\\
&=\sum_{n=0}^k s_nz^n - \sum_{n=0}^{k-1} s_nz^{n+1}\\
&=s_kz^k+(1-z)\sum_{n=0}^{k-1} s_nz^n
\end{align*}
Note that 
\begin{align*}
  \hspace{1cm}(1-z)\sum_{n=0}^{\infty}z^n=1\hspace{0.5cm}(\abso{z}<1)
\end{align*}
This give us 
\begin{align*}
\lim_{z\to 1^-}\Big( \sum_{n=0}^{\infty}c_nz^n- \sum_{n=0}^\infty c_n \Big)&= \lim_{z\to 1^-} \Big( \lim_{k\to \infty} s_kz^k  + (1-z)\sum_{n=0}^{k-1} s_nz^n - s\Big)\\
&=\lim_{z\to 1^-}  (1-z)\sum_{n=0}^{\infty} (s_n-s)z^n\hspace{0.5cm}(\because \forall z\inc:\abso{z}<1, \lim_{k\to \infty}s_kz^k=0)
\end{align*}
We reduce the problem into 
\begin{align*}
  \vi{\text{ finding }\delta\text{ such that }\abso{(1-z)\sum_{n=0}^{\infty}(s_n-s)z^n}\leq \epsilon \text{ for all }z \in \mathbb{S}_M(1)\cap D_\delta(1)}
\end{align*}
Because $s_n \to s$, we know there exists $N$ such that $\abso{s_n-s}<\frac{1}{2M}$ for all $n>N$. We claim 
\begin{align*}
\vi{\delta= \frac{\epsilon }{2\sum_{n=0}^N \abso{s_n-s}}\text{ suffices }}
\end{align*}
Note that $\sum_{n=0}^{\infty}(s_n-s)z^n$ absolutely converges by direct comparison test. Then we can deduce
\begin{align*}
  \abso{(1-z)\sum_{n=0}^{\infty}(s_n-s)z^n}&=  \abso{1-z}\cdot \abso{\sum_{n=0}^{N}(s_n-s)z^n+\sum_{n=N+1}^{\infty}(s_n-s)z^n}\\
 &\leq \abso{1-z}\Big(\sum_{n=0}^N \abso{s_n-s}+ \frac{\epsilon }{2M} \sum_{n=N+1}^\infty \abso{z}^n \Big)\\
 &\leq  \frac{\epsilon}{2}+ \frac{\epsilon }{2M} \cdot \frac{\abso{1-z}}{1-\abso{z}} \cdot \abso{z}^{N+1}\leq \epsilon \hspace{0.5cm}(\because \abso{z}<1\text{ and }\frac{\abso{1-z}}{1-\abso{z}}<M)\vdone
\end{align*}
We now prove 
\begin{align*}
\blue{\text{ when $R\inr^+$ }}
\end{align*}
Fix $\epsilon $. We wish 
 \begin{align*}
   \blue{\text{ to find }\delta\text{ such that }\abso{\sum_{n=0}^{\infty}c_nz^n-c_nR^n}<\epsilon \text{ for all }z \in \mathbb{S}_M(R)\cap D_\delta(R)}
\end{align*}
Fix  
\begin{align*}
a_n=c_nR^n\text{ and }g(z)\triangleq \sum_{n=0}^{\infty}a_n z^n=\sum_{n=0}^{\infty}c_nR^nz^n\hspace{0.5cm}(\abso{z}<1)
\end{align*}
By premise and \vi{our result}, we know 
\begin{align*}
g(1)\text{ exists and there exists $\delta'$ such that }\abso{g(z)-g(1)}<\epsilon \text{ for all $z \in\mathbb{S}_M(1)\cap D_{\delta'}(1)$ }
\end{align*}
We claim 
\begin{align*}
  \blue{\delta=R\delta'\text{ suffices }}
\end{align*}
First note that 
\begin{align*}
\frac{\abso{R-z}}{R-\abso{z}}\in (0,M)\implies \frac{\abso{1-\frac{z}{R}}}{1-\abso{\frac{z}{R}}}\in (0,M)
\end{align*}
This tell us 
\begin{align*}
z\in \mathbb{S}_M(R) \implies \frac{z}{R}\in \mathbb{S}_M(1)
\end{align*}
Fix $z\in \mathbb{S}_M(R)\cap D_\delta(R)$. We now have
\begin{align*}
\frac{z}{R}\in \mathbb{S}_M(1)\cap D_{\delta'}(1)
\end{align*}
This then let us conclude
\begin{align*}
\abso{\sum_{n=0}^{\infty}c_nz^n-c_nR^n}&=\abso{g(\frac{z}{R})-g(1)}<\epsilon \bdone
\end{align*}
\end{proof}
\begin{mdframed}
\begin{Example}{\textbf{(Identity of $\ln$ derived from Abel's Theorem)}}{}
\begin{align*}
  \ln (1+x)=\sum_{n=1}^{\infty} (-1)^{n-1} \frac{x^n}{n}\text{ for all $x \in (-1,1]$ }
\end{align*}
Check that both side satisfy $y'=\frac{1}{1+x}$, and $y(0)=0$. This tell us that two sides equal on  $(-1,1)$. Now using Abel's Theorem and the continuity of $\ln$, we have 
\begin{align*}
\ln 2= \sum_{n=1}^{\infty} (-1)^{n-1} \frac{1}{n}
\end{align*}
\end{Example}
\begin{Example}{\textbf{($1-1+1-1+\cdots=\frac{1}{2}$)}}{}
\begin{align*}
1-1+1-1+\cdots = \frac{1}{2}\text{ is WRONG!!! }
\end{align*}
When people say: "$1-1+1-1+\cdots =\frac{1}{2}$", they mean the sum of the series in the sense of Abel. Compute the Macularin series of $\frac{1}{1+r}$
\begin{align*}
\frac{1}{1+r}=\sum_{n=0}^{\infty}(-1)^nr^n
\end{align*}
Check both side do equal on $(-1,1)$ by direct computation. Apply Abel's Theorem to see the magic. 
\end{Example}
\end{mdframed}

\chapter{Calculus in Euclidean Space}
\section{Operator Norm}
\begin{mdframed}
In this section, and particularly in functional analysis, we say a function $T$ between two metric space is a  \textbf{bounded operator} if $T$ always map bounded set to bounded set. In particular, if $T$ is a linear transformation between two normed space, we say $T$ is a \textbf{bounded linear operator}.\\

Suppose $\mathcal{X},\mathcal{Y}$ are two normed space over $\R\text{ or }\C$. In space $L(\mathcal{X},\mathcal{Y})$, alternatively, we can define the boundedness for each linear transformation $T$ by 
\begin{align*}
T\text{ is bounded } \overset{\triangle}{\iff} \exists M\inr,\forall x\in \mathcal{X}, \norm{Tx}\leq M\norm{x}
\end{align*}
The proof of equivalency is simple. For $(\longrightarrow )$, let $E\triangleq \set{y \in \mathcal{X}: \norm{y}=1}$ is non-empty. Clearly, $E$ is bounded. Let $M=\sup_{y\in E} \norm{Ty}$. We now have 
\begin{align*}
\norm{Tx}= \norm{x}\cdot\norm{T \frac{x}{\norm{x}}}\leq M\norm{x}
\end{align*}
For $(\longleftarrow)$, just observe $\norm{Tx-Ty}=\norm{T(x-y)}\leq M\norm{x-y}$.\\


We first show that a linear transformation is continuous if and only if it is bounded. (\myref{Theorem}{LOB}) 
\end{mdframed}
\begin{theorem}
\label{LOB}
\textbf{(Liner Operator is Bounded if and only if it is Continuous)} Given two normed space $\mathcal{X},\mathcal{Y}$ over $\R\text{ or }\C$ and  $T\in L(\mathcal{X},\mathcal{Y})$, we have 
\begin{align*}
T\text{ is a bounded operator }\iff T\text{ is continuous on $\mathcal{X}$}
\end{align*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$ We show 
\begin{align*}
\vi{T\text{ is Lipschitz continuous on }V}
\end{align*}
Because $T$ is bounded, we can let $M\inr^+$ satisfy $\norm{Tx}\leq M\norm{x}$. We see 
\begin{align*}
\norm{Tx-Ty}\leq \norm{T(x-y)}\leq M \norm{x-y}\vdone
\end{align*}
$(\longleftarrow)$\\

Because $T$ is linear and continuous at $0$, we know there exists  $\epsilon $ such that 
\begin{align*}
\sup_{\norm{y}\leq \epsilon } \norm{Ty} \leq 1
\end{align*}
We claim 
\begin{align*}
\blue{\hspace{3cm}\norm{Tx}\leq \frac{1}{\epsilon }\norm{x}\hspace{1.5cm}(x\in \mathcal{X})}
\end{align*}
Fix $x\in V$. Compute 
\begin{align*}
\norm{Tx}= \frac{\norm{x}}{\epsilon } \bnorm{T \frac{\epsilon x}{\norm{x}}}\leq \frac{\norm{x}}{\epsilon}\bdone
\end{align*}
\end{proof}
\begin{mdframed}
Here, we introduce a new terminology, which shall later show its value. Given a set $X$, we say two metrics $d_1,d_2$ on $X$ are \textbf{equivalent}, and write $d_1\sim d_2$, if we have 
\begin{align*}
\exists m,M \inr^+, \forall x ,y\in X, md_1(x,y)\leq d_2(x,y) \leq Md_1(x,y)
\end{align*}
Now, given a fixed vector space $V$, naturally, we say two norms $\norm{\cdot}_1,\norm{\cdot}_2$ on $V$ are \textbf{equivalent} if 
\begin{align*}
\exists m,M \inr^+, \forall x\in X, m \norm{x}_1\leq \norm{x}_2 \leq M\norm{x}_1
\end{align*}
We say two metric $d_1,d_2$ on  $X$ are  \textbf{topologically equivalent} if the topology they induce on $X$ are identical.\\

A few properties can be immediately spotted.  
\begin{enumerate}[label=(\alph*)]
  \item Our definition of $\sim$ between metrics of a fixed $X$ is an equivalence relation.
  \item Our definition of $\sim$ between norms on a fixed $V$ is an equivalence relation.
  \item Equivalent norms induce equivalent metrics.
  \item Equivalent metrics are topologically equivalent. 
\end{enumerate}

We now prove that if $V$ is finite-dimensional, then all norms on  $V$ are equivalent (\myref{Theorem}{ANoF}). This property will later show its value, as used to prove that linear map of finite-dimensional domain is alwasy continuous (\myref{Theorem}{LmoF})
\end{mdframed}
\begin{theorem}
\label{ANoF}
\textbf{(All Norms on Finite-dimensional space are Equivalent)} Suppose $V$ is a finite-dimensional vector space over $\R\text{ or }\C$. Then 
\begin{align*}
\text{ all norms on $V$ are equivalent }
\end{align*}
\end{theorem}
\begin{proof}
Let $\set{e_1,\dots ,e_n}$ be a basis of $V$. Define $\infty$-norm $\norm{\cdot}_\infty$ on $V$ by 
\begin{align*}
\bnorm{\sum \alpha_i e_i}_{\infty}\triangleq  \max \abso{\alpha_i} 
\end{align*}
It is easily checked that $\norm{\cdot}_\infty$ is indeed a norm. Fix a norm $\norm{\cdot}$ on $V$. We reduce the problem into  
\begin{align*}
  \vi{\text{ finding $m,M\inr^+$ such that }m\norm{x}_\infty \leq \norm{x}\leq M\norm{x}_\infty}
\end{align*}
We first claim 
\begin{align*}
\blue{M=\sum \norm{e_i}\text{ suffices }}
\end{align*}
Compute 
\begin{align*}
\norm{x}= \bnorm{\sum \alpha_ie_i} \leq \sum \abso{\alpha _i} \norm{e_i} \leq \norm{x}_\infty \sum \norm{e_i}= M \norm{x}_\infty \bdone
\end{align*}
Reverse triangle inequality give us 
\begin{align*}
\Big|\norm{x}-\norm{y}\Big|\leq \norm{x-y} \leq M \norm{x}_\infty
\end{align*}
This implies that $\norm{\cdot}:\Big(V,\norm{\cdot}_\infty \Big)\rightarrow \R$ is Lipschitz continuous.\\


 
Note that $S\triangleq \set{y \in V: \norm{y}_\infty =1}$ is non-empty. Check that $S$ is compact in $\norm{\cdot}_\infty$ by checking  $S$ is sequentially compact using the fact $\R^{n-1}$ is locally compact.\\


Now, by EVT, we know $\min _{y \in S}\norm{y}$ exists. Note that $\min_{y \in S}\norm{y}>0$, since $0 \not\in S$.\\ 

We claim 
\begin{align*}
  \olive{m= \min_{y \in S}\norm{y}\text{ suffices }}
\end{align*}
Fix $x \in V$. Compute 
\begin{align*}
m\norm{x}_\infty = \norm{x}_\infty (\min_{y \in S} \norm{y})\leq \norm{x}_\infty \cdot  \bnorm{ \frac{x}{\norm{x}_\infty}}=\norm{x}\odone \vdone
\end{align*}






\end{proof}
\begin{theorem}
\label{LmoF}
\textbf{(Linear map of Finite-dimensional Domain is always Continuous)} Given a finite-dimensional normed space $\mathcal{X}$ over $\R\text{ or }\C$, an arbitrary normed space  $\mathcal{Y}$ over $\R\text{ or }\C$ and a linear transformation  $T:\mathcal{X}\rightarrow \mathcal{Y}$, we have 
\begin{align*}
T\text{ is continuous }
\end{align*}
\end{theorem}
\begin{proof}
Fix $x\in \mathcal{X},\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find $\delta$ such that }\forall h\in \mathcal{X}: \norm{h}\leq \delta, \norm{T(x+h)-Tx}\leq \epsilon }
\end{align*}
Let $\set{e_1,\dots ,e_n}$ be a basis of $\mathcal{X}$. Note that $ \norm{ \sum \alpha_i e_i }_1: = \sum \abso{\alpha _i}$ is a norm. By \myref{Theorem}{ANoF}, we know $\norm{\cdot}$ and $\norm{\cdot}_1$ are equivalent. Then, we can fix $M\inr^+$ such that 
\begin{align*}
\hspace{2cm}\norm{x}_1 \leq M\norm{x}\hspace{0.5cm}(x \in V)
\end{align*}
We claim 
\begin{align*}
\vi{\delta= \frac{\epsilon }{M(\max \norm{Te_i} )}\text{ suffices }} 
\end{align*}
Fix $\norm{h}\leq \delta$ and express $h=\sum \alpha_i e_i$. Compute using linearity of $T$
\begin{align*}
  \norm{T(x+h)-Tx}&=\norm{\sum \alpha_i T e_i}\\
  &\leq \sum \abso{\alpha _i} \norm{Te_i}\\
  &\leq  \norm{h}_1 (\max \norm{Te_i} )\\
  &\leq M \norm{h}(\max \norm{Te_i})\leq \epsilon \vdone
\end{align*}
\end{proof}
\begin{mdframed}
As a corollary of \myref{Theorem}{LOB} and \myref{Theorem}{LmoF}, we now see that, if $\mathcal{X}$ is finite-dimensional, then all linear map of domain $\mathcal{X}$ are bounded. A counter example to the generalization of this statement is followed. 
\begin{Example}{\textbf{(Differentiation is an Unbounded Linear Operator)}}{}
\begin{align*}
\mathcal{X}=\Big(\R[x]|_{[0,1]}, \norm{\cdot}_\infty \Big), D(P)\triangleq P'
\end{align*}
Note that $\set{x^n}_{n\inn}$ is bounded in $\mathcal{X}$ and $\set{D(x^n)}_{n\inn}$ is not.   
\end{Example}
Now, suppose $\mathcal{X},\mathcal{Y}$ are two fixed normed spaces over $\R$ or $\C$. We can easily check that the set $BL(\mathcal{X},\mathcal{Y})$ of bounded linear operators from $\mathcal{X}$ to $\mathcal{Y}$ form a vector space over whichever field $\mathcal{Y}$ is over.\\

Naturally, our definition of boundedness of linear operator derive us a norm on $BL(\mathcal{X},\mathcal{Y})$, as followed 
\begin{align}
\label{tnop}
\norm{T}_{\text{op}}\triangleq \inf \set{M\inr^+ :\forall x \in \mathcal{X}, \norm{Tx}\leq M\norm{x}}
\end{align}
Before we show that our definition is indeed a norm, we first give some equivalent definitions and prove their equivalency. 
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definitions of Operator Norm)} Given two fixed normed space $\mathcal{X},\mathcal{Y}$ over $\R$ or  $\C$, a bounded linear operator  $T:\mathcal{X}\rightarrow \mathcal{Y}$, and define $\norm{T}_{\text{op}}$ as in (\myref{}{tnop}), we have 
\begin{align*}
\norm{T}_{\text{op}}=\sup_{x\in \mathcal{X},x\neq 0} \frac{\norm{Tx}}{\norm{x}}
\end{align*}
\end{theorem}
\begin{proof}
Define $J=\set{M\inr^+:\forall x \in \mathcal{X},\norm{Tx}\leq M\norm{x}}$, so that we have $\norm{T}_{\text{op}}=\inf J$. Now, observe 
\begin{align*}
J&=\set{M\inr^+:M\geq \frac{\norm{Tx}}{\norm{x}},\forall x\neq 0\in \mathcal{X}}\\
&=\bset{M\inr^+ : M \geq \sup_{x \in \mathcal{X},x\neq 0} \frac{\norm{Tx}}{\norm{x}}}
\end{align*}
This let us conclude 
\begin{align*}
\norm{T}_\text{op}=\inf J=\sup_{x \in \mathcal{X},x\neq 0} \frac{\norm{Tx}}{\norm{x}}
\end{align*}
\end{proof}
\begin{mdframed}
It is now easy to see 
\begin{align}
  \norm{T}_{\text{op}}&=\sup_{x \in \mathcal{X},x\neq 0} \frac{\norm{Tx}}{\norm{x}}\label{equivdefopnorm1}\\
&=\sup_{x\in \mathcal{X},\norm{x}=1} \norm{Tx}\label{equivdefopnorm}
\end{align}
It is not all in vain to introduce the equivalent definitions. See that the verification of  $\norm{\cdot}_{\text{op}}$ being a norm on $BL(\mathcal{X},\mathcal{Y})$ become simple by utilizing the equivalent definitions. 
\begin{enumerate}[label=(\alph*)]
  \item For positive-definiteness, fix non-trivial $T$ and fix $x\in \mathcal{X}\setminus N(T)$. Use (\myref{}{equivdefopnorm1}) to show $\norm{T}_{\text{op}}\geq \frac{\norm{Tx}}{\norm{x}}>0$. 
  \item For absolute homogenity, use (\myref{}{equivdefopnorm}) and $\norm{Tcx}=\abso{c}\cdot \norm{Tx}$.
  \item For triangle inequality, use (\myref{}{equivdefopnorm}) and $\norm{(T_1+T_2)x}\leq \norm{T_1x}+\norm{T_2x}$. 
\end{enumerate}
Naturally, and very very importantly, (\myref{}{equivdefopnorm1}) give us 
\begin{align*}
\hspace{3cm}\norm{Tx}\leq \norm{T}_\text{op}\cdot \norm{x}\hspace{1.5cm}(x\in \mathcal{X})
\end{align*}
This inequality will later be the best tool to help analyze the derivatives of functions between Euclidean spaces, and perhaps better, it immediately give us 
\begin{align*}
 \frac{\norm{T_1T_2x}}{\norm{x}}\leq \frac{\norm{T_1}_\text{op}\cdot \norm{T_2}_\text{op}\cdot \norm{x}}{\norm{x}}=\norm{T_1}_\text{op}\cdot \norm{T_2}_{\text{op}}
\end{align*}
Then (\myref{}{equivdefopnorm1}) give us  
\begin{align*}
\norm{T_1T_2}_\text{op}\leq \norm{T_1}_\text{op}\cdot \norm{T_2}_\text{op}
\end{align*}
\end{mdframed}
\section{Chain Rule}
\begin{mdframed}
Given two normed space $\mathcal{X},\mathcal{Y}$, suppose $f$ maps an open neighborhood $O$ around $x$ in $\mathcal{X}$ to $\mathcal{Y}$. We say $f$ is \textbf{differentiable at} $x$ if there exists a bounded linear transformation $A_x:\mathcal{X}\rightarrow \mathcal{Y}$ (from now, $A_x$ will be denoted $df_x$) such that  
\begin{align}
\label{defdi}
  \lim_{h\to 0} \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}=0
\end{align}
Immediately, we should check that the linear approximation is unique. Suppose $df_x$ and  $df_x'$ both satisfy (\myref{}{defdi}). We are required to show $(df_x-df_x')h=0$ for all $\norm{h}_\mathcal{X}=1$. Fix $h\in \mathcal{X}$ such that $\norm{h}_\mathcal{X}=1$. Note that 
\begin{align*}
 \frac{(df_x-df'_x)th}{t}\text{ is a constant in $t$ for $t\neq 0$ }
\end{align*}
This then reduced the problem into showing 
\begin{align}
\label{goaldib}
\frac{(df_x-df'_x)th}{t\norm{h}_{\mathcal{X}}}\to 0\text{ as $t\to 0$ }
\end{align}
Observe 
\begin{align*}
  (df_x-df'_x)th&= \Big( f(x+th)-f(x)-df'_x(th)\Big) - \Big(f(x+th)-f(x)-df_x(th) \Big)
\end{align*}
which implies 
\begin{align*}
\norm{(df_x-df_x')th}_\mathcal{Y}\leq \norm{f(x+th)-f(x)-df'_x(th)}_{\mathcal{Y}}+ \norm{f(x+th)-f(x)-df_x(th)}_\mathcal{Y}
\end{align*}
and thus implies (\myref{}{goaldib}).\\

It shall be quite clear that a function $f$ differentiable at $x$ must be continuous at $x$, by noting the nominator of (\myref{}{defdi}) must tend to $0$.\\

We now prove the Chain Rule for function between normed space. 
\end{mdframed}
\begin{theorem}
\textbf{(Chain Rule)} Given three normed space $\mathcal{X},\mathcal{Y},\mathcal{Z}$, a point $x \in \mathcal{X}$, a function $g$ that map an open set $U\subseteq \mathcal{Y}$ containing $f(x)$ into $\mathcal{Z}$, a function $f$ that map an open-neighborhood around $x$ into $U$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable at $x$
   \item $g$ is differentiable at $f(x)$
\end{enumerate}
we have 
\begin{align*}
d(g\circ f)_x=dg_{f(x)}\circ df_{x}
\end{align*}
\end{theorem}
\begin{proof}
For brevity, we use $F\triangleq g\circ f$. We wish to prove 
\begin{align*}
\vi{\lim_{h\to 0}\frac{\norm{F(x+h)-F(x)-dg_{f(x)}df_x(h)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}=0}
\end{align*}
Fix $k\triangleq f(x+h)-f(x)$. Observe 
\begin{align*}
 F(x+h)-F(x)-dg_{f(x)}df_x(h)=\Big(g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) \Big) + dg_{f(x)}\big(k-df_x(h)\big)
\end{align*}
This now implies 
\begin{align*}
&\frac{\norm{F(x+h)-F(x)-dg_{f(x)}df_x(h)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}\text{ is smaller than }
\end{align*}
\begin{align*}
\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z+ \norm{dg_{f(x)}\big(k-df_x(h)\big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}} 
\end{align*}
This let us reduce the problem into proving 
\begin{align*}
  &\olive{ \lim_{h\to 0}\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} =0}\\
  \text{ and }&\blue{\lim_{h\to 0} \frac{\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}=0}
\end{align*}
We first prove 
\begin{align*}
\olive{ \lim_{h\to 0}\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} =0}
\end{align*}
Note that if $\norm{k}_\mathcal{Y}=0$, we have 
\begin{align*}
\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} =0
\end{align*}
Now, observe that 
\begin{align*}
\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} = \frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{k}_\mathcal{Y}}\cdot \frac{\norm{k}_\mathcal{Y}}{\norm{h}_\mathcal{X}} 
\end{align*}
Because $h \to 0 \implies k\to 0$, we can now reduce the problem into proving 
\begin{align*}
\olive{\limsup_{h\to 0} \frac{\norm{k}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\text{ exists }}
\end{align*}
Observe 
\begin{align*}
\frac{\norm{k}_\mathcal{Y}}{\norm{h}_\mathcal{X}}&= \frac{\norm{f(x+h)-f(x)-df_x(h)+df_x(h)}_\mathcal{X}}{\norm{h}_\mathcal{X}}\\
&\leq \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}+\frac{\norm{df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\\
&\leq \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}} + \norm{df_x}_\text{op} \odone
\end{align*}
We now prove 
\begin{align*}
\blue{\lim_{h\to 0} \frac{\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}=0}
\end{align*}
Note that if $f(x+h)-f(x)-df_x(h)=0$, then $\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}=0$. Now, observe 
\begin{align*}
\frac{\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}&= \frac{\norm{dg_{f(x)}\big(f(x+h)-f(x)-df_x(h) \big)}_\mathcal{Z}}{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}\cdot \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\\
&\leq \norm{dg_{f(x)}}_\text{op}\cdot \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\to 0 \bdone
\end{align*}
\end{proof}
\begin{mdframed}
Interestingly, if $f:\Big(\R,\norm{\cdot}_2 \Big)\rightarrow \Big(\R^n , \norm{\cdot}_2\Big)$ is a curve in $\R^n$ 
\begin{align*}
f(t)=(f_1(t),\cdots , f_n(t))
\end{align*}
and we define 
\begin{align*}
f'(t)\triangleq (f_1'(t),\cdots ,f_n'(t))
\end{align*}
We have 
\begin{align*}
\abso{f'(t)}= \norm{df_t}_{\text{op}}
\end{align*}
This give us the following expected result (\myref{Corollary}{BPoD}). 
\end{mdframed}
\begin{theorem}
\textbf{(Basic Property of Derivative)} Suppose $f$ maps a convex open set $E\subseteq \Big(\R^n,\norm{\cdot}_2 \Big)$ into $\Big(\R^m,\norm{\cdot}_2 \Big)$,  $f$ is differentiable on $E$, and there exists $M\inr$ such that 
\begin{align*}
\hspace{2.5cm}\norm{df_x}_\text{op}\leq M\hspace{1.5cm}(x \in E)
\end{align*}
Then for all $a,b \in E$, we have
\begin{align*}
\abso{f(b)-f(a)}\leq M \abso{b-a}
\end{align*}
\end{theorem}
\begin{proof}
Define $\gamma:[0,1]\rightarrow E$ by 
\begin{align*}
\gamma (t)\triangleq a+(b-a)t
\end{align*}
Now, note that 
\begin{align*}
\abso{f(b)-f(a)}&= \abso{(f\circ \gamma )(1)-(f\circ \gamma )(0)}\\
&= \abso{\int_0^ 1(f\circ \gamma )'(t)dt}\\
&\leq \int_0^1 \abso{(f\circ \gamma )'(t)}dt\\
&=\int_0^1 \norm{d(f\circ \gamma )_t}_\text{op}dt\\
&\leq \int_0^1 \norm{df_{\gamma (t)}}_\text{op}\cdot \norm{d\gamma_t}_\text{op}dt\\
&=\int_0^1 M\cdot \abso{b-a}dt=M\abso{b-a}
\end{align*}
\end{proof}
\begin{corollary}
\label{BPoD}
\textbf{(Basic Property of Derivative)} Suppose $f$ maps a convex open set $E\subseteq \Big(\R^n,\norm{\cdot}_2 \Big)$ into $\Big(\R^m,\norm{\cdot}_2 \Big)$, $f$ is differentiable on $E$, and $df_x=0$ for all $x\in E$, then
\begin{align*}
f\text{ stay constant on $E$ }
\end{align*}
\end{corollary}






\section{Differentiability Theorem}
\begin{mdframed}


Suppose $W$ is equipped with an inner product. Given a function $f:E\rightarrow W$ and an orthonormal basis $\set{e_\ld}$ of $W$, depending on over $\R\text{ or }\C$, the notation $f_j$ represent a function from $E$ to  $\R\text{ or }\C$ defined by $f_j(x)=\langle f(x),e_j\rangle_W$.\\



Given a function $f:V \to \R$ and a normal vector $e_i\in V$, the \textbf{partial derivative} $\partial_if:V \to \R$ is defined by 
\begin{align*}
\partial_i f(x)=\lim_{t\to 0} \frac{f(x+te_i)-f(x)}{t}
\end{align*}


\end{mdframed}
\begin{theorem}
\label{DiJ}
\textbf{(Derivative is Jacobian)} Suppose  $\alpha =\set{e_1,\dots ,e_n}$ is an orthonormal basis of $\R^n$, and $\beta =\set{q_1,\dots ,q_m}$ is an orthonormal basis of  $\R^m$. Suppose  $f$ maps an open neighborhood $O$ around $x\inr^n$ to $\R^m$.  Then 
\begin{align*}
  f\text{ is differentiable at }x\implies \begin{cases}
    \partial_{i}f_j(x)\text{ exists for all }i,j\\
    [df_x]_{\alpha }^{\beta }=\begin{bmatrix}
      \partial_1f_1(x)&\cdots & \partial_nf_1(x)\\
      \vdots & \ddots & \vdots\\
      \partial_1f_m(x) & \cdots & \partial_nf_m(x)
    \end{bmatrix}
  \end{cases}
\end{align*}
\end{theorem}
\begin{proof}
Fix $i,j$. We wish to show 
 \begin{align*}
\vi{\lim_{t\to 0} \frac{f_j(x+te_i)-f_j(x)}{t}\text{ exists }}
\end{align*}
Because $f$ is differentiable at $x$, by definition of $df_x$, we have
\begin{align*}
\lim_{t\to 0} \frac{\abso{f(x+te_i)-f(x)-df_x(te_i)}}{\abso{te_i}}=0
\end{align*}
Set $R_i:\R\rightarrow \R^m$ by $R_i(t)\triangleq f(x+te_i)-f(x)-df_x(te_i)$. We have 
\begin{align}
\label{Dij1}
  \lim_{t\to 0}\frac{\abso{R_i(t)}}{\abso{t}}=0
\end{align}
Compute
\begin{align*}
f_j(x+te_i)-f_j(x)&=\big(f(x+te_i)-f(x) \big)\cdot q_j\\
&=\big(R_i(t)+df_x(te_i) \big) \cdot q_j\\
&= R_i(t)\cdot q_j+ tdf_x(e_i)\cdot q_j
\end{align*}
This then give us 
\begin{align*}
\frac{f_j(x+te_i)-f_j(x)}{t}=\frac{R_i(t)\cdot q_j}{t}+ df_x(e_i)\cdot q_j
\end{align*}
and 
\begin{align*}
  df_x(e_i)\cdot q_j - \frac{\abso{R_i(t)\cdot q_j}}{\abso{t}} \leq \frac{f_j(x+te_i)-f_j(x)}{t}\leq df_x(e_i)\cdot q_j + \frac{\abso{R_i(t)\cdot q_j}}{\abso{t}}
\end{align*}
By Cauchy-Schwarz Inequality, we now have 
\begin{align*}
  df_x(e_i)\cdot q_j - \frac{\abso{R_i(t)}}{\abso{t}} \leq \frac{f_j(x+te_i)-f_j(x)}{t}\leq df_x(e_i)\cdot q_j + \frac{\abso{R_i(t)}}{\abso{t}}
\end{align*}
Now applying Squeeze Theorem and \myref{Equation}{Dij1}, we have  
\begin{align*}
\lim_{t\to 0} \frac{f_j(x+te_i)-f_j(x)}{t}=df_x(e_i)\cdot q_j \vdone
\end{align*}
We now have 
\begin{align*}
\hspace{2cm}df_x(e_i)\cdot q_j=\partial _if_j(x)\hspace{1.5cm}(\forall i,j)
\end{align*}
This now give us 
\begin{align*}
df_x(e_i)=\sum_{j=1}^m \Big(df_x(e_i)\cdot q_j \Big)q_j=\sum_{j=1}^m \partial_if_j(x)q_j
\end{align*}
and suggest the matrix representation. 
\end{proof}
\begin{mdframed}
Note that the converse is not always true, that is, given $f:\R^n\rightarrow \R^m$, even if the partial derivative $\partial_if_j$ all exists at $x$, the function $f$ can still not be differentiable at $x$. In fact, it doesn't even have to be continuous at $x$. See the following examples. 
\begin{Example}{\textbf{(Discontinuous Function with Partial Derivative)}}{}
\begin{align*}
f:\R^2\rightarrow \R\text{ and }f(x,y)=\begin{cases}
  1& \text{ otherwise}\\
  0& \text{ if }x=0\text{ or }y=0
\end{cases}
\end{align*}
\end{Example}
\begin{Example}{\textbf{(Discontinuous Function with Partial Derivative)}}{}
\begin{align*}
f:\R^2\rightarrow \R\text{ and }f(x,y)=\begin{cases}
  \frac{x^2y}{x^4+y^2}& \text{ if }(x,y)\neq 0\\
  0& \text{ if }(x,y)=0
\end{cases}
\end{align*}
We have
\begin{align*}
f(h,h^2)\to \frac{1}{2}\text{ as $h\to 0$ and }\partial_xf(0)=\partial _yf(0)=0
\end{align*}
\end{Example}
\begin{Example}{\textbf{(Non-differentiable Continuous Funciton with Partial Derivative)}}{}
\begin{align*}
f:\R^2 \rightarrow \R\text{ and }f(x,y)=\begin{cases}
  \frac{x\abso{y}}{\sqrt{x^2+y^2} }& \text{ if $(x,y)\neq 0$ }\\
  0& \text{ if $(x,y)=0$ }
\end{cases}
\end{align*}
We have 
\begin{align*}
\partial_xf(0)=\partial_yf(0)=0
\end{align*}
By \myref{Theorem}{DiJ} (Derivative is Jacobian), if $f$ is differentiable at $0$, then  $df_0$ must be trivial. Yet  
\begin{align*}
\frac{\abso{f(h,h)-f(0)-df_0(h,h)}}{\abso{(h,h)}}=\frac{h }{2\abso{h} }\not\to 0 
\end{align*}
Note that $f$ is continuous at  $0$, by observing 
\begin{align*}
  x^2+y^2- 2\abso{xy}=\big(\abso{x}-\abso{y} \big)^2\geq 0 \implies \frac{x^2+y^2}{2}\geq \abso{xy}
\end{align*}
which implies 
\begin{align*}
\abso{f}\leq \frac{\sqrt{x^2+y^2} }{2}
\end{align*}
\end{Example}
\end{mdframed}
\begin{mdframed}
Given two normed space $\mathcal{X},\mathcal{Y}$, and an open $E\subseteq \mathcal{X}$, we say $f:E\rightarrow \mathcal{Y}$ is \textbf{continuously differentiable } on $\mathcal{Y}$ if the map $D:(E,\norm{\cdot}_\mathcal{X})\rightarrow \Big(BL(\mathcal{X},\mathcal{Y}), \norm{\cdot}_\text{op} \Big)$ defined by 
\begin{align*}
D(x)=df_x
\end{align*}
is continuous. Note that the definition of the term "continuously differentiable" coincide when $\mathcal{X}=\mathcal{Y}=\R$ and $df_x$ is just $h\mapsto f'(x)h$.  
\end{mdframed}
\begin{theorem}
\textbf{(Differentiability Theorem)} Suppose  $\alpha =\set{e_1,\dots ,e_n}$ is an orthonormal basis of $\R^n$, and $\beta =\set{q_1,\dots ,q_m}$ is an orthonormal basis of  $\R^m$. Suppose  $f$ maps an open set $E\subseteq\R^n$ to $\R^m$.  Then  
\begin{align*}
f\text{ is continuously differentiable on $E$ }\iff \partial_if_j\text{ exists and is continuous on $E$ for all $i,j$ }
\end{align*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

Fix $i,j$. We wish to show 
 \begin{align*}
\vi{\text{ $\partial_if_j$ exists and is continuous on $E$ }}
\end{align*}
Because $f$ is differentiable on $E$, we know  $\partial_i f_j$ exists on $E$. (\myref{Theorem}{DiJ}). Fix $x \in E$. We only have to show 
\begin{align*}
  \vi{\partial_i f_j\text{ is continuous at $x$ }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find }\delta\text{ such that } \abso{\partial_i f_j(y)-\partial_i f_j(x)}\leq \epsilon \text{ for all $\abso{y-x}<\delta$ }}
\end{align*}
Because $f$ is continuously differentaible at $x$, we know there exists  $\delta$ such that 
\begin{align*}
\norm{df_y-df_x}_\text{op}< \epsilon \text{ for all $\abso{y-x}\leq \delta$ }
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such $\delta$ suffices }}
\end{align*}
By \myref{Theorem}{DiJ} (Derivative is Jacobian), we know
\begin{align*}
\partial_i f_j(y)-\partial_i f_j(x)=(df_y-df_x)e_i \cdot q_j
\end{align*}
By Cauchy-Inequality, we then have 
\begin{align*}
  \abso{\partial_i f_j(y)-\partial_if_j(x)}&\leq \abso{(df_y-df_x)e_i}\\
  &\leq \norm{df_y-df_x}_\text{op}<\epsilon \vdone
\end{align*}

$(\longleftarrow)$\\






\end{proof}



\section{Inverse Function Theorem}
\begin{theorem}
\textbf{(Inverse Function Theorem)} Given a map $f$ from open set $E\subseteq \R^n$ to $\R^n $ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is conitnuously differentiable on $E$ 
   \item $df_a$ is invertible  
  \item $f(a)=b$
\end{enumerate}
Then there exists open neighborhood $U$ around $a$ and  $V$ around  $b$ such that  
\begin{align*}
f\text{ is a diffeomorphism from $U$ to  $V$}
\end{align*}
\end{theorem}
\section{Implicit Function Theorem}
\section{Appendix: Linear Algebra}
\begin{mdframed}
This section contains
\begin{enumerate}[label=(\alph*)]
  \item definitions and basic property for \textbf{norm}
  \item definitions and basic property for \textbf{inner product}
  \item definitions and basic property of \textbf{positive semi-definite Hermitian form}
  \item full statement and proof of \textbf{Cauchy Schwarz Inequality} for both inner product space and positive semi-definite Hermitian form  
  \item statement and proof of \textbf{SVD} (singular value decomposition). 
\end{enumerate}
\end{mdframed}
\textbf{(Norm Axiom Part)}
\begin{mdframed}
Recall that by a \textbf{normed space} $V$, we mean a vector space over a sub-field $\F$ of $\C$ equipped with  $\norm{\cdot}:V\to \R_0^+$ satisfying the following $\underline{\text{axioms}}$: 
\begin{enumerate}[label=(\alph*)]
  \item $\norm{x}=0 \implies x=0$ (positive-definiteness)
  \item $\norm{sx}=\abso{s}\cdot \norm{x}$ for all $s \in \F$ and $x\in V$ (absolute-homogenity)
  \item $\norm{x+y}\leq \norm{x}+\norm{y}$ for all $x,y \in V$ (triangle inequality)
\end{enumerate}
Observe
\begin{align*}
\norm{0}=\norm{0+x}\leq \norm{0}+\norm{x}\text{ for all $x\in V$ }
\end{align*}
This shows that $\norm{x}\geq 0$ for all $x\in V$. Also observe 
\begin{align*}
\norm{0}=\norm{0(x)}=\abso{0}\cdot \norm{x}=0
\end{align*}

We can now rewrite the normed space axioms into
\begin{enumerate}[label=(\alph*)]
  \item $\norm{x}=0\iff x=0$ (positive-definiteness)
  \item $\norm{sx}=\abso{s}\cdot \norm{x}$ for all $s \in \F$ and $x\in V$ (absolute-homogeneity)
  \item $\norm{x+y}\leq \norm{x}+\norm{y}$ for all $x,y \in V$ (triangle inequality)
  \item $\norm{x}\geq 0$ for all $x \in V$ (non-negativity)
\end{enumerate}
\end{mdframed}
\textbf{(Inner Product Axiom Part)}
\begin{mdframed}
Recall that by an \textbf{inner product space} $V$, we mean a vector space over  $\R$ or $\C$ equipped with  $\langle \cdot,\cdot\rangle : V^2 \rightarrow \R\text{ or }\C$ satisfying the following $\underline{\text{axioms}}$
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle >0$ for all $x\neq 0$ (Positive-definiteness)
  \item $\langle x,y\rangle =\overline{\langle y,x\rangle }$ (Conjugate symmetry)
  \item $\langle x+y,z\rangle =\langle x,z\rangle +\langle y,z\rangle $ and $\langle cx,z\rangle=c\langle x,z\rangle $ (Linearity in the first argument)
\end{enumerate}
Note that conjugate symmetry let us deduce
\begin{align*}
\langle x,x\rangle =\overline{\langle x,x\rangle }\implies \langle x,x\rangle \inr
\end{align*}
Also, one can easily use linearity in first argument to deduce 
\begin{align*}
\langle 0,0\rangle =2\langle 0,0\rangle \implies \langle 0,0\rangle =0
\end{align*}
This now let us rewrite the inner product space over $\C$ axioms into 
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle \geq 0$ for all $x\in V$ (non-negativity) 
  \item $\langle x,x\rangle =0 \iff x=0$ (positive-definiteness)
  \item $\langle x,y\rangle =\overline{\langle y,x\rangle }$ (conjugate symmetry)
  \item $\langle cx+y,z\rangle =c\langle x,z\rangle +\langle y,z\rangle $ and $\langle x,cy+z\rangle =\overline{c}\langle x,y\rangle +\langle x,z\rangle $ (Linearity)
\end{enumerate}
Note that using $c=1$ and $y=0$, ($\because \langle 0,z\rangle =0\langle x,z\rangle=0 $) one can check that the latter expression of linearity implies the first expression.
\end{mdframed}
\begin{mdframed}
If the scalar field is $\R$, then conjugate symmetry is just symmetry and we also have linearity in the second argument.\\

This now let us rewrite the inner product space over $\R$ axioms into 
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle \geq 0$ for all $x\in V$ (non-negativity) 
  \item $\langle x,x\rangle =0 \iff x=0$ (positive-definiteness)
  \item $\langle x,y\rangle =\langle y,x\rangle $ (symmetry)
  \item Linearity in both arguments
\end{enumerate}
\end{mdframed}
\begin{mdframed}
If we do not require $\langle \cdot,\cdot\rangle $ to be positive-definite, but only non-negative, i.e. $\langle x,x\rangle \geq 0$ for all $x\in V$, then we have a \textbf{positive semi-definite Hermitian form}. Formally speaking, a positive semi-definite Hermitian form $\langle\cdot,\cdot \rangle :V^2 \rightarrow \R\text{ or }\C$ satisfy the following axioms
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle \geq 0$ for all $x\in V$ (non-negativity)
  \item $\langle x,y\rangle =\overline{\langle y,x\rangle }$ (conjugate symmetry)
  \item $\langle x+y,z\rangle =\langle x,z\rangle +\langle y,z\rangle $ and $\langle cx,z\rangle=c\langle x,z\rangle $ (Linearity in the first argument)
\end{enumerate}
\begin{Example}{\textbf{(Example of Positive semi-definite Hermitian form)}}{}
\begin{align*}
\text{ arbitrary }V\text{ over $\R$ or  $\C$ }\hspace{0.5cm}\langle x,y\rangle \triangleq 0\text{ for all $x,y$ }\hspace{1cm}
\end{align*}
\end{Example}
\end{mdframed}
\textbf{(Norm Induce Part)}
\begin{mdframed}
Given a vector space $V$ over $\R$ or  $\C$, one can check that if  $V$ is equipped with an inner product  $\langle \cdot,\cdot\rangle :V^2\rightarrow \R\text{ or }\C$, then we can induce a norm on $V$ by 
\begin{align*}
\hspace{3cm}\norm{x}\triangleq \sqrt{\langle x,x\rangle }\hspace{1.5cm}(x \in V)
\end{align*}
Note that 
\begin{align*}
\norm{x}=0\iff \langle x,x\rangle =0 
\end{align*}
This implies that if $\langle \cdot,\cdot\rangle $ is an inner product (satisfy positive-definiteness), then $\norm{\cdot}$ is also positive-definite. And if $\langle \cdot,\cdot\rangle $ is not positive-definite, then there exists  $x\neq 0\in V$ such that $\norm{x}=0$, which make $\norm{\cdot}$ a \textbf{semi-norm}.\\

Absolute homogeneity follows from the linearity of inner product.\\

To check triangle inequality, we first have to prove Cauchy-Schwarz inequality.
\end{mdframed}
\begin{theorem}
\label{BPoP}
\textbf{(Basic Property of Positive semi-definite Hermitian form)} Given a positive semi-definite Hermitian form $\langle \cdot,\cdot\rangle:V^2 \rightarrow \R\text{ or }\C $ and $x,y \in V$, we have 
\begin{align*}
\langle x,x\rangle =0 \implies \langle x,y\rangle =0
\end{align*}
\end{theorem}
\begin{proof}
\As{$\langle x,y\rangle\neq 0 $}. Fix $t> \frac{\norm{y}^2 }{2\abso{\langle x,y\rangle }^2}$. Compute 
\begin{align*}
\norm{y-t\langle y,x\rangle x}^2 &= \norm{y}^2 + \norm{(-t)\langle y,x\rangle x }^2  + \langle -t\langle y,x\rangle x,y \rangle + \langle y,-t\langle y,x\rangle x\rangle \\
&=\norm{y}^2 + t^2 \abso{\langle x,y\rangle }^2 \norm{x}^2 -t \langle y,x\rangle \langle x,y\rangle - t \langle x,y\rangle \langle y,x\rangle  \\
&= \norm{y}^2- 2t \abso{\langle x,y\rangle }^2 <0 \tCaC
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(Cauchy-Schwarz Inequality)} Given a positive semi-definite Hermitian form $\langle \cdot,\cdot\rangle :V^2\rightarrow \C$ on vector space $V$ over $\C$, we have 
\begin{enumerate}[label=(\alph*)]
  \item $\abso{\langle x,y\rangle}\leq  \norm{x}\cdot\norm{y}\hspace{0.5cm}(x,y \in V)$ 
  \item the equality hold true if $x,y$ are linearly dependent
  \item the equality hold true if and only if $x,y$ are linearly dependent (provided $\langle \cdot,\cdot\rangle $ is an inner product)
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\begin{align*}
  \vi{\hspace{3cm}\abso{\langle x,y\rangle }\leq \norm{x}\cdot\norm{y}\hspace{1.5cm}(x,y \in V)}
\end{align*}
Fix $x,y \in V$. \myref{Theorem}{BPoP} tell us $\norm{x}=0 \implies  \langle x,y\rangle =0$. Then we can reduce the problem into proving 
\begin{align*}
\vi{\frac{\abso{\langle x,y\rangle }^2}{\norm{x}^2}\leq\norm{y}^2  }
\end{align*}
Set $z\triangleq y-\frac{\langle y,x\rangle }{\norm{x}^2}x$. We then have 
\begin{align*}
\langle z,x\rangle =\langle y-\frac{\langle y,x\rangle }{\norm{x}^2}x,x \rangle =\langle y,x\rangle - \frac{\langle y,x\rangle }{\norm{x}^2}\langle x,x\rangle =0
\end{align*}
Then from $y=z+\frac{\langle y,x\rangle }{\norm{x}^2}x$, we can now deduce
\begin{align*}
\langle y,y\rangle &= \langle z+\frac{\langle y,x\rangle }{\norm{x}^2}x,z+ \frac{\langle y,x\rangle }{\norm{x}^2}x\rangle  \\
&=\langle z,z\rangle + \abso{\frac{\langle y,x\rangle }{\langle x,x\rangle }}^2 \langle x,x\rangle \\
&=\langle z,z\rangle + \frac{\abso{\langle x,y\rangle }^2}{\langle x,x\rangle }
\end{align*}
Because $\langle z,z\rangle\geq 0 $, we now have
\begin{align*}
\langle y,y\rangle = \langle z,z\rangle + \frac{\abso{\langle x,y\rangle }^2}{\langle x,x\rangle }\geq  \frac{\abso{\langle x,y\rangle }^2}{\langle x,x\rangle }\vdone
\end{align*}
The equality hold true if and only if $\langle z,z\rangle =0$. This explains the other two statements regarding the equality. 
\end{proof}
\begin{mdframed}
The proof is clearly geometrical. If one wish to remember the proof, one should see the trick we use is exactly 
\begin{align*}
z\triangleq y-\abso{y}(\cos \theta )\hat{x}\text{ is the projection of $y$ onto $x^{\perp}$ }
\end{align*}
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
       \includegraphics[height=5cm,width=12cm]{CSp.jpeg}
   \end{minipage}
\end{center}
Then all we do rest is just expanding $\abso{y}^2=\abso{z+\tilde{x}}^2$, where $\tilde{x}=y-z=\abso{y}(\cos \theta)\hat{x}$, which give the answer and is easy to compute since $z\cdot \tilde{x}=0$.\\

Now, with Cauchy-Schwarz Inequality, we can check the triangle inequality 
\begin{align*}
  \norm{x+y}^2&=\langle x+y,x+y\rangle \\
&=\langle x,x\rangle +\langle x,y\rangle +\langle y,x\rangle +\langle y,y\rangle \\
&= \langle x,x\rangle + \langle y,y\rangle +2\text{ Re }\langle x,y\rangle \\
&\leq \norm{x}^2 + \norm{y}^2 + 2\abso{\langle x,y\rangle }\\
&\leq \norm{x}^2 + \norm{y}^2 + 2\norm{x}\cdot \norm{y}=\Big(\norm{x}+\norm{y}\Big)^2
\end{align*}
\end{mdframed}
\textbf{(Euclidean Space Abstract Part)} 
\begin{mdframed}
  By a \textbf{concrete Euclidean Space}, we mean some space of $n$-tuple  $(x_1,\dots ,x_n)$ over $\R$, equipped with inner product  $\langle \cdot,\cdot\rangle_E$ defined by 
\begin{align*}
  \langle  (x_1,\dots,x_n)  ,(y_1,\dots ,y_n)\rangle_E= \sqrt{\sum_{k=1}^n (y_k-x_k)^2} 
\end{align*}
By an \textbf{Euclidean Space}, we simply mean a finite dimensional vector space $V$ over $\R$,  equipped with an inner product $\langle \cdot,\cdot\rangle $ such that there exists a concrete Euclidean space $E$ and an isomorphism  $\phi:V\to E$ such that 
\begin{align*}
\hspace{3cm}\langle x,y\rangle= \langle \phi (x),\phi (y)\rangle_E \hspace{1.5cm}(x,y \in V)
\end{align*}
Note that if you define $\langle \cdot,\cdot\rangle $ on the space of $n$-tuples $(x_1,\dots ,x_n)$ over $\R$ by 
\begin{align*}
  \langle  (x_1,\dots,x_n)  ,(y_1,\dots ,y_n)\rangle=2 \sqrt{\sum_{k=1}^n (y_k-x_k)^2} 
\end{align*}
Then, the space of $n$-tuple is clearly not a concrete Euclidean space, and clearly an Euclidean space. 

\end{mdframed}
\textbf{(SVD)}

\chapter{Beauty}
\section{Euler's Formula}
\begin{mdframed}
Suppose that we define 
\begin{align*}
  \hspace{2cm}\exp(z)&\triangleq \sum_{n=0}^\infty \frac{z^n}{n!}\hspace{1.5cm}(z\inc)\\
  \hspace{2cm}\sin (z)&\triangleq \sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)!}z^{2n+1}\hspace{1.5cm}(z\inc)\\
  \hspace{2cm}\cos (z)&\triangleq \sum_{n=0}^\infty \frac{(-1)^n}{(2n)!}z^{2n}\hspace{1.5cm}(z\inc)
\end{align*}
Some properties we are familiar with is now easily seen using basic technique we learned in Chapter 3. 
\begin{enumerate}[label=(\alph*)]
  \item $\exp(z),\sin(z),\cos(z)$ are well defined on $\C$ by \myref{Cauchy-Hadamard}{Cauchy-Hadamard}.  
  \item $\begin{cases}
    \exp(x)\\
    \sin(x)\\
    \cos(x)
  \end{cases}\inr$ provided $x\inr$. 
  \item $\begin{cases}
    \exp(0)=1 \\
    \sin(0)=0\\
    \cos (0)=1
  \end{cases}$
  \item $\exp(x)$ is strictly increasing on $\R\hspace{0.5cm}(\because \abso{y^n-x^n}\leq \abso{y^n}+\abso{x^n})$
  \item $\exp(x)\nearrow \infty$ as $x\to \infty\hspace{0.5cm}(x\inr)$ 
  \item $\exp(x)\cdot \exp(y)=\exp(x+y)\hspace{0.5cm}(x\inc)$ by \myref{Merten's Theorem of Cauchy Product}{Merten Cau}
  \item $\exp (x)\searrow 0$ as $x \to -\infty\hspace{0.5cm}(x\inr)$
  \item $\begin{cases}
\frac{d}{dz}\exp(z)=\exp(z)\\
  \frac{d}{dz}\sin (z)=\cos (z)\\
  \frac{d}{dz}\cos (z)=- \sin (z)
  \end{cases}(z\inc)$, using \myref{Term-by-Term Differentiation}{AfaS}. 
  \item $\exp (x)$ is convex on $\R\hspace{0.5cm}(\because (e^x)''=e^x>0)$  
  \item $\exp (nz)=\big(\exp (z) \big)^n\hspace{0.5cm}(z\inc,n\inz)$, by induction and \myref{Merten's Theorem of Cauchy Product}{Merten Cau}. 
\end{enumerate}
In particular, we have \textbf{Euler's Formula}. 
\end{mdframed}
\begin{theorem}
\textbf{(Euler's Formula)}  
\begin{align*}
\hspace{2cm}\exp(iz)=\cos (z) + i \sin (z)\hspace{1.5cm}(z\inc)
\end{align*}
\end{theorem}
\begin{proof}
Define 
\begin{align*}
I_i(n)\triangleq \begin{cases}
  1& \text{ if $n \equiv 0$ (mod $4$) }\\
  i& \text{ if $n \equiv 1$ (mod $4$) }\\
  -1& \text{ if $n \equiv 2$ (mod $4$) }\\
  -i& \text{ if $n \equiv 3$ (mod $4$) }
\end{cases}
\end{align*}
Compute 
\begin{align}
\label{expiz}
\exp(iz)&=\sum_{n=0} \frac{(iz)^n}{n!}\notag\\ 
&=\sum_{n=0}^{\infty} \frac{I_i(n)z^n}{n!}\\
&=\sum_{n=0}^{\infty} \frac{I_i(2n)}{(2n)!}z^{2n}+\frac{I_i(2n+1)}{(2n+1)!}z^{2n+1}\hspace{0.5cm}\big(\because \text{ this is a sub-sequence of \myref{(}{expiz})}\big)\notag\\
&= \sum_{n=0}^{\infty} \frac{(-1)^{n}}{(2n)!}z^{2n}+ i\cdot \frac{(-1)^n}{(2n+1)!}z^{2n+1} \notag
\end{align}
Now, we can conclude 
\begin{align*}
\cos (z)+i \sin (z)&= \sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n)!}z^{2n} + i \cdot \sum_{n=0}^{\infty} \frac{(-1)^{n}}{(2n+1)!}z^{2n+1}\\
&=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n)!}z^{2n} +  \sum_{n=0}^{\infty} i\cdot  \frac{(-1)^{n}}{(2n+1)!}z^{2n+1} \\
&=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n)!}z^{2n} +  i\cdot  \frac{(-1)^{n}}{(2n+1)!}z^{2n+1} =\exp (iz)
\end{align*}
\end{proof}


\section{Alternative Definitions of $e^x$}
\begin{theorem}
\textbf{(First Characterization)} for all $x\inr$, the sequence $\set{(1+\frac{x}{n})^n}_{n\inn}$ has limit 
\begin{align*}
\lim_{n\to \infty} \Big(1+\frac{x}{n}\Big)^n = \sum_{k=0}^\infty \frac{x^k}{k!}
\end{align*}
\end{theorem}
\begin{proof}
The proof is trivial if $x=0$. First suppose  $x\inr^+$. Set 
\begin{align*}
t_n\triangleq \Big(1+\frac{x}{n}\Big)^n\text{ and }s_n\triangleq \sum_{k=0}^n \frac{x^k}{k!}
\end{align*}
We wish to show 
\begin{align*}
\vi{\limsup_{n\to\infty} t_n\leq \lim_{n\to \infty}s_n\leq \liminf_{n\to\infty} t_n}
\end{align*}
We first prove 
\begin{align*}
\blue{\limsup_{n\to\infty} t_n\leq \lim_{n\to \infty}s_n}
\end{align*}
Use Binomial Theorem to compute 
\begin{align*}
t_n=\Big(1+\frac{x}{n} \Big)^n&= \sum_{k=0}^n \binom{n}{k} \Big(\frac{x}{n} \Big)^k\\
&=\sum_{k=0}^n \frac{x^k}{k!} \cdot \frac{n(n-1)\cdots (n-k+1)}{n^k} \leq \sum_{k=0}^n \frac{x^k}{k!}=s_n \bdone
\end{align*}
We now prove 
\begin{align*}
\olive{\lim_{n\to \infty}s_n\leq \liminf_{n\to\infty} t_n}
\end{align*}
Fix $\epsilon $. Because $s_n\nearrow $, we know there exists  $m$ such that 
 \begin{align*}
s_m> \lim_{n\to \infty}s_n -\epsilon 
\end{align*}
Fix such $m$. Observe 
 \begin{align*}
\hspace{1.5cm}t_n=\sum _{k=0}^n \binom{n}{k}\Big(\frac{x}{n} \Big)^k\geq \sum_{k=0}^m \frac{x^k}{k!}\cdot \frac{n(n-1)\cdots (n-k+1)}{n^k}\hspace{1.5cm}(n\geq m)
\end{align*}
Clearly, there exists $N$ such that 
\begin{align*}
  \forall n>N, \abso{\Big( \sum_{k=0}^m \frac{x^k}{k!}\cdot \frac{n(n-1)\cdots (n-k+1)}{n^k}\Big) - s_m }\leq  s_m-(\lim_{n\to \infty}s_n-\epsilon )
\end{align*}
Then, we see for all $n>\max \set{N,m}$
\begin{align*}
  t_n\geq \sum_{k=0}^m \frac{x^k}{k!}\cdot \frac{n(n-1)\cdots (n-k+1)}{n^k} &\geq s_m - (s_m-\lim_{n\to \infty}s_n -\epsilon )\\
  &=\lim_{n\to \infty}s_n -\epsilon \odone \vdone
\end{align*}
For negative real, we only wish to show 
\begin{align*}
\vi{\sum_{n=0}^\infty \frac{(-x)^n}{n!}=\Big(\sum_{n=0}^\infty \frac{x^n}{n!} \Big)^{-1}}\text{ and }\blue{\lim_{n\to \infty}\Big(1-\frac{x}{n} \Big)^{n}=\Bigg(\lim_{n\to\infty    }\Big(1+\frac{x}{n} \Big)^{n} \Bigg)^{-1}}
\end{align*}
Because the convergence is absolute, we can use Merten's Theorem for Cauchy product (\myref{Theorem}{Merten Cau})to compute 
\begin{align*}
\Big( \sum_{n=0}^{\infty} \frac{(-x)^n}{n!}\Big)\cdot \Big( \sum_{n=0}^\infty \frac{x^n}{n!} \Big)&= \sum_{n=0}^\infty \Big(\sum_{k=0}^n \binom{n}{k}(-1)^{n-k} \Big) \frac{x^n}{n!}\\
&=\sum_{n=0}^\infty (1-1)^{n} \frac{x^n}{n!}=1 \vdone
\end{align*}
We first prove 
\begin{align*}
\olive{\lim_{n\to \infty}\Big(1-\frac{x^2}{n^2} \Big)^n=1}
\end{align*}
Computing term by term, it is clear that 
\begin{align*}
\limsup_{n\to\infty} \Big(1-\frac{x^2}{n^2} \Big)^n \leq 1
\end{align*}
Using Bernoulli's Inequality (\myref{Theorem}{Bernoulli's Inequality}), we see that for large enough $n$, we have 
\begin{align*}
  \Big(1-\frac{x^2}{n^2} \Big)^n\geq 1-\frac{x^2}{n} 
\end{align*}
This then implies 
\begin{align*}
\liminf_{n\to\infty} \Big(1-\frac{x^2}{n^2} \Big)^n \geq 1\odone
\end{align*}
Compute
\begin{align*}
\lim_{n\to \infty}\Big(1-\frac{x}{n} \Big)^{n} &= \lim_{n\to \infty}\Bigg(\frac{1-\frac{x^2}{n^2}}{1+\frac{x}{n}}\Bigg)^n \\
&=\frac{\lim_{n \to \infty} (1-\frac{x^2}{n^2})^n}{\lim_{n\to \infty}(1+\frac{x}{n})^n}=\frac{1}{\lim_{n\to \infty}(1+\frac{x}{n})^n}\bdone
\end{align*}





\end{proof}
\begin{mdframed}
\begin{align*}
\ln (x)\triangleq \int_1^x \frac{1}{t}dt
\end{align*}
By FTC (\myref{Theorem}{FTC1}), it is easy to see that 
 \begin{align*}
\hspace{1.5cm}\frac{d}{dx}\ln (x)=\frac{1}{x}\hspace{1.5cm}(x\inr^+)
\end{align*}
To see 
\begin{align*}
\ln (xy)=\ln (x)+ \ln (y)
\end{align*}
Fix $y\inr^+$ and set 
\begin{align*}
f(x)\triangleq \ln (x)\text{ and }g(x)\triangleq \ln(xy)
\end{align*}
Conclude $f'(x)=g'(x)$, and use FTC (\myref{Theorem}{FTC2}) to conclude $f-g$ is some fixed constant $k$. Now, see that 
\begin{align*}
g(1)=f(1)+k \implies k=\ln(y)
\end{align*}
Then, we have 
\begin{align*}
\ln(xy)=g(x)=f(x)+k=\ln (x)+ \ln (y)
\end{align*}
Using induction, it is now easy to see 
\begin{align*}
  \hspace{3cm}\ln (x^n)=n \ln (x)\hspace{2.5cm}(n\inz_0^+)
\end{align*}
\end{mdframed}
\begin{theorem}
\textbf{(Second Characterization)}
\end{theorem}
\section{Alternative Definition for $\sin$ and $\cos$}
\section{Fundamental Theorem of Algebra}
\begin{theorem}
\textbf{(Fundamental Theorem of Algebra)}
\end{theorem}




\end{document}
