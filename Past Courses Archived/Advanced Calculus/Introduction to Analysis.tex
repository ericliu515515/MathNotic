\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}


\title{\Huge{NCKU 112.1,112.2}\\Untitled}
\author{\huge{Eric Liu}}
\date{}
\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak
\setcounter{chapter}{-1}
\chapter{XXX Theory as Foundation}
\section{ZF}
\fbox{\begin{minipage}{39em
}
This section is written as a confirmation of knowledge. For a even more rigorous treatment, one can look for another note where the discussion of ZF is put after that of formal system and zeroth and first order logic.\\

Notice that in our discussion, the domain of discourse are sets. Plain speaking, every variable should be interpreted as set. In this section, we never interpret symbol $x$ as a real number (we never use symbol $x$ to denote a real number), and we never interpret symbol $G$ as a group  (we never use symbol $G$ to denote a group). Whatever variable symbol we use, we interpret the symbol a set (we use the symbol to denote a set), albeit $x,G\text{ or }h$.\\

However, if you wish, you can still interpret the symbol $x$ as a number or structure or any mathematical object or even cats, but I doubt there exists any interpretation that isn't set won't make people feel weird.\\   

Notice that ZF is a first order logic theory. We must first declare the following 
\end{minipage}}
\begin{definition}
\label{0.1.1}
\textbf{(Atomic Formula)} In our discussion in this section, there are precisely two atomic formulas:
\begin{equation*}
x\inA
\end{equation*}
and
\begin{equation*}
A=B
\end{equation*}
\end{definition}
\fbox{\begin{minipage}{39em}
The above definition may be confusing if one has no basic knowledge for formal system. I here highly recommend the first two chapters of the excellent book \textit{Elementary Formal System } written by Smullyan, and one can understand what does the words "formula" and "axiom" mean. \\

Now we start our discussion of ZF.
\end{minipage}}
\begin{axiom}\textbf{(Axiom of Extension)}
\label{0.1.2}
\begin{equation*}
\forall x(x\inA \longleftrightarrow x\inB)\longrightarrow A=B
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
TBH, I have no clue why this axiom is named axiom of extension. This axiom axiomatically define equality (between sets).\\

The practice of putting the word \textit{between sets} in a  parenthesis in the above paragraph serve as a reminder that the domain of discourse of our discussion should be interpreted as the class of all sets.\\ 

ZF in our discussion is a theory where the only two non-logical symbol is $\in\text{ and }=$. In our discussion, whenever the symbols $\subseteq,\cup ,\cap, \neq$ appear, one should immediately realize the sentence in which the symbols appear is an interpretation.\\

In ZF, we never define $A \subseteq B$. We never define $A\neq B$. We never define $A \cup B$. We merely construction a formal sentence that one feel it make sense to  interpret as $A \subseteq B$ or the English sentence "$A$ is a subset of  $B$.". 
\end{minipage}}
\begin{definition}
\label{0.1.3}
\textbf{(Interpretation)} We can interpret the formal sentence $\forall x(x\in A\longrightarrow x\in B)$ as informal sentence $A \subseteq B$. If we ever say $A \subseteq B$, we mean $\forall x(x\in A\longrightarrow  x\in B)$
\end{definition}
\begin{definition}
\label{0.1.4}
\textbf{(Interpretation)} We can interpret the formal sentence $\exists x((x\in A \land \neg x\in B)\lor(x \in B \land \neg x \in A))$ as the sentence $A\neq B$.
\end{definition}
\begin{axiom}
\label{0.1.5}
\textbf{(Axiom of Subset)} Let $P$ be a predicate. The axiom states
\begin{equation*}
\forall A\exists B\forall x(x\inB\longleftrightarrow x\inA\land P(x))
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
Because of axiom of subset, when we informally says $B=\set{x\inA: P(x)}$ in our argument, we know $B$ exists if  $P$ is a predicate. Notice that Russell's paradox rely on the construction of such set: $\set{y:P(y)}$ where predicate $P(x)$  is defined by $\neg x=x$. The axiom of subset only allow us to construct  "subset" (not set) filled with elements satisfying certain predicate.\\

Also, the axiom of subset allow us to construct intersection of two sets. Consider the set $\set{x\inA:P(x)}$ where $P(x)$ is defined by $x\inB$.\\

Notice that one at this point can trivially and formally deduce the existence of two sets that are ought to be interpreted as $A\cap B$ and $B\cap A$, and formulate a formal sentence that is ought to be interpreted as "$A\cap B$ is unique" and another formal sentence that is ought to be interpreted as "$A\cap B=B\cap A$", and deduce these two formal sentences.\\   

One can also formulate and deduce the formal sentence of "intersection operation is associative" as an exercise.\\

Now, before we introduce the next axiom, we first declare  that the symbol $\varnothing$ is in our alphabet.
\end{minipage}}
\begin{axiom}
\label{0.1.6}
\textbf{(Axiom of Existence of Empty Set)}
\begin{equation*}
\exists \varnothing\forall y\neg (y\in \varnothing)
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
In some people's formation of ZF, the axiom of existence of empty set isn't necessary, since it is possible to construct the empty set with axiom of subset. If we already know that the universe is nonempty, we can arbitrarily pick a set $A$ and construct  $\set{x\inA:P(x)}$ where $P(x)$ is defined $\neg x=x$.\\

At this point, one can try to formulate the formal sentence of "empty set is unique and is a subset of every set" and formally deduce such using axiom of extension.\\


We now introduce an axiom to construct new set.
\end{minipage}}
\begin{axiom}
\label{0.1.7}
\textbf{(Axiom of Pairing)}
\begin{equation*}
\forall x\forall y\exists A\forall v(v\inA\longleftrightarrow v=x \lor v=y)
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
    By axiom of pairing, one can deduce the formal sentence of "$\set{x,y}$ exists if $x,y$ exist".\\  

    Notice that we can also deduce the formal sentence of "$\set{x}$ exists if $x$ exists".
\end{minipage}}
\begin{axiom}
\label{0.1.8}
\textbf{(Axiom of Union)}
\begin{equation*}
\forall A\exists B \forall x(x\in B\longleftrightarrow \exists C(C\inA\land x\in C))
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
By axiom of union, one can deduce the formal sentence of "$\bigcup X$ exists, if $X$ exist".\\

Also, one can try to formulate and deduce the formal sentence of "For all set  $X$, we know  $\bigcup X$ is unique", that of "For all sets $x,y$, we know $x \cup y=y \cup x$" and that of "For all sets $x,y,z$, we know  $(x\cup y)\cup z=x\cup (y\cup z)$".\\    

Now we introduce a new axiom to construct even more sets.
\end{minipage}}
\begin{axiom}
\label{0.1.9}
\textbf{(Axiom of Power Set)} 
\begin{equation*}
\forall A\exists B\forall x(x\in B \longleftrightarrow \forall y(y\in x\longrightarrow y\in A))
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
The axiom of power set can be interpreted as that every set has at least one power set (and it can be proved that there exists only one for every set and that if two set have the same power set then the two set are the same). Normally and informally, we denote the power set of $x$ as $\mathcal{P}(x)$\\  

Now we introduce the standard way to "construct a model for natural numbers", that is, to "formally formulate sets that can be interpreted as natural numbers". The construction is straight forward: we interpret $\varnothing$ as $0$, and  while interpreting $x$ as a natural, interpret $x\cup \set{x}$ as $x+1$.\\

This construction seems to satisfy Peano axioms, but a problem will emerge when one trying to verify: one can construct any natural number, for instance $3$, by constructing  $\set{\varnothing,\set{\varnothing},\set{\varnothing,\set{\varnothing}}}$, but one can not construct the set of all natural numbers, that is, informally writing, $\N=\set{0,1,2,3,\dots }=\set{\varnothing, \set{\varnothing},\set{\varnothing,\set{\varnothing}},\set{\varnothing,\set{\varnothing},\set{\varnothing,\set{\varnothing}}},\dots }$.\\

The construction of the set $\N$, rely on the next axiom (while the construction of no element in $\N$ rely on the next axiom). In fact, the formulation of any infinite set rely on the next axiom. Without this axiom, we can not formulate any infinite set (after we construct the predicate $P(x)$ to be interpreted as "$P(x)$ is true if $x$ is infinite").
\end{minipage}}
\begin{axiom}
\label{0.1.10}
\textbf{(Axiom of Infinity)} 
\begin{equation*}
\exists I(\varnothing \in I \land \forall y(y\in I\longrightarrow y\cup \set{y}\in I))
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
Notice that every set interpreted as a natural number can be proved to belong to the set $I$ in  \myref{Axiom}{0.1.10}. In other (informal) words, the set interpreted as $\N$ is a subset of $I$.\\

Before we formulate such subset of  $I$, we first state the five Peano Axioms, outside of our formal system. One can and should read the following five axioms algebraically, not formally.  
\end{minipage}}
\begin{axiom}
\label{0.1.11}
\textbf{(Peano Axioms, outside of our formal system)} We say $\gen{U,S}$, where $S$ is a function, is a model of Peano axioms if    
\begin{equation*}
 0\in U\text{ (First Peano axiom) }
\end{equation*}
\begin{equation*}
a \in U \implies S(a)\in U\text{ (Second Peano axiom) }
\end{equation*}
\begin{equation*}
\forall x \in U, S(x)\neq 0 \text{ (Third Peano axiom) }
\end{equation*}
\begin{equation*}
S(a)=S(b)\implies a=b\text{ (Forth Peano axiom) }
\end{equation*}
\begin{equation*}
0  \in X\text{ and } \forall u \in U, u\in X\longrightarrow S(u)\in X\implies U\subseteq X \text{ (Induction axiom) }
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
Before reading the proof of the following theorem, notice that the set $\set{y\in\bigcup  x:\forall z(z\in x\longrightarrow y\in x)}$ is what we mean by $\bigcap  x$, and notice that the set $\set{a\in x: \neg a\in y}$ is what we mean by $x\setminus y$.\\

One can try to completely formalize $\bigcap x\text{ and }x\setminus y$  
\end{minipage}}
\begin{theorem}
\label{0.1.12}
\textbf{(Existence of Model of Natural Numbers in ZF)}
We first define five predicate resembling the five Peano axioms.
\begin{equation*}
P_1(x):=\varnothing\in x
\end{equation*}
\begin{equation*}
P_2(x):=\forall a(a\in x\longrightarrow a\cup \set{a}\in x)
\end{equation*}
\begin{equation*}
P_3(x):=\forall a(a\in x\longrightarrow a\cup \set{a}\neq \varnothing)
\end{equation*}
\begin{equation*}
P_4(x):=\forall a\forall b(a\in x \land b\in x \land a\cup \set{a}=b\cup \set{b}\longrightarrow a=b)
\end{equation*}
\begin{equation*}
  P_5(x):=\forall a([\forall b(b\in a\longrightarrow b \in x)\land \varnothing\in a\land \forall b(b\in a\longrightarrow b\cup \set{b}\in a)]\longrightarrow x=a)
\end{equation*}
Reminder: Notice that the notation $\cup $ is not in our formal language, I use the notation only for abbreviation.\\

The theorem states:
\begin{equation*}
\exists A(P_1(A)\land P_2(A)\land  P_3(A)\land  P_4(A)\land  P_5(A))
\end{equation*}
\end{theorem}
\begin{proof}
  Let $I$ be the set in \myref{Axiom}{0.1.10}. Define $ \call:=\set{X\in\power{I}:\varnothing \in X \land \forall a(a \in X \longrightarrow a\cup \set{a}\in X)} $. From now on, we will call $a\cup \set{a}$ the successor $S(a)$ of $a$ in our proof.\\

Then  $\call$ can be precisely described as the set that contain all subsets $X$ of  $I$ such that the set interpreted as  $0$ is in  $X$ and that the successor of each element in $X$ belong to  $X$.\\

Let $Y=\bigcap \call$. We seek to show that 
\begin{equation*}
\teal{P_1(Y)\land P_2(Y) \land P_3(Y) \land P_4(Y)\land P_5(Y)}
\end{equation*}
\vi{$P_1(Y)$} and \blue{$P_2(Y)$} are trivial. $\vdone \bdone$\\

\vi{$P_3(Y)$} is also trivial.$\vdone$\\


We now prove $\blue{P_5(Y)}$\\

Let $E \subseteq \bigcap  \call,\varnothing\in E\text{ and }\forall b(b\in E\longrightarrow b \cup \set{b}\in E)$. Notice that $E\subseteq \bigcap \call \implies \forall X\in \call ,E\subseteq X \subseteq I$, so $E\in \call $. Then $\bigcap \call \subseteq E \bdone$\\

We now prove \vi{$P_4(Y)$}\\

Before reading the following, one may want to bear in mind that $\forall x(x\in x\cup \set{x}\text{ and }x\subseteq x\cup \set{x})$ \\

Define $S:=\set{n\in \bigcap \call:\forall m(m \in n \longrightarrow  m\subseteq n) }$. We see that $S\subseteq \bigcap \call $ and that $\set{\varnothing}\in S$, trivially, so by $P_5(Y)$, we only have to show $n \in S \longrightarrow n\cup \set{n}\in S$ to show $S=\bigcap  \call $. To show such, observe that if $n\in S\text{ and }m \in n \cup \set{n}$, then we know $m \in n\text{ or }m \in \set{n}$, which is equivalent to $m \in n \text{ or } m=n$, which implies $m\subseteq n$, since by the definition of $S$, both  $m \in n\text{ and }m=n$ implies $m\subseteq n$. Then we know $n\in S\text{ and }m \in n \cup \set{n}\longrightarrow  m\subseteq n \subseteq n\cup \set{n}$.\\

The above prove that every element of element $n$ of  $\bigcap \call $ is a subset of $n$. \As{$m\neq n\in\bigcap \call $ but $m\cup \set{m}=n\cup \set{n}$}. Because $n\in n \cup  \set{n}=m\cup  \set{m}$ and $n\neq m$, we can deduce $n \in m$, which implies $n\subseteq m$. Using the same method we can also deduce $m \subseteq n\tCaC \vdone$    
\end{proof}
\fbox{\begin{minipage}{39em}
Notice that we didn't prove Peano axioms give a unique structure up to isomorphism. I personally haven't proved it yet, but I guess to prove such  isn't trivial and will require to show structure like $\set{0,1,2,\dots ;a,b}$ where $S(a)=b\text{ and }S(b)=a$ does not satisfy the induction axiom.\\

Say, we have proved the uniqueness of model of Peano axiom. We still haven't show in the universe of ZF, our choice of model for natural number is unique. For all we know, maybe the construction $\varnothing, \set{\varnothing},\set{\set{\varnothing}},\set{\set{\set{\varnothing}}},\dots $ also works. (Hint: Let's say there are other construction for natural numbers, there are still  reasons we pick $\bigcap \call $ as our model, since later you will see $a<b\inn\iff  a\in b\text{ and }a\subset b$)\\

So, it seems like \myref{Theorem}{0.1.12} is relatively weak. This is not the case however. The fact is that, we don't really require anything extra of \myref{Theorem}{0.1.12} to do further discussion.\\

The core concept of Axiom is that we don't care about what a mathematical object is, \textit{we only care about what a mathematical object does}. One should \textit{never} view axiom as something that is apparently true and can not be proved. One should know: in most domains of mathematics,  individually, axioms are simply a convention between one mathematician to another. \textit{Axioms are properties we wish a object to have so that we can make knowledge that is truly important using the property of the object}.\\

For instance, we never care about what are sets. We never care about what is $\N$. We never care about what is  $\R$. We only care about that  $\R$ is completed and we can do induction on $\N$. It doesn't matter if $\N$ contain a cat or Homer Simpson, as long as it satisfy Peano axiom and we can deduce all the Theorem.\\


Notice the construction of natural number: $\varnothing, \set{\varnothing},\set{\varnothing,\set{\varnothing}},\dots $ can each be formally proved to belong to $\bigcap  \call $. We now introduce the last axiom.  
\end{minipage}}
\begin{axiom}
\label{0.1.13}
\textbf{(Axiom of Foundation)}
\begin{equation*}
\forall X(X \neq \varnothing \longrightarrow \exists Y (Y\in X \land \forall z(z\in Y \longrightarrow \neg z\in X)))
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
In English, the axiom of foundation states that every nonempty set $X$ has an element $Y$ such that  $X\cap Y=\varnothing$. This axiom is sometimes discarded by people who use ZFC for mathematics not so related to set theory, since in the construction of most mathematical object, this axiom is unnecessary. This is also the reason this axiom is introduced last in this section. But of course, this axiom have purpose. We now introduce some results.  
\end{minipage}}
\begin{theorem}
\label{0.1.14}
\textbf{(No set belong to itself)}
\begin{equation*}
\forall x, \neg x\in x
\end{equation*}
\end{theorem}
\begin{proof}
\As{$\exists x,x\in x$}. Observe that by the axiom of foundation we have $x\cap \set{x}=\varnothing$, but we also have $x\in x\text{ and }x\in \set{x}$, so $x\in x \cap  \set{x}\tCaC$
\end{proof}
\begin{theorem}
\label{0.1.15}
  \textbf{(No infinitely descending chain of sets)}
  Informally, we can say that the following situation will never happen
  \begin{equation*}
  \dots \in x_3 \in x_2 \in x_1
  \end{equation*}
  Formally, we say
  \begin{equation*}
  \neg \exists X(\forall  x(x\in X \longrightarrow \exists y(y \in X \land y \in x)))
  \end{equation*}
  which reads "There does not exist a set $X$ such that for all $x\in X$ there exists a set  $y\in X$ belong to $x$"
\end{theorem}
\begin{proof}
\As{there exists a set $X$ such that for all $x\in X$ there exists $y\in X \cap x$}, we immediately see that for all $x\in X$, the intersection $x\cap X$ is nonempty \CaC.
\end{proof}
\section{Ordinal}
\fbox{\begin{minipage}{39em}
In previous section, we employed a purely set-theoretic formal language to delve into Zermelo-Fraenkel Set Theory. While this methodology is undoubtedly elegant for discussing set theory, it becomes notably cumbersome in mathematical domains with limited connections to set theory. This is mainly because many of the tools we typically rely on (such as functions, groups, real numbers, relations, and more) become unavailable. To utilize them, we must first construct all the sets that are interpreted as these tools, leading to extensive and convoluted formulae.\\

In this section, our goal isn't to rigorously prove everything using a formal language. Instead, we'll explain our proofs in sufficient detail so that, if desired, they can be formalized with minimal additional effort, albeit taking a bit more time.\\

Please note that this section primarily serves as a foundational toolkit for the subsequent sections. We will introduce all the necessary tools, and that's the extent of it.\\

In last section, we use a very philosophical and logical way to discuss. Here, we back to our usual algebraic and analytical way to discuss. We firs introduce axioms for ordered sets. 
\end{minipage}}
\begin{axiom}
\label{0.2.1}
\textbf{(Axiom for Poset)} Let $\leq $ be a relation on set $S$. We say $\gen{S,\leq }$ is partially ordered if   
\begin{equation*}
\forall x\in S, x\leq  x \text{ (Reflexive) }
\end{equation*}
\begin{equation*}
\forall x,y,z\in S, x\leq y\text{ and }y\leq z\implies x\leq z\text{ (Transitive) }
\end{equation*}
\begin{equation*}
\forall x,y\in S, x\leq y\text{ and }y\leq x\implies x=y \text{ (Antisymmetric) }
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
Notice that poset dose not require every two element to be comparable (trichotomy). Totally ordered set require such. 
\end{minipage}}
\begin{axiom}
\label{0.2.2}
\textbf{(Axiom for Totally Ordered Set)} Let $\leq $ be a relation on set $S$. We say $\gen{S,\leq }$ is totally ordered (or linearly ordered) if 
 \begin{equation*}
\forall x\in S, x\leq x\text{ (Reflexive) }
\end{equation*}
\begin{equation*}
\forall x,y\in S, x\leq  y\text{ or }y\leq x \text{ (Connected) }
\end{equation*}
\begin{equation*}
\forall x,y,z\in S, x\leq y\text{ and }y\leq z\implies x\leq z \text{ (Transitive) } 
\end{equation*}
\begin{equation*}
\forall x,y\in S,x\leq y\text{ and }y\leq x\implies x=y\text{ (Antisymmetric) }
\end{equation*}
\end{axiom}
\fbox{\begin{minipage}{39em}
We now give the axioms for strictly ordered sets.  
\end{minipage}}
\begin{axiom}
\label{0.2.3}
\textbf{(Axiom for Strictly Totally and Partially Ordered Set)} Let $<$ be a relation on set $S$. We say  $\gen{S,<}$ is strictly totally ordered if 
\begin{equation*}
\forall x\in S, x\not < x \text{ (Irreflexive) }
\end{equation*}
\begin{equation*}
\forall x,y\in S, x<y \implies y\not<x \text{ (Asymmetric) }
\end{equation*}
\begin{equation*}
\forall x,y,z\inS, x<y\text{ and }y<z\implies x<z \text{ (Transitive) }
\end{equation*}
\begin{equation*}
\forall x,y\in S, x\neq y \implies x<y\text{ or }y<x \text{ (Connected) }
\end{equation*}
$\gen{S,<}$ is strictly partially ordered if satisfy irreflexive, asymmetric and transitive axiom. 
\end{axiom}
\fbox{\begin{minipage}{39em}
Notice that if $S$ is totally ordered, then we can let $S$ be strictly totally ordered by defining  $a<b$ if  $a\leq b\text{ and }a\neq b$, and if $S$ is strictly totally ordered, we can let  $S$ be totally ordered by defining  $a\leq b$ if $a<b\text{ or }a=b$.\\


Again, if $S$ is partially ordered, then we can let $S$ be strictly partially ordered by defining  $a<b$ if  $a\leq b\text{ and }a\neq b$, and if $S$ is strictly partially ordered, we can let  $S$ be partially ordered by defining  $a\leq b$ if $a<b\text{ or }a=b$.\\

We now give definition of some basic notions.  
\end{minipage}}
\begin{definition}
\label{0.2.4}
\textbf{(Definition of Minimal and Maximal)} A maximal element $a$ of a subset $X$ of poset $Y$ or totally ordered set $Y$ is an element that no element $b\in X$ satisfy $a<b$. Minimal element is defined in similar fashion.
\end{definition}
\begin{definition}
\label{0.2.5}
\textbf{(Definition of Well Ordered Set)} A totally ordered set $S$ is said to be well ordered if every nonempty subset $X$ of  $S$ has a minimal element $x$. 
\end{definition}
\fbox{\begin{minipage}{39em}
Notice that $\N$ is well ordered, but all  $\Z,\Q,\Q^+,\set{0}\cup \Q^+,\R$ is not well ordered.\\

We now give the definition of ordinal, which is the core of this section.\\

Notice that there are other ways to define ordinal, but Von Neumann ordinal is the most elegant that can be formalized by our system in section 2.1. 
\end{minipage}}
\begin{definition}
\label{0.2.6}
\textbf{(Definition of Von Neumann Ordinal)} A set $S$ is an ordinal if  $S$ is a strictly totally well ordered set with respect to  $\in$ and every element of $S$ is a subset  of $S$. Precisely,  we say a set $S$ is an ordinal  if satisfy the following:
\begin{equation*}
\forall x\in S,x\not\in x\text{ (Irreflexive) }
\end{equation*}
\begin{equation*}
\forall x,y\in S, x\in y\implies y \not\in x \text{ (Asymmetric) }
\end{equation*}
\begin{equation*}
\forall x,y,z\in S, x\in y \text{ and } y\in z \implies x\in z\text{ (Trnasitive) }
\end{equation*}
\begin{equation*}
\forall x,y\in S, x\neq y\implies x\in y\text{ or }y\in x \text{ (Connected) }
\end{equation*}
\begin{equation*}
\forall V\subseteq S, \exists x\in V,\forall y\in V, x\in y \text{ (Well Order) }
\end{equation*}
\begin{equation*}
\forall x\in S, x\subseteq S \text{ (Subset) }
\end{equation*}
In other words, $S$ is an ordinal if  $\gen{S,\in}$ is strictly totally well ordered and every element of $S$ is a subset of  $S$. 
\end{definition}
\fbox{\begin{minipage}{39em}
Von Neumann's definition of ordinal may seem simple and straight forward, but is in fact very strong. Here we prove properties of Von Neumann ordinal we intend to have by definition.\\

Notice that in ZF, irreflexive and antisymetric is trivially satisfied by axiom of foundation. 
\end{minipage}}
\begin{theorem}
\label{0.2.7}
\textbf{(Every Element of Ordinal is an Ordinal)} If $S$ be an ordinal, then  $A \in S$ is an ordinal.
\end{theorem}
\begin{proof}
Notice that if $A=\varnothing$, then the proof is all trivial, so we only have to consider $A\neq \varnothing$. Because $A \subseteq S$, so transitive,  connected and well ordered is trivially proved.\\




We now prove \vi{$\forall x\in A, x \subseteq A$}.\\

Let $x\in A$, if $x$ are all empty, the proof is trivial. Let $\alpha \in x$, so we have $\alpha \in x \in A$\\

Because $A\in S$, we know $A \subseteq S$, so we know $x \in A \subseteq S$. Then we know $x \subseteq S$, so we know $\alpha \in x \subseteq S$. So we know $\alpha ,x,A$ all belong to $S$, then because $S$ is transitive, we conclude  $\alpha \in A\vdone $
\end{proof}
\begin{theorem}
\label{0.2.8}
\textbf{(Ordinal Successor)}
If $a$ is an ordinal, then  $a\cup \set{a}$ is an ordinal.
\end{theorem}
\begin{proof}
  Notice that $b\in a\cup \set{a}\implies b \in a\text{ or }b\in \set{a}\implies b\in a\text{ or }b=a$. If $b=a\text{ and }b\in a$, we see $a\in a$, so we know $b\in a\cup \set{a}\implies b\in a\text{ exclusively or }b=a$. \\

The proofs for subset part and connected part are relatively simple. Let $x,y\in a \cup \set{a}$. If $x\in a$, then $x \subseteq a\subseteq a\cup \set{a}$. If $x=a$, then  $x=a\subseteq a\cup \set{a}$. Let $x\neq y$. If $x,y$ are both in  $a$, then the proof is finished. If, WOLG, $y=a$, then  $x \in a= y$.\\ 

We now prove \vi{the transitive part}.\\

 Let $x,y,z\in a \cup  \set{a}$ and $x\in y\text{ and }y\in z$. By axiom of foundation, it is impossible more than one of $x,y,z$ equals to  $a$. If $x=a$, then we have $y \in a$, so we see $\cdots a=x\in y\in a\in y \in a $, which implies it is impossible $x=a$. Similarly, we can deduce $y\neq a$. So far, we have proven $x\in a\text{ and }y\in a$. If $z\in a$, then because $a$ is an ordinal, our proof if done. If $z=a$, we see  $x\in a=z\vdone$\\


We now prove \blue{the well ordered part}.\\

Let $V\subseteq a \cup \set{a}$. By axiom of foundation, we know $a$ and $\set{a}$ are disjoint, so if $a\not \in V$, then $V \subseteq a$, and by the definition of $a$, our proof is finished.  If $a \in V$, we see either $a$ is the only element of  $V$ or  there exist element other than $a$ in $V$. If the situation is former, then the proof is trivially finished. If the situation is latter, then we observe $V \setminus \set{a} \subseteq a$, so $V \setminus \set{a}$ has a minimal element $x$, and  $x\in a$ indicate $x$ is also the minimal element of  $V\bdone$ 
\end{proof}
\begin{theorem}
\label{0.2.9}
  \textbf{(Basic Property of Ordinals)} If $a,b$ are ordinals, we have
   \begin{equation*}
  a \in b \iff a \subset b
  \end{equation*}
\end{theorem}
\begin{proof}
From left to right is relatively simple. If $a\in b$, then we have $a \subseteq b$. By axiom of foundation, we know $a \in b \implies  a\neq  b$, so we have $a \subset b$.\\

$(\longleftarrow)$\\

Let $a \subset b$. We wish to prove $a=x:=\min b\setminus a$, so that $a=\min b\setminus a\in b$. We now prove \vi{$x \subseteq a$} and prove  \blue{$a \subseteq x$}.\\

By definition of $x$, we have $\forall y\in b \setminus a, y\not\in x$. This tell us $b \setminus a$ and $x$ are disjoint. Because $x\subseteq b$, the fact that $b \setminus a$ and $x$ are disjoint tell us that $x \subseteq a\vdone$\\

Let $z\in a$, we wish to prove $z \in x$. Notice that $z \in a \subset b$, so because  $b$ is an ordinal and  $x \in b$, we know either $z\in x\text{ or }z=x\text{ or }x \in z$. \As{$x=z$}. We have  $\min b\setminus a =x =z \in a\tCaC$. \As{$x\in z$}. Because $a$ is an ordinal and  $z\in a$, we have $\min  b \setminus a = x \in z \subseteq a\tCaC$. Now we have $z \in x$ as desired $\bdone$
\end{proof}
\begin{lemma}
\label{0.2.10}
If $A,B$ are ordinals, then  $A \cap B$ is an ordinal.
\end{lemma}
\begin{proof}
Transitive and connected and well ordered of $A\cap B$ trivially inherit from that of $A$. Observe $x\in A \cap  B\implies x \in A\text{ and }x\in B\implies x\subseteq A\text{ and }x\subseteq B\implies x\subseteq A\cap B$, and we are finished.      
\end{proof}
\begin{theorem}
\label{0.2.11}
\textbf{(Trichotomy of Ordinals)} If $a,b$ are ordinals, we have
\begin{equation*}
a \subset b \text{ or }a=b\text{ or }b \subset a
\end{equation*}
\end{theorem}
\begin{proof}

\As{$a\not \subset b\text{ and }a\neq b\text{ and }b\not \subset a$}. We have $a\setminus b\neq \varnothing\text{ and }b\setminus a\neq \varnothing$, since, say, if $a\setminus b=\varnothing$, we have $a \subseteq b$.\\

By  \myref{Lemma}{0.2.10}, we know $a\cap b$ is an ordinal. Because $a \setminus b\neq \varnothing$, we know $a \cap  b \subset a$, and then by \myref{Theorem}{0.2.9} we have $a \cap  b \in a$. Similarly, we have $a \cap b \in b$. Then we have $a\cap  b \in a \cap  b\tCaC$
\end{proof}
\begin{corollary}
\label{0.2.12}
\textbf{(Trichotomy of Ordinals)} If $a,b$ are ordinals, we have
 \begin{equation*}
a \in b\text{ or }a=b\text{ or }b\in a
\end{equation*}
\end{corollary}
\begin{corollary}
\label{0.2.13}
\textbf{(The Class of All Ordinal is Totally Ordered)} 
The class of all ordinals is totally ordered by set-membership $\in$. 
\end{corollary}
\begin{proof}
The irreflexive and assymmetric part is trivially satisfied by axiom of foundation. The transitive part is simple: $x\in y\text{ and }y\in z\implies x\in y\subseteq z$. The connected part is just proven by \myref{Corollary}{0.2.12}. 
\end{proof}
\begin{theorem}
\label{0.2.14}
\textbf{(The Class of All Ordinals is Well Ordered)} Every set of ordinals has a $\in$-minimal element.
\end{theorem}
\begin{proof}
If not, then it contradicts to \myref{Theorem}{0.1.15}
\end{proof}
\begin{theorem}
\label{0.2.15}
\textbf{(Burali-Forti Paradox)}
All ordinals do not form a set.\\

Formally speaking,  define $P(S)$ are true if and only if $S$ is an ordinal. Then we have 
\begin{equation*}
\neg \exists A\forall x(x \in A\longleftrightarrow P(x))
\end{equation*}
\end{theorem}
\begin{proof}
\As{$\Omega$ is the set of all ordinals}. By \myref{Theorem}{0.2.14}, we see $\Omega$ satisfy at least irreflexive, asymmetric, trnasitive, connected and well ordering requirement of ordinal.\\

Let $x \in \Omega$, and $y \in x$. By \myref{Theorem}{0.2.7}, we know $y \in \Omega$, so in fact we have $x \subseteq \Omega$, which prove $\Omega$ is an ordinal. Then we have $\Omega \in \Omega \tCaC$
\end{proof}
\fbox{\begin{minipage}{39em}
Whewww, we have proved a lot of intended properties of ordinal. Notice that although there does not exist a set consist precisely of all ordinals. We still can mention the class of all ordinal by $Ord$.\\

 Notice that $Ord$ is in fact an extension of  $\N$, and it goes like this
  \begin{equation*}
  0,1,2,3,\dots ;\omega, \omega+1,\omega +2,\dots
  \end{equation*}
\end{minipage}}
\begin{theorem}
\label{0.2.16}
  \textbf{(All Naturals are Ordinals)} All element of  $\bigcap \call$ are ordinals. 
\end{theorem}
\begin{proof}
We prove by induction, which is  the $P_5(\bigcap  \call )$ of \myref{Theorem}{0.1.12}.\\

Notice that $\varnothing$ trivially satisfy all property of ordinals, which finish the proof for base step, and notice that by \myref{Theorem}{0.2.8}, the proof for induction step is also finished.  
\end{proof}
\begin{theorem}
\label{0.2.17}
\textbf{($\N$ as a set is an ordinal)} $\bigcap \call $ is an ordinal. 
\end{theorem}
\begin{proof}
By \myref{Theorem}{0.2.16}, we have proved that $\cap \call $ is a strictly totally ordered set. Notice that every subset of a well ordered set is well order, so it only left to prove the \vi{subset part} of definition of ordinal.\\

We prove by induction. Let $X:=\set{x \in \bigcap \call  : x \subseteq \bigcap \call }$. Base case: $\varnothing \in X$ is trivial. Let $a \in X$. Then we have $a\in \bigcap   \call$ and $a \subseteq \bigcap   \call $, so we have $a \cup \set{a} \subseteq \bigcap \call  \vdone$ 
\end{proof}
\begin{theorem}
\label{0.2.18}
\textbf{(Transfinite Induction)} 
Let $X$ be a nonempty ordinal. For all $x \in X$, we define 
\begin{equation*}
S_x:=\set{z \in X: z\in x }
\end{equation*}
Let $Y\subseteq X$. If $Y$ satisfy the property:
\begin{equation*}
\forall x\in X( S_x \subseteq Y \longrightarrow x \in Y )
\end{equation*}
we have $Y=X$
\end{theorem}
\begin{proof}
\As{$Y$ satisfy the property, but $Y\neq X$}. 
We first show \vi{$Y$ is nonempty}.  Because $X$ is an ordinal, we can pick an element $r \in X$ such that $\forall u \in X, r\in u$, so we have $S_r=\varnothing \subseteq Y$. Then because $Y$ satisfy the property, we have  $r \in Y\vdone$\\

Because $X$ is an ordinal, we know there exists  $x_0 \in X\setminus Y$ such that $\forall x\neq x_i\in X\setminus Y, x_0 \in x$.\\

Let $a \neq  x_0 \in X$, we know either $a \in Y$ or $a \in X\setminus Y$, we now prove \blue{$a \in x_0 \implies a \in Y$}. \As{$a \not \in Y$}, we then have $a\in X\setminus Y$, so we have $x_0 \in a \tCaC \bdone$.\\

The \blue{blue part} show that $S_{x_0} \subseteq Y$, so by the property of $Y$, we have  $x_0 \in Y$, but $x_0 \in X \setminus Y \tCaC$.

\end{proof}
\section{Axiom of Choice and its equivalents}

\fbox{\begin{minipage}{39em}
In this section, we aim to establish the equivalence between the Axiom of Choice and Zorn's Lemma. Additionally, we will explore and provide proofs for various applications of Zorn's Lemma. We'll begin by laying down foundational theorems in order theory, which will serve as our lemmas. Using these, we will demonstrate the progression from the Axiom of Choice to Zorn's Lemma. Interestingly, the reverse direction, proving from Zorn's Lemma to the Axiom of Choice, is considerably more straightforward and can be viewed as an "application" of Zorn's Lemma. Towards the end, we will delve into other notable applications of Zorn's Lemma.\\

Notice in this section, $P$ is a non-empty partially ordered set (poset).
\end{minipage}}
\begin{definition}
\label{0.3.1}
\textbf{(Definition of Chain)} We say  $X\subseteq P$ is a chain if $X$ is totally ordered. 
\end{definition}
\begin{definition}
\label{0.3.2}
\textbf{(Definition of Well Ordered Set)} Let $X\subseteq P$. We say  $X$ is well ordered only if $X$ is totally ordered.
\end{definition}
\begin{definition}
\label{0.3.3}
\textbf{(Definition of Upper Bound)} Let $X\subseteq P$. We say $X$ is bounded above by  $t$ if
\begin{equation*}
\forall x \in X, x\leq t
\end{equation*}
\end{definition}
\begin{definition}
\label{0.3.4}
\textbf{(Definition of Initial Segment)} Let $C$ be a chain of $P$. We say $T$ is an initial segment of $C$ if
\begin{equation*}
T\subseteq C \text{ and }\forall  v\in T,\forall u\in C, u \leq v \implies  u \in T
\end{equation*}
In this section, we denote the statement: $T$ is an initial segment of  $C$ by $T\subseteq' C$, and we denote $T\subseteq'C\text{ and }T\neq C$ by $T\subset' C$
\end{definition}
\begin{lemma}
\label{0.3.5}
An initial segment of a chain is also a chain
\end{lemma}
\begin{lemma}
\label{0.3.6}
Let  $S \subseteq P$ be well ordered and $t$ be an upper bound of  $S$. We have $S \cup \set{t}$ is also well ordered. 
\end{lemma}
\begin{proof}
Let $V \subseteq S\cup \set{t}\text{ and }V\neq \varnothing$ . If $V\cap S$ is empty, then we know $V=\set{t}$ and the proof is trivially finished. If $V\cap S$ is nonempty, let $x= \min V\cap S$. We know $y\in V\implies y \in V \cap S\text{ or }y=t$. If $y \in V\cap S$, then $x\leq y$ by definition of $x$. If  $y=t$, then  $x\leq t=y$ by $x\in S\text{ and the definition of $t$}$  
\end{proof}
\begin{lemma}
\label{0.3.7}
If $\cc$ is a set of well ordered chains in $P$ where 
 \begin{equation*}
X,Y \in \cc \implies X\text{ is an initial segment of $Y$ or $Y$ is an initial segment of  $X$}
\end{equation*}
then $\bigcup \cc$ is a well ordered chain 
\end{lemma}
\begin{proof}
Notice that the definition of $\cc$ implies
\begin{equation*}
X,Y \in \cc \implies  X \subseteq Y\text{ or }Y\subseteq X
\end{equation*}

Let $a,b \in \bigcup \cc $, and let $a \in A \in \cc, b \in B\in\cc$. Observe $A \subseteq B$ or $B \subseteq A\implies a,b \in A\text{ or }a,b \in B$, because $A,B$ are chains, we deduce $a,b$ are  comparable.  All the other properties of totally ordered set inherit from $P$, so we have proven  $\bigcup \cc$ is a chain.\\

We first prove \vi{$\gen{\cc,\subseteq'}$ is totally ordered.}\\

Connected part and antisymmetric part are implied by the definition of $\cc$. One can verify the reflexive part trivially hold true. Let  $A\subseteq'B\subseteq'C\text{ and }a\in A, c \in C\text{ and }c\leq a$. Because $a \in B$ and $B \subseteq' C$, we can deduce $c \in B$ from $c \leq a$. Then we can deduce $c \in A$ from $c \in B\text{ and }A\subseteq' B$, finishing the proof for transitive part $\vdone$\\


Let $V \subseteq \bigcup \cc$. We now prove \blue{$V$ contain a minimal element}.\\

Arbitrarily pick $X\in \cc$ so that $V\cap X$ is nonempty. Because $V \cap X \subseteq X$ and $X$ is well ordered, we know $\min V \cap X$ exists. \As{$\exists v' \in V, v'< \min V \cap  X$}. We know $\min  V\cap  X\in X\in \gen{\cc,\subseteq'}$. Let $S=\set{Z \in \cc : v' \in Z}$. If $X \in S$, then $v'\in V \cap  X$ contradicting to $v' <\min V\cap  X$, so we know $X \not\in S$. Observe $v' \in Z\subseteq ' X \implies  v' \in X $ causing the same contradiction, so we know every element in $S$ is greater than  $X$ with respect to  $\subseteq '$. We know $S$ is non empty, since  $v' \in V \subseteq \bigcup \cc$. Then we can arbitrarily pick $Z\in S$. By definition of initial segment, we can deduce $v' \in X$ from $v'<\min V \cap  X\text{ and } Z\subseteq' X \tCaC\bdone$ 
\end{proof}
\begin{lemma}
\label{0.3.8}
The class of initial segments of a chain is totally ordered by $\subseteq' $
\end{lemma}
\begin{proof}
Antisymmetric, reflexive and transitive parts are implied by definition of $\subseteq'$, as shown in the proof of  \myref{Lemma}{0.3.7}. Now we let $X$ be a chain and let  $A\subseteq' X\text{ and }B\subseteq' X$, and prove \vi{$A \subseteq' B\text{ or }B\subseteq' A$}\\

\As{$A\not\subseteq B\text{ and }B\not\subseteq A$}. Arbitrarily pick $x \in A\setminus B\text{ and }y \in B\setminus A$. We know $x\neq y$, since $y\not\in A\setminus B$. WOLG, let $x<y$. Because $x\in A\subseteq X$, where $y\in B$ and $B\subseteq' X$, so we deduce $x \in B\tCaC$\\

WOLG, let $A \subseteq B$. \As{$A\not\subseteq' B$}. Then there exists $a\in A,b\in B$ such that $b<a\text{ and }b\not\in A$. Yet, we see $A\subseteq' X,b\in X\text{ and }a \in A$, so we can deduce $b \in A\tCaC$ 
\end{proof}
\fbox{\begin{minipage}{39em}
While \myref{Lemma}{0.3.7} might appear to be a potent assertion, implying that the union of a totally ordered (by inclusion) set of well-ordered subsets is itself well-ordered, this is far from the truth. For instance, consider the union of the sets $\set{\set{-1},\set{-1,-2},\set{-1,-2,-3},\dots}$. This union is the set of negative integer, apparently not well-ordered, even though the set $\set{\set{-1},\set{-1,-2},\dots}$ is a set of well-ordered subsets of $\Z$ when ordered by inclusion. This counterexample is even more compelling than our original premise.\\

Next, we'll move on to demonstrating the link from the Axiom of Choice to Zorn's Lemma. But before delving into that, let's first formulate the concept of a function within the ZF set theory.
\end{minipage}}
\begin{definition}
\label{0.3.9}
\textbf{(Construction of Function)} In ZF, we interpret the set
\begin{equation*}
\set{\set{a},\set{a,b}}
\end{equation*}
as $(a,b)$ 
and we interpret the set 
\begin{equation*}
\set{(x,y):x\in X, y \in Y}
\end{equation*}
as a function from $X$ to  $Y$. Carefully notice we can construct predicate $Q(f)$ as below to tell if a set should be interpreted as a function.
\begin{equation*}
\forall A (A \in f \longrightarrow \forall x (\set{x} \in A\longrightarrow  \exists yH(A,x,y)\lor A=\set{\set{x}})\land  G(f,A,x))
\end{equation*}
where predicate $H(A,x,y)$ is defined by
\begin{equation*}
x\neq y\land \set{x,y}\in A\land \forall z(z \in A\longrightarrow z=\set{x}\lor z=\set{x,y} )
\end{equation*}
and predicate $G(f,A,x)$ is defined by 
\begin{equation*}
\forall B (B \in f\longrightarrow B=A\lor \set{x}\not\in B)
\end{equation*}
Also, carefully notice we can construct predicate $Q_{f}(x,y)$
\begin{equation*}
\exists A\in f(\set{x}\in A\land \set{x,y}\in A)
\end{equation*}
which if hold true should be interpreted as $f(x)=y$
\end{definition}
\begin{theorem}
\label{0.3.10}
\textbf{(Axiom of Choice Implies Zorn's Lemma)}
If \begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
\textbf{AC:} For each set of nonempty sets  $Y$, there exists a function $f:Y\rightarrow \bigcup Y$ such that $f(X)\in X$ 
\end{minipage}
\end{center}
Then
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
       \textbf{Zorn's Lemma:} For every partially ordered set $P$ such that every chain is bounded, there exists a maximal element. 
    \end{minipage}
\end{center}
\end{theorem}
\begin{proof}
Because the set of upper bound of chain is always nonempty, we can let $g$ be a function that map each chain $C$ in  $P$ to the set of upper bounds of $C$, and let $h$ be a choice function on image of $g$. Then, we define $f=h\circ g$, so we have  $\forall x \in C, x\leq f(C)$.\\

The above use the axiom of choice. We now define 
\begin{multline}
\cc:=\set{C\subseteq P: C\text{ is well ordered},\min C=p,T\subset' C\longrightarrow f(T)=\min C\setminus T}
\end{multline}
We see $\set{p}\in \cc$, so $\cc$ is nonempty. We first prove \vi{$\gen{\cc,\subseteq'}$ is totally ordered}.\\

Reflexive part, transitive part and antisymmetric part are implied by the definition of $\subseteq'$, as shown in the proof of \myref{Lemma}{0.3.7}. Let $A,B \in \cc$, and define
\begin{equation*}
R:=\bigcup \set{T: T \subseteq' A\text{ and }T\subseteq' B}
\end{equation*}
Let $x\in R\text{ and }y \in A$. Then we find $T$ such that $x \in T\subseteq' A$. Observe if $y<x$, then $y\in T \subseteq R$. This prove $R\subseteq' A$. Similarly, we can prove $R\subseteq' B$. Then by the definition of $R$, we know  $R$ is greatest common initial segment of  $A\text{ and }B$.\\

\As{$R\subset' A\text{ and }R\subset' B$}. Then we see $f(R)=\min A\setminus R=\min B\setminus R$. Let $a\inA\text{ and }r'\in R\cup \set{f(R)}$. Observe
\begin{equation*}
a<r' \implies \begin{cases}
  a<f(R)=\min A\setminus R\\
  a<r \in R\subseteq' A
\end{cases}\implies \begin{cases}
  a \not\in A\setminus R\\
  a \in R
\end{cases}\implies a\in R
\end{equation*}
So we have $R\cup \set{f(R)}\subseteq' A$. Similarly we have $R\cup \set{f(R)}\subseteq' B$. Because $f(R)=\min A\setminus R\not\in R$, so we know $R\subset R\cup \set{f(R)}\subseteq R\text{ by definition of $R$ }\tCaC$\\

Because $R\subseteq' A\text{ and }R\subseteq' B$. WOLG, we can let $R=A$, and we see $A=R\subseteq' B\vdone$\\

Let 
\begin{equation*}
U:=\bigcup \cc
\end{equation*}
We now prove \blue{$U\in \cc$}.\\

Notice that by \myref{Lemma}{0.3.7}, we know $U$ is a well ordered chain. Because $x \in U \implies \exists D,x \in D \in \cc\implies p\leq x$, so $p$ is also the minimal element of  $U$. Then now we only have to prove \blue{$T\subset' U\longrightarrow  f(T)=\min U\setminus T$}. \\

Arbitrarily find $T\subset' U$, and arbitrarily pick $s \in U\setminus T$. By definition of $U$, we know there exists $S\in \cc$ such that $s \in S$.\\

We wish to prove \teal{$S\subseteq' U$}. Arbitrarily pick $v \in U$ less than $s$ and let $v \in V \in\cc$. By \vi{violet part}, we know $S\subseteq' V\text{ or }V\subseteq' S$. If $V\subseteq' S$, then we prove $v<s \implies v \in V\subseteq S$. If $S\subseteq ' V$, then $v<s \implies  v \in S$. Notice both of them mean $S \subseteq' U$, since $v$ is arbitrarily picked from  $U\tdone$.\\



Because $T\subseteq' U\text{ and }S\subseteq' U$, by \myref{Lemma}{0.3.8}, we know either $S\subseteq' T\text{ or }T\subseteq' S$. Notice $s \in S\text{ and }s\in U\setminus T$, so we know $T\subset' S$. Then because $S \in \cc$, we know $f(T)=\min S\setminus T$. Notice that it can be trivially proved  $S\subseteq' U \implies S\setminus T \subseteq' U\setminus T$, so in fact we have $f(T)=\min S\setminus T=\min U\setminus T\bdone$\\

\As{$U$ does not contain a maximal element of  $P$}. We now prove \vi{$U\cup \set{f(U)}\in \cc$}\\

Notice that by \myref{Lemma}{0.3.6}, we know $U\cup \set{f(U)}$ is a well ordered chain, and because $f(U)$ is an upper bound of $U$, we know $p=\min U\cup \set{f(U)}$, so we only have to prove \vi{$T\subset' U\cup \set{f(U)}\longrightarrow f(T)= \min U\cup \set{f(U)}\setminus T} $}.\\

Because $f(U)$ is an upper bound of $U$, we see  $f(U)=\max U\cup \set{f(U)}$, and because $U$ contain no maximal element, we know  $f(U)\not\in U$, that is, $U\subset U\cup \set{f(U)}$.\\

Notice we have $f(U)\not\in T$, since $f(U)\in T\implies T=U\cup \set{f(U)}$. Then observe 
\begin{gather*}
f(U)\not \in T\text{ and }T\subset' U \cup \set{f(U)}\\
\implies T\subseteq' U\\
\implies T=U\text{ or }T\subset' U\\
\implies f(T)=f(U)=\min U\cup \set{f(U)}\setminus U=\min U\cup \set{f(U)}\setminus T\\
\text{ or }f(T)=\min U\setminus T=\min U\cup \set{f(U)}\setminus T\vdone
\end{gather*}


Then we see $U\cup \set{f(U)}\in \cc\implies U\cup \set{f(U)}\subseteq \bigcup \cc= U \tCaC$
\end{proof}
\fbox{\begin{minipage}{39em}
We will first delve into a straightforward application of Zorn's Lemma. Following that, we'll demonstrate how Zorn's Lemma inherently suggests the Axiom of Choice.
\end{minipage}}
\begin{theorem}
\label{0.3.11}
\textbf{(Zorn's Lemma Implies Every Vector Space Has a Basis)} If
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
       \textbf{Zorn's Lemma:} For every partially ordered set $P$ such that every chain is bounded, there exists a maximal element. 
    \end{minipage}
\end{center}
then 
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
        Every vector space has a basis.
    \end{minipage}
\end{center}
\end{theorem}
\begin{proof}
  Let $V$ be a vector space and let  $\mathcal{X}$ be the set of all linearly independent set of  $V$.  Order  $\mathcal{X}$ by inclusion, we now prove \vi{every chain $\cc$ in $\mathcal{X}$ is bounded above by $\bigcup \cc$}.\\

We first prove \teal{$\bigcup \cc$ is linearly independent}. Arbitrarily pick a finite subset $S$ of  $\bigcup \cc$, and for each vector $s_i \in S$, pick a linearly independent set $S_i \in \cc$ such that $s_i \in S_i$. Let $S_n$ be the maximal element of  $\set{S_i}$ by inclusion. Then we have $S\subseteq S_n$, so $S$ is linearly independent $\tdone$\\

By definition, we have $C\in \cc\implies C \subseteq \bigcup \cc\vdone$.\\

Then by Zorn's Lemma, we know there exist a maximal element $W$ of $\mathcal{X}$. We now prove \blue{$U$ spans  $V$}.\\

\As{there exists $v\in V\setminus \text{span}(U)$}. Then we see $U\subset U\cup \set{v}$ and see $U\cup \set{v}\in \mathcal{X}\tCaC\bdone$
\end{proof}
\fbox{\begin{minipage}{39em}
In direct application of  Zorn's Lemma, especially those involving chains ordered by inclusion, a common tactic is to use the union of a chain as an upper bound. Conceptually, this can be likened to computing the limit or supremum of a monotonically increasing sequence.\\

We now prove that Zorn's Lemma also implies axiom of choice, so we know Zorn's Lemma is equivalent to axiom of choice.
\end{minipage}}
\begin{theorem}
\label{0.3.12}
\textbf{(Zorn's Lemma Implies Axiom of Choice)} If 
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
\textbf{Zorn's Lemma:} For every partially ordered set $P$ such that every chain is bounded above, there exists a maximal element     
    \end{minipage}
\end{center}
Then
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
        \textbf{AC:} For each set of nonempty set  $Y$, there exists a function $f:Y\rightarrow \bigcup Y$ such that $f(X)\in X$ 
    \end{minipage}
\end{center}
\end{theorem}
\begin{proof}
Notice that we prove the theorem in the sense of ZF.\\

  Let $P$ be the set of all function $f$ from subset of  $Y$ to  $\bigcup Y$ such that $f(X)\in X$. In $P$, we define
\begin{equation*}
g\leq h\text{ if }g\subseteq h
\end{equation*}
Let $\cc$ be a chain in $P$, we  now prove \vi{$\bigcup \cc$ is an upper bound of $\cc$}.\\

Let $A \in \bigcup \cc$, so we know there exists $f \in \cc$, such that $A\in f$, then we know $A$ is indeed a set of the form  $\set{\set{x},\set{x,y}}$.\\

Let $B=\set{\set{x},\set{x,y}}\in \bigcup \cc$. \As{$\exists z, \exists C\in \cc, C=\set{\set{x},\set{x,z}}\in \bigcup \cc$}. Arbitrarily find $B',C'$ such that  $B\in B' \in \cc$ and $C \in C'\in \cc$. Then WOLG, let $B'\subseteq C'$, so we see $B\in C'$ then we see $C'$ is not a function  \CaC. For now we have proven $\bigcup \cc \in P$, then the fact that $\bigcup \cc$ is an upper bound of $\cc$ trivially follows. $\vdone$ \\

Let $f$ be a maximal element of  $P$. \As{Domain of  $f$ is a proper subset of  $Y$}. Then we can arbitrarily select an element $X\in Y\setminus Dom(f)$, and arbitrarily select an element $x\in X$, then define $f'=f\cup \set{\set{X},\set{X,x}}\tCaC$
\end{proof}
\fbox{\begin{minipage}{39em}
In the proof above, there are a couple of crucial observations to make:\\

\textbf{Brevity of the Proof:} The proof is notably more concise than proving that the Axiom of Choice (AC) implies Zorn's Lemma. This conciseness is a reason why some might prefer to take Zorn's Lemma as an axiomatic starting point, even though it's just an equivalent formulation of AC.\\

\textbf{Choice from a Set:} In the concluding section of the proof, we "select" an element from specific sets on two occasions. Beginners might assume that a choice function can only be defined if AC is assumed. However, this isn't the case for a finite collection of sets. One doesn't necessarily need the Axiom of Choice for this.\\

The real potency of the Axiom of Choice lies in its ability to address \textit{situations that demand an infinite sequence of selections, where there's no predefined order or method to the selection process}.\\

In our proof, when we "select" an element, it's from a singular set. Consequently, the domain of this choice function is of cardinality $1$. It's evident that the Axiom of Choice isn't necessary here.\\

To delve into why the Axiom of Choice isn't required for finite situations, consider attempting a formal construction of a choice function for the set $\set{\set{1,2},\set{3,4},\set{5,6}}$.\\

At this point, one may wonder, is it true that no construction of choice function on countable set of nonempty set require Axiom of Choice? This question in simple words want to ask: if a set $ X$ is countable, meaning I have already build a bijective function from $\N$ to $X$, meaning I can well order $X$, and to select an element from one nonempty set is doable as above explained, why can't I recursively do this and have a choice function on $X$? The answer is: To \textit{recursively} do such, what we should do is to construct a function $g$ that maps nonempty set to an element of itself, and recursively define the desired choice function by $f(x)=g(x)$, but one can see this fake "technique" is not only trivial, and more importantly, wrong, since without AC, we don't know if $g$ exists, let alone using $g$ to construct  $f$.
\end{minipage}}
\begin{theorem}
\label{0.3.13}
\textbf{(Zorn's Lemma Implies Well Ordering Theorem)}  
If
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
        \textbf{Zorn's Lemma:} For every partially ordered set $P$ such that every chain is bounded above, there exists a maximal element.
    \end{minipage}
\end{center}
then
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
       \textbf{Well Ordering Principle:} Every set $X$ can be well ordered.
    \end{minipage}
\end{center}
\end{theorem}
\begin{proof}
Define
\begin{equation*}
P:=\set{(A_i,\leq_i): A_i\subseteq X\text{ and }A_i\text{ is well ordered by $\leq_i$ }}
\end{equation*}
and we define a partial order $\leq $ for $P$ by
\begin{equation*}
  (A_i,\leq_i)\leq (A_j,\leq _j)\text{ if }A_i\subseteq' A_j\text{ with respect to $\leq_j$ and }x_i\leq_i y_i\longrightarrow x_i\leq_j y_i 
\end{equation*}
Let $\cc_d$ be a chain of $P$. Define order $\leq_d$ on $\bigcup \cc_d$ by
\begin{equation*}
x\leq_d y\text{ if }\exists A_i\in\cc_d, x,y\in A_i\text{ and }x\leq_i y
\end{equation*}
we first prove \vi{$\leq_d$ is well defined}, which is just to prove that $\leq_d$ is antisymmetric. Let $x\leq_d y\text{ and }y\leq _dx$, and let $x,y\in A_i\cup A_j$ such that $x\leq_i y\text{ and }y\leq _jx$. Because $\cc$ is a chain, WOLG, let $A_i\leq A_j$, we then have  $x\leq_j y\text{ and }y\leq _jx$, so $x=y\vdone$\\

We now prove  \blue{$\leq_d$ is reflexive, connected and transitive}.\\

For reflexive, notice $x\in \bigcup\cc_d$ implies $\exists A_i\in\cc_d, x \in A_i$, and the rest is trivial. For connected, for $x,y\in \bigcup \cc_d$, let $x\in A_i \in\cc_d\text{ and }y\inA_j\in\cc_d$. Because $\cc$ is a chain, WOLG, we can let $A_i\leq A_j$, and then we see $x,y\in A_j$, so because $A_j$ is totally ordered by $\leq_j$, we know $x,y$ is comparable by  $\leq_d$. For transitive, let $x\leq_d y\leq _d z$, and let $x,y\in A_i\in\cc_d\text{ and }x\leq _iy\text{ and }y,z\inA_j\in\cc_d\text{ and }y\leq _jz$. If $A_j\leq A_i$, we see $x\leq_i y\leq_i z$, then the proof is finished. If $A_i\leq A_j$, we see $x\leq_j y\leq_j z\bdone$.\\

We now prove \vi{$\bigcup \cc_d\in P$}, which is just to prove $\bigcup \cc_d$ is well ordered by $\leq_d$.\\

Let $V\subseteq \bigcup \cc_d$, and arbitrarily pick $A_i\in \cc_d$ such that $A_i\cap V$ is nonempty, because $A_i$ is well ordered by  $\leq _i$, we know there exists an minimal element $v'$ of $A_i\cap V$ with respect to $\leq _i$. Let $v \in V$, if $v\in A_i\cap V$, then by definition of $\leq _d$, we see $v'\leq_d v$. If $v\in V\setminus A_i$, then for each $A_j\in\cc$ containing $v$, we know $A_i\subset A_j$, and because $\cc$ is a chain, we know $A_i\leq A_j$. \As{$v\leq _j v'$}, because $v'\in A_i\subseteq' A_j$, we can deduce $v'\in A_i\tCaC$, so we have $v'<_j v$, which implies $v' <_d v$. We have proved $v'$ is the smallest element of $V\vdone$ \\

We now prove  \blue{$\bigcup \cc_d$ is an upper bound of $\cc_d$ with respect to $\leq $}. Arbitrarily pick $x_i,y_i\in A_i\in \cc_d$,  we immediately see $x_i\leq _iy_i\implies x_i\leq _dy_i$, so we only have to prove $A_i\subseteq' \bigcup \cc_d$ with respect to $\leq _d$. \teal{Let $v_i\in A_i,v_j\in\bigcup \cc_d\text{ and }v_j\leq _dv_i$}. Arbitrarily pick $A_j$ containing  $v_j$ and  \As{$v_j\not\inA_i$}, so we have $A_i<A_j$. Because $v_j\leq_d v_i\text{ and }v_i,v_j\in A_j$, we know $v_j\leq _jv_i$. Then because $A_i\subseteq' A_j$ with respect to $\leq _j$, we have $v_j\in A_i\tCaC$. We have proved $v_j\in A_i$ by rejecting the red assumption. Notice the \teal{premise}. We have proved $A_i\subseteq' \bigcup \cc_d$ with respect to $\leq _d\bdone$\\

Let $(A_k,\leq _k)$ be a maximal element of $P$.  \As{$A_k\subset X$}, we can arbitrarily pick $x\in X\setminus A_k$, and for all $a_k\in A_k$ define $a_k\leq_k x$. By \myref{Lemma}{0.3.6}, we see $(A_k\cup \set{x},\leq _k)$ is well ordered, so $(A_k\cup \set{x},\leq _k)\in P$. To prove $(A_k,\leq _k)<(A_k\cup \set{x},\leq _k)$ is trivial. \CaC
\end{proof}
\section{Cardinality and Cardinal}
\fbox{\begin{minipage}{39em}
In this section, we first present a definition and a theorem as a primer rather than the primary focus. This serves as a foundation to instill confidence in the reader regarding the use of set theory language.
\end{minipage}}
\begin{definition}
\label{0.4.1}
\textbf{(Formulation of Property of Function)}
We use predicate $P(f)$ 
\begin{equation*}
\forall A\forall B(A\in f\land B\in f\land \exists x(\set{x}\in A\land \set{x}\in B)\longrightarrow A=B)
\end{equation*}
to tell if $f$ is injective, and we use predicate  $P(f,Y)$ 
\begin{equation*}
\forall y\exists A\exists B((A=\set{\set{y}}\land A\in f)\lor (A\in f\land B\in A\land y  \in B\land \exists x(x\in B\land x\neq y) ))
\end{equation*}
\end{definition}
\begin{theorem}
\label{0.4.2}
\textbf{(Basic Property of Function)} There exists a one-to-one function from $A$ to  $B$ if and only if there exists an onto function from  $B$ to  $A$
\end{theorem}
\fbox{\begin{minipage}{39em}
While it's straightforward to "prove" the theorem, formalization of such proof offers a robust assurance of its correctness. I encourage you to attempt formalization. With that, let's delve into the topic of cardinality, and introduce a highly useful theorem.
\end{minipage}}
\begin{definition}
\label{0.4.3}
\textbf{(Definition of Cardinality)} 
We write 
\begin{equation*}
A\leq _c B\text{ or write }B\geq _cA
\end{equation*}
If there exists a one-to-one function from $A$ to  $B$, or, equivalently, if there exists an  onto function from $B$ to  $A$. Also, we write 
\begin{equation*}
A=_c B
\end{equation*}
if there exists a bijective function from $A$ to  $B$  (one-to-one correspondence between $A,B$) 
\end{definition}
\begin{theorem}
\label{0.4.4}
\textbf{(Schroder-Berstein Theorem)} Let $A,B$ be sets. We have 
\begin{equation*}
A\leq_c B\text{ and }B\leq _c A\iff  A=_c B
\end{equation*}
\end{theorem}
\begin{proof}
Let $f:A\rightarrow B$, $g:B\rightarrow A$ be two one-to-one function.\\

Define $C_0:=A\setminus g(B) $, and for all nonnegative integer $k$, define 
\begin{equation*}
C_{k+1}:=g(f(C_k))
\end{equation*}
Also, Define
\begin{equation*}
C:=\bigcup _{k=0}^{\infty} C_k
\end{equation*}
Define
\begin{equation*}
h(x):=\begin{cases}
  f(x) & x\in C \\
  g^{-1}(x) & x\inA\setminus C
\end{cases}
\end{equation*}
We first prove \vi{$h$ is defined every where on $A$}.\\

Because $C_0\subseteq C$, we know that $A\setminus C \subseteq A\setminus C_0=A \setminus (A\setminus g(B))$. Then we can deduce
\begin{gather*}
x\in A \setminus C \implies x\inA \setminus (A \setminus g(B))\\
\liff x\inA\text{ and }x\not\inA \setminus g(B)\\
\liff  x\inA\text{ and }(x\not\inA\text{ or }x\in g(B))\\
\liff (x\inA\text{ and }x\not\in A)\text{ or }(x\inA\text{ and }x\in g(B))\\
\liff x\inA\text{ and }x\in g(B)\\
\liff x\in g(B) \vdone
\end{gather*}
We now prove \blue{$h$ is one-to-one}.\\

\As{there exists $x\neq y\inA$ such that $h(x)=h(y)$}. We know $x,y$ are not both in $C$, otherwise  $f$ is not one-to-one. We also know $x,y$ are not both in $A \setminus C$, otherwise $g$ is not a function. WOLG, let $x\in C\text{ and }y\inA \setminus C$, so we have $f(x)=g^{-1}(y)$. Then, we have $y=g(f(x))$. Because $x\in C$ , we know there exists a nonnegative integer $n$ such that $x\in C_n$. Then we see $y\in g(f(C_n))=C_{n+1}\tCaC\bdone$\\

Lastly we prove  \vi{$h$ is onto}.\\

We wish to prove $B=h(A)$, where 
 \begin{equation*}
h(A)=f(C)\cup g^{-1}(A \setminus C)\text{ and }h(A)\subseteq B
\end{equation*}
So we just prove
\begin{equation*}
B\subseteq f(C)\cup g^{-1}(A \setminus C)
\end{equation*}
Let $x\in B$. If all $x$ is in  $g^{-1}(A \setminus C)$, then the proof is over. If not, keep in mind that $g(x)\not\in C_0$, since $g(x)\in C_0=A \setminus g(B)$ implies $g(x)\not\in g(B)$ and observe 
\begin{gather*}
x\not\in g^{-1}(A \setminus C)\\
\liff g(x)\not\in A \setminus C\\
\liff g(x)\in C \\
\liff \exists n\inn, g(x)\in C_n\\
\liff \exists n\inn, \exists u\in C_{n-1}, g(x)=g(f(u))\\
\liff \exists n\inn,\exists u\in C_{n-1}, x=f(u)\\
\liff x\in f(C) \vdone 
\end{gather*}
\end{proof}
\fbox{\begin{minipage}{39em}
Schroder-Berstein Theorem in simple word is just that the relation $\leq_c$ between sets is antisymmetric. One may intuitively think such "simple" consequence should be easy to prove but in fact isn't.\\ 

That the relation $\leq _c$ is reflexive and transitive is trivial to prove. What being said, this does not implies $\leq _c$ is a "total order", since we haven't proved it is connected. 
\end{minipage}}
\begin{theorem}
\label{0.4.5}
\textbf{(Caridinality of All Set are Comparable if Given AC)} If given AC, for all sets $A,B$, we have 
 \begin{equation*}
A\leq _c B\text{ or }B\leq _c A
\end{equation*}
\end{theorem}
\begin{proof}
Let $P$ be the set of one-to-one function from subset of  $A$ to  $B$. We define order relation on  $P$ by 
\begin{equation*}
f\leq g\text{ if }\forall x\in \text{Dom}(f), f(x)=g(x)\text{ and }\text{Dom}(f)\subseteq \text{Dom}(g)
\end{equation*}
To prove this do define a order relation is trivial. By Zorn's Lemma, we can pick a maximal element of $f$. If Domain of  $f$ is not  $A$ and range of $f$ is not  $B$,  then we can define a larger function by arbitrarily picking  $x\in A\setminus \text{Dom}(f)$ and $y\in B\setminus \text{Im}(f)$ and defining  $f'(z)=\begin{cases}
  f(z)& \text{ if  }x\in \text{Dom}(f)\\
  y& \text{ if  }z=x
\end{cases}$. It is trivial to prove $f<f'$. 
\end{proof}
\fbox{\begin{minipage}{39em}
So far, we have proved $\leq _c$ is a total order on the class of all sets. Now, we give a necessary and sufficient condition of one set having strictly less cardinality than that of the other.
\end{minipage}}
\begin{definition}
\label{0.4.6}
\textbf{(Definition of Cardinality)} Let  $A,B$ be sets. If $A\geq _c B$ and there does not exist a one-to-one correspondence between $A$ and $B$, which we denote $A \neq _c B$, then we write
\begin{equation*}
A>_c B\text{ or write }B<_c A
\end{equation*}
\end{definition}
\begin{theorem}
\label{0.4.7}
\textbf{(Basic Property of Cardinality)} For all sets $A,B$, we have  
\begin{equation*}
A>_c B\iff \text{ no function from $A$ to $B$ is one-to-one}
\end{equation*}
\begin{equation*}
A >_c B\iff \text{ no function from $B$ to $A$ is onto }
\end{equation*}
\end{theorem}
\begin{proof}
The fact that an one-to-one function from $A$ to $B$ exists if and only if an onto function from $B$ to $A$ exists is proved in the beginning of the section, so we only have to prove the first iff and the proof is finished. From left to right, \As{there exists an one-to-one function from $A$ to $B$}. Because $A >_c B\implies A\geq _c B\implies $ there exists another one-to-one function from $B$ to $A$, so by Schroder-Berstein Theorem, we have $A =_c B\tCaC$. From right to left, \As{the negate of  $A >_c B$}. Notice the negate of $A>_c B$ is no function from  $B$ to $A$ is one-to-one or $A =_c B$. If $A =_c B$, then there exists a bijective, so an one-to-one function from $A$ to $B$. If no function from  $B$ to $A$ is one-to-one, and no function from $A$ to $B$ is one-to-one, then the cardinality of $A,B$ is incomparable \CaC  
\end{proof}
\fbox{\begin{minipage}{39em}

Notice that we haven't define the cardinality in a "constructive" manner. We defined cardinality by defining relation between sets. We now define cardinal, not cardinality, in a constructive manner compatible to our old definition.
\end{minipage}}
\begin{definition}
\label{0.4.8}
\textbf{(Von Neumann Cardinal Assignment)} We now assign each set $X$ to a cardinal
 \begin{equation*}
\abso{X}=\min  \set{\sigma\in\text{ Ord }:\text{ and }\sigma =_c X}
\end{equation*}
And we say an ordinal is a cardinal if the ordinal if an caridnality of some sets. 
\end{definition}
\begin{theorem}
\label{0.4.9}
\textbf{(Every Set Can be Assigned to a Cardinal if Given AC, Needed to be formalized)} For arbitrary set $X$, there exists an ordinal of same cardinality. More precisely, every well order set (some set can not be well ordered without AC) can be assigned a cardinal.
\end{theorem}
\begin{proof}
By AC, we can well order $X$. Define function $f$ that map ordinal to  element in $X$ by
 \begin{equation*}
f(0):=\min X\text{ and }f(\alpha ):=\min X\setminus \set{f(\beta ):\beta <\alpha }
\end{equation*}

\As{$\forall \alpha \in\text{Ord }, f[\alpha ]\subset X$}. Then we know $\text{Ord$=$Dom}(f)$, and then we can formally construct a inverse function $f^{-1}$, and formally construct the set that is the image of $f^{-1}$, which we know is Ord. \CaC
\end{proof}
\begin{theorem}
\label{0.4.10}
\textbf{(Compatibility between Von Neumann Cardinal and Ordinal)} For all set $A,B$, we have
 \begin{equation*}
A<_c B \implies  \abso{A}\in \abso{B}\text{ and } A=_c B\implies \abso{A}=\abso{B}\text{ and }n\in \bigcap  \call \implies \abso{n}=n
\end{equation*}
where infimum is with respect to strict well order $\in$ on the class of ordinals. 
\end{theorem}
\begin{proof}
\As{$\abso{B}\in \abso{A}$}. Notice that trivially we can prove the relation $=_c$ on the class of all sets is symmetric, so we have $A<_c B$, since if $A=_c B$, we should by definition have $\abso{B}=\abso{A}$. Notice $\abso{B}\in \abso{A}\implies \abso{B}\subset \abso{A}$, so we have an one-to-one function from $\abso{B}$ to $\abso{A}$ by mapping each element of $B$ to itself, then we have $\abso{B}\leq_c \abso{A}$, notice that by definition we have $B=_c \abso{B}\leq _c \abso{A}=_c A$. Then we can construct function by composition to show $B\leq _c A\tCaC$.\\




We prove by induction. Base case is simple: notice $\varnothing=_c \varnothing$ implies $\varnothing \in \set{\sigma \in\text{Ord}:\sigma =_c \varnothing}$, so we have $\abso{\varnothing}=\varnothing$. For induction case, let $k\in \bigcap \call \text{ and }\abso{k}=k$, we wish to prove \vi{$k\cup \set{k}=\min \set{\sigma\in\text{Ord}:\sigma=_c k\cup \set{k}}$}. Trivial, we have $k\cup \set{k}\in \set{\sigma\in\text{Ord}:\sigma=_c k \cup \set{k}}$. We first prove \teal{$k$ is the greatest ordinal in  $k\cup \set{k}$}. Let $m \in k\cup \set{k}$. \As{$k \in m$}. Then by definition of ordinal we  have  $k\subseteq m$, and because $k \in m$, we then have $k\cup \set{k}\subseteq m$. On the other side, notice by definition of ordinal we have $m\subseteq k\cup \set{k}$, so in summary we have $m=k\cup \set{k}\tCaC\tdone$. So, we see if $k\cup \set{k}$ is not the smallest element of $\set{\sigma:\sigma=_c k\cup \set{k}}$, then $k$ belong to  $\set{\sigma:\sigma=_c k \cup \set{k}}$. We now prove $k\neq_c k\cup \set{k}$ by induction. Base case is trivial: $\varnothing\neq _c \set{\varnothing}$. We now prove $k\neq _c k \cup \set{k}\longrightarrow k\cup \set{k}\neq _c k\cup \set{k}\cup \set{k \cup \set{k}}$. \As{there exists a bijective function $f$ from $k \cup  \set{k}$ to $k\cup \set{k}\cup \set{k \cup \set{k}}$ }. Then we can define a bijective function $g(x):k\rightarrow k\cup \set{k}$ by $\begin{cases}f(k)& \text{ if  }x=f^{-1}(\set{k\cup \set{k}})\\
  f(x)& \text{ if  }x\neq f^{-1}(\set{k\cup \set{k}})
\end{cases}\tCaC\vdone$
\end{proof}
\fbox{\begin{minipage}{39em}
The theorem discussed above is fundamental to the theory of ordinals and cardinals. However, it is infrequently employed in the discussions of real analysis. This is because, using the elementary methods of set description, the sets under consideration often possess cardinalities equivalent to either $\mathbb{N}$, $\mathbb{R}$, or are finite.\\

We now present the standard definition of \textit{infinite} in the language of set theory, a definition widely accepted in most analysis texts. Subsequently, we introduce the concept of \textit{Dedekind-infinite}. This definition harnesses our intuition: that shifting every term in an infinite sequence is permissible. It's important to note that the traditional definition of \textit{infinite} and that of \textit{Dedekind-infinite} are not always equivalent. However, if the Axiom of Choice (AC) holds true, then these definitions become equivalent. Since most people in the mathematical community accept AC, the two definitions are often treated as equivalent in practice.
\end{minipage}}
\begin{definition}
\label{0.4.11}
\textbf{(Definition of Infinite)} We say a set $A$ is finite if there exists an ordinal $\sigma$ in $\bigcap  \call $ such that $\abso{A}=\sigma$, and we say $A$ is infinite if $A$ is not finite. 
\end{definition}
\begin{definition}
\label{0.4.12}
  \textbf{(Definition of Dedekind Infinite)} Let $A$ be a set. We say $A$ is Dedekind-infinite if there exists a proper subset of $A$ that have the same cardinality as $A$, and we say $A$ is Dedekind-finite if $A$ is not Dedekind-infinite.
\end{definition}
\begin{theorem}
\label{0.4.13}
\textbf{(Equivalence of Infinite and Dedekind Infinite under AC)} Under AC, a set $A$ is infinite if and only if  $A$ is Dedekind-infinite.
\end{theorem}
\begin{proof}
We first prove \vi{$A$ is finite  $\implies A$ is Dedekind-finite} by induction. Base case is trivial, since $\varnothing$ doesn't even have  proper subsets. For induction case, let $k\in \bigcap \call $ be Dedekind-finite. \As{$\exists m\subset k\cup \set{k},m=_c k\cup \set{k}$}. If $m$ contain  $k$, then we have  $m\setminus \set{k}\subseteq k\text{ and }m\setminus \set{k}=_c k$, where we use the same technique in proof of \myref{Theorem}{0.4.10}. So we know $m\subseteq k$. Then we have $k\cup \set{k}=m\leq_k k\tCaC\vdone$.\\

We now prove \teal{$A$ is infinite  $\implies A$ is Dedekind-infinite}. Define function 
\begin{equation*}
f:\N\rightarrow \power{\power{A}}, n\mapsto \set{X:\power{A}:\abso{X}=n}
\end{equation*}
We first prove \vi{for all naturals  $n$, we have  $f(n)$ is nonempty} by induction.\\

$f(1)$ is nonempty since we can arbitrarily select an element and use axiom of pairing to construct a singleton subset. If $f(k)$ is nonempty, we can arbitrarily select an element $X$ from $f(k)$ and to see if $A=X$ then $A$ is finite, so we can arbitrarily select an element from $A\setminus X$ and attach to $X$ to show  $f(k+1)$ is nonempty.  $\vdone$\\

We now \blue{construct a bijective from $A$ to a proper subset missing one element by constructing a countable subset $U$ of $A$ and map each element  in $U$ to its successor}.\\

By AC, we know there exists a function $g(n)$ such that $\forall n\inn, g(n)\in f(n)$. Let $U=\bigcup g[\N]$. Define $h:U\rightarrow \N$ by mapping elements in $g(n)$ to elements in $\set{x \inn: \frac{(n-1)n}{2}<x\leq \frac{n(n+1)}{2}}$. $h$ is onto, since  $U=\bigcup g[\N]$, and because $\abso{g(n)}=n=\abso{x\inn:\frac{(n-1)n}{2}x\leq \frac{n(n+1)}{2}}$, we know $h$ is one-to-one. Now we define function $B:A\rightarrow A\setminus \set{h^{-1}(1)}$, where $y$ is the element in  $U$ that was mapped to  $1$ by
\begin{equation*}
B(x)=\begin{cases}
  x& \text{ if  }x\not\in U\\
  h^{-1}(h(x)+1)& \text{ if  }x\inU\bdone\tdone
\end{cases}
\end{equation*}
\end{proof}
\begin{corollary}
\label{0.4.14}
\textbf{($\N$ is the "smallest" infinite set)} Every infinite set have either same or greater cardinality than $\N$ 
\end{corollary}
\begin{proof}
Consider the set $U$ in proof above.
\end{proof}
\begin{corollary}
\label{0.4.15}
\textbf{(Another Definition of Infinity)} A set is infinite if and only if it has cardinality equal to or greater than $\N$
\end{corollary}
\fbox{\begin{minipage}{39em}
Now, we will demonstrate a few theorems specific to countable cardinalities. While these can be generalized to encompass all cardinalities using more advanced set theory tools, our final theorem will be applicable to all cardinalities without exception.
\end{minipage}}
\begin{definition}
\label{0.4.16}
\textbf{(Definition of Countable)} We say a set is countable if the set has cardinality less than or equal to $\N$
\end{definition}
\begin{theorem}
\label{0.4.17}
\textbf{(Finite Cartesian Product of Countable Set is Countable)} For all naturals $n$, we have $\abso{\N^n}=\abso{\N}$ 
\end{theorem}
\begin{proof}
  Because we can map $p\mapsto (p,1,\dots ,1)$, we know $\abso{\N}\leq \abso{\N^n}$. Because we know $\abso{X_m:=\set{(p_1,\dots, p_n) \in \N^n :\sum p_i=m}}=H^n_{m-n}$, we can map each element in $X_m$ one-to-one into  $(\sum_{i=3}^{m-1} H^n_{i-n},\sum ^m_{i=3}H^n_{i-n}]$, so we know $\abso{\N^n}\leq \abso{\N}$
\end{proof}
\begin{theorem}
\label{0.4.18}
\textbf{(Countable Union of Countable Set is Countable)} Let $S=\set{E_n:n\inn}$ be a countable set of countable sets. We have $\bigcup S$ is countable.  
\end{theorem}
\begin{proof}
For each $n\inn$, because $E_n$ is countable, we can define  bijective $f_n:\N\rightarrow E_n$. Define $g:\N \rightarrow  \bigcup S$ by
\begin{equation*}
g(x)=f_{m-(h-2)}(h)\text{ where }m=\max \set{n\inn: \frac{(n+1)n}{2}< x}\text{ and }h=x-\frac{(m+1)m}{2}
\end{equation*}
We now prove \vi{$g$ is onto}.\\

Let $x\in \bigcup S$, we know $\exists p\inn, x\in E_p$, so we know $\exists p,q\inn, x=f_p(q)$. One can verify $g(q+\frac{(p+q-2)^2+(p+q-2)}{2})=f_p(q)=x$ where $m=p+q-2\text{ and }n=q\vdone$
\end{proof}
\begin{theorem}
\label{0.4.19}
\textbf{(Every Nonempty Set Has Cardinality Strictly Less than Its Power Set)} If $A$ is nonempty, then  $\abso{A}<\abso{\power{A}}$.
\end{theorem}
\begin{proof}
  Let $f$ be a function from $A$ to  $\power{A}$. Define $T=\set{a\in A:a\not\in f(a)}$. Observe $b \in T\implies b\not\in f(b)\implies T\neq f(b)$ and observe $b\not\in T\implies b\in f(b)\implies T\neq f(b)$, so $T\in \power{A}\setminus f[A]$.  
\end{proof}
\section{Ordinal Arithmetic}
\section{Cardinal Arithmetic}
\section{Measure, $\sigma$-Algebra and Set Limits}
\begin{axiom}
\label{0.7.1}
\textbf{(Axioms of Set Algebra, Monotone Class and $\sigma$-Algebras)} 
\begin{enumerate}[label=(\alph*)]
  \item $\varnothing\in \Sigma $ (Background)
  \item $A \in \Sigma \implies A^c \in \Sigma $ (Closed under Complement)
  \item $A,B \in \Sigma \implies A\cap B \in \Sigma$ (Closed under Finite Intersection)
   \item $\set{E_n}\subseteq \Sigma \implies \bigcap_{n=1}^\infty E_n \in \Sigma$ (Closed under Countable Intersection)
   \item $\begin{cases}
     E_n \nearrow \bigcup_{n\inn} E_n \implies \bigcup_{n\inn} E_n \in \Sigma \\
     E_n \searrow \bigcap_{n\inn} E_n \implies \bigcap_{n\inn} E_n \in \Sigma 
   \end{cases}$ (Closed under Set Limit)
\end{enumerate}
We say $(X,\Sigma)$ is an
\begin{enumerate}[label=(\roman*)]
  \item algebra of set if $(a),(b),(c)$ are satisfied. 
  \item $\sigma$-algebra if $(a),(b),(c),(d)$ are satisfied. 
  \item monotone class if $(e)$ is satisfied.
\end{enumerate}
\end{axiom}
\begin{theorem}
\label{0.7.2}
\textbf{(Simplification of $\sigma$-Algebras Axioms)} $(X,\Sigma)$ is a $\sigma$-algebra if 
\begin{enumerate}[label=(\alph*)]
  \item $\varnothing \in \Sigma$
  \item $A\in \Sigma \implies A^c \in \Sigma$ 
  \item $A_1,A_2 \in \Sigma \implies A_1\cap A_2 \in \Sigma$
  \item Given disjoint $\set{A_n}_{n\inn}\subseteq\Sigma$, we have $ \bigsqcup_{n\inn} A_n \in \Sigma  $
\end{enumerate}
\end{theorem}
\begin{proof}
Thanks to De Morgan formula, we only have to check for countable union axiom. Given a sequence of set $\set{A_n}_{n\inn} \subseteq \Sigma$ that might not be disjoint, we can define 
\begin{align*}
B_n=A_n \Big\setminus \Big(\bigcup_{k=1}^{n-1} A_k \Big)
\end{align*}
and see $\set{B_n}_{n\inn}\subseteq \Sigma$ are disjoint. We now have 
\begin{align*}
\bigcup A_n= \bigsqcup B_n \in \Sigma
\end{align*}
as wished.
\end{proof}
\begin{theorem}
\label{0.7.3}
\textbf{(Generating of $\sigma$-Algebras)} 
\begin{enumerate}[label=(\alph*)]
  \item $\set{\Sigma_\ld}_{\ld  \in \Lambda }\text{ is a collection of $\sigma$-algebras on }X \implies \bigcap_{\ld  \in \Lambda }\Sigma_\ld $ is a $\sigma $-algebra on $X$ 
  \item $\set{\Sigma_\ld}_{\ld  \in \Lambda }\text{ is a collection of $\sigma$-algebras on }X \not \nRightarrow \bigcup _{\ld  \in \Lambda }\Sigma_\ld $ is a $\sigma $-algebra on $X$ 
  \item $Y\subseteq X$ and $\Sigma$ is a $\sigma$-algebra on $X\implies \set{Y\cap B:B \in \Sigma}$ is a $\sigma$-algebra on $Y$ 
  \item $\Sigma_X,\Sigma_Z$ are respectively $\sigma$-algebras on $X,Z\nRightarrow\set{A\times B:A \in \Sigma_X, B\in \Sigma_Z}$ is a $\sigma$-algebra on $X\times Z$
\end{enumerate}
\end{theorem}
\begin{corollary}
\label{0.7.4}
\textbf{(Existence and Uniqueness of $\sigma$-Algebra Generated from an Arbitrary Family of Subsets)} Let  $\mathcal{F}$ be a collection of subsets of $X$.  
\begin{align*}
\sigma(\mathcal{F})\equiv \bigcap \set{\Sigma:\Sigma\text{ is a $\sigma$-algebra on }X}\text{ is a $\sigma$-algebra on $X$}
\end{align*}
\end{corollary}
\begin{mdframed}
For $(b)$, see the following example. 
\end{mdframed}
\begin{Example}{\textbf{(Union of $\sigma$-algebras need not be a $\sigma$-algebra)}}{}
\begin{align*}
X\equiv \set{1,2,3}\text{ and }\Sigma_1\equiv\set{\varnothing,\set{1,2},\set{3},X}\text{ and }\Sigma_2 \equiv \set{\varnothing,\set{1},\set{2,3},X}
\end{align*}
The intersection
\begin{align*}
\Sigma_1\cup  \Sigma_2=\set{\varnothing,\set{1},\set{3},\set{1,2},\set{2,3},X}
\end{align*}
is not a $\sigma$-algebra, since it is not closed under countable union, as one can see  $\set{1}\cup  \set{3} \not \in \Sigma_1\cup \Sigma_2$.
\end{Example}
\begin{mdframed}
For $(d)$, see the following example. 
\end{mdframed}
\begin{Example}{\textbf{(Set of Product of Elements of $\sigma$-Algebras generally is not a $\sigma$-Algebra)}
}{}
\begin{align*}
X\equiv \set{1,2},\Sigma_X\equiv \set{\varnothing,\set{1},\set{2},X}\text{ and }Z\equiv \set{a,b},\Sigma_Z \equiv \set{\varnothing,\set{a},\set{b},Z}
\end{align*}
We see 
\begin{align*}
  \set{B_X\times B_Z : B_X \in \Sigma_X , B_Z \in \Sigma_Z}=\Big\{&\varnothing,\set{(1,a)},\set{(1,b)},\set{(1,a),(1,b)}\\
&\set{(2,a)},\set{(2,b)},\set{(2,a),(2,b)}\\
&\set{(1,a),(2,a)},\set{(1,b),(2,b)},X\times Z\Big\}
\end{align*}
is not a $\sigma$-algebra, since $\set{(1,a)}^c=\set{(1,b),(2,a),(2,b)}$ is not an element. 
\end{Example}
\begin{mdframed}
Given $f:X\rightarrow (Y,\Sigma_Y)$, and with  \myref{Corollary}{0.7.9}, we can now define 
\begin{align*}
\sigma(f)\equiv \sigma \Big(\set{f^{-1}[B]:B \in \Sigma_Y}\Big)\text{ and }\sigma(\Sigma_X\times \Sigma_Z)=\sigma\Big(\set{A\times B:A\in \Sigma_X,B\in \Sigma_Z}\Big)
\end{align*}
\end{mdframed}
\begin{definition}
\label{0.7.5}
\textbf{(Definition of Set Limits)} 
\begin{align*}
\liminf_{n\to\infty} E_n=\bigcup_{n=1} \bigcap_{m=n} E_m \text{ and } \limsup_{n\to\infty} E_n = \bigcap_{n=1} \bigcup_{m=n} E_m
\end{align*}
\end{definition}
\begin{theorem}
\label{0.7.6}
\textbf{(Main Property of Set Limits)} We have the following 
\begin{align*}
\begin{cases}
   x \in \liminf_{n\to\infty} E_n \iff  \exists n ,\forall m\geq n, x\in E_m \\
   x \in \limsup_{n\to\infty} E_n\iff \forall n, \exists m\geq n, x \in E_m
\end{cases}
\end{align*}
\end{theorem}
\begin{corollary}
\label{0.7.7}
\textbf{(Main Property of Set Limits)} 
\begin{align*}
\bigcap_{n=1} E_n \subseteq \liminf_{n\to\infty} E_n \subseteq \limsup_{n\to\infty} E_n \subseteq \bigcup_{n=1} E_n 
\end{align*}
\end{corollary}
\begin{corollary}
\label{0.7.8}
\textbf{(Monotone Set Sequence)}
\begin{align*}
\begin{cases}
  E_n \nearrow \implies E_n \nearrow \bigcup_{n=1} E_n\\
  E_n \searrow \implies E_n \searrow \bigcap_{n=1} E_n
\end{cases}
\end{align*}
\end{corollary}
\begin{proof}
Given $E_n\nearrow$, we see  $\bigcap_{m=n} E_m=E_n$, which give us 
\begin{align*}
\liminf_{n\to\infty} E_n=\bigcup_{n=1} \bigcap_{m=n} E_m=\bigcup_{n=1}E_n
\end{align*}
Then because $\liminf_{n\to\infty} E_n\subseteq\limsup_{n\to\infty} E_n\bigcup_{n=1}E_n$, we have $E_n\nearrow \bigcup_{n=1}E_n$. The proof for decreasing sequence is similar.
\end{proof}
\begin{theorem}
\label{0.7.9}
\textbf{(Equivalent Definition of Set Limits)}
\begin{align*}
\liminf_{n\to\infty} E_n= \set{x \in X: \liminf_{n\to\infty} 1_{E_n}(x)=1}
\end{align*}
\end{theorem}
\begin{proof}
Observe 
\begin{align*}
  x \in \liminf_{n\to\infty} E_n &\implies \exists n , \forall m\geq n, x \in E_m \\
&\implies \exists n ,\forall m\geq n, 1_{E_m}(x)=1 \\
&\implies \liminf_{n\to\infty} 1_E_n(x)=1
\end{align*}
since the second last line implies that every sub-sequence $1_{E_{n_k}}(x)$ ends by $1$.\\

For the other direction, notice that $\set{1_{E_n}(x)}$ either has finite or infinite $0$. If $\set{1_{E_n}(x)}$ has infinite $0$, then we have a sub-sequence converge to $0$, which is impossible as  $\liminf_{n\to\infty} 1_{E_n}(x)=1$. Now, because $\set{1_{E_n}(x)}$ has only finite $0$, we see $1_{E_n}(x)$ must end by $1$, which give us 
 \begin{align*}
x \in \liminf_{n\to\infty}  E_n
\end{align*}
\end{proof}
\begin{axiom}
\label{0.7.10}
\textbf{(Axioms of Countably Additive Measure)} We say $\mu:\Sigma \rightarrow \R^+_0\cup \set{\infty}$ is measure function if 
\begin{enumerate}[label=(\alph*)]
  \item $\mu (\varnothing)=0$
  \item $\forall E \in \Sigma , \mu(E)\geq 0$ 
  \item $\set{E_n}_{n\inn}\subseteq \Sigma $ are disjoint $\implies \mu(\bigsqcup_{n=1}^\infty E_n)=\sum_{n=1}^\infty \mu(E_n)$
\end{enumerate}
\end{axiom}
\begin{theorem}
\label{0.7.11}
\textbf{(Monotone Measurable Sets)} Suppose $\set{E_n}_{n\inn}\subseteq \Sigma$. We have 
\begin{align*}  
E_n\nearrow \bigcup_{n=1}^\infty E_n\implies \mu(E_n)\nearrow \mu (\bigcup_{n=1}^\infty E_n)
\end{align*}
\end{theorem}
\begin{proof}
Let
\begin{align*}
\forall n\geq 2,A_n=E_n\setminus E_{n-1}
\end{align*}
We see 
\begin{align*}
\bigcup_{n=1}^\infty E_n =E_1 \sqcup \Big( \bigsqcup_{n=2}^\infty A_n\Big)
\end{align*}
Then 
\begin{align}
\mu(\bigcup_{n=1}^\infty E_n)=\mu (E_1)+\sum_{n=2}^\infty \mu (E_n)-\mu (E_{n-1})\label{(2)}
\end{align}
where 
\begin{align}
\mu (E_1)+ \sum_{n=2}^m \mu (E_n)-\mu (E_{n-1})=\sum_{n=1}^m \mu (E_n)-\sum_{n=1}^{m-1}\mu (E_n)=\mu (E_m)\label{(3)}
\end{align}
Combining \myref{Equation}{(2)} and \myref{Equation}{(3)}, we have
\begin{align*}
\mu (E_m)\to \mu (E_1)+\sum_{n=2}^\infty \mu (E_n)-\mu (E_{n-1})=\mu (\bigcup_{n=1}^\infty E_n)\text{ as }m \to \infty
\end{align*}
\end{proof}
\begin{mdframed}
Seeing, \myref{Theorem}{0.7.11}, one may presume that the analog 
\begin{align*}
E_n\nearrow \bigcap_{n=1}^\infty E_n \implies \mu (E_n)\setminus \mu (\bigcup_{n=1}^\infty E_n)
\end{align*}
is correct. Yet, this is not true, as one can see the below example.
\end{mdframed}
\begin{Example}{\textbf{(Behavior of non-finite Measure)}}{}
\begin{align*}
X=\R, E_n=[n,\infty)
\end{align*}
We see 
\begin{align*}
\bigcap_{n=1}^\infty E_n=\varnothing\text{ and }\forall n\inn,\mu(E_n)=\infty 
\end{align*}
\end{Example}
\begin{theorem}
\label{0.7.12}
\textbf{(Main Property of Limit of Measurable Sets)} 
\begin{align*}
 \mu \Big(\liminf_{n\to\infty}  E_n\Big)\leq  \liminf_{n\to\infty}  \mu(E_n)
\end{align*}
\end{theorem}
\begin{proof}
Given an arbitrary convergent sub-sequence $\mu (E_{n_k})$ in the sense of extended reals, we wish to prove 
\begin{align*}
\mu\Big(\liminf_{n\to\infty} E_n \Big)\leq \lim_{k\to\infty} \mu(E_{n_k})
\end{align*}
Notice 
\begin{align*}
  \mu \Big(\bigcap_{m=1}E_m \Big)\leq \mu \Big(\bigcap_{m=2}E_m \Big) \cdots \leq \mu \Big(\bigcup_{n=1} \bigcap_{m=n} E_m \Big)=\mu \Big(\liminf_{n\to\infty} E_n \Big)
\end{align*}
where $\set{\bigcap_{m=k}E_m}_{k\inn}$ is an increasing sequence. Also, notice 
\begin{align*}
\forall n, \exists n_k, \bigcap_{m=n}E_m \subseteq E_{n_k} \implies \forall n, \exists n_k, \mu \Big(\bigcap_{m=n}E_m \Big)\leq \mu (E_{n_k})
\end{align*}
The result now follows from \myref{Theorem}{0.7.11}. 
\end{proof}
\begin{mdframed}
Again, one may wish to prove the analog 
\begin{align*}
\limsup_{n\to\infty} \mu (E_n)\leq \mu \Big(\limsup_{n\to\infty} E_n \Big)
\end{align*}
This again isn't true, as one can see the below example.
\end{mdframed}
\begin{Example}{\textbf{(Behavior of non-finite Measure)}}{}
\begin{align*}
X=\R, E_n=(n,n+1)
\end{align*}
We see 
\begin{align*}
\bigcap_{n=1}^\infty\bigcup_{m=n}^\infty E_n=\varnothing \text{ and }\limsup_{n\to\infty} \mu (E_n)=1
\end{align*}
\end{Example}
\begin{mdframed}
However, the analog can hold true if the measure is finite. See  \myref{Theorem}{0.7.13}
\end{mdframed}
\begin{theorem}
\label{0.7.13}
\textbf{(Set Limit and Finite Measure)} If $\mu (X)<\infty$, we have 
\begin{align*}
\begin{cases}
  E_n \searrow\bigcap_{n=1}^\infty E_n \implies \mu (E_n)\searrow \mu (\bigcap_{n=1}^\infty E_n) \\
\limsup_{n\to\infty} \mu (E_n)\leq \mu (\limsup_{n\to\infty}  E_n)
\end{cases}
\end{align*}
\end{theorem}
\begin{proof}
The first result follows from De Morgan's formula and \myref{Theorem}{0.7.11}. The second result follows from the first result with proof similar to that of \myref{Theorem}{0.7.12}.
\end{proof}
\begin{corollary}
\label{0.7.14}
\textbf{(Set Limit and Finite Measure)} If $\mu(X)<\infty$, we have 
\begin{align*}
E_n \to E \implies \mu(E_n)\to \mu (E)
\end{align*}
\end{corollary}
\textbf{(Part 3)} 
\begin{definition}
\label{0.7.15}
\textbf{(Definition of Atoms of $\sigma$-Algebra)} We say $B \in \Sigma$ is an atom if there are no non-empty measurable proper subset of $B$. We say $\Sigma$ is an atomic $\sigma$-algebra if the atoms form a partition of $X$. 
\end{definition}
\begin{theorem}
\label{0.7.16}
\textbf{(Equivalent Definition of Atomic $\sigma$-Algebra)} 
\begin{align*}
\Sigma\text{ is an atomic }\sigma\text{-algebra }\iff \text{ every measurable set is the union of some atoms }
\end{align*}
\end{theorem}
\begin{proof}
Let $\set{B_\alpha }_{\alpha  \in \Lambda }$ be the set of atoms. From left to right, let $E$ be measurable and let $\set{B_\alpha }_{\alpha \in \Lambda _E}$ be the set of atoms contained by $E$. Use the partition premise to argue that $\bigcup_{\alpha  \in \Lambda  _E} B_\alpha =E$. From right to left, notice that each atom must be disjoint and try to cause a contradiction from assumption $\bigcup_{\alpha  \in \Lambda }B_\alpha \subset \Sigma$.
\end{proof}
\begin{theorem}
\label{0.7.17}
\textbf{(Property of Atomic $\sigma$-Algebra)} 
\begin{enumerate}[label=(\alph*)]
  \item If $\set{B_\alpha :\alpha \in \Lambda }$ is a partition of $X$, then  $\sigma \Big(\set{B_\alpha :\alpha \in \Lambda } \Big)$ has $\set{B_\alpha :\alpha \in \Lambda }$ as atoms.
\end{enumerate}
\end{theorem}
\begin{proof}
Notice that 
\As{$E \in \sigma\Big(\set{B_\alpha :\alpha  \in \Lambda } \Big)$ and $\exists \alpha ,E\subset B_\alpha $}
\end{proof}
\begin{theorem}
\label{0.7.18}
\textbf{(Atomic Structure of Countably generated $\sigma$-algebras)} Let  
\begin{align*} \Sigma=\sigma\Big(\set{E_n}\Big)\text{ and }\forall I \in \power{\N}, B_I=\bigcap_{i \in I}E_i \cap \bigcap_{i\not\in I}E_i^c
\end{align*}
Then $(X,\Sigma)$ are atomic, and the atoms are exactly 
\begin{align*}
\mathcal{A}=\Big\{B_I :I \in \power{\N}\Big\}
\end{align*}
\end{theorem}
\begin{proof}
There are two statements we have to prove 
\begin{enumerate}[label=(\alph*)]
  \item $\vi{\bigsqcup \mathcal{A}=X}$
  \item \blue{$\mathcal{A}$ is a family of atoms}. 
\end{enumerate}
For \vi{the first statement}, we have to prove $\mathcal{A}$ is a partition of  $X$. For each $x\in X$, let $I_x=\set{i\inn: x \in E_i}$. We see $x\in B_{I_x}$. This shows that $\bigcup \mathcal{A}=X$.\\

Now, given distinct $I,J \in \power{\N}$, say,  $k\in I \setminus J$, we see 
\begin{align*}
B_I \subseteq E_k
\end{align*}
Also, we see 
\begin{align*}
B_J\subseteq E_k^c
\end{align*}
This shows that given distinct $I,J$, two sets  $B_I,B_J$ are disjoint. $\vdone$\\

For the \blue{second statement}, \As{$\exists E\neq \varnothing\in \Sigma, \exists B_I \in \mathcal{A}, E\subset B_I$}. Clearly the set of all possible union of sets in $\mathcal{A}$ form a $\sigma$-algebra. Denote the set $U(\mathcal{A})$. By definition of generation of $\sigma$-algebra, we see 
\begin{align*}
\Sigma\subseteq \sigma(\mathcal{A})\subseteq U(\mathcal{A})
\end{align*}


\end{proof}
\begin{theorem}
\label{0.7.19}
\textbf{(Cardinality of Countably generated $\sigma$-algebra is either Finite or $2^{\aleph_0}$)} Given a countable family $\set{E_n}_{n\inn}$, we have
\begin{align*}
  \abso{\sigma \Big(\set{E_n}_{n\inn} \Big)}<\aleph_0\text{ or }\abso{\sigma\Big(\set{E_n}_{n\inn} \Big)}=2^{\aleph_0}
\end{align*}
\end{theorem}
\section{Construction of $\R$: Dedekind Cut}
\section{Writing Status}
\begin{mdframed}
This first chapter remains unfinished as the understanding of both its nature and technical is largely incomplete.\\

It shall be noted here, that everything claims to "work in ZFC" is a large pool of proposition encoded by sets. 

Future advice: 

\begin{enumerate}[label=(\alph*)]
  \item Rework the remarks in "ZF" 
  \item Gain a deeper understanding of ordinal
\end{enumerate}



\end{mdframed}
\chapter{The Real and Complex Number System}
\section{L.U.B.}
\begin{mdframed}
In this section, we prove that 
\begin{enumerate}[label=(\alph*)]
  \item L.U.B $\iff $ G.L.B in \myref{Theorem}{1.1.1}
  \item $\sqrt{2} \not\inq$ in \myref{Theorem}{1.1.2}
  \item $\Q$ does not satisfy L.U.B. in \myref{Corollary}{1.1.4}
\end{enumerate}
Notice that the proof for \myref{Corollary}{1.1.4} use both \myref{Theorem}{1.1.2} and \myref{Theorem}{1.1.3}.\\

In order to prove $\Q$ doe not satisfy L.U.B. the proof for \myref{Corollary}{1.1.4} first show the set of upper bounds of $\set{p\inq^+:p^2<2}$ is exactly $\set{p\inq^+:p^2>2}$. Then it follows from \myref{Theorem}{1.1.3} that $\sup \set{p\inq^+:p^2<2}=\min \set{p\inq^+:p^2>2}$ D.N.E.\\

The tricky part is in fact proving  \myref{Theorem}{1.1.3}. The proof use an algebraic method to directly construct functions that implies the non-existence of $\max \set{p\inq^+:p^2<2}$ and $\min \set{p\inq^+:p^2>2}$.
\end{mdframed}
\begin{theorem}
\label{1.1.1}
\textbf{(L.U.B. $\iff $ G.L.B.)} 
\begin{align*}
X\text{ satisfy L.U.B. }\iff X\text{ satisfy G.L.B.}
\end{align*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

Let $E$ be a bounded below set in  $X$, and let $T$ be the set of lower bounds of  $E$. By premise, $\sup T$ exists. \As{$\sup T\not\in T$}. Because $\sup T$ is not a lower bound of $E$, we know there exists  $e\in E$ such that $e<\sup T$, then we know there exists $t\in T$ such that $e<t\tCaC$. We have proved $\sup T\in T$. This tell us $\inf E=\max T$ exists.\\

The converse is done similarly. 
\end{proof}
\begin{theorem}
\label{1.1.2}
\textbf{($\sqrt{2}\not\inq$)}
There exists no rational $p$ such that $p^2=2$
\end{theorem}
\begin{proof}
\As{there exists $p\inq$ such that $p^2$}. Express $p$ in the form of  $\frac{n}{m}$ where $m,n$ are relatively prime integers. Deduce 
\begin{align}
p^2=2 \implies  \Big(\frac{n}{m} \Big)^2=2 \implies n^2=2m^2 \implies n|2 
\end{align}
Notice that $n|2\implies n^2 |4$. Now, we can deduce
\begin{align}
\begin{cases}
  n^2|4\\
  n^2=2m^2
\end{cases}\implies 2m^2|4\implies m^2|2 \implies m|2
\end{align}
We have proved that $n,m$ are both divisible by  $2$. This  \CaC to the fact that they are relatively prime.
\end{proof}
\begin{theorem}
\label{1.1.3}
\begin{align*}
\begin{cases}
  \max \set{p\inq^+:p^2<2}\text{ D.N.E. }\\
  \min \set{p\inq^+:p^2>2}\text{ D.N.E. }
\end{cases}
\end{align*}
\end{theorem}
\begin{proof}
We first reduce the problem into finding a function $f:\Q^+\rightarrow \Q^+$ such that 
\begin{align*}
\begin{cases}
  p<f(p)<\sqrt{2}& \text{ if $p<\sqrt{2} $ }\\
  \sqrt{2} <f(p)<p & \text{ if $p>\sqrt{2} $ }
\end{cases}
\end{align*}
We then reduce the problem by setting $h,g:\Q^+\rightarrow \Q^+$ to 
\begin{align}
\label{1.3}
\begin{cases}
  f(x)-x=\frac{2-x^2}{h(x)}\\
  \big(f(x) \big)^2-2=\frac{x^2-2}{g(x)}
\end{cases}
\end{align}
There are two things to notice in \myref{Equation}{1.3}. First, as long as $h,g$ map $\Q^+$ to  $\Q^+$ and \myref{Equation}{1.3} stand true, the function  $f$ satisfy our need. Second, we can and should treat $g$ as a dependent variable of $h$.\\

In other words, we transform our original problem of finding the appropriate $f$ into (1) finding the relationship between $h,g$ and (2) find $h$ that satisfy $h,g>0$ based on (1).\\

Now, transform \myref{Equation}{1.3} into
\begin{align}
\label{1.4}
\begin{cases}
  f(x)=\frac{2-x^2}{h(x)}+x\\
  \big(f(x) \big)^2=\frac{x^2-2}{g(x)}+2
\end{cases}
\end{align}
With some elementary algebra skill, we see that $h,g$ only has to satisfy    
 \begin{align}
\label{1.5}
\Big(\frac{2-x^2}{h(x)}+x \Big)^2=\frac{x^2-2}{g(x)}+2
\end{align}
Separate $h,g$ in \myref{Equation}{1.5}. We now have 
\begin{align}
\label{1.6}
\frac{(x^2-2)^2-2x(x^2-2)\big(h(x)\big)+(x^2-2)\big(h(x)\big)^2}{\big(h(x)\big)^2}=\frac{x^2-2}{g(x)}
\end{align}
Simplify \myref{Equation}{1.6} by dividing $x^2-2$ and multiplying with $\big(h(x)\big)^2g(x)$ on both side. We now have 
\begin{align*}
g(x)\Big((x^2-2)-2x\big(h(x) \big)+\big(h(x) \big)^2\Big)=\big(h(x) \big)^2
\end{align*}
Separate $g,h$ and complete the square. We now have 
\begin{align}
\label{1.7}
g(x)=\frac{\big(h(x) \big)^2}{\big(h(x)-x \big)^2-2}
\end{align}
Now, it is easy to see a solution of $f$ is setting $f=\frac{2-x^2}{h(x)}+x$ and $h(x)=x+2$. 
\end{proof}
\begin{corollary}
\label{1.1.4}
\textbf{($\Q$ does Not satisfy L.U.B.)} 
\begin{align*}
\Q\text{ does not satisfy L.U.B. }
\end{align*}
\end{corollary}
\begin{proof}
\As{$\Q$ satisfy L.U.B.} Let $E=\set{p\inq^+:p^2<2}$. We first prove 
\begin{align*}
  \vi{\set{q\inq^+:q^2>2}=\set{q\inq:\forall p\in E,q\geq p}}
\end{align*}
It is easy to check $\set{q\inq^+:q^2>2}\subseteq \set{q\inq:\forall p\in E,q\geq p}$.  Let $p$ satisfy  $\forall q\in E,p\geq q$. We know $p$ must be positive. This reduce the problem into proving $p^2$ is greater than  $2$.\\

\myref{Theorem}{1.1.2} tell us $p^2\neq 2$. Then we know $p^2$ is either greater than  $2$ or less than  $2$. If  $p^2$ is less than  $2$, by  \myref{Theorem}{1.1.3}, there exists an element of  $E$ greater than  $p$, which is impossible. This shows that  $p^2$ is greater than $2$ $\vdone$ \\

By \myref{Theorem}{1.1.3}, we now see $\sup E$ does not exists. \CaC
\end{proof}
\fbox{\begin{minipage}{39em}
Below are specification.
\end{minipage}}
\begin{definition}
\label{1.1.5}
\textbf{(Definition of Totally Ordered Sets)} We say $\leq $ is an totally order relation on $X$ if for all  $x,y,z \in X$, we have 
\begin{enumerate}[label=(\alph*)]
  \item (Reflexive) $x\leq x$
  \item (Connected) $x\leq y\text{ or }y\leq x$
  \item (Transitive) $x\leq y\text{ and }y\leq z \implies x\leq z$ 
  \item (Anti-Symetric) $x\leq y$ and $y\leq x\implies x=y$
\end{enumerate}
\end{definition}
\begin{definition}
\label{1.1.6}
\textbf{(Definition of Maximum and Supremum)} Let $X$ be an totally ordered set and $E\subseteq X$. We define the maximum and supremum of $E$ by  
\begin{enumerate}[label=(\alph*)]
  \item $\max E \in E\text{ and }\forall x\in E, x\leq \max E$
  \item $\sup E=\min \set{x\in X:\forall y\in E, x\leq y}$
\end{enumerate}
\end{definition}
\begin{definition}
\label{1.1.7}
\textbf{(Definition of Completed Ordered Set)} We say an ordered set $X$ is complete or satisfy L.U.B. if 
\begin{align*}
  \text{ the supremum of every set bounded above exists.}
\end{align*}
\end{definition}
\section{Dedekind Cut}
\begin{mdframed}
In this section, we  
\begin{enumerate}[label=(\alph*)]
  \item construct $\R$ using Dedekind cuts.   
  \item prove that the existence of completed ordered field is unique up to isomorphic. 
\end{enumerate}
\end{mdframed}
\begin{definition}
\textbf{(Definition of Cut)} A cut $\alpha $ is a subset of $\Q$ that satisfy 
\begin{enumerate}[label=(\alph*)]
  \item $\varnothing \subset \alpha \subset \Q$ (proper nonempty)
  \item  $p\in \alpha \iff  \exists q \in \alpha ,p<q$ (left-side structure)
\end{enumerate}
We define order, addition and multiplication by 
\begin{enumerate}[label=(\alph*)]
  \item $0=\Q^-$
  \item $\alpha \leq \beta \iff \alpha \subseteq \beta$ 
  \item $\alpha +\beta =\set{p+q\inq: p\in  \alpha ,q \in \beta }$ 
  \item $\forall \alpha ,\beta\inr^+,\alpha \cdot \beta=\set{pq\inq:p\in \alpha ,q\in \beta } $ 
  \item $\forall \alpha \inr^+,\forall \beta \inr^-, \alpha \cdot \beta = - (\alpha \cdot (-\beta ))$
  \item $\forall \alpha ,\beta \inr^-, \alpha \cdot \beta  =-((-\alpha )\cdot (-\beta ))$
\end{enumerate}
\end{definition}
\begin{theorem}
\textbf{($0\inr$)} We are required to check 
\begin{enumerate}[label=(\alph*)]
  \item $\varnothing \subset \Q^-\subset \Q$ 
  \item $p\inq^- \iff  \exists q\inq^-,p<q$
\end{enumerate}
\end{theorem}
\begin{theorem}
\textbf{(Cuts and Addition are Abelian Group)} We are required to check 
\begin{enumerate}[label=(\alph*)]
  \item $$
\end{enumerate}
\end{theorem}
\begin{proof}

\end{proof}


\section{Real Numbers Field}
\fbox{\begin{minipage}{39em}
Although the title of this section is "Real Numbers Field", here, we will not construct the real numbers field, nor use any common property of real numbers. In fact, we will not even use the symbol $\R$ in this section, since we are merely proving theorems about an ordered field with least-upper-bound property. We don't know if there exists any ordered field with least-upper-property. Let's say there does; yet, we don't know if such structure is unique. Let's say it is unique; yet, we don't know if that structure have relation with $\R$. Here, we will use the symbol $\F$ to denote an ordered field with least-upper-bound property. One should realize that we can use algorithm to define a subset containing $1\inF$ that is isomorphic to $\N$, and thereby we abuse the notation to denote that subset $\N$. A subfield of $\F$ isomorphic to $\Q$ can also be defined after we define $\Z$, so we also thereby abuse the notation to denote that subfield  $\Q$.         
\end{minipage}}
\begin{theorem}
\label{1.3.1}
$\N$ is unbounded above.
\end{theorem}
\begin{proof}
 \As{$\N$ is bounded above}. Because $1>0$, we know $\sup \N -1<\sup \N$. Then $\sup \N -1$ is not an upper bound of $\N$. Arbitrarily pick any $m \inn$ greater than $\sup \N -1$. We see $m>\sup \N-1\implies m+1>\sup \N$, where $m+1\inn\tCaC$
\end{proof}
\begin{corollary}
\label{1.3.2}
Both $\Z\text{ and }\Q$ are unbounded both above and below.
\end{corollary}
\begin{corollary}
\label{1.3.3}
  
\textbf{(Divided by $1$)} Given any $x\inF$, there exists $n\inz$ such that $n\leq x<n+1$
\end{corollary}
\begin{proof}
If $x>0$, let $S=\set{n\inn: n>x}$. Notice $S=\varnothing$ implies $\N$ is bounded above by $x$, so $S$ is nonempty. Then by well-ordering principle, we know $\min S$ exists. We now show \vi{$\min S-1\leq x<\min S$}. Observe that $\min S\in S\implies x<\min S$. \As{$\min S-1>x$}. We immediately see $\min S-1\in S\tCaC\vdone$.\\

If $x<0$, let $S=\set{n\inn:n\geq -x}$. Again, $S=\varnothing$ implies $\N$ is bounded above by $-x$, so $S$ is nonempty. Then by well-ordering principle, we know $\min S$ exists. We now show \blue{$-\min S\leq x<-\min S+1$}. Observe that $\min S\inS\implies \min S\geq -x\implies x\geq -\min S$. \As{$-\min S+1\leq x$}. Then $\min S-1\geq -x>0$; thus $\min S-1\inS\tCaC\bdone$\\

If $x=0$, then we let $n=0$. 
\end{proof}
\begin{theorem}
\label{1.3.4}
\textbf{(Archimedean Property)} Given $x,y\inF\text{ and }0<x$, there exists $n\inn$ such that $nx>y$ 
\end{theorem}
\begin{proof}
 Because $\N$ is unbounded above, we know  $\frac{y}{x}$ can not be an upper bound of $\N$, so we know  $\exists n\inn,n>\frac{y}{x} $. Then because $x>0$, we can deduce $nx>y$.
\end{proof}
\begin{theorem}
\label{1.3.5}
\textbf{($\Q$ is dense in $\F$)} Given $x,y\inF\text{ and }x<y$, we know there exists $p \inq$ such that $x<p<y$
\end{theorem}
\begin{proof}
Every rational, positive or negative, can be expressed in the form $\frac{m}{n}$ for some integer $m$ and naturals $n$. We seek to find some integer $m$ and $n$ such that $x<\frac{m}{n}<y$. Notice that $x<\frac{m}{n}<y\iff nx<m<ny$. Because $m$ has to be an integer, we know for $nx<m<ny$ to hold true, we must first have $ny-nx>1$. Because $y-x>0$, by Archimedean Property, there exists $n\inn$ such that $ny-nx=n(y-x)>1$. By \hyperref[1.3.3]{Corollary 1.3.3}, we know there exists $m \inz$ such that $m\leq  ny<m+1$.\\

Notice $m=ny$ if and only if $ y\inq$. So we can split the proof into two cases.\\

\vi{Case 1: $y\inq$}\\

We see that the set $\set{r\inq:r<y}$ have supremum $y$, since $y\inq$. Then $x<y$ tell us $x$ is not an upper bound of the set, then we can pick some rational $r$ in the set greater than $x$, so $x<r<y\vdone$.\\

\blue{Case 2: $y\not\inq$}\\ 

We know $m<ny<m+1$. $ny<m+1$ tell us $nx<ny-1<m$, so $nx<m<ny\bdone$ 
\end{proof}
\begin{theorem}
\label{1.3.6}
\textbf{(Positive root of power uniquely exists)} For all natural $n$ and $y>0$, there exists a unique positive $x$ such that $x^n=y$
\end{theorem}
\begin{proof}
By \hyperref[1.2.10]{Theroem 1.2.10}, we know two different positive numbers $0<x<x'$ are different when raised to the power of $n$, being $0<x^n<(x')^{n}$, so if such positive power exists, it must be unique.\\

We have handled the uniqueness part of the proof. Denote $E:= \set{m \inF^+: m^n<y}\text{ and }x:=\sup E$. Now we do the existence part by proving \vi{$x$ exists} and  \blue{$x^n=y$}.\\

To show \vi{ $x=\sup \set{m \inF^+: m^n<y}$ exists}, we only have to show the set $\set{m \inF^+: m^n<y}$ is nonempty and bounded above. In other word, we wish to construct function $a\inF^+$ and $b$ of $y$ such that for all positive input $y>0$, we have $a^n<y$ and $(0<m^n<y\longrightarrow m<b)$. In the followings, the domain of $a$ and $b$ are only positives.\\

First we construct $a$. By \myref{Theorem}{0.2.9}, we know if $a<\min \set{1,y}$ , then $a^n<a<y$, so we construct $a$ such that $0<a<\min \set{1,y}$. Notice that $a$ must be positive because we are constructing a number in $E$, where $E$ contain only positives. Express $a$ in the form  $a=\frac{p}{q}$ where $p,q$ are both function of $y$. In the process of  construction, We must be careful to make sure $a$ exists for all positive $y$.\\

To satisfy $0<a$, we need only guarantee $p,q$ are always of the same sign for all positive $y$. If such $p,q$ exists, we can change both sign of  $p,q$ when they are negative, and get two positive function. So, we can just require $p,q$ to be positive for all positive $y$.\\      

To satisfy $a<1$, observe $a<1\iff \frac{p}{q}<1\iff p<q$. The easiest construction is to let $q=p+c$ where $c$ is positive.\\

To satisfy $a<y$, observe $a<y\iff \frac{p}{q}<y\iff p(y-1)+cy=qy-p>0$. The easiest construction is to let $p(y-1)+cy=y^2$, which is possible, if we let $p=y$ and $c=1$. In this case $p=y>0$ and $q=y+1>0$, and  $a=\frac{y}{y+1}$. We finished proving $E$ is nonempty. \textbf{(Notice $c=y^2,p=y^3,q=y^3+y^2,a=\frac{y^3}{y^3+y^2}$ also do the trick)}\\

Now we construct $b$. By \myref{Theorem}{0.2.10}, we know if $0<b\text{ and }0<m^n<b^n$, then $m<b$, so we construct $b$ such that $y<b^n$ which lead to $0<m^n<y<b^n$ if  $m^n<y$. Because $y>0$, this is fairly easy. Simply let $b=y+1$, so we have $b>1\text{ and }b>y$; thus by \myref{Theorem}{0.2.9}, we have $b^n>b>y$, finishing proving $E$ is bounded above, where  $b=y+1$ is an upper bound. $\vdone$ \\

To show \blue{$x^n=y$}, we show $x^n\geq y$ and $x^n\leq y$. We will assume that $x^n<y$ or $x^n>y$, but before we do such, let's see what property from which can we possibly draw contradiction. Notice that because we just prove the existence of the supremum of $E$, we haven't use the fact that $x=\sup E$ in anywhere of our proof. We know
\begin{equation*}
x=\sup E\iff \forall d>0,
\begin{cases}
x+d\not\in E\text{ ($x$ is an upper bound) }\\
\text{ and }\\
x-d \text{ is not an upper bound of $E$ (the \emph{least} upper bound)}
\end{cases}    
\end{equation*}
So, you see, we wish to construct  a small and positive $h\text{ and }k$ such that if we assume $x^n<y\text{ or }x^n>y$ we can draw $x+h\in E$ or $x-k$ is an upper bound of $E$. \textbf{(We are going to assume $\sup E$ is smaller or greater than $\sqrt[n]{y}$)} \\

Observe $x+h\inE\iff (x+h)^n<y\iff (x+h)^n-x^n<y-x^n$, and observe $x-k$ is an upper bound of $E\iff (m^n<y\longrightarrow m<x-k)\iff (m\geq x-k\longrightarrow m^n\geq y)\iff (m\geq x-k\longrightarrow x^n-m^n\leq x^n-y)$.\\

Notice that the act of  subtracting  $x^n$ at the both side of the inequality play an important role in our proof: not only does the act allow us to use the identity $a^n-b^n=(a-b)(a^{n-1}+a^{n-2}b+\cdots +b^{n-1})$, and the act also tell us between $x+h \in E\text{ and }x-k$ is an upper bound of $E$, which contradiction statement should we draw from $x^n>y$. If $y-x^n<0$, then  $y-x^n<0<(x+h)^n-x^n$, so we can not possibly draw $x+h\inE$ from $x^n>y$. \textbf{(If $\sup E>\sqrt[y]{n}$, then it is too big, we can find a smaller upper bound of $E$)}\\

\As{$x^n>y$}. We wish to construct positive $k$ such that \teal{$m\geq x-k \longrightarrow x^n-m^n\leq x^n-y$}, so we can draw the contradiction $x-k$ is an upper bound of $E$. \\ 

Notice that $m\geq x-k\implies x^n-m^n\leq x^n-(x-k)^n$, so if $x^n-(x-k)^n\leq x^n-y$, our proof at this part is finished. Now our job is to single out the $k$ in the inequality to give an condition such that $x^n-(x-k)^n\leq x^n-y$ hold if the condition hold. It is easy to see that computing the polynomial of $k$ on left hand side of inequality and to prove such positive $k$ exists for all $n$ is almost impossible. Thus, we take a possible but actually non-existing risk in our next step of the proof. Use the $a^n-b^n$ identity and that $x-k<x$ to deduce
\begin{equation*}
x^n-(x-k)^n=k(x^{n-1}+x^{n-2}(x-k)+\cdots +(x-k)^{n-1})\leq knx^{n-1}
\end{equation*}
So, if we have $knx^{n-1}\leq x^n-y$, which is equivalent to $k\leq \frac{x^n-y}{nx^{n-1}}$, our proof is partially finished. Notice that $x^n-y>0\text{ and }nx^{n-1}>0$, so $\frac{x^n-y}{nx^{n-1}}>0$; thus the positive $k$ exists.  $\tdone\tCaC$\textbf{(The reason I use the word "possible risk" is that if we use an identity that show us $x^n-(x-k)^n$ smaller than a quantity greater than $x^n-y$, the proof can not be done.)} \\

\As{$x^n<y$}. We wish to construct positive $h$ such that \teal{$(x+h)^n-x^n<y-x^n$}, so we can draw the contradiction $x+h\in E$.\\

Again, we use the same identity to deduce
 \begin{equation*}
   (x+h)^n-x^n=h((x+h)^{n-1}+(x+h)^{n-2}x+\cdots +x^{n-1})<hn(x+h)^{n-1}
\end{equation*}
To single out the $h$ in the  $hn(x+h)^{n-1}$, notice that we can take the risk to add the constraint $h<1$ at the end of our construction to have $(x+h)^n-x^n<hn(x+h)^{n-1}<hn(x+1)^{n-1}$. Then, if we have $hn(x+1)^{n-1}<y-x^n$, which is equivalent to $h<\frac{y-x^n}{n(x+1)^{n-1}} $, our proof is finished. To sum up, any $h$ satisfy  $h<\min \set{1,\frac{y-x^n}{n(x+1)^{n-1}}}$ does the trick, and such $h$ exists, since  $0<\min \set{y-x^n,n(x+1)^{n-1}}$. $\tdone\tCaC\bdone$   
\end{proof}
\fbox{\begin{minipage}{39em}
If you want a proof with less of my commentary, here you go.
\end{minipage}}
\begin{proof}
  Observe $\min \set{1,\frac{y}{2}}\in E$ and $\max \set{1,y}$ is an upper bound of $E$, so $\sup E$ exists.  Denote $x:=\sup E$. \As{$x^n>y$}. Observe $x-k\text{ is an upper bound of $E$ }\iff m^n<y\longrightarrow m<x-k\iff m\geq x-k\longrightarrow m^n\geq y$. We know $(x-k)^n\geq y\iff x^n-(x-k)^n\leq x^n-y$, and know $x^n-(x-k)^n\leq knx^{n-1}$. If we let $0<k\leq \frac{x^n-y}{nx^{n-1}}$, then $x-k$ is an upper bound of  $E\tCaC.$ \As{$x^n<y$}. Observe $x+h\inE\iff (x+h)^n-x^n<y-x^n$. We know that $(x+h)^n-x^n<hn(x+h)^{n-1}$ and that if $h<1$, we have  $hn(x+h)^{n-1}<hn(x+1)^{n-1}$. So we let  $h=\min \set{1,\frac{y-x^n}{n(x+1)^{n-1}}}$, and we can see $x+h\inE\tCaC$. 
\end{proof}
\begin{definition}
\label{1.3.7}
The number $x$ in \hyperref[1.3.6]{Theorem 1.3.6} is written  $x=\sqrt[n]{y}$ or $x=y^{\frac{1}{n}}$.
\end{definition}
\fbox{\begin{minipage}{39em}
The above theorem is by far the trickiest we have seen. Often the theorem is proven only in special case $\sqrt{2}$ as a classical example in the first class of analysis. Here, we prove a general result. The following theorem is to make sure the definition of rational power make sense . In last section we prove some inequalities concerning integer power. Here, we prove those inequalities are inherited by rational power. Of course, the arithmetic properties of rational power, also inherited from those of integer power, will be proven after these inequalities.   \\ 


About the coverage of definition, notice that we didn't and won't define $y^{\frac{1}{n}}$ when $y$ is negative. Also, notice that for all nonzero rational $s$, we can define $0^s=0$. 
\end{minipage}}
\begin{theorem}
\label{1.3.8}
Given $m,p\inz\text{ and }n,q\inn\text{ and }a>0\text{ and }\frac{m}{n}=\frac{p}{q}$, we have 
\begin{equation*}
(a^m)^{\frac{1}{n}}=(a^p)^{\frac{1}{q}}
\end{equation*}
\end{theorem}
\begin{proof}
Observe
\begin{align*}
  x&= (a^m)^{\frac{1}{n}}\\
  \liff x^n&=a^m\\
  \liff (x^n)^q&=(a^m)^q\\
\liff x^{nq}&= a^{mq}=a^{np} \rap{because $n,m,q\inz$}\\
  \liff (x^q)^n&=(a^p)^n \rap{again, because $n,p,q\inz$}\\
  \liff x^q&=a^p\rap{by \hyperref[1.3.6]{Theorem 1.3.6}}\\
  \liff x&=(a^p)^{\frac{1}{q}}\rap{by \hyperref[1.3.6]{Theroem 1.3.6}}\\
  \liff (a^m)^{\frac{1}{n}}&=x=(a^p)^{\frac{1}{q}} 
\end{align*}
\end{proof}
\begin{definition}
\label{1.3.9}
\textbf{(Definition of Rational Powers)} Given a rational $r=\frac{p}{q}$, where $q\inn$. For all $a$, we define $a^r=(a^p)^{\frac{1}{q}}$
\end{definition}
\begin{theorem}
\label{1.3.10}
\textbf{(Inequality when base is fixed)} Given a positive $a$ and two rational  $x,y$ where  $x<y$, we have
 \begin{equation*}
\begin{cases}
  a^x<a^y\iff a>1\\
  a^x=a^y\iff a=1\\
  a^x>a^y\iff 0<a<1
\end{cases}
\end{equation*}
\end{theorem}
\begin{proof}
  Express $x,y$ in the form $x=\frac{q}{p},y=\frac{n}{m}$ where $q,n\inz\text{ and }m,p\inn$. Notice $x<y\implies \frac{q}{p}<\frac{n}{m}\implies mq<np$. Observe
  \begin{align*}
    a^x<a^y &\implies a^{\frac{q}{p}}<a^{\frac{n}{m}}\\
  &\implies a^q=(a^{\frac{q}{p}})^p<(a^{\frac{n}{m}})^p\rap{by \myref{Theorem}{0.2.10}}\\
  &\implies a^{mq}=(a^q)^m<((a^{\frac{n}{m}})^p)^m=((a^{\frac{n}{m}})^m)^p=(a^n)^p=a^{np}\\
  &\implies a>1\rap{by \myref{Theorem}{0.2.9}} 
  \end{align*}
  Notice that the above implication still hold true if $<$ is replaced by $>$ and $>$ is replaced by  $<$, and notice that the above implication still hold true if $<$ and  $>$ are all replaced by $=$, so we in fact have three implications. The Theorem follows from the three implication.
   
\end{proof}
\begin{theorem}
\label{1.3.11}
\textbf{(Inequality when rational power is fixed)} Given $0<b<c$ and  $z\inq$, we have
\begin{equation*}
\begin{cases}
  b^z<c^z \iff 0<z\\
  b^z=c^z \iff 0=z\\
  b^z>c^z\iff 0>z
\end{cases}
\end{equation*}
\end{theorem}
\begin{proof}
Express $z=\frac{q}{p}$, where $q$ is an integer and $p$ is a natural. Notice $q\text{ and }z$ are of the same sign. Observe by \myref{Theorem}{0.2.10}, we have $b^{\frac{q}{p}}<c^{\frac{q}{p}}\implies b^q=(b^{\frac{q}{p}})^p<(c^{\frac{q}{p}})^p=c^q\implies 0<q$.\\

  Notice that the above implication still hold true if $<$ is replaced by $>$ and $>$ is replaced by  $<$, and notice that the above implication still hold true if $<$ and  $>$ are all replaced by $=$, so we in fact have three implications. The Theorem follows from the three implication.
\end{proof}
\fbox{\begin{minipage}{39em}
Now we prove the arithmetic properties of rational power concerning only positive base. Notice there is no definition of a negative raised to the power of a rational. \red{From Here}
\end{minipage}}
\begin{theorem}
\label{1.3.12}
 \textbf{(Arithmetic property of rational power)} Given $r,s\inq\text{ and }a>0$, we have
\begin{equation*}
a^{r+s}=a^ra^s
\end{equation*}
\end{theorem}
\begin{proof}
Express $r,s$ in the form $r=\frac{p}{q},s=\frac{m}{n}$ where $q,n\inn\text{ and }p,m \inz$. Observe
\begin{align*}
  (a^{r+s})^{nq}&=(a^{\frac{np+mq}{nq}})^{nq}\\
  &=a^{np+mq} 
\end{align*}
Then observe
\begin{align*}
  (a^ra^s)^{nq}&= (a^{\frac{p}{q}}a^{\frac{m}{n}})^{nq} \\
  &= (a^{\frac{p}{q}})^{nq}(a^{\frac{m}{n}})^{nq}\rap{by \myref{Theorem}{0.2.14}}\\
  &=((a^{\frac{p}{q}})^q)^n((a^{\frac{m}{n}})^n)^q\\
&=(a^p)^n(a^m)^q\\
&= a^{np+mq}=(a^{r+s})^{nq}\rap{\myref{Theorem}{0.2.12}\text{ and }\myref{Theorem}{0.2.13}} 
\end{align*}
Then by \myref{Theorem}{0.3.6}, we can deduce $a^ra^s=a^{r+s}$ 
\end{proof}
\begin{theorem}
\label{1.3.13}
\textbf{(Arithmetic property of rational power)} Given $r,s\inq\text{ and }a>0$, we have
\begin{equation*}
  (a^r)^s=a^{rs}
\end{equation*}
\end{theorem}
\begin{proof}
Express $r,s$ in the form $r=\frac{p}{q},s=\frac{m}{n}$ where $q,n\inn\text{ and }p,m \inz$. Observe
\begin{align*}
  (a^{rs})^{nq}&=(a^{\frac{mp}{nq}})^{nq}\\
  &= a^{mp}
\end{align*}
Then observe
\begin{align*}
  ((a^r)^s)^{nq}&=(((a^{\frac{p}{q}})^{\frac{m}{n}})^n)^q\\
  &=((a^{\frac{p}{q}})^m)^q\\
 &=(a^{\frac{p}{q}})^{mq}\\
  &=((a^{\frac{p}{q}})^q)^m \\
  &=(a^p)^m=a^{mp}=(a^{rs})^{nq}  
\end{align*}
\end{proof}
\begin{corollary}
\label{1.3.14}
$a^{\frac{q}{p}}=(a^{\frac{1}{p}})^q$
\end{corollary}
\begin{theorem}
\label{1.3.15}
\textbf{(Arithmetic property of rational power)} Given $r\inq$ and $a,b>0$, we have
 \begin{equation*}
a^rb^r=(ab)^r
\end{equation*}
\end{theorem}
\begin{proof}
Express $r$ in the form  $r=\frac{q}{p}$, and observe $(a^rb^r)^p=(a^r)^p(b^r)^p=(a^{\frac{q}{p}})^p(b^{\frac{q}{p}})^p=a^qb^q=(ab)^q=((ab)^r)^p$. 
\end{proof}
\section{Irrational Power}
\fbox{\begin{minipage}{39em}
    After the rational power, we now try to define irrational power. In most of the textbooks, irrational power as a rigorous definition often come after the definition of Euler number, but here, we define the irrational power with a technique not so advanced and to be fair quite cumbersome compared to the approaches in most textbooks. Of course, the common inequalities and arithmetic properties of irrational power will be presented, although their order of presentation is different to how we present in the section before. 
\end{minipage}}
\begin{lemma}
\label{1.4.1}
For $b,x\inF$, define  $B(x):=\set{b^t:t\inq,t\leq x}$. Then for all $x\inF$, $b>1$ implies  $\sup B(x)$ exists, and $0<b\leq 1$ implies $\inf B(x)$ exists.
\end{lemma}
\begin{proof}
  For all $x$,  we know $B(x)$ is nonempty, since if not, $\Q$ is bounded below. Because $\Q$ is nonbounded above, we can pick a rational  $y$ greater than $x$. If $b>1$, we deduce  $\forall b^t\inB(x),b^t\leq b^y $; thus $b^y$ is an upper bound of  $B(x)$. If $0<b<1$, we deduce $\forall b^t\in B(x),b^y\leq b^t$; thus $b^y$ is an lower bound of $B(x)$. If $b=1$, just notice  $B(x)=\set{1}$.     
\end{proof}
\begin{definition}
\label{1.4.2}
\textbf{(Irrational Power Definition)} For all $b,x\inF$, define $B(x)$ as in \myref{Lemma}{1.4.1}, and for all $x\inF$, if $b>1$, define $b^x:=\sup B(x)$, and if $0<b\leq 1$, define $b^x:=\inf B(x)$ 
\end{definition}
\fbox{\begin{minipage}{39em}
The above definition is in fact not very appropriate, since we have already define $b^x$ where  $x\inq$. We don't know if the above definition is consistent with our old definition. The next theorem is to show it is.  
\end{minipage}}
\begin{theorem}
\label{1.4.3}
  \textbf{(Consistency of power definitions)} Given $r\inq$, if $b>1$, then  $b^r=\sup B(r)$, and if $b<1$, then  $b^r=\inf B(r)$
\end{theorem}
\begin{proof}
If $b>1$, deduce  $b^r=\max  B(r)=\sup B(r)$. If $b<1$, deduce $b^r=\min B(r)=\inf B(r)$.  
\end{proof}
\fbox{\begin{minipage}{39em}
Now, we are going to prove one common inequalities. The other will be proven after some work. 
\end{minipage}}


\begin{theorem}
\label{1.4.4}
\textbf{(Inequality when base is fixed)} Given a positive $b$ and $x<y$, we have
 \begin{equation*}
\begin{cases}
  b^x<b^y\iff b>1\\
  b^x=b^y\iff b=1\\
  b^x>b^y\iff 0<b<1
\end{cases}
\end{equation*}
\end{theorem}
\begin{proof}
  By \myref{Theorem}{0.3.5}, we can pick a rational $q$ such that  $x<q<y$. We have $b^q\in B(y)$. We now consider three possible situations.\\



  If $b>1$, then $b^q\leq \sup B(y)$. Because $\forall b^t\inB(x), t\leq x<q$, we can  deduce $\forall b^t\in B(x), b^t<b^q$; that is, $b^q$ is an upper bound of $B(x)$. Notice that there exists rational between $x<q$, so  $b^q$ can not be the least upper bound of  $B(x)$.  Then we can deduce $\sup B(x)< b^q\leq \sup B(y)$.\\

  If $b=1$, then  $b^x=1=b^y$.\\

  If  $b<1$, then $\inf B(y)\leq b^q$. Because $\forall b^t\in B(x), t\leq x<q$, we can deduce $\forall b^t\in B(x), b^t>b^q$ ; that is, $b^q$ is an lower bound of  $B(x)$. Notice that there exists rational between $x<q$, so  $b^q$ can not be the greatest lower bound of  $B(x)$. Then we can deduce $\inf B(y)\leq b^q< \inf B(x)$.\\

  The Theorem follows from the three implications of the three situation. 
\end{proof}

\begin{lemma}
\label{1.4.5}
Let $S,U$ and be two bounded above subset of  $\F$, containing only nonnegative numbers. Define $SU:=\set{su:s\inS,u\inU}$. We have    
\begin{equation*}
\sup SU=\sup S \sup U 
\end{equation*}
and have 
\begin{equation*}
\inf SU=\inf S\inf U
\end{equation*}
where the infimum part hold true even if $S,U$ are not bounded above 
\end{lemma}
\begin{proof}
Because $S,U$ contain only nonnegative numbers, we know $s\leq \sup S$ and $u\leq \sup U$ implies $su\leq \sup S \sup U$, so $\sup S \sup U$ is an upper bound of $SU$. Now we show \vi{$\sup S \sup U$ is the \textit{least} upper bound of $SU$}.\\

\As{there is an upper bound $x$ of $AB$ smaller than $\sup A \sup B$}. Express $x$ in the form  $x=\sup S \frac{x}{\sup S}$. Because $x<\sup S \sup U $, we know $\frac{x}{\sup S}<\sup U$, which implies there is a number  $u\inU$ greater than $\frac{x}{\sup S}$. Observe $u>\frac{x}{\sup S}\implies \frac{x}{u}<\sup S$, which implies that there is a number $s\in S$ greater than  $\frac{x}{u}$. Observe  $\frac{x}{u}<s\implies x<su \tCaC\vdone$\\ 

Clearly, we know $s\geq \inf  S$ and $u\geq \inf  U$ implies $su\geq \inf  S \inf  U$, so $\inf  S \inf  U$ is an lower bound of $SU$. Now we show \blue{$\sup S \sup U$ is the \textit{greatest} lower bound of $SU$}.\\

\As{there is an lower bound $x$ of $AB$ greater than $\sup A \sup B$}. Express $x$ in the form  $x=\inf  S \frac{x}{\inf  S}$. Because $x>\inf  S \inf  U $, we know $\frac{x}{\inf  S}>\inf  U$, which implies there is a number  $u\inU$ less than $\frac{x}{\inf  S}$. Observe $u<\frac{x}{\inf  S}\implies \frac{x}{u}>\inf  S$, which implies that there is a number $s\in S$ less than  $\frac{x}{u}$. Observe  $\frac{x}{u}>s\implies x>su \tCaC\bdone$  
\end{proof}

\begin{theorem}
\label{1.4.6}
\textbf{(Arithmetic property)} Given $r,s\inF$ and $0<a$, we have
\begin{equation*}
  a^{r+s}=a^ra^s
\end{equation*}
\end{theorem}

\begin{proof}  
We prove \vi{$A(r+s)=\set{xy:x\inA(r)\text{ and }y\inA(s)}$}. Denote the set on right side $E$. Observe that $xy\in E \implies \exists q\leq r\inq,x=a^q\text{ and }\exists m\leq s\inq,y=a^m\implies xy=a^{q+m} \text{ where $q+m\leq r+s$}\implies xy\in A(r+s)$. Then we know $E\subseteq A(r+s)$. Given $a^d\in A(r+s)$, we know $d\leq r+s$, so if we express $a^d=a^ra^{d-r}$, we are sure $a^{d-r}\in A(s)$ and $a^r\in A(r)$, which implies $a^d\in E$. Then we can deduce $A(r+s)\subseteq E$.$\vdone$\\

By \myref{Lemma}{1.4.5}, our proof is finished.
\end{proof}
\fbox{\begin{minipage}{39em}
Now, we introduce an idea to aid our proof of inequality.
\end{minipage}}
\begin{definition}
\label{1.4.7}
\textbf{(Definition of order homomorphism)} Let $S$ be an subset of  $\F$, the function  $\phi :S \rightarrow  \F $ is an \textit{order homomorphism}, if for all $a,b\in S$, we have 
\begin{equation*}
  a<b\implies \phi (a)<\phi (b)
\end{equation*}
Two subset $U,V$ are said to be  \textit{order isomorphic}, if there exists an bijective order homomorphism from one to another.
\end{definition}
\begin{lemma}
\label{1.4.8}
Let $S,U$ be two order isomorphic bounded above subset of  $\F$, containing only nonnegative numbers, where $\phi :S\rightarrow U$ is an order isomorphism. Let $US_\phi = \set{s\phi (s):s\in S}$. We have
\begin{equation*}
\sup S \sup U= \sup US_\phi 
\end{equation*}
and have
\begin{equation*}
\inf S\inf U=\inf US_\phi 
\end{equation*}
where the infimum part hold true even if $S,U$ are not bounded above.
\end{lemma}
\begin{proof}
For all $s\phi (s)$ in $US_\phi $, we know $0\leq s\leq \sup S$ and $0\leq \phi (s)\leq \sup U$, so we can deduce $s\phi (s)\leq \sup S\sup U$. We have proven that $\sup S\sup U$ is an upper bound of $US_\phi $. We now prove it is the least. \As{there exists an upper bound $x$ of $US_\phi $ smaller than $\sup S\sup U$}. Because $\frac{x}{\sup U}<\sup S$, we know there exists $s\inS$ such that $x<(\sup U)s$. Then because $\frac{x}{s}<\sup U$, we know there exists $\phi (s')\in U$ such that $x<s\phi (s')$. If $s<s'$, then we see  $x<s\phi (s')<s'\phi (s')\in US_\phi $. If $s>s'$, then we see  $x<s\phi (s')<s\phi (s)$. If $s=s'$, then we see $x<s\phi (s')=s\phi (s)\tCaC$\\


For all $s\phi (s)$ in $US_\phi $, we know $s\geq \inf  S$ and $ \phi (s)\geq \inf  U$, so we can deduce $s\phi (s)\geq \inf  S\inf  U$. We have proven that $\inf  S\inf  U$ is an lower bound of $US_\phi $. We now prove it is the greatest. \As{there exists an lower bound $x$ of $US_\phi $ greater than $\inf  S\inf  U$}. Because $\frac{x}{\inf  U}>\inf  S$, we know there exists $s\inS$ such that $x>(\inf  U)s$. Then because $\frac{x}{s}>\inf  U$, we know there exists $\phi (s')\in U$ such that $x>s\phi (s')$. If $s<s'$, then we see  $x>s\phi (s')>s\phi (s)\in US_\phi $. If $s>s'$, then we see  $x>s\phi (s')>s'\phi (s')$. If $s=s'$, then we see $x>s\phi (s')=s\phi (s)\tCaC$      
\end{proof}

\begin{theorem}
\label{1.4.9}
\textbf{(Arithmetic property)} Given $b,c>0$ and  $z\inF$, we have
\begin{equation*}
b^zc^z=(bc)^z
\end{equation*}
\end{theorem}
\begin{proof}
 From now, we will use $C(z)$ to denote $\set{c^u: u\inq, u\leq z}$. Define $\phi :B(z)\rightarrow C(z)$ as $b^t\mapsto c^t$. Observe that if $b,c$ are both greater than $1$, then  $b^t<b^m\implies t<m\implies c^t<c^m$, and that if $b,c$ are both less than $1$, then  $b^t<b^m\implies t>m\implies c^t<c^m$. So If $b,c$ are both greater than $1$ or both less than $1$,  then $\phi$ is an order isomorphism. Notice that $\set{(bc)^u:u\inq, u\leq z}=\set{b^t \phi(b^t):t\inq, t\leq z}=C(z)B(z)_\phi$. Then by \myref{Lemma}{1.4.8}, our proof is finished. We now show \vi{$(b^{-1})^zb^z=1$}.\\

WOLG, let $b<1$. Denote $H:=\set{b^t:t\inq,t\leq z},E:=\set{b^{-t}:t\inq,t\leq z}$, so $b^z=\inf H\text{ and }(b^{-1})^z=\sup E$. \As{$(\sup E)(\inf H)>1$}. Notice $\sup E>\frac{1}{\inf H}$ implies that there exists $t\leq z$ such that $b^{-t}>\frac{1}{\inf H}$, which leads to $\inf H>b^t\tCaC$. \As{$(\sup E)(\inf H)<1$}. Notice $\inf H<\frac{1}{\sup E}$ implies that there exists $t\leq z$ such that $b^{-t}<\frac{1}{\sup E}$, which leads to $\sup E<b^t\tCaC\vdone$\\     

 If $b<1<c$ and $bc>1$, observe that $(bc)^z(b^{-1})^z=c^z\implies (bc)^z=(bc)^z(b^{-1})^zb^z=b^zc^z$. If $b<1<c$ and  $bc<1$, observe that $(bc)^z(c^{-1})^z=b^z\implies (bc)^z=(bc)^z(c^{-1})^zc^z=b^zc^z $  
\end{proof}
\begin{theorem}
\label{1.4.10}
\textbf{(Inequality when power is fixed)} Given $0<b<c$ and  $z\inF$, we have
\begin{equation*}
\begin{cases}
  b^z<c^z \iff 0<z\\
  b^z=c^z \iff 0=z\\
  b^z>c^z\iff 0>z
\end{cases}
\end{equation*}
\end{theorem}
\begin{proof}
By \myref{Theorem}{1.4.4}, $z>0\implies (\frac{c}{b})^z>(\frac{c}{b})^0=1\implies c^z>b^z$, and $z=0\implies b^z=1=c^z$, and $z<0\implies (\frac{c}{b})^z<(\frac{c}{b})^0=1$ 
\end{proof}

\fbox{\begin{minipage}{39em}
Before the last arithmetic property, we first prove the existence of logarithm, which play an important role in the proof of last arithmetic property.
\end{minipage}}
\begin{theorem}
\label{1.4.11}
\textbf{(Existence of logarithm)} For all positive  $b,y$ where $b\neq 1$, there exists an unique  $x\inF $ such that $b^x=y$  
\end{theorem}
\begin{proof}
The uniqueness part have been handled by \myref{Theorem}{1.4.4}. Now we split the proof into two cases: \vi{$b>1$} and \blue{$b<1$}\\

Case 1: \vi{$b>1$}\\

Define $A:=\set{w:b^w<y}$ and $x=\sup A$.\\

We first prove the identity \teal{for all positive integer $m$ and  $c>1$, we have $c-1\geq (c^{\frac{1}{m}}-1)m$}\\

Let $d=c^{\frac{1}{m}}$, so we have $c-1=d^m-1=(d-1)(d^{m-1}+d^{m-1}+\cdots +1)\geq (d-1)m=(c^{\frac{1}{m}}-1)m\tdone$\\


\As{$b^x>y$}. Let $t=\frac{b^x}{y}$ and $\frac{b-1}{t-1}<n=m$ and $b=c$ to use the identity, so we have  $b-1\geq n(b^{\frac{1}{n}}-1)$. Then we have $\frac{b-1}{n}\geq b^{\frac{1}{n}}-1$. Notice that $b^x>y\implies t-1>0$, so because $n>\frac{b-1}{t-1}$, we then have $t-1>\frac{b-1}{n}\geq b^{\frac{1}{n}}-1$, which implies $t> b^{\frac{1}{n}}$.\\

 Observe $b^{x-\frac{1}{n}}=\frac{b^x}{b^{\frac{1}{n}}}>\frac{b^x}{t}=y$. Then $x-\frac{1}{n}$ is an upper bound of $A\tCaC$.\\


\As{$b^x<y$}. Let $t=\frac{y}{b^x}$ and let $\frac{b-1}{t-1}<n=m$ and $b=c$ to use the identity, so we have  $b-1\geq n(b^{\frac{1}{n}}-1)$. Then we have $\frac{b-1}{n}\geq b^{\frac{1}{n}}-1$. Notice that $b^x<y\implies t-1>0$, so because $n>\frac{b-1}{t-1}$, we then have $t-1>\frac{b-1}{n}\geq b^{\frac{1}{n}}-1$, which implies $t>b^{\frac{1}{n}}$.\\

Observe $b^{x+\frac{1}{n}}=b^xb^{\frac{1}{n}}<b^xt=y$. Then $x+\frac{1}{n}\inA\tCaC\vdone$\\

Case 2: \blue{$b<1$}\\

Define $B:=\set{w:b^w<y}$ and $x:=\inf B$.\\

We first prove the identity \teal{for all positive integer $m$ and $0<c<1$, we have  $c-1\leq (c^{\frac{1}{m}}-1)m$}\\

Let $d=c^{\frac{1}{m}}$, so we have $c-1=d^m-1=(d-1)(d^{m-1}+d^{m-2}+\cdots +1)\leq (d-1)m=(c^{\frac{1}{m}}-1)m\tdone$.\\

\As{$b^x>y$}. Let $t=\frac{y}{b^x}$ and $\frac{b-1}{t-1}<n=m\text{ and }b=c$ to use the identity, so we have $b-1\leq (b^{\frac{1}{n}}-1)n$. Then we have $\frac{b-1}{n}\leq b^{\frac{1}{n}}-1$. Notice that $b^x>y\implies t-1<0$, so because $n>\frac{b-1}{t-1}$, we then have $t-1<\frac{b-1}{n}\leq b^{\frac{1}{n}}-1$, which implies $t<b^{\frac{1}{n}}$.\\

Observe $b^{x+\frac{1}{n}}=b^xb^{\frac{1}{n}}>b^xt=y$. Then $x+\frac{1}{n}$ is an lower bound of $B\tCaC$.\\

\As{$b^x<y$}. Let $t=\frac{b^x}{y}\text{ and }\frac{b-1}{t-1}<n=m\text{ and }b=c$ to use the identity, so we have $b-1\leq (b^{\frac{1}{n}}-1)n$. Then we have $\frac{b-1}{n}\leq b^{\frac{1}{n}}-1$. Notice that $b^x<y\implies t-1<0$, so because $n>\frac{b-1}{t-1}$, we then have $t-1<\frac{b-1}{n}\leq b^{\frac{1}{n}}-1$, which implies $t<b^{\frac{1}{n}}$.\\

Observe $b^{x-\frac{1}{n}}=\frac{b^x}{b^{\frac{1}{n}}}<\frac{b^x}{t}=y$. Then $x-\frac{1}{n}\inB\tCaC\bdone$
\end{proof}
\begin{definition}
\label{1.4.12}
\textbf{(Definition of logarithm)} Define $\log_b y:=x$, where $x,b,y$ are in \myref{Theorem}{1.4.11}.
\end{definition}
\begin{lemma}
\label{1.4.13}
Given $0<x<rs$ and $0<r$ and  $0<s$, there exist positive rational $u<s$ and positive rational $t<r$ such that   $x<tu<rs$. 
\end{lemma}
\begin{proof}
  $0<x<rs \implies \frac{x}{s}<r$. Then there exists rational $t$ such that  $\frac{x}{s}<t<r$, which implies $\frac{x}{t}<s$. Then there exists rational $u$ such that $\frac{x}{t}<u<s$. Then $x<tu<rs$.
\end{proof}
\begin{theorem}
\label{1.4.14}
\textbf{(Arithmetic property)} Given $r,s\inF\text{ and }a>0$, we have
\begin{equation*}
  (a^r)^s=a^{rs}
\end{equation*}
\end{theorem}
\begin{proof}
  The proof is very very long. We first denote three important sets:
\begin{equation*}
T:=\set{a^t:t\inq,t\leq r}
\end{equation*}
\begin{equation*}
U:=\set{(a^r)^u:u\inq,u\leq s}
\end{equation*}
\begin{equation*}
X:=\set{a^x:x\inq,x\leq rs}
\end{equation*}

  So, we have 
\begin{equation*}
a^r=(\sup \text{ or }\inf )T
\end{equation*}
\begin{equation*}
(a^r)^s=(\sup \text{ or }\inf )U
\end{equation*}
\begin{equation*}
a^{rs}=(\sup \text{ or }\inf )X
\end{equation*}
Notice that from now, when variables $t,u,x$ and their variants $t',u',x'$ are mentioned, they are rational and respectively less than or equal to $r,s,rs$. \\



  Now, we split the proof into sixteen cases: $a$ may be smaller or greater than $1$; $r,rs$ may be positive or negative; $rs$ is rational or irrational. \\



  We first do all 8 cases of $rs\not\inq$. Notice that $rs\not\inq\implies x<rs$.\\  


  The easiest case first:  \vi{$a>1\text{ and }r>0\text{ and }s>0\text{ and }rs\not\inq$}.\\

  \As{$a^{rs}>(a^r)^s$}; that is, $\sup X>\sup U$. Then $\exists x,\forall u,a^x>(a^r)^u=(\sup T)^u$.  Let $u>0$, so we have $\forall t, a^{\frac{x}{u}}>a^t$. By \myref{Lemma}{1.4.13} \CaC. \As{$a^{rs}<(a^r)^s$}; that is, $\sup X<\sup U$. Then $\exists u,\forall x,(\sup T)^u=(a^r)^u>a^x$. Because $a^r>1$, we know $u>0$, since $u<0\implies (a^r)^u<1$. Then we have  $\exists u,\forall x,\sup T>a^{\frac{x}{u}}$, so we have $\exists u,\forall x,\exists t,a^t>a^{\frac{x}{u}}$. But we see that for all $u$, we can let  $x>ru$, so $a^{\frac{x}{u}}>a^r.\tCaC\vdone$   \\



  The second case: \blue{$a>1\text{ and }r>0\text{ and }s<0\text{ and }rs\not\inq$}.\\

  \As{$a^{rs}>(a^r)^s$}; that is, $\sup X>\sup U$. Then $\exists x,\forall u, a^x>(a^r)^u=(\sup T)^u$. Because $u\leq s<0$, we have $\exists x,\forall u,a^{\frac{x}{u}}<\sup T$. Then, we have $\exists x,\forall u,\exists t,a^{\frac{x}{u}}<a^t$. Express $x=rm$, and let $0>s>u>m$, so we have  $\frac{x}{u}=\frac{rm}{u}>r\tCaC$. \As{$a^{rs}<(a^r)^s$}; that is, $\sup X<\sup U$. Then $\exists u,\forall x,a^x<(a^r)^u=(\sup T)^u$. Because $u\leq s<0$, we have $\exists u,\forall x, a^{\frac{x}{u}}>\sup T$. Then, we have $\exists u,\forall x,\forall t, a^{\frac{x}{u}}>a^t$. If $s\not\inq$, then we notice that because $u<s<0$, we have $r(\frac{s}{u})<r$. Pick $t$ such that $r(\frac{s}{u})<t<r$, so we have $rs>tu$. Then pick $x$ such that $0>rs>x>tu$, and observe $a^{\frac{x}{u}}<a^t\tCaC$. If $s\inq$, then it is possible to happen $u=s$. Then $a^{\frac{x}{u}}=a^{\frac{x}{s}}>a^{\frac{rs}{s}}=a^r\geq a^t\tCaC\bdone$  \\



  \\




The third case: \vi{$a>1\text{ and }r<0\text{ and }s>0\text{ and }rs\not\inq$}\\

\As{$a^{rs}>(a^r)^s$}; that is, $\sup X>\inf  U$. Then $\exists x,\exists u,a^x>(a^r)^u=(\sup T)^u $. Notice that $u\leq 0\implies (a^r)^u\geq 1>a^x$, so $u$ must be positive. Then we have $\exists x,\exists u,a^{\frac{x}{u}}>\sup T$. Then, we have $\exists x,\exists u,\forall t,a^{\frac{x}{u}}>a^t$. Notice that $\frac{x}{u}<\frac{rs}{u}<r$, so there  exists $t$ such that  $a^{\frac{x}{u}}<a^t\tCaC.$ \As{$a^{rs}<(a^r)^s$}; that is, $\sup X<\inf U$. Let $\sup X<m<\inf U$, and let $n=\log_a m$, so $a^{rs}<a^n<\inf  U$. Pick a rational $k$ so that  $a^{rs}<a^k<a^n<\inf U$. Then we know for all positive $u$, we have $a^{\frac{k}{u}}<a^r=\sup T$. This give us $\forall u>0,\exists t,a^k<a^{tu}$. Because $rs<k$, we know  $s>\frac{k}{r}$. Let $s>u'>\frac{k}{r}$. Then we see $\forall t,u't<u'r<k\tCaC\vdone$\\

The forth case: \blue{$a>1\text{ and }r<0\text{ and }s<0\text{ and }rs\not\inq$}\\

\As{$a^{rs}>(a^r)^s$}; that is, $\sup X>\inf U$. Then $\exists x,\exists u,a^x>(a^r)^u$. Because $u\leq s<0$, we have $\exists x,\exists u, a^{\frac{x}{u}}<a^r=\sup T$. Then $\exists x,\exists u,\exists t, a^x>a^{tu}$. Observe $tu\geq ru\geq rs>x\tCaC$. \As{$a^{rs}<(a^r)^s$}; that is, $\sup X<\inf U$. Let $\sup X<m<\inf U$, and let $n=\log_a m$, so $a^{rs}<a^n<\inf  U$. Pick a rational $k$ so that  $a^{rs}<a^k<a^n<\inf U$. Then, $\forall u, a^{\frac{k}{u}}>a^r=\sup T$. This implies $\forall u,\forall t,a^k<a^{tu}$. Because $rs<k$, we know  $s>\frac{k}{r}$. Let $s>u'>\frac{k}{r}$. Then $u'r<k$, which implies $r>\frac{k}{u'}$. Let $r>t'>\frac{k}{u'}$. Then we see $t'u'<k\tCaC\bdone$\\

The next four cases where $a<1$ use the same logic, and  reference to \myref{Theorem}{1.4.9}.\\

The forth to eighth cases:  \vi{$a<1\text{ and }rs\not\inq$}\\

By the above four cases, we know $((\frac{1}{a})^r)^s=(\frac{1}{a})^{rs}$. Observe $a^{rs}(\frac{1}{a})^{rs}=1^{rs}=1$, and observe $(a^r)^s((\frac{1}{a})^r)^s=(a^r(\frac{1}{a})^r)^s=((1)^r)^s=1\vdone$.\\

The final eight cases:  \blue{$rs\inq$}\\

If $r,s$ are both rational, the proof reference to \myref{Theorem}{1.3.13}. Notice that if only one of $r,s$ is rational, then  $rs$ is irrational, so we can assume  $r,s$ are both irrational. Observe by the eight cases above, $(a^{rs})^{\frac{1}{s}}=a^r$, so by \myref{Definition}{0.3.9}, $(a^r)^s=((a^{rs})^{\frac{1}{s}})^s=a^{rs}\bdone$\\

Notice that outside of the sixteen cases above, if $a=1\text{ or }s=0\text{ or }s=0$, the proof is trivial.

\end{proof}
\section{Euclidean Space}
\fbox{\begin{minipage}{39em}
In this section, we will use the notion of vector space to define Euclidean space, so we will first prove a simple and fundamental theorem about vector space. 
\end{minipage}}
\begin{theorem}
\label{1.5.1}
Let $V$ be a vector space over arbitrary filed $\F$ and of dimension $n$. We have that  $V$ is isomorphic to  $\F^n$  
\end{theorem}
\begin{proof}
For people who don't know, $\F^n$ in most textbooks is defined as a vector space consisting of $n$-tuples, and $n$-tuples can be defined as follows for any natural $n$.\\

 \begin{gather*}
   (c,d):=\set{\set{c},\set{c,d}}\\
   (b,c,d):=(b,(c,d))=\set{\set{b},\set{b,(c,d)}}=\set{\set{b},\set{b,\set{\set{c},\set{c,d}}}}\\
   (a,b,c,d):=(a,(b,c,d))=\cdots 
\end{gather*}


  Let $\set{v_1,\dots,v_n}$ be a basis for $V$, and define $\phi: \set{v_1,\dots ,v_n} \rightarrow \F^n$ by $c_1v_1+\cdots+c_nv_n\mapsto (c_1,\dots,c_n)$. Notice that $\phi$ can verified to be linear and bijective by using the theorem that every vector in $V$ can be uniquely expressed as a linear combination of  $v_1,\dots ,v_n$.  
\end{proof}
\fbox{\begin{minipage}{39em}
Notice that the existence and uniqueness of real numbers field will be handled in Section 1.7,1.8,1.9, and from now on, let's just pretend that there exists an unique completed ordered field, which we call Real Numbers Field and denote $\R$.\\


In this section, we will introduce the definition of Euclidean space and Cauchy-Schwarz inequality in Euclidean space. 
\end{minipage}}
\begin{definition}
\label{1.5.2}
\textbf{(Definition of absolute value)} Let $x\inR$. We define the absolute value $\abso{x}$ of $x$ by
\begin{equation*}
\abso{x}:=\begin{cases}
  x & \text{ if $x>0$ }\\
  -x& \text{ if $x\leq 0$  }
\end{cases} 
\end{equation*}
\end{definition}
\begin{theorem}
\label{1.5.3}
$\forall x\inr,\abso{x}=\sqrt{x^2} $
\end{theorem}
\begin{proof}
If $x>0$, then  $\abso{x}>0\text{ and }\abso{x}^2=x^2$. If $x\leq 0$, then $\abso{x}\geq 0\text{ and }\abso{x}^2=(-x)^2=x^2$. 
\end{proof}
\begin{definition}
\label{1.5.4}
\textbf{(Definition of $L_p$ norm, or, $p$-norm)} Let $p\geq 1\inR$, and let $\vecta{x}\inr^n$
\begin{equation*}
\norm{\vecta{x}}_p:=(\sum_{i=1}^{n}\abso{x_i}^p)^{\frac{1}{p}} 
\end{equation*}
\end{definition}
\begin{definition}
\label{1.5.5}
\textbf{(Definition of Euclidean $n$-space)}
When we use the word \textit{Euclidean $n$-Space}, we mean $\R^n$ as a vector space equipped with a function $\gen{\cdot,\cdot}:\R^n\times\R^n\rightarrow \R$ called \textin{Euclidean inner product} defined by
\begin{equation*}
\gen{\vecta{x},\vecta{y}}:=\sum_{i=1}^{n}x_iy_i
\end{equation*}
and equipped with $2$-norm
\begin{equation*}
\norm{\vecta{x}}:=\norm{\vecta{x}}_2=(\sum_{i=1}^{n}\abso{x_i}^2)^{\frac{1}{2}} 
\end{equation*}

$2$-norm is also called \textit{Euclidean norm}  or  $L_2$ norm. Most time we use the notation $\vecta{x}\cdot\vecta{y}$ in place of $\gen{\vecta{x},\vecta{y}}$  and the notation $\abso{\vecta{x}}$ in place of $\norm{\vecta{x}}$, for abbreviation. Notice that by \myref{Theorem}{1.5.3}, when $\vecta{x}=(x_1)\inr^1$, we have $\abso{\vecta{x}}=\abso{x_1}$, so we don't have to worry about the compatibility of abbreviation.  Lastly, we can use \myref{Theorem}{1.5.3}, to verify $\abso{\vecta{x}}=(\vecta{x}\cdot\vecta{x})^{\frac{1}{2}}$ in Euclidean Space. 
\end{definition}
\fbox{\begin{minipage}{39em}
Although the common usage of notation $\R^n$ only refer to the set without any structure, but from now, we will use $\R^n$ to denote Euclidean  $n$-space.\\

Notice that Euclidean 3-space is a great tool to describe the physical world, and the idea of Euclidean norm and Euclidean inner product captures the essence of length and angle really well as we will explain in later section and chapter.\\

Lastly, we close this section with a proof of Cauchy-Schwarz inequality in Euclidean space, and leave some important properties of Euclidean norm and Euclidean inner product to next section.  
\end{minipage}}
\begin{theorem}
\label{1.5.6}
\textbf{(Special Case of Schwarz inequality)}  Let  $\vecta{x},\vecta{y}\inR^n$. We have  
 \begin{equation*}
\abso{\vecta{x}}\abso{\vecta{y}}\geq \abso{\vecta{x}\cdot \vecta{y}}
\end{equation*}
The equality hold only if $\exists \ld\inr, \vecta{x}=\ld\vecta{y}$
\end{theorem}
\begin{proof}
Let $f(t)=\sum_{i=1}^{n}(x_it-y_i)^2$, so we have
\begin{align*}
f(t)&=\sum (x_it-y_i)^2\\
&=\sum x_i^2t^2-2x_iy_it+y_i^2 \\
&=t^2\sum x_i^2 -2t\sum x_iy_i+\sum y_i^2\\
&=\abso{\vecta{x}}^2t^2-2(\vecta{x}\cdot\vecta{y})t+\abso{\vecta{y}}^2
\end{align*}
$f(t)$ is an nonnegative quadratic polynomial of $t$, since $f(t)$ is a sum of squares. Then $D(f)\leq 0$; that is $4(\vecta{x}\cdot\vecta{y})^2-4\abso{\vecta{x}}^2\abso{\vecta{y}}^2\leq 0$, which implies $(\vecta{x}\cdot\vecta{y})^2\leq \abso{\vecta{x}}^2\abso{\vecta{y}}^2$, and further implies $\abso{\vecta{x}\cdot \vecta{y}}\leq \abso{\vecta{x}}\abso{\vecta{y}}$     \\

If $\abso{\vecta{x}}\abso{\vecta{y}}=\abso{\vecta{x}\cdot\vecta{y}}$, Then $D(f)=0$, that is, there exists a unique $t'\inr$ such that $f(t')=0$. Then we see $0=f(t')=\sum (x_it'-y_i)^2$, which implies $\forall i, y_i=x_it'$
\end{proof}
\renewcommand{\inC}{\in\C}
\section{Complex Numbers}
\begin{theorem}
\label{1.6.1}
If we define vector multiplication in $\R^2$  as
 \begin{equation*}
 \vecta{xy}=(x_1y_1-x_2y_2,x_1y_2+x_2y_1)
\end{equation*}\\
, then $\R^2$ become a field.
\end{theorem}
\begin{proof}
The whole proof is long and tedious, here we only present some that are worth mentioning.\\
\begin{equation*}
\vecta{y}\vecta{x}=(y_1x_1-y_2x_2,y_1x_2+y_2x_1)=(x_1y_1-x_2y_2,x_1y_2+x_2y_1)=\vecta{x}\vecta{y}
\end{equation*}
\begin{align*}
  (\vecta{x}\vecta{y})\vecta{z}&=(x_1y_1-x_2y_2,x_1y_2+x_2y_1)(z_1,z_2)\\
  &=(x_1y_1z_1-x_2y_2z_1-x_1y_2z_2-x_2y_1z_2,x_1y_1z_2-x_2y_2z_2+x_1y_2z_1+x_2y_1z_1)\\
  &=(x_1(y_1z_1-y_2z_2)-x_2(y_2z_1+y_1z_2),x_1(y_1z_2+y_2z_2)+x_2(y_1z_1-y_2z_2))\\
  &=(x_1,x_2)(y_1z_1-y_2z_2,y_1z_2+y_2z_1)=\vecta{x}(\vecta{y}\vecta{z})
\end{align*}
\begin{equation*}
  (1,0)\vecta{x}=(1,0)(x_1,x_2)=(x_1,x_2)\text{ and }\vecta{x}(1,0)=(x_1,x_2)(1,0)=(x_1,x_2)
\end{equation*}
\begin{align*}
\vecta{x}(\frac{x_1}{x_1^2+x_2^2},\frac{-x_2}{x_1^2+x_2^2})&=(\frac{x_1^2+x_2^2}{x_1^2+x_2^2},\frac{-x_1x_2+x_1x_2}{x_1^2+x_2^2})\\
&=(1,0)
\end{align*}
\begin{align*}
\vecta{x}(\vecta{y}+\vecta{z})&=(x_1,x_2)(y_1+z_1,y_2+z_2)\\
&=(x_1y_1+x_1z_1-x_2y_2-x_2z_2,x_2y_1+x_2z_1+x_1y_2+x_1z_2)\\
&=(x_1y_1-x_2y_2,x_1y_2+x_2y_1)+(x_1z_1-x_2z_2,x_2z_1+x_1z_2)\\
&=\vecta{xy}+\vecta{xz}
\end{align*}
\end{proof}
\fbox{\begin{minipage}{39em}
The fact that the field in \myref{Theorem}{1.6.1} is isomorphic to the customary  $\C$ can be easily verified by checking $(a,b)\mapsto a+bi$ is field isomorphism. We now define $\C$ in this way. Notice that this definition is an abuse of notation, since in this way, $\R\not\in\C$. The motivation behind this definition is to emphasize the geometric nature of $\C$.      
\end{minipage}}
\begin{definition}
\label{1.6.2}
\textbf{(Definition of Complex Numbers)} We define $\C$ as the field in  \myref{Theorem}{1.6.1}. We write $(a,b)$ as $a+bi$ and $(a,-b)$ as $a-bi$. We define $\text{Re}(a+bi):=a$ and $\text{Im}(a+bi):=b$. We say the real part of  $a+bi$ is  $a$ and the imaginary part of  $a+bi$ is  $b$. We define the absolute of value of complex number as its length when it is treated as a vector in complex plane, which in our definition is a Euclidean space. Precisely, we define $\abso{a+bi}:=(a^2+b^2)^{\frac{1}{2}}=\abso{(a,b)}$. Moreover, we define the conjugate as  $\overline{a+bi}=a-bi$. We abuse the notation so that $a+0i\inr\subseteq\C$. 
\end{definition}
\begin{theorem}
\label{1.6.3}
Let $z,w\inC$. We have
\begin{equation*}
\overline{(\overline{z})}=z
\end{equation*}
\begin{equation*}
\overline{z+w}=\overline{z}+\overline{w} 
\end{equation*}
\begin{equation*}
\overline{zw}=(\overline{z})(\overline{w})
\end{equation*}
\begin{equation*}
  \overline{z^n}=(\overline{z})^n
\end{equation*}
\begin{equation*}
z+\overline{z}=2\text{Re}(z)\text{ and }z-\overline{z}=2i\text{Im}(z)
\end{equation*}
\begin{equation*}
\abso{\overline{z}}=\abso{z}
\end{equation*}
\begin{equation*}
\abso{zw}=\abso{z}\abso{w}
\end{equation*}
\begin{equation*}
\abso{z^n}=\abso{z}^n
\end{equation*}
\end{theorem}
\begin{proof}
We prove only the following. Let $z=z_1+z_2i$ and $w=w_1+w_2i$. We have
 \begin{align*}
   \overline{(\overline{z})(\overline{w})}&=\overline{(z_1-z_2i)(w_1-w_2i)}\\
   &=\overline{z_1w_1-z_2w_2-(z_2w_1+z_1w_2)i}\\
   &=z_1w_1-z_2w_2+(z_2w_1+z_1w_2)i\\
   &=zw
\end{align*}
\begin{equation*}
\abso{\overline{z}}=(z_1^2+(-z_2)^2)^{\frac{1}{2}}=(z_1^2+z_2^2)^{\frac{1}{2}}=\abso{z}
\end{equation*}
\begin{align*}
\abso{zw}&=\abso{z_1w_1-z_2w_2+(z_2w_1+z_1w_2)i}\\
&=[(z_1w_1-z_2w_2)^2+(z_2w_1+z_1w_2)^2]^{\frac{1}{2}}\\
&=[(z_1w_1)^2+(z_2w_2)^2-2z_1z_2w_1w_2+(z_2w_1)^2+(z_1w_2)^2+2z_1z_2w_1w_2]^{\frac{1}{2}}\\
&=[z_1^2w_1^2+z_2^2w_1^2+z_1^2w_2^2+z_2^2w_2^2]^{\frac{1}{2}}\\
&=[(z_1^2+z_2^2)(w_1^2+w_2^2)]^{\frac{1}{2}}=(z_1^2+z_2^2)^{\frac{1}{2}}(w_1^2+w_2^2)^{\frac{1}{2}}=\abso{z}\abso{w}
\end{align*}
\end{proof}
\fbox{\begin{minipage}{39em}
In last section we define the Euclidean norm. Here, we give the axioms for norm function and verify that Euclidean norm satisfy all of them. Notice that the four axioms of norm function each describe a property of length in Euclidean space, which is so "obviously true" if we use a non-rigorous approach in geometry. 
\end{minipage}}
\begin{axiom}
\label{1.6.4}
\textbf{(Axioms for norm function)} Let $V$ be a vector space over  $\F\subseteq\C$. A norm, or norm function, $p:V\rightarrow \R$ is a function that satisfy 
\begin{equation*}
  \forall \vecta{x,y}\inV, p(\vecta{x}+\vecta{y})\leq p(\vecta{x})+p(\vecta{y})  \text{( Triangle inequality)}
\end{equation*}
\begin{equation*}
\forall \vecta{x}\inV,\forall \ld\inF, p(\ld\vecta{x})=\abso{\ld}p(\vecta{x})\text{ ( Absolute homogenity) }
\end{equation*}
\begin{equation*}
\forall \vecta{x}\inV,p(\vecta{x})\geq 0\text{( Nonnegativity)}
\end{equation*}
\begin{equation*}
\forall \vecta{x}\inV, p(\vecta{x})=0\implies \vecta{x}=\vecta{0} \text{ ( Positive definiteness) }
\end{equation*}

\end{axiom}
\begin{theorem}
\label{1.6.5}
Euclidean norm satisfy the four axioms.
\end{theorem}
\begin{proof}
We started from the last sentence of \myref{Definition}{1.5.5}, for which we have $\abso{\vecta{x}+\vecta{y}}=((\vecta{x}+\vecta{y})\cdot(\vecta{x}+\vecta{y}))^{\frac{1}{2}}$. Observe
\begin{align*}
\abso{\vecta{x}+\vecta{y}}^2&=(\vecta{x}+\vecta{y})\cdot(\vecta{x}+\vecta{y})\\
&=\vecta{x}\cdot(\vecta{x}+\vecta{y})+\vecta{y}\cdot(\vecta{x}+\vecta{y})\\
&=\abso{\vecta{x}}^2+\abso{\vecta{y}}^2+2(\vecta{x}\cdot\vecta{y})
\end{align*}
and observe
\begin{equation*}
  (\abso{\vecta{x}}+\abso{\vecta{y}})^2=\abso{\vecta{x}}^2+\abso{\vecta{y}}^2+2\abso{\vecta{x}}\abso{\vecta{y}}
\end{equation*}
by \myref{Theorem}{1.5.6}, we then have $\abso{\vecta{x}+\vecta{y}}^2\leq (\abso{\vecta{x}}+\abso{\vecta{y}})^2$. The triangle inequality follows.\\

Observe
\begin{align*}
\abso{\ld\vecta{x}}&=(\sum (\ld x_i)^2)^{\frac{1}{2}}\\
&=(\ld ^2\sum x_i^2)^{\frac{1}{2}}\\
&=\abso{\ld }(\sum x_i^2)^{\frac{1}{2}}\\
&=\abso{\ld }\abso{\vecta{x}}
\end{align*}
Notice that $\abso{\vecta{x}}=\sum x_i^2\geq 0$ and notice that $\abso{\vecta{x}}=0\implies \sum x_i^2=0\implies \forall i,x_i=0\implies \vecta{x}=\vecta{0}$
\end{proof}
\fbox{\begin{minipage}{39em}
In later chapters, we will introduce norms defined in different ways and spaces. For example, in Euclidean space we can define  $\norm{\vecta{x}}:=\max (\abso{x_1},\abso{x_2},\dots ,\abso{x_n})$. One can check this definition does satisfy four axioms. Also, notice that because  $\C$ is an Euclidean 2-space,  by \myref{Theorem}{1.6.5}, the four axioms of norm function also apply to the absolute of complex numbers.\\

Next, we introduce the three axioms for inner product. 
\end{minipage}}
\begin{axiom}
\label{1.6.6}
\textbf{(Axioms for inner product)} Let $V$ be a vector space over $\F$, where $\F$ is either $\R$ or $\C$. An inner product $\gen{\cdot,\cdot}:V\times V\rightarrow \F $ is a function that satisfy the following three axioms: 
\begin{equation*}
\forall \vecta{x,y}\inV,\gen{\vecta{x},\vecta{y}}=\overline{\gen{\vecta{y},\vecta{x}}}\text{ ( Conjugate Symmetry)}
\end{equation*}
\begin{equation*}
\forall \vecta{x,y,z}\inV, \forall a,b\inF, \gen{a\vecta{x}+b\vecta{y},\vecta{z}}=a\gen{\vecta{x},\vecta{z}}+b\gen{\vecta{y},\vecta{z}} \text{ ( Linearity in first argument)} 
\end{equation*}
\begin{equation*}
\vecta{x}\neq \vecta{0}\longrightarrow \gen{\vecta{x},\vecta{x}}>0 \text{ ( Positive Definitness) }  
\end{equation*}
Notice that to satisfy the third axiom, we first have to guarantee that $\forall \vecta{x}\inV,\gen{\vecta{x},\vecta{x}}\inr$. This is implied by the first axiom, since $\gen{\vecta{x},\vecta{x}}=\overline{\gen{\vecta{x},\vecta{x}}}$ 
\end{axiom}
\begin{theorem}
\label{1.6.7}
Euclidean inner product satisfy the three axioms. 
\end{theorem}
\begin{proof}
Let $\vecta{x},\vecta{y},\vecta{z}\inR^n$, and $a,b\inr$. Observe
\begin{equation*}
\vecta{x}\cdot\vecta{y}=\sum x_iy_i=\sum y_ix_i=\vecta{y}\cdot\vecta{x}=\overline{\vecta{y}\cdot\vecta{x}}
\end{equation*}
\begin{equation*}
  (a\vecta{x}+b\vecta{y})\cdot\vecta{z}=\sum (ax_i+by_i)z_i=a\sum x_iz_i + b\sum y_iz_i=a(\vecta{x}\cdot\vecta{z})+b(\vecta{y}\cdot\vecta{z})
\end{equation*}
\begin{equation*}
\vecta{x}\cdot\vecta{x}=\sum x_i^2\geq 0
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
    In 2 or 3 dimension, Euclidean inner product captures the essence of angle really well, as one may remember $\vecta{x}\cdot\vecta{y}=\abso{\vecta{x}}\abso{\vecta{y}}\cos\theta $. However, to rigorously explain why $\sum x_iy_i=\abso{\vecta{x}}\abso{\vecta{y}}\cos\theta $ is for now impossible, since we haven't define  $\cos$. Notice that one can define $\cos$ using Taylor series.\\

Normally, one doesn't use the notation $\vecta{x}\cdot\vecta{y}$ in place of $\gen{\vecta{x},\vecta{y}}$, since these two notations have completely two different meaning in modern mathematics. The former is called dot product, defined only in Euclidean space and defined only by $\vecta{x}\cdot\vecta{y}:=\sum x_iy_i$. The latter is a wide notation of every function that satisfy the three axioms. Obviously the former is only an example of the latter, and we only use $\vecta{x}\cdot\vecta{y}$ in place of $\gen{\vecta{x},\vecta{y}}$, when the latter in context is equivalent to the former, e.g. Euclidean inner product.\\

A quite meaningless example of an inner product is $\gen{\vecta{x},\vecta{y}}:=0$. A more complicated example of an inner product is for space of continuous complex valued function from real interval $[a,b]$ defined by $\gen{f,g}:=\int_a^b f(t)\overline{g(t)}dt$\\

In last section, we "verified" that in Euclidean space, we have $\norm{\vecta{x}}=\gen{\vecta{x},\vecta{x}}^{\frac{1}{2}}$. Now, we prove that every inner product induce a norm, using Parallelogram Law. However, here we have to emphasize that \textit{even though every inner product space come with a norm, not any normed space come with an inner product}.
\end{minipage}}
\begin{definition}
\label{1.6.8}
\textbf{(Parallelogram Law)} We say a normed space $V$ satisfy Parallelogram Law if for all $\vecta{x},\vecta{y}\inV$, we have
 \begin{equation*}
\norm{\vecta{x}+\vecta{y}}^2+\norm{\vecta{x}-\vecta{y}}^2=2(\norm{\vecta{x}}^2+\norm{\vecta{y}}^2)
\end{equation*}
\end{definition}
\begin{lemma}
\label{1.6.9}
\textbf{(Arising from inner product $\longrightarrow $ Satisfying Parallelogram Law)} For an inner product space, if we define $\norm{\vecta{x}}=\gen{\vecta{x},\vecta{x}}^{\frac{1}{2}}$, then we have 
\end{lemma}
\begin{proof}
Notice that $\gen{\vecta{a},\vecta{b}+\vecta{c}}=\gen{\vecta{a},\vecta{b}}+\gen{\vecta{a},\vecta{c}}$ and observe 
\begin{align*}
\norm{\vecta{x}+\vecta{y}}^2+\norm{\vecta{x}-\vecta{y}}^2&=\gen{\vecta{x}+\vecta{y},\vecta{x}+\vecta{y}}+\gen{\vecta{x}-\vecta{y},\vecta{x}-\vecta{y}}\\
&=\gen{\vecta{x},\vecta{x}+\vecta{y}}+\gen{\vecta{y},\vecta{x}+\vecta{y}}+\gen{\vecta{x},\vecta{x}-\vecta{y}}-\gen{\vecta{y},\vecta{x}-\vecta{y}}\\
&=2\gen{\vecta{x},\vecta{x}}+2\gen{\vecta{y},\vecta{y}}+\gen{\vecta{x},\vecta{y}}+\gen{\vecta{y},\vecta{x}}-\gen{\vecta{x},\vecta{y}}-\gen{\vecta{y},\vecta{x}}\\
&=2\gen{\vecta{x},\vecta{x}}+2\gen{\vecta{y},\vecta{y}}=2(\norm{\vecta{x}}^2+\norm{\vecta{y}}^2)
\end{align*}
\end{proof}
\begin{theorem}
\label{1.6.10}
Every inner product give rise to a definition of norm $\norm{\vecta{x}}=\gen{\vecta{x},\vecta{x}}^{\frac{1}{2}}$
\end{theorem}
\begin{proof}
Let $V$ be an inner product space and define  $\norm{\vecta{x}}:=\gen{\vecta{x},\vecta{x}}^{\frac{1}{2}}$. \As{there exists $\vecta{x},\vecta{y}$ such that $\norm{\vecta{x}+\vecta{y}}>\norm{\vecta{x}}+\norm{\vecta{y}}$}. Observe
\begin{equation*}
\norm{\vecta{x}+\vecta{y}}^2>\norm{\vecta{x}}^2+\norm{\vecta{y}}^2+2\norm{\vecta{x}}\norm{\vecta{y}}
\end{equation*}
Notice $\gen{-\vecta{a},-\vecta{a}}=-\gen{\vecta{a},-\vecta{a}}=\overline{-\gen{-\vecta{a},\vecta{a}}}=\overline{\gen{\vecta{a},\vecta{a}}}=\gen{\vecta{a},\vecta{a}}$ and observe
\begin{align*}
  \norm{\vecta{x}-\vecta{y}}^2&>\norm{\vecta{x}}^2+\norm{-\vecta{y}}^2+2\norm{\vecta{x}}\norm{-\vecta{y}}\\
  &= \norm{\vecta{x}}^2+\norm{\vecta{y}}^2+2\norm{\vecta{x}}\norm{\vecta{y}}
\end{align*}
So we have
\begin{equation*}
\norm{\vecta{x}+\vecta{y}}^2+\norm{\vecta{x}-\vecta{y}}^2>2(\norm{\vecta{x}}^2+\norm{\vecta{y}}^2)+4\norm{\vecta{x}}\norm{\vecta{y}}\geq 2(\norm{\vecta{x}}^2+\norm{\vecta{y}}^2)
\end{equation*}
This \CaC to \myref{Lemma}{1.6.8}.\\

Observe
\begin{equation*}
\norm{\ld \vecta{x}}=(\gen{\ld \vecta{x},\ld \vecta{x}})^{\frac{1}{2}}=(\ld \overline{\ld }\gen{\vecta{x},\vecta{x}})^{\frac{1}{2}}=(\ld \overline{\ld })^{\frac{1}{2}}(\gen{\vecta{x},\vecta{x}})^{\frac{1}{2}}=\abso{\ld }\norm{\vecta{x}}
\end{equation*}
Nonnegativity and Positive definiteness of norm function follows from Positive definiteness of inner product and $\gen{\vecta{0},\vecta{0}}=0$. 
\end{proof}
\fbox{\begin{minipage}{39em}
For the last sentence of the last paragraph, we here state it in precision: Let $f(x,y)$ be an inner product, then the function $g(x)=\sqrt{f(x,x)}$ satisfy the norm axiom, but if let $l(x)$ be a norm function, it doesn't always exists a function  $h(x,y)$ such that $l(x)=\sqrt{h(x,x)}\text{ and }h$ satisfy the inner product axioms.\\

An amazing fact is that for a normed space $V$ over $\R$ or  $\C$,  if $V$ satisfy Parallelogram Law, then we can define an inner product on  $V$ by  $\gen{\vecta{x},\vecta{y}}:=\begin{cases}
  \frac{\norm{\vecta{x}+\vecta{y}}^2-\norm{\vecta{x}-\vecta{y}}^2}{4}& \text{ if over $\R$ }\\
  \frac{\norm{\vecta{x}+\vecta{y}}^2-\norm{\vecta{x}-\vecta{y}}^2}{4}+i \frac{\norm{i\vecta{x}-\vecta{y}}^2-\norm{i\vecta{x}+\vecta{y}}^2}{4}& \text{ if over $\C$ }
\end{cases}$ so that this inner product not only satisfy all the axioms for inner product, we also have $\norm{\vecta{x}}=(\gen{\vecta{x},\vecta{x}})^{\frac{1}{2}}$.\\

So, in other word, a norm is induced by an inner product if and only if the norm satisfy the Parallelogram Law. Isn't this amazing? For the proof of only if part, we put it in complicated exercises.\\

We close this section with a special case of Cauchy-Schwarz inequality and the most general Cauchy-Schwarz inequality.
\end{minipage}}
\begin{theorem}
\label{1.6.11}
\textbf{(Cauchy-Schwarz inequality in $\C^n$)} Let $\vecta{v}=(v_1,\dots ,v_n)\inC^n$ and $\vecta{w}=(w_1,\dots ,w_n)\inC^n$ 
 \begin{equation*}
   \abso{\sum v_j\overline{w_j}}\leq (\sum \abso{v_j}^2)^{\frac{1}{2}}(\sum \abso{w_j}^2)^{\frac{1}{2}} 
\end{equation*}
and the equality hold if and only if  $\exists \ld \inC,\vecta{w}=\ld\vecta{v}$\\

Also, if we define inner product on $\C^n$ as $\gen{\vecta{v},\vecta{w}}:=\sum v_j\overline{w_j}$ and induce norm as $\norm{\vecta{v}}:=\gen{\vecta{v},\vecta{v}}^{\frac{1}{2}}$, the special case of Cauchy-Schwarz inequality in $\C^n$ can also be shortened to
 \begin{equation*}
\abso{\gen{\vecta{v},\vecta{w}}}\leq (\norm{\vecta{v}})(\norm{\vecta{w}})
\end{equation*}
\end{theorem}
\begin{proof}
Define $A:=\sum \abso{v_j}^2=\norm{\vecta{v}}^2,B:=\sum \abso{w_j}^2=\norm{\vecta{w}}^2,C:=\sum v_j\overline{w_j}=\gen{\vecta{v},\vecta{w}}$\\

Notice that $B=0$ implies $\forall j,w_j=0$, then two sides of inequality are both $0$ and  $\ld=0$. We have proven the case of $B=0$, now we prove the case of  $B>0$. Keep in mind that $A,B\inR$ and observe
\begin{align*}
\sum \abso{Bv_j-Cw_j}^2&=\sum (Bv_j-Cw_j)\overline{(Bv_j-Cw_j)}\\
&=\sum (Bv_j-Cw_j)(B\overline{v_j}-\overline{C}\overline{w_j})\\
&=\sum B^2\abso{v_j}^2-BC\overline{v_j}w_j-B\overline{C}v_j\overline{w_j}+C\overline{C}\abso{w_j}^2\\
&=B^2A-BC\overline{C}-B\overline{C}C+C\overline{C}B\\
&=B^2A-BC\overline{C}\\
&=B(AB-\abso{C}^2) 
\end{align*}

Because $B>0$, then we can deduce
\begin{equation*}
\sum \abso{v_j}^2\sum \abso{w_j}^2 - \abso{\sum v_j\overline{w_j}}^2=AB-\abso{C}^2=\frac{1}{B}\sum \abso{Bv_j-Cw_j}^2\geq 0
\end{equation*}
So we can deduce
\begin{equation*}
\sum \abso{v_j}^2\sum \abso{w_j}^2\geq \abso{\sum v_j\overline{w_j}}^2
\end{equation*}
Square both side then the Theorem follows.\\

Notice that the equality hold true if and only if $\sum \abso{Bv_j-Cw_j}^2=0$, which is equivalent to $\forall j, w_j=\frac{B}{C}v_j$, and equivalent to $\vecta{w}=\frac{B}{C}\vecta{v}$, where $\ld =\frac{B}{C}=\frac{\norm{\vecta{w}}^2}{\gen{\vecta{v},\vecta{w}}}$
\end{proof}
\begin{theorem}
\label{1.6.12}
\textbf{(General Cauchy-Schwarz inequality)} For all inner product space $V$ where the norm is induced by the inner product, we have
\begin{equation*}
\abso{\gen{\vecta{v},\vecta{w}}}\leq (\norm{\vecta{v}})(\norm{\vecta{w}})
\end{equation*}
and the equality hold if and only if  $\vecta{w}=\ld \vecta{v}$
\end{theorem}
\begin{proof}
Define $A:=\norm{\vecta{v}}^2,B:=\norm{\vecta{w}}^2,C:=\gen{\vecta{v},\vecta{w}}$\\

Notice that $B=0$ implies $\vecta{w}=\vecta{0}$, which further implies $\gen{\vecta{v},\vecta{w}}=\overline{\gen{\vecta{0},\vecta{v}}}=\overline{\gen{\vecta{a}-\vecta{a},\vecta{v}}}=\overline{\gen{\vecta{a},\vecta{v}}-\gen{\vecta{a},\vecta{v}}}=0$, so two side of the inequality are both  $0$ and the equality hold true, where $\ld =0$. We have proven the case of $B=0$. Now we prove the case of $B>0$\\

Keep in mind that $A,B\inR$ and observe
\begin{align*}
\norm{B\vecta{v}-C\vecta{w}}^2&=\gen{B\vecta{v}-C\vecta{w},B\vecta{v}-C\vecta{w}}\\
&=\gen{B\vecta{v},B\vecta{v}}-\gen{C\vecta{w},B\vecta{v}}-\gen{B\vecta{v},C\vecta{w}}+\gen{C\vecta{w},C\vecta{w}}\\
&=B^2\gen{\vecta{v},\vecta{v}}-CB\gen{\vecta{w},\vecta{v}}-B\overline{C}\gen{\vecta{v},\vecta{w}}+C\overline{C}\gen{\vecta{w},\vecta{w}}\\
&=B^2A-CB\overline{C}-B\overline{C}C+C\overline{C}B\\
&=B(AB-\abso{C}^2)
\end{align*}
Because $B>0$, we can deduce 
 \begin{equation*}
\norm{\vecta{v}}^2\norm{\vecta{w}}^2-\abso{\gen{\vecta{v},\vecta{w}}}^2=AB-\abso{C}^2=\frac{\norm{B\vecta{v}-C\vecta{w}}^2}{B}\geq 0
\end{equation*}
So we can deduce
\begin{equation*}
\norm{\vecta{v}}^2\norm{\vecta{w}}^2\geq \abso{\gen{\vecta{v},\vecta{w}}}^2
\end{equation*}
Square both side then the Theorem follows. 
\end{proof}
\begin{corollary}
\label{1.6.13}
\textbf{(Verification of Triangle Inequality)} Let $V$ be an inner product space where the norm is induced by the inner product, we have 
 \begin{equation*}
   \norm{\vecta{v}+\vecta{w}}\leq\norm{\vecta{v}}+\norm{\vecta{w}}
\end{equation*}
where the equality hold true only when $\gen{\vecta{v},\vecta{w}}\geq 0$ and $\vecta{v},\vecta{w}$ are linearly dependent.
\end{corollary}
\begin{proof}
Observe
\begin{align*}
\norm{\vecta{v}+\vecta{w}}^2&=\gen{\vecta{v}+\vecta{w},\vecta{v}+\vecta{w}}\\
&=\norm{\vecta{v}}^2+\gen{\vecta{v},\vecta{w}}+\gen{\vecta{w},\vecta{v}}+\norm{\vecta{w}}^2\\
&=\norm{\vecta{v}}^2+\norm{\vecta{w}}^2+\gen{\vecta{v},\vecta{w}}+\overline{\gen{\vecta{v},\vecta{w}}}\\
&=\norm{\vecta{v}}^2+\norm{\vecta{w}}^2+2\text{Re}(\gen{\vecta{v},\vecta{w}})\\
&\leq \norm{\vecta{v}}^2+\norm{\vecta{w}}^2+2\abso{\gen{\vecta{v},\vecta{w}}}\\
&\leq \norm{\vecta{v}}^2+\norm{\vecta{w}}^2+2(\norm{\vecta{v}})(\norm{\vecta{w}})\\
&=(\norm{\vecta{v}}+\norm{\vecta{w}})^2
\end{align*}
\end{proof}
\section{Existence of Real Numbers - Dedekind Cut*}
\section{Existence of Real Numbers - Decimal*}
\section{Uniqueness of Real Numbers*}
\section{Exercises}
\begin{question}{}{}
Given a nonzero rational $r$ and an irrational $x$, prove that $r+x$ and $rx$ are irrational. 
\end{question}
\begin{question}{}{}
Prove that no rational $r$ satisfy  $r^2=12$
\end{question}
\begin{question}{}{}
Let $E$ be a nonempty subset of an ordered set; suppose $\alpha$ is a lower bound and $\beta$ is an upper bound of $E$. Prove $\alpha<\beta$ 
\end{question}
\begin{question}{}{}
Let $A$ be a nonempty subset of real numbers which is bounded below.  Define $-A:=\set{-x: x\inA}$. Prove that
\begin{equation*}
\inf A=-\sup (-A)
\end{equation*}
\end{question}
\begin{question}{}{}
Prove that no ordered relation can be defined on $\C$ so that $\C$ become an ordered field.
\end{question}
\begin{question}{}{}
Let $w=u+vi$ and  $a=(\frac{\abso{w}+u}{2})^{\frac{1}{2}}$ and $b=(\frac{\abso{w}-u}{2})^{\frac{1}{2}}$ \\

Verify that if $v\geq 0$, then  $w=(a+bi)^2$, and that if $v<0$, then $w=(a-bi)^2$\\

Prove that every complex number have at least $2$ square roots.
\end{question}
\begin{question}{}{}
Let $z_1,\dots ,z_n\inC$. Prove that
\begin{equation*}
\abso{\sum z_j}\leq \sum \abso{z_j}
\end{equation*}
\end{question}
\begin{question}{}{}
Let $x,y\inC$. Prove that
 \begin{equation*}
\abso{\abso{x}-\abso{y}}\leq \abso{x-y}
\end{equation*}
\end{question}
\begin{question}{}{}
Let $z\inC\text{ and }\abso{z}=1$. Compute
\begin{equation*}
\abso{1+z}^2+\abso{1-z}^2
\end{equation*}
\end{question}
\begin{question}{}{}
Let $\vecta{x}\inR^k\text{ and }\vecta{x}\neq \vecta{0}$. When $k=1$,  prove that there does not exists $\vecta{y}\inR^k$, such that $\vecta{x}\cdot\vecta{y}$. When $k\geq 2$, prove that there exists infinitely amount of $\vecta{y}\inR^k$ such that $\vecta{x}\cdot\vecta{y}=0$ 

\end{question}
\begin{question}{}{}
Let $k\geq 3, \vecta{x},\vecta{y}\inR^k,\abso{\vecta{x}-\vecta{y}}=d>0\text{ and }r>0$. Prove that if $2r<d$, then there exists no  $z\inR^k$ such that $\abso{\vecta{z}-\vecta{x}}=\abso{\vecta{z}-\vecta{y}}=r$. Prove that if $2r=d$, there exists exactly one $\vecta{z}\inR^k$ that satisfy $\abso{\vecta{z}-\vecta{x}}=\abso{\vecta{z}-\vecta{y}}=r$. Prove that if $2r>d$, then there are infinitely many $\vecta{z}\inR^k$ that satisfy $\abso{\vecta{z}-\vecta{x}}=\abso{\vecta{z}-\vecta{y}}=r$. Prove that if $k=2$ and  $2r>d$, then there exists exactly $2$ unique  $\vecta{z}\inR^k$ such that $\abso{\vecta{z}-\vecta{x}}=\abso{\vecta{z}-\vecta{y}}=r$. Prove that if $k=1$ and $2r>d$ then there exists no $\vecta{z}\inR^k$ that satisfy $\abso{\vecta{z}-\vecta{x}}=\abso{\vecta{z}-\vecta{y}}=r$
\end{question}

\section{Complicated Exercises}
In \myref{Axiom}{0.2.1}, we present the two ordered field axioms:
\begin{equation*}
y<z\longrightarrow x+y<x+z
\end{equation*}
\begin{equation*}
x>0\text{ and }y>0\longrightarrow xy>0
\end{equation*}\\
If we define the order for $\R$ completely in reverse, that is; for any $x,y$, where originally we  have  $x\leq y$, we now define $x\geq y$, then we can see the new order relation does not satisfy the second axiom, i.e. $x>0\text{ and }y>0\rightarrow xy>0$, by observing $-1>0\implies 1=(-1)^2<0$
\begin{question}{
  Uniquely Orderd}{}
Prove that $\Q$ and $\R$ are uniquely ordered field; that is, any order relation defined on $\Q$ and  $\R$ must be exactly the same as how we usually define it to satisfy the two ordered field axioms. Notice that you have to first come up with a way to describe our usual ways for ordering $\Q\text{ and }\R$ in your proof and make sure that the description do let us tell weather $x< y$ or $y< x$ for all $x,y$.       
\end{question}
Next question refers to \myref{Definition}{1.6.8}
\begin{question}{}{}
  Give an example of a normed vector space $V$ such that for all inner product that can be defined on $V$, there exists  $\vecta{x}\inV$ such that $\norm{\vecta{x}}\neq \sqrt{\gen{\vecta{x},\vecta{x}}}$, and show that this normed space does not satisfy Parallelogram Law.\\

Prove a norm is induced by some inner product if and only if the norm satisfy Parallelogram Law.
\end{question}
\begin{question}{}{}
Let $\vecta{a},\vecta{b}\inR^n\text{ and }m \inR^+$. Find $\vecta{c}\inR^n\text{ and }r>0$ such that
\begin{equation*}
\abso{\vecta{x}-\vecta{a}}=m\abso{\vecta{x}-\vecta{b}}\iff \abso{\vecta{x}-\vecta{c}}=r
\end{equation*}
\end{question} 
\begin{question}{}{}
State and prove the Pythagorean Law in Euclidean $n$-space, where  $n\geq 2$
\end{question}
\chapter{Basic Topology} 
\section{Different but Equivalent Ways to Define A Topology}
\fbox{\begin{minipage}{39em}
This section illustrates three methods to define a topology. However, in practice, the approach using open sets is predominantly employed.
\end{minipage}}
\begin{definition}
\label{2.1.1}
\textbf{(Definition of Topology, via Open Set)} Given a set $X$, called a topological space, if we say a family $\tau$  of subsets of $X$ is a topology on $X$, then we mean
\begin{gather*}
X, \varnothing \in \tau\\
A,B\in\tau\implies A\cap B\in\tau\\
\mathfrak{U}\subseteq \tau\implies \bigcup \mathfrak{U}\in \tau
\end{gather*}
If a subset of $X$ belong to  $\tau$, we say it is an open set, and if a subset of $X$ is an complement of an open set, we say it is an closed set.
\end{definition}
\fbox{\begin{minipage}{39em}
The method above is the usual way to define a topology. Next, we'll look at some properties of closed sets. Keep in mind that the related properties for open sets are basic axioms
\end{minipage}}
\begin{theorem}
\label{2.1.2}
\textbf{(Property of Closed Sets)} Let $\mathfrak{F}$ be the family of precisely all closed sets in a topological space. We have
\begin{gather*}
X,\varnothing \in \mathfrak{F}\\
A,B\in\mathfrak{F}\implies A\cup  B\in\mathfrak{F}\\
\mathfrak{B}\subseteq \mathfrak{F}\implies \bigcap \mathfrak{B}\in \mathfrak{F}
\end{gather*}
\end{theorem}
\begin{proof}
\begin{equation*}
X=\varnothing^c\text{ and }\varnothing=X^c\in \mathfrak{F}
\end{equation*}
\begin{equation*}
A,B\in \mathfrak{F}\implies A^c,B^c \in \mathfrak{O}\implies A^c \cap B^c=(A\cup B)^c \in \mathfrak{O} \implies A\cup  B \in \mathfrak{F}
\end{equation*}
\begin{equation*}
\mathfrak{B}\subseteq \mathfrak{F}\implies \set{F^c:F \in \mathfrak{B}}\subseteq \mathfrak{O}\implies \bigcup_{F\in \mathfrak{B}}F^c \in \mathfrak{O}\implies \bigcap \mathfrak{B}= (\bigcup_{F \in\mathfrak{B}} F^c)^c \in \mathfrak{F}
\end{equation*}
\end{proof}
\begin{theorem}
\label{2.1.3}
\textbf{(Equivalent Ways to Define The Same Topology, Part 1)} Define $f$ on $\power{\power{X}}$ by 
\begin{equation*}
f(\mathfrak{A})=\set{A^c:A\in \mathfrak{A}}
\end{equation*}
If $\mathfrak{O}$ is a topology on $X$, then  $f(\mathfrak{O})$ are the family of precisely all closed sets on $X$. Also, we have
\begin{equation*}
f=f^{-1}
\end{equation*}
This mean if we are given a family of closed sets, the family of closed sets given by the topology induced by our original family is our original family.   
\end{theorem}
\begin{proof}
Trivial. 
\end{proof}
\fbox{\begin{minipage}{39em}
Let's move on to the axioms for the neighborhood system. After covering that, we'll demonstrate that these axioms and those for open sets aren't just interconnected  they're essentially the same thing. 
\end{minipage}}
\begin{definition}
\label{2.1.4}
\textbf{(Definition of Neighborhood Function)} We say a function $\mathcal{N}:X\rightarrow \power{\power{X}}$ is neighborhood function if 
\begin{enumerate}[label=(\alph*)]
  \item \text{ The set of neighborhood $\mathcal{N}(x)$ for $x$ is non-empty.  }\\
  \item \text{ A neighborhood around $x$ must contain $x$ }\\
  \item \text{ A set containing a neighborhood around $x$ is a neighborhood around  $x$ }\\
  \item \text{ The intersection of two neighborhoods around $x$ is a neighborhood around  $x$ }\\
  \item Every neighborhood $N$ around $x$ contain an neighborhood $M$ around $x$ such that $N$ is an neighborhood of all $m \in M$
\end{enumerate}
\end{definition}
\begin{theorem}
\label{2.1.5}
\textbf{(Equivalent Ways to Define The Same Topology, Part 2)} There exists a one-to-one correspondence between the class of all topology on $X$ and the class of all neighborhood function on $X$.\\

The correspondence can be written in the form of 
\begin{equation*} (g(\tau))(x)=\set{N\subseteq X: \exists O\in \tau,x\in O\subseteq N}
\end{equation*}
In English, $g$ define
\begin{equation*}
N\text{ is an neighborhood around $x$ }\iff \text{ there exists an open set $O$ s.t. }x\in O\subseteq N
\end{equation*}
The inverse of $g$ will be proved to be
\begin{equation*}
   h(\mathcal{N})=\set{O:\forall x \in O, O\in \mathcal{N}(x)}
\end{equation*}
In English, $h$ define
\begin{equation*}
\text{ $O$ is an open set }\iff O \text{ is an neighborhood around all its element }
\end{equation*}
\end{theorem}
\begin{proof}
  We first prove \vi{$g$ do map topology to neighborhood function}.\\

Try to prove that $X$ is an neighborhood around $x$. The fact an neighborhood around $x$ must contain  $x$ and the fact follows from definition.\\

For proving intersection of two neighborhood around $x$ is an neighborhood around  $x$, use the fact the intersection of two open set is open.\\

For proving each neighborhood $N$ around $x$ contain an neighborhood $M$ around $x$ such that $N$ is an neighborhood of all  $m \in M$, verify for $M=O$, where  $O$ is the open set such that  $x\in O\subseteq N\vdone$\\


We now prove  \blue{$h$ do map neighborhood function to topology}.\\

Proving $\varnothing \in h(\mathcal{N})$ is trivial. We have $X \in h(\mathcal{N})$ because  $\mathcal{N}(x)$ is nonempty for all $x$ and  $X$ is a super set of any neighborhood.\\

Let $A,B$ be open. To prove $A\cap B$ is open, first realize $A$ and  $B$ are both neighborhood around  $x\in A\cap B$, and use the fact the intersection of neighborhood is an neighborhood.\\

Let $\mathfrak{A}$ be a class of open sets. To prove $\bigcup \mathfrak{A}$ is open, first use the fact $x\in \mathfrak{A}$ implies $x$ is in some open set  $A\in \mathfrak{A}$. Then use the fact $A$ is neighborhood around  $x$, and deduce the super set $\bigcup \mathfrak{A}$ must also be a neighborhood.$\bdone$\\

Lastly, we prove \teal{$h$ is the inverse of $g$}.\\

To prove such, we have to first prove \vi{$h(g(\tau))=\tau$}. \\

We first have
\begin{align*}
  Z\in h(g(\tau))&\iff \forall z\in Z, Z\in (g(\tau))(z)\\
      &\iff \forall z\in Z, \exists O_z\in \tau, z\in O_z\subseteq Z 
\end{align*}

It is now quite clear that $\tau \subseteq h(g(\tau))$. To see $h(g(\tau))\subseteq \tau$, collect $O_z$ for each  $z\in Z$ and use the fact the union of open set is open. $\vdone$\\

We now prove \blue{$g(h(\mathcal{N}))=\mathcal{N}$. More precisely, we want to prove $\forall x, g(h(\mathcal{N}))(x)=\mathcal{N}(x)$}\\

Let $x\in X$. Observe
\begin{equation*}
Y \in g(h(\mathcal{N}))(x)\iff  \exists O \in h(\mathcal{N}),x \in O \subseteq Y
\label{1}
\end{equation*}
By definition of $h$, we see $O\in \mathcal{N}(x)$. Then by super set property, we know $Y \in \mathcal{N}(x)$. We have proved $ g(h(\mathcal{N}))(x)\subseteq \mathcal{N}(x)$.\\

Arbitrarily pick $N\in \mathcal{N}(x)$. We wish to prove $N\in g(h(\mathcal{N}))$. Let $M$ be the largest neighborhood around  $x$ contained by $N$ such that  $N$ is an neighborhood around all  $m \in M$. Such greatest $M$ exists because the union of neighborhood  around  $x$ is again an neighborhood around  $x$.\\

We now have 
\begin{equation*}
M=\set{m\in N:N\in \mathcal{N}(m)}
\label{2}
\end{equation*}
since if $M\subset \set{m \in N:N\in \mathcal{N}(m)}$, we can pick any $m$  not in $M$ to see  $M\cup \set{m}$ is even larger.\\

We wish to show the neighborhood $M\subseteq N$ is open to close the proof, by using $M=O$ in \myref{equation}{1}.\\

Arbitrarily pick $m \in M$. We know $N$ is an neighborhood around  $m$, then there exists an neighborhood $Q_m$ around $m$ contained by $N$ such that $N$ is an neighborhood around all  $q\in Q_m$. We see $Q_m\subseteq M$ as \myref{equation}{2} suggest. By super set property, we are done. $\bdone \tdone$
\end{proof}
\fbox{\begin{minipage}{39em}
We now give the formal definition of neighborhood while a topology is given, and translate \myref{Theorem}{2.1.5} into language without neighborhood function. 
\end{minipage}}
\begin{definition}
\label{2.1.6}
\textbf{(Definition of Neighborhood)} We say a set $E$ is a neighborhood around a point $x$, if  $E$ contains an  open set containing $x$.
\end{definition}
\begin{corollary}
\label{2.1.7}
\textbf{(Property of Neighborhood, Part 1)} 
\begin{enumerate}[label=(\alph*)]
  \item For all points $x$, there exists a neighborhood around $x$.\\
  \item The super set of a neighborhood around  $x$ is a neighborhood around $x$.\\
  \item The intersection of two neighborhood around  $x$ is a neighborhood around $x$.\\
  \item 
For each neighborhood $N$ around $x$, there exists a smaller neighborhood $M$ around $x$ such that  $N$ is a neighborhood around all $m \inM$.
\end{enumerate}
\end{corollary}
\begin{theorem}
\label{2.1.8}
\textbf{(Property of Neighborhood, Part 2)} 
\begin{equation*}
  \text{ $E$ is open $\iff E$ is an neighborhood around all points in $E$}
\end{equation*}
\end{theorem}
\begin{proof}
From left to right, it trivially follows from our definition of neighborhood.\\

From right to left, use the definition of neighborhood for each $x\in E$, and the fact that union of open set is open. 
\end{proof}
\section{Closure and Interior Operator}
\fbox{\begin{minipage}{39em}
This section won't go into any detailed general topology. It merely work as a comparison for the next section. 
\end{minipage}}
\begin{definition}
\label{2.2.1}
\textbf{(Definition of Closure and Interior Operator)} We define the closure $\overline{E}$ of $E$ to be smallest closed set containing $E$
\begin{equation*}
  \overline{E}:=\bigcap \set{F\in\mathfrak{F}:E\subseteq F}
\end{equation*}
and define the interior $E^\circ$ of  $E$ to be the largest open set contained by  $E$ 
\begin{equation*}
E^\circ :=\bigcup  \set{O\in\tau :O \subseteq E}
\end{equation*}
\end{definition}
\begin{theorem}
\label{2.2.2}
\textbf{(Basic Property of Closure and Interior Operator)} We have
\begin{gather*}
  \overline{E}=E\iff E\in\mathfrak{F}\text{ and }E^\circ=E\iff E\in\tau\\
\overline{(\overline{E})}=\overline{E}\text{ and }(E^\circ)^\circ=E^\circ\\
  S\subseteq T \implies S^\circ \subseteq T^\circ\text{ and }\overline{S}\subseteq \overline{T}
\end{gather*}
\end{theorem}
\begin{theorem}
\label{2.2.3}
\textbf{(Basic Property of Closure and Interior Operator)} We have 
\begin{equation*}
   \overline{E\cup F}=\overline{E}\cup \overline{F}\text{ and }(E\cap F)^\circ=E^\circ \cap F^\circ
\end{equation*}
\begin{equation*}
\overline{E\cap F}\subseteq \overline{E}\cap \overline{F}\text{ and }E^\circ \cup F^\circ \subseteq (E\cup F)^\circ 
\end{equation*}
\begin{equation*}
  (E^\circ )^c=\overline{E^c}
\end{equation*}
\end{theorem}
\begin{proof}
\As{$C$ is closed and  $E\cup F\subseteq C\subset \overline{E}\cup \overline{F}$}. WOLG, $E\subseteq C\cap \overline{E} \subset \overline{E}\tCaC$. \As{$O$ is open and  $E^\circ \cap F^\circ  \subset O\subseteq E\cap F $}. WOLG, $ E^\circ  \subset O\cup  E^\circ \subseteq E\tCaC$\\

Because $E^\circ \subseteq E$, we have $E^c \subseteq (E^\circ )^c$. \As{$C\text{ is closed and }E^c\subseteq C\subset (E^\circ )^c$}. We see $E^\circ  \subset C^c\subseteq E\tCaC$
\end{proof}
\fbox{\begin{minipage}{39em}
Now, we define interior point and limit point without referencing interior and closure. We'll also demonstrate that the interior, described as the largest open set within $E$, can in turn define interior points.\\ 

Aside from the last section, this is another example in general topology where definitions can be expressed in several ways, yet they remain equivalent.
\end{minipage}}
\begin{definition}
\label{2.2.4}
\textbf{(Definition of Interior Point)} 
\begin{equation*}
\text{ $p$ is an interior point of }E\iff E \text{ is an neighborhood around  $p$ }
\end{equation*}
\end{definition}
\begin{definition}
\label{2.2.5}
\textbf{(Definition of Limit and Isolated Point)} 
\begin{gather*}
p\text{ is limit point of $E$ }\\
\iff \text{ every open neighborhood around $p$ contain some $q\in E$ that isn't $p$ }
\end{gather*}
We denote the set of limit point of $E$ by  $E'$. 
\end{definition}
\begin{theorem}
\label{2.2.6}
\textbf{(Interior Contain Exactly All Interior Points)} 
\begin{equation*}
\text{ Interior contain exactly all interior point }
\end{equation*}
\end{theorem}
\begin{proof}
All interior points has an open neighborhood contained by $E$, which is contained by  $E^\circ $ by definition. Each point in interior is an interior point because interior itself is an open neighborhood.
\end{proof}
\begin{theorem}
\label{2.2.7}
\textbf{(Closure Contain Exactly All Limit Points of $E$ and Points in $E$)} We have
\begin{equation*}
\overline{E}= E' \cup  E
\end{equation*}
\end{theorem}
\begin{proof}
Let $p\in (E'\cup E)^c$. Because $p$ is not a limit point of  $E$, we know there exists an open neighborhood $N$ such that
 \begin{equation*}
N\cap E\subseteq \set{p}
\end{equation*}
Because $p\not\in E$, we then know
\begin{equation*}
N\in E^c
\end{equation*}
We have proved $E'\cup E$ is closed. We wish to show  $E'$ is contained by all closed set $F$ containing  $E$. Let $p\in F^c$. Because $F^c$ is open, we see  $p$ is not a limit point of  $E$.  
\end{proof}
\begin{corollary}
\label{2.2.8}
\textbf{(Every Point in Closure And Not in the Set is a Limit Point)}
\begin{equation*}
\overline{E}\setminus E\subseteq E'
\end{equation*}
\end{corollary}
\fbox{\begin{minipage}{39em}
    Above illustrate two different but equivalent way to define closure and interior. One must be familiar with both of them.\\

    We now give some useful facts that shall later be compared with.
\end{minipage}}
\begin{theorem}
\label{2.2.9}
\textbf{(Point in Closure if and only if No Neighborhood is Disjoint)} We have
\begin{equation*}
\overline{E}=\set{p: \forall M\in \mathcal{N}(p), M\cap E\neq \varnothing}
\end{equation*}
\end{theorem}
\begin{proof}
  There are two possibilities for $p\in \overline{E}$
  \begin{equation*}
  p\in E\text{ or }p\in E'
  \end{equation*}
  There are two possibilities for point have no disjoint neighborhood
  \begin{equation*}
  p\in E\text{ or }p\in E'
  \end{equation*}
  Notice each first possibilities are identical, and each second possibilities are equivalent.
\end{proof}
\begin{theorem}
\label{2.2.10}
\textbf{(Point is Limit Point if and only if In the Closure of Set Excluding The Point)} We have  
\begin{equation*}
E'=\set{p:p \in \overline{E\setminus \set{p}}}
\end{equation*}
\end{theorem}
\begin{proof}
  Notice each limit point $p\in  E'$ is still a limit point of $E\setminus \set{p}$, and notice each limit point of  $E$ is a limit point of $E\cup \set{p}$.
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce boundary.
\end{minipage}}
\begin{definition}
\label{2.2.11}
\textbf{(Definition of Boundary)} We define the boundary of $S$ by the closure of $S$ minus the interior of  $S$
 \begin{equation*}
\partial S=\overline{S}\setminus S^\circ 
\end{equation*}
\end{definition}
\begin{theorem}
\label{2.2.12}
\textbf{(Equivalent Definition for Boundary)} We have
\begin{equation*}
\partial S=\overline{S}\cap \overline{S^c}
\end{equation*}
and
\begin{equation*}
\partial S=\set{p:\text{for every open neighborhood $O$ around $p$, }O\cap S\neq \varnothing\neq O\cap S^c}
\end{equation*}
\end{theorem}
\begin{proof}
Notice $\overline{S^c}=(S^\circ )^c$ from \myref{Theorem}{2.2.3} and we are done. The second result follows from $\partial S=\overline{S}\cap \overline{S^c}$.
\end{proof}
\begin{corollary}
\label{2.2.13}
\textbf{(Boundary is Closed)} We have
\begin{equation*}
\partial S\text{ is closed }
\end{equation*}
\end{corollary}
\begin{corollary}
\label{2.2.14}
\textbf{(Boundary of Boundary is Subset of Boundary)} For all $S$, we have
\begin{equation*}
\partial \partial S\subseteq \partial S
\end{equation*}
\end{corollary}
\begin{proof}
$\partial S$ is closed tell us
\begin{equation*}
\partial \partial S= \partial S\setminus (\partial S)^\circ 
\end{equation*}
\end{proof}
\begin{theorem}
\label{2.2.15}
\textbf{(Boundary is also the Boundary of Complement)} 
\begin{equation*}
\partial S=\partial (S^c)
\end{equation*}
\end{theorem}
\begin{proof}
This follows from $\partial S=\overline{S}\cap \overline{S^c}$
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce exterior
\end{minipage}}
\begin{definition}
\label{2.2.16}
\textbf{(Definition of Exterior)} We define the exterior by
\begin{equation*}
\text{ext}(S):=(\overline{S})^c
\end{equation*}
\end{definition}
\begin{theorem}
\label{2.2.17}
\textbf{(Equivalent definition of Interior)} We have
\begin{equation*}
X=S^\circ \cup \partial S\cup \text{ ext }(S)
\end{equation*}
and have
\begin{equation*}
\text{ext}(S)=(S^c)^\circ 
\end{equation*}
\end{theorem}
\begin{proof}
The first result follows straight from definition, as $\partial S\cup S^\circ =\overline{S}$.Rewrite \myref{Theorem}{2.2.3} into
\begin{equation*}
  (E^\circ )^c =\overline{E^c}\implies ((E^c)^\circ )^c=\overline{E}
\end{equation*}
Then we have
\begin{equation*}
\text{ext}(S)=(\overline{S})^c=(S^c)^\circ 
\end{equation*}
\end{proof}
\begin{theorem}
\label{2.2.18}
\textbf{(Basic Property of Exterior)} We have
\begin{equation*}
S\subseteq T \implies  \text{ext}(T)\subseteq \text{ext}(S)
\end{equation*}
\begin{equation*}
S^\circ  \subseteq \text{ext}(\text{ext}(S))
\end{equation*}
\end{theorem}
\begin{proof}
\begin{equation*}
S\subseteq T\implies \overline{S}\subseteq \overline{T}\implies \text{ext}(T)=(\overline{T})^c\subseteq (\overline{S})^c=\text{ext}(S)
\end{equation*}
Deduce
\begin{equation*}
S\subseteq \overline{S}\implies (\overline{S})^c \subseteq S^c\implies \overline{(\overline{S})^c}\subseteq \overline{S^c}=(S^\circ )^c\implies S^\circ \subseteq (\overline{(\overline{S})^c})^c=\text{ext}(\text{ext}(S))
\end{equation*}
\end{proof}
\section{Different Types of Connected}
\begin{definition}
\label{2.3.1}
  \textbf{(Definition of Path)} By a path from $a$ to  $b$ in a subset $S\subseteq X$, we mean a
  \begin{equation*}
  \text{ Continuous function $f:[0,1]\rightarrow S$ such that $f(0)=a$ and $f(1)=b$ }
  \end{equation*}
\end{definition}
\begin{definition}
\label{2.3.2}
\textbf{(Definition of Path-Connected)} We say a set $S$ is path-connected if 
\begin{equation*}
\text{ For each $a,b\in S$, there exists a path $f$ in $S$ from $a$ to $b$ }
\end{equation*}
\end{definition}
\begin{definition}
\label{2.3.3}
\textbf{(Definition of Separated)} We say $A,B$ are separated if 
\begin{equation*}
\overline{A}\cap B=\varnothing=A\cap \overline{B}\text{ and }A,B\text{ are non-empty }
\end{equation*}
\end{definition}
\begin{definition}
\label{2.3.4}
\textbf{(Definition of Connected)} We say a set $V$ is connected if 
 \begin{equation*}
V\text{ can not be expressed as union of  two non-empty separated sets}
\end{equation*}
\end{definition}
\begin{theorem}
\label{2.3.5}
\textbf{(Equivalent Definition of Connected)} The following are equivalent
\begin{enumerate}[label=(\alph*)]
  \item $V$ is connected
  \item $V$ has no proper non-empty subset  $U$ relatively clopen to  $V$
\end{enumerate}
\end{theorem}
\begin{proof}
  We first prove \vi{$V$ is connected  $\implies $$V$ has no proper non-empty subset  $U$ relatively clopen to  $V$}.\\

\As{$U$ is a proper non-empty subset of $V$ relatively clopen to $V$}. Define 
\begin{equation*}
W:=V\setminus U
\end{equation*}
We know $W$ is also a proper non-empty subset of $V$ relatively clopen to $V$. WOLG, we only wish to prove
 \begin{equation*}
\overline{U}\cap W=\varnothing
\end{equation*}
Because $U$ is relatively closed to  $V$, we know there exists a close set  $F$ such that 
 \begin{equation*}
U=F\cap V
\end{equation*}
Then by definition of closure, we know 
\begin{equation*}
\overline{U}\subseteq F
\end{equation*}
Notice that $F$ and $W$ are disjoint, otherwise  there exists some $x\in F\cap W$, and we can deduce
\begin{equation*}
x\in W\implies x\in V\implies x\in F\cap V=U\left(\because x\in F\cap W\right)
\end{equation*}
which is clearly impossible, since $W$ and $U$ are disjoint.\\

Then because $F$ and $W$ are disjoint, we see
\begin{equation*}
\overline{U}\cap W\subseteq F\cap W=\varnothing
\end{equation*}
WOLG, we see $U$ and $W$ are separated and $V=U\cup W\tCaC\vdone$\\

We now prove \blue{$V$ has no proper non-empty subset $U$ relatively clopen to $V\implies V$ is connected}.\\

\As{$V$ is disconnected}. Then we can express $V$ into union of non-empty separated subsets $A,B$
 \begin{equation*}
V=A\cup B\text{ and }\overline{A}\cap B=\varnothing=A\cap \overline{B}
\end{equation*}
We wish to prove 
\begin{equation*}
A\text{ is relatively clopen to }V
\end{equation*}
We first prove 
\begin{equation*}
\teal{A=\overline{A}\cap V}
\end{equation*}
It is clear that 
\begin{equation*}
A\subseteq\overline{A}\cap V
\end{equation*}
Let $x\in \overline{A}\cap V$. Because $\overline{A}\cap B=\varnothing$, we know 
\begin{equation*}
x\not\in B
\end{equation*}
Then because $x\in V$, we know 
\begin{equation*}
x\in A\tdone
\end{equation*}
WOLG, from \teal{teal} part, we also know $B$ is relatively closed to $V$, then we can deduce  $A$ is relatively clopen to $V$.\\

Because $B$ is non-empty, we know  $A$ is proper. We have proved  $A$ is an non-empty proper subset of $V$ relatively clopen to  $V\tCaC$ 

\end{proof}
\begin{theorem}
\label{2.3.6}
\textbf{(Union of non-Disjoint Connected Sets is Connected)} 
\begin{equation*}
  \text{ $A,B$ are both connected and }A\cap B\neq \varnothing\implies A\cup  B\text{ is connected } 
\end{equation*}
\end{theorem}
\begin{proof}
\As{$A\cup B$ is disconnected}. Let
\begin{equation*}
V\cup W=A\cup B\text{ and }V,W\text{ be separated }
\end{equation*}
Let $x\in A\cap B$. Because $A\cup B=V\cup W$, we see $x$ must belong to one of  $V,W$. WOLG, let  $x\in V$. We know $W$ must intersect with one of $A,B$. WOLG, let  $W\cap  A\neq \varnothing$. We see 
\begin{equation*}
x\in A\cap B\text{ and }x\in V\implies V\cap A\neq \varnothing
\end{equation*}
Notice
\begin{equation*}
\overline{W\cap A}\subseteq \overline{W}\cap \overline{A}\text{ and }\overline{V\cap A}\subseteq \overline{V}\cap \overline{A}\text{ and }(V\cap A)\cup (W\cap A)=A\tCaC
\end{equation*}
\end{proof}
\begin{theorem}
\label{2.3.7}
\textbf{(Path Connected is Stronger than Connected)} 
\begin{equation*}
S\text{ is path-connected }\implies S\text{ is connected }
\end{equation*}
\end{theorem}
\begin{proof}
\As{$S$ is not connected}. Express $S$ into union of two proper non-empty separated subsets $A,B$
\begin{equation*}
S=A\cup B\text{ where $A,B$ are separated }
\end{equation*}
Let 
\begin{equation*}
a\in A\text{ and }b\in B
\end{equation*}
Because $S$ is path-connected, we know there exists a path $f$ in  $S$ from  $a$ to $b$. Consider
\begin{equation*}
c :=f\left(\sup f^{-1}\left[A \right] \right)
\end{equation*}
and denote $x:=\sup f^{-1}\left[A \right]\inr$. Remember
\begin{equation*}
c=f\left(x \right)
\end{equation*}
Because $c\in S$, we know
\begin{equation*}
c\text{ belong to either $A$ or  $B$ }
\end{equation*}
To cause contradiction, we wish to prove
\begin{equation*}
\begin{cases}
  c\in B\implies c\in \overline{A}\\
  c\in A\implies c\in \overline{B}
\end{cases}
\end{equation*}
We first prove 
\begin{equation*}
  \vi{c\in B\implies  c\in \overline{A}}
\end{equation*}
Let $C$ be an open neighborhood around  $c$, we only wish to prove 
\begin{equation*}
\text{ There exists $c'\neq c$ such that $c'\in A\cap C$ }
\end{equation*}
We know 
\begin{equation*}
f\left(x \right)=c\in C
\end{equation*}
Then because $f$ is continuous, we know there exists an open set $O$ in  $\left[0,1 \right]$ such that  
\begin{equation*}
x\in O\text{ and }f\left[O \right]\subseteq C
\end{equation*}
Because $O$ is an open neighborhood around $x\in\R$, we know there exists an open ball $B_r\left(x \right)$ contained by $O$. Clearly,
\begin{equation*}
x-\frac{r}{2}<x=\sup f^{-1}\left[A \right]
\end{equation*}
Then we know there exists some $y\in f^{-1}\left[A \right]$ such that 
\begin{equation*}
x-\frac{r}{2}<y\leq x 
\end{equation*}
Clearly, 
\begin{equation*}
  y\in \left(x-\frac{r}{2},x \right]\subseteq B_r\left(x \right)\subseteq O
\end{equation*}
We now prove \teal{$f\left(y \right)$ is the wanted  $c'$} to finish the partial proof.\\

Notice that 
\begin{equation*}
y\in f^{-1}\left[A \right]\implies f\left(y \right)\in A\implies f\left(y \right)\neq c\text{  }\text{     $\left(\because c\not\in A \right)$ }
\end{equation*}
Because $x-\frac{r}{2}<y\leq x$, we can deduce
\begin{equation*}
y\in B_r\left(x \right)\subseteq O
\end{equation*}
and deduce 
\begin{equation*}
f\left(y \right)\in f\left[O \right]\subseteq C
\end{equation*}
We have proved
\begin{equation*}
f\left(y \right)\neq c\text{ and }f\left(y \right)\in A\cap C\tdone\vdone
\end{equation*}
We now prove 
\begin{equation*}
\blue{c\in A\implies c\in \overline{B}}
\end{equation*}
Let $C$ be an open neighborhood around $c$, we only wish to prove 
 \begin{equation*}
\text{ There exist $c'\neq c$ such that $c'\in B\cap C$ }
\end{equation*}
We know 
\begin{equation*}
f\left(x \right)=c\in C
\end{equation*}
Then because $f$ is continuous, we know there exists an open set $O$ in  $\left[0,1 \right]$ such that  
\begin{equation*}
x\in O\text{ and }f\left[O \right]\subseteq C
\end{equation*}
Because $f\left(x \right)=c\in A$, we know $x\neq 1$, otherwise $f\left(x \right)\in A\cap B$, which is clearly impossible.\\

Then we know there exists $y\in O$ such that 
\begin{equation*}
x<y
\end{equation*}
Because $x=\sup f^{-1}\left[A \right]$, we know 
\begin{equation*}
y \not\in f^{-1}\left[A \right]
\end{equation*}
Then we can deduce 
\begin{equation*}
f\left(y \right)\in B
\end{equation*}
and from $y\in O$, deduce
\begin{equation*}
f\left(y \right)\in f\left[O \right]\subseteq C
\end{equation*}
Notice that from $f\left(y \right)\in B$ and $c\in A$, we know 
\begin{equation*}
f\left(y \right)\neq c
\end{equation*}
We have proved
\begin{equation*}
f\left(y \right)\neq c\text{ and }f\left(y \right)\in B\cap C\bdone
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce connected component. 
\end{minipage}}
\begin{theorem}
\label{2.3.8}
\textbf{(Path-Connected Component)} Define a relation $\sim$ in $S$ by 
\begin{equation*}
a\sim b\text{ if there exists a path in $S$ from $a$ to  $b$}
\end{equation*}
Then 
\begin{equation*}
\sim\text{ is an equivalence relation }
\end{equation*}
\end{theorem}
\begin{proof}
To see $a\sim a$
\begin{equation*}
\text{ define }f(x)=a
\end{equation*}
The inverse image of open set is either $\varnothing$ or $\left[0,1 \right]$. Both are open in $\left[0,1 \right]$.\\

We now prove 
\begin{equation*}
  \vi{a\sim b\implies b\sim a}
\end{equation*}
Let $f$ be a path in $S$ from $a$ to $b$. It is clear that $g:\left[0,1 \right]\rightarrow \left[0,1 \right]$
\begin{equation*}
g(x)=1-x\text{ is continuous }
\end{equation*}
Then we see 
\begin{equation*}
f\circ g\text{ is a path in $S$ from $b$ to $a$ }\vdone
\end{equation*}
We now prove 
\begin{equation*}
\blue{a\sim b\text{ and }b\sim c\implies a\sim c}
\end{equation*}
Let $f$ be a path in $S$ from  $a$ to $b$, and let $g$ be a path in $S$ from $b$ to $c$. Define $h:\left[0,1 \right]\rightarrow S$ by
\begin{equation*}
h(x)=\begin{cases}
  f(2x)& \text{ if  }x\in [0,\frac{1}{2} )\\
  g(2(x-\frac{1}{2}))& \text{ if  }x\in [\frac{1}{2},1]
\end{cases}
\end{equation*}
Suppose $x\in [0,\frac{1}{2})$. We wish to prove 
\begin{equation*}
  \teal{h\text{ is continuous at }x}
\end{equation*}
Let $W\subseteq S$ be an neighborhood around  $h(x)$. We wish to find an neighborhood $V\subseteq [0,1]$ around $x$ such that 
\begin{equation*}
h[V]\subseteq W
\end{equation*}
Because $f$ is continuous at $2x$, we know there exist an neighborhood $T\subseteq [0,1]$ around $2x$ such that 
 \begin{equation*}
h(x)=f(2x)\in f[T]\subseteq W
\end{equation*}
Let
\begin{equation*}
T_{\frac{1}{2}}=\set{\frac{t}{2}:t \in T}
\end{equation*}
Clearly, $T_{\frac{1}{2}}$ is an neighborhood around $x$, such that $T_{\frac{1}{2}}\subseteq [0,\frac{1}{2}]$, and we can see
\begin{equation*}
 h[T_{\frac{1}{2}}]=f[T]\subseteq W\tdone
\end{equation*}
Suppose $x\in [\frac{1}{2},1]$. We wish to prove 
\begin{equation*}
\olive{h\text{ is continuous at $x$ }}
\end{equation*}
Let $W\subseteq S$ be an neighborhood around $h(x)$. We wish to find an neighborhood $V\subseteq [0,1]$ around $x$ such that 
\begin{equation*}
h[V]\subseteq W
\end{equation*}
Because $g$ is continuous at $2(x-\frac{1}{2})$, we know there exists an neighborhood $T\subseteq [0,1]$ around $2(x-\frac{1}{2})$ such that 
\begin{equation*}
h(x)=g(2(x-\frac{1}{2}))\in g[T]\subseteq W
\end{equation*}
Let 
\begin{equation*}
T_{\frac{1}{2}}'=\set{\frac{t}{2}+\frac{1}{2}:t \in T}
\end{equation*}
Clearly, $T_{\frac{1}{2}}'$ is an neighborhood around $x$, such that  $T'_{\frac{1}{2}}\subseteq [\frac{1}{2},1]$, and we can see
\begin{equation*}
h[T'_{\frac{1}{2}}]\subseteq g[T]\subseteq W\odone
\end{equation*}
\end{proof}
\begin{definition}
\label{2.3.9}
\textbf{(Definition of Path-Connected Component)} Partition $S$ by equivalence relation  $\sim$ defined in \myref{Theorem}{2.3.8} into 
\begin{equation*}
\set{S_\ld:\ld  \in \Lambda }
\end{equation*}
We say each $S_\ld $ is an path-connected component.
\end{definition}
\begin{theorem}
\label{2.3.10}
\textbf{(Connected Components are Mutually Separated)} 
\begin{equation*}
A,B\text{ are non-equal connected component of $S$ }\implies A,B\text{ are separated }
\end{equation*}
\end{theorem}
\begin{proof}

\end{proof}
\begin{theorem}
\label{2.3.11}
\textbf{(Topologist's Sine Curve)} In standard topology of $\R^2$
 \begin{equation*}
\set{\left(x,\sin \frac{1}{x} \right):x\in \left(0,1 \right)}\cup \set{\left(0,0 \right)}\text{ is connected }
\end{equation*}
\end{theorem}
\begin{proof}
Denote $S=\set{\left(x,\sin \frac{1}{x} \right):x\in \left(0,1 \right)}\cup \set{\left(0,0 \right)}$, and arbitrarily partition $S$ into non-empty disjoint $A,B$
 \begin{equation*}
S=A\cup B
\end{equation*}
We wish to prove 
\begin{equation*}
\overline{A}\cap B\neq \varnothing\text{ or }A\cap \overline{B}\neq \varnothing
\end{equation*}
WOLG, suppose $\left(0,0 \right)\in A$. Denote $\left(0,0 \right)$ by $g$. Consider the set 
\begin{equation*}
\set{x: (x,\sin \frac{1}{x})\in B}
\end{equation*}
and let 
\begin{equation*}
\alpha =\inf \set{x:(x,\sin \frac{1}{x})\in B}
\end{equation*}
If $\alpha =0$, there are two possibilities
\begin{equation*}
  \begin{cases}
    \exists t\inr^+, \forall x\in (0,t), (x,\sin \frac{1}{x})\in B\\
    \forall t\inr^+, \exists x\in (0,t), (x,\sin \frac{1}{x})\not\in B
  \end{cases}
\end{equation*}
If $\vi{\exists t\inr^+, \forall x\in (0,t), (x,\sin \frac{1}{x})\in B}$, we can construct a sequence 
\begin{equation*}
\set{(\frac{1}{n\pi},(\sin n\pi))}_{n\inn}
\end{equation*}
which converge to $g$, and its tail is in  $B$. It is now easy to see
\begin{equation*}
  (0,0)\in \overline{B}\vdone
\end{equation*}
If $\blue{\forall t\inr^+, \exists x\in (0,t), (x,\sin \frac{1}{x})\not\in B}$, arbitrarily pick a path connected component $B_1$ of $B$. Consider $f(\inf p[B_1])$




\begin{theorem}
\label{2.3.12}
\textbf{(Conjecture)} In $\R^n$, two connected subset $A,B$ are disjoint implies  $A,B$ are separated.
\end{theorem}
\section{Continuous Function} 
\begin{definition}
\label{2.4.1}
\textbf{(Definition of Continuous Function)} We say a function $f:X\rightarrow Y$ is continuous if  
\begin{equation*}
V\text{ is open }\implies f^{-1}\left[V \right]\text{ is open }
\end{equation*}
\end{definition}
\begin{definition}
\label{2.4.2}
\textbf{(Definition of Continuous at a point)} We say $f$ is continuous at $x$ if 
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       For all neighborhood $V$ around $f\left(x \right)$, there exists neighborhood $U$ around $x$ such that $f\left[U \right]\subseteq V$
   \end{minipage}
\end{center}
\end{definition}
\begin{lemma}
\label{2.4.3}
Given $f:X\rightarrow Y$, we have
\begin{equation*}
f\left[f^{-1}\left[V \right] \right]\subseteq V
\end{equation*}
where the equality hold true when $V\subseteq f\left[X \right]$
\end{lemma}
\begin{proof}
Deduce 
\begin{align*}
  y\in f\left[f^{-1}\left[V \right] \right]&\implies \exists x\in f^{-1}\left[V \right], y=f\left(x \right)\\
&\implies \exists y'\in V, y'=f\left(x \right)=y
\end{align*}
Suppose $V\subseteq f\left[X \right]$ and $y\in V$. We know 
\begin{equation*}
f^{-1}\left[y \right]\text{ is non empty }
\end{equation*}
Then We know there exists $x$ such that 
 \begin{equation*}
y=f\left(x \right)
\end{equation*}
Because $y\in V$, we know $x\in f^{-1}\left[V \right]$, then we see 
\begin{equation*}
y\in f\left[f^{-1}\left[V \right] \right]
\end{equation*}
\end{proof}
\begin{theorem}
\label{2.4.4}
\textbf{(Equivalent Definition of Continuous)} Given $f:X\rightarrow Y$. The following are equivalent
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous
  \item every inverse image of closed set is closed
  \item $f$ is continuous at all $x\in X$
  \item for all $A\subseteq X$, we have $f[\overline{A}]\subseteq \overline{f[A]}$
\end{enumerate}
\end{theorem}
\begin{proof}
From  $\left(a \right)$ to $\left(b \right)$, deduce from $V$ is closed to $f^{-1}\left[V^c \right]$ is open, and deduce $f^{-1}\left[V \right]=\left(f^{-1}\left[V^c \right] \right)^c$.\\

From $\left(b \right)$ to $\left(a \right)$, deduce from $V$ is open to $f^{-1}\left[V^c \right]$ is closed, and deduce $f^{-1}\left[V \right]=\left(f^{-1}\left[V^c \right] \right)^c$.\\

We now prove \vi{$f\text{ is continuous }\implies f\text{ is continuous at all $x\in X$ }$}. Let $V$ be an neighborhood around $f\left(x \right)$. We wish to find an neighborhood $U$ around $x$ such that $f\left[U \right]\subseteq V$. By definition of neighborhood, we know there exists an open set $O$ such that
\begin{equation*}
  f\left(x \right)\in O\subseteq V
\end{equation*}
Because $f$ is continuous, we know 
\begin{equation*}
f^{-1}\left[O \right]\text{ is open }
\end{equation*}
Observe that by \myref{lemma}{2.4.3}, we have
\begin{equation*}
x\in f^{-1}\left[O \right]\text{ and }f\left[f^{-1}\left[O \right] \right]\subseteq O\subseteq V\vdone
\end{equation*}


We now prove \blue{$f$ is continuous at all  $x\in X\implies f$ is continuous}. Let $V\subseteq Y$ be open. We wish to prove $f^{-1}\left[V \right]$ is open.\\

Notice that $V$ is an neighborhood around all  $y\in V$, then because $f$ is continuous at all $x\in X$, we know for all $x\in f^{-1}\left[V \right]$, there exists an neighborhood $U_x$ around  $x$ such that  
\begin{equation*}
f\left[U_x \right]\subseteq V 
\end{equation*}
Then for all $x\in f^{-1}\left[V \right]$, there exists an open neighborhood $O_x$ around  $x$ such that 
\begin{equation*}
f\left[O_x \right]\subseteq V
\end{equation*}
This give us 
\begin{equation*}
O_x \subseteq f^{-1}\left[V \right]
\end{equation*}
Then if we collect all such open neighborhood $O_x$, we see 
 \begin{equation*}
\bigcup_{x\in f^{-1}\left[V \right]}O_x=f^{-1}\left[V \right]\bdone
\end{equation*}
We now prove 
\begin{equation*}
\vi{f\text{ is continuous at all $x\in X$ }\implies \forall A\subseteq X, f[\overline{A}]\subseteq \overline{f[A]}}
\end{equation*}
It is clear that 
\begin{equation*}
f[A]\subseteq \overline{f[A]}
\end{equation*}
Let $a'\in \overline{A}\setminus A$, we reduce the problem into proving 
\begin{equation*}
f(a')\in \overline{f[A]}
\end{equation*}
In fact, we will only have to prove
\begin{equation*}
f(a')\in (f[A])'
\end{equation*}
Let $N$ be an neighborhood around  $f(a')$, we reduce the problem into finding an $p\in  f[A]$ such that
\begin{equation*}
p\neq f(a')\text{ and }p\in N
\end{equation*}
Because $a'\not\in A$, we don't have to worry about  $p\neq f(a')$. Because $f$ is continuous at  $a'$, we know there exists a neighborhood  $N_{X}$ around $a'$ such that 
 \begin{equation*}
f[N_X]\subseteq N
\end{equation*}
Then because $a'\in \overline{A}\setminus A$ is a limit point of $A$, we know there exists  $a \in A$   such that 
\begin{equation*}
a\in N_X
\end{equation*}
This give us 
\begin{equation*}
f(a)\in N\vdone
\end{equation*}
We now prove 
\begin{equation*}
  \blue{\forall A\subseteq X, f[\overline{A}]\subseteq \overline{f[A]}\implies \text{ every inverse image of closed set is closed }}
\end{equation*}
Let $B\subseteq Y$, and $A=f^{-1}[B]$. Suppose
 \begin{equation*}
\overline{B}=B
\end{equation*}
We reduce the problem into proving
\begin{equation*}
\overline{A}\subseteq A
\end{equation*}
From premise, we have
\begin{equation*}
f[\overline{A}]\subseteq \overline{f[A]}=B
\end{equation*}
This implies
\begin{equation*}
\overline{A}\subseteq f^{-1}[B]=A\bdone
\end{equation*}






\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce other concepts we are familiar.
\end{minipage}}
\begin{theorem}
\label{2.4.5}
\textbf{(Composition of Continuous Function is Continuous)} Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be continuous. We have
\begin{equation*}
g\circ f\text{ is continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Let $V\subseteq Z$ be open. We know 
\begin{equation*}
g^{-1}[V]\text{ is open }
\end{equation*}
This give us
\begin{equation*}
f^{-1}[g^{-1}[V]]\text{ is open }
\end{equation*}
Deduce
\begin{align*}
  x\in (g\circ f)^{-1}[V]&\iff g(f(x))\in V\\
  &\iff f(x)\in g^{-1}[V]\\
  &\iff f^{-1}[g^{-1}[V]]
\end{align*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce connected.
\end{minipage}}
\begin{theorem}
\label{2.4.6}
\textbf{(Continuity Map Connected Set to Connected Set)} Let $D$ be the domain of continuous function $f$. Let $A\subseteq D$.  
 \begin{equation*}
A\text{ is connected }\implies f[A]\text{ is connected }
\end{equation*}
\end{theorem}
\begin{proof}
We prove the contrapositive
\begin{equation*}
f[A]\text{ is disconnected }\implies A\text{ is disconnected }
\end{equation*}
Suppose 
\begin{equation*}
f[A]=V\cup W\text{ and $V,W$ are separated }
\end{equation*}
Notice
\begin{align*}
  (f^{-1}[V]\cap A)\cup (f^{-1}[W]\cap A)&=(f^{-1}[V]\cup  f^{-1}[W])\cap A\\
  &=f^{-1}[V \cup W]\cap A\\
  &=A
\end{align*}
WOLG, we only wish to prove
\begin{equation*}
\overline{f^{-1}[V]\cap A}\cap (f^{-1}[W]\cap A)=\varnothing
\end{equation*}
Notice
\begin{align*}
  \overline{f^{-1}[V]\cap A}\subseteq \overline{f^{-1}[V]}\cap \overline{A}\subseteq \overline{f^{-1}[\overline{V}]}\cap \overline{A}=f^{-1}[\overline{V}]\cap \overline{A}
\end{align*}
We have reduce our problem into proving 
\begin{equation*}
f^{-1}[\overline{V}]\cap \overline{A}\cap f^{-1}[W]\cap A=\varnothing
\end{equation*}
\As{$x\in f^{-1}[V]\cap  A \cap f^{-1}[W]$}. We see $f(x)\in \overline{V}$ and $f(x)\in W$, which is impossible, since $V,W$ are separated.  \CaC
\end{proof}
\fbox{\begin{minipage}{39em}
Lastly, about compact.
\end{minipage}}
\begin{theorem}
\label{2.4.7}
\textbf{(Continuous Function Maps Compact Set to Compact Set)} Let $D$ be the domain of $f$ and $f$ be continuous. Let $A\subseteq D$.
\begin{equation*}
A\text{ is compact }\implies f[A]\text{ is compact }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{G_i}$ be an open cover of $f[A]$. We first wish to show
\begin{equation*}
  \vi{\set{f^{-1}[G_i]}\text{ is an open cover for $A$ }}
\end{equation*}
It is clear that $f^{-1}[G_i]$ are all open.\\

Let $x\in A$. We know $f(x)\in f[A]$. Then we know there exists some $G_j$ such that 
 \begin{equation*}
f(x)\in G_j
\end{equation*}
Then we see $x\in f^{-1}[G_j]\vdone$\\

Because $A$ is compact, we know there exists a finite sub-cover  $\set{f^{-1}[G_\ld] :\ld  \in \Lambda }$.\\

We wish to prove
\begin{equation*}
\blue{\set{G_\ld :\ld \in \Lambda }\text{ is an open cover for $f[A]$ }}
\end{equation*}
Let $y\in f[A]$. We know there exists $x\in A$ such that 
\begin{equation*}
y=f(x)
\end{equation*}
Then we know there exists  $\ld _j$ such that
\begin{equation*}
x\in f^{-1}[G_{\ld _j}]
\end{equation*}
Then we see 
\begin{equation*}
y=f(x)\in G_{\ld _j}\bdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{2.4.8}
\textbf{(Closed Subset of Compact Set is Compact)} Let $X$ be compact, and  $E\subseteq X$
\begin{equation*}
E\text{ is closed }\implies E\text{ is compact }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{G_{\ld }:\Lambda }$ be an open cover of $E$. Consider 
\begin{equation*}
U:=\set{G_{\ld }}\cup \set{E^c}
\end{equation*}
Because $E$ is closed, clearly $U$ is an open cover of $X$. Then $U$ has a finite sub-cover $U'$ that cover $X$. If $U'$ contain no  $E^c$, we see that $U'$ is also a finite sub-cover of  $\set{G_\ld}$ that cover $E$. If $U'$ contain  $E^c$,  we see that 
\begin{equation*}
U'\setminus \set{E^c}\text{ is a finite sub-cover of $\set{G_\ld }$ that cover $E$ }
\end{equation*}
Since 
\begin{equation*}
e\in E\subseteq X\implies e\in \bigcup U'\text{ and }e\not \in E^c\implies \exists S\in U'\setminus \set{E^c}, e\in S
\end{equation*}

\end{proof}
\section{Homeomorphism}
\begin{theorem}
\label{2.5.1}
\textbf{(Inverse Function of One-to-One Continuous Function is Again Continuous)} Let $f:X\rightarrow Z$ be continuous and one-to-one. Let $Y$ be the range of  $f$. Consider the sub-space topology  $(Y,\tau)$. The inverse function 
\begin{equation*}
f^{-1}:(Y,\tau)\rightarrow X\text{ is also continuous }
\end{equation*}
\end{theorem}
\begin{proof}
  \red{Wrong Conjecture}
\end{proof}

\section{Three Non-Equivalent Compactness and Separable}
\fbox{\begin{minipage}{39em}
In this section, we will first give two equivalent definition of base and prove they are equivalent.
\end{minipage}}
\begin{definition}
\label{2.6.1}
\textbf{(Definition of Base)} We say a sub-collection $\mathcal{O}$ of topology $\tau$ on $X$ is a base if every open neighborhood $O$ around $x$ contain an open set $U$ in $\mathcal{O}$ containing $U$. That is
\begin{equation*}
\forall x\in X, \forall O\in\tau:x\in O,\exists U\in \mathcal{O}, x\in U\subseteq O
\end{equation*}
We call open sets in a base basic open sets.
\end{definition}
\begin{theorem}
\label{2.6.2}
\textbf{(Equivalent Definition of Base)} A sub-collection of topology $\tau$ is a base if and only if every open set can be expressed as a union of a set of basic open sets. That is, the necessary and sufficient condition for $\mathcal{O}$ to be a base is 
\begin{equation*}
\forall O\in \tau, \exists \mathcal{U}\subseteq \mathcal{O}, O=\bigcup \mathcal{U}
\end{equation*}
\end{theorem}
\begin{proof}
  From left to right, for each point $x\in O$, we know there exists $U_x\in \mathcal{O}$ such that $x\in U_x\subseteq O$. Collect $U_x$ for each $x\in O$ as $\mathcal{U}$, and we are partially done.\\

  From right to left, let $O$ be an open neighborhood around $x$. If there exists no $U\in \mathcal{O}$ such that $x\in U\subseteq O$, then every union of sub-collection $\mathcal{U}\subseteq\mathcal{O}$ that contain $x$ contain some points not in  $O$, which is impossible.  
\end{proof}
\fbox{\begin{minipage}{39em}
Then, we will define separable, and give a sufficient condition of separable.
\end{minipage}}
\begin{definition}
\label{2.6.3}
\textbf{(Definition of Separable)} We say $X$ is separable if 
\begin{equation*}
  X\text{ has a countable dense subset }
\end{equation*}
\end{definition}
\begin{theorem}
\label{2.6.4}
\textbf{(Base and Dense Set)} 
\begin{equation*}
\text{ A set intersecting with all basic open set is dense }
\end{equation*}
\end{theorem}
\begin{proof}
For each point $p$ not in $A$ and for each open neighborhood $N_p$ around $p$, there exists an basic open set  $O_p$ contained by $N_p$. The fact  $O_p$ intersect $A$ implies  $N_p$ intersect with  $A$. We have proved every point  $p$ not in $A $ is a limit point of $A$.
\end{proof}
\begin{corollary}
\label{2.6.5}
\textbf{(Countable Base Implies Separable)} 
\begin{equation*}
X\text{ has a countable base }\implies X\text{ is separable }
\end{equation*}
\end{corollary}
\begin{proof}
Let $\mathcal{O}$ be a countable base for $X$. For each $O\in \mathcal{O}$, we can collect a point $x_O\in O$, and have $A:=\set{x_O:O\in \mathcal{O}}$. Because $\mathcal{O}$ is countable, we know $A$ is countable, and by  \myref{Theorem}{2.6.4}, we know $A$ is dense.
\end{proof}
\fbox{\begin{minipage}{39em}
The above let us draw a Venn diagram, where existence of countable base is a small circle in the big circle separable.\\

It is clear that compact implies countably compact, as every countable cover in compact space has a finite sub-cover. We now explore the relation of the three compactness.
\end{minipage}}
\begin{theorem}
\label{2.6.6}
\textbf{(Compact Implies Countably Compact)} 
\begin{equation*}
 X\text{ is compact }\implies X\text{ is countably compact }
\end{equation*}
\end{theorem}
\begin{theorem}
\label{2.6.7}
\textbf{(Countably Compact Implies Limit Point Compact)} 
\begin{equation*}
X\text{ is countably compact }\implies X\text{ is limit point compact }
\end{equation*}
\end{theorem}
\begin{proof}
Arbitrarily pick an infinite $A$. We know  $A$ is either countable or uncountable. We first prove  \vi{if $A$ is countably infinite, then $A$ has a limit point}.\\

\As{$A$ has no limit point}. For each $a\in A$, because $a$ is not a limit point of $A$, we know there exists an open neighborhood  $O_a$ around $a$ such that  $O_a\cap A=\set{a}$. Notice that $A$ has no limit point let us deduce
\begin{equation*}
A'=\varnothing \implies \overline{A}=A\implies A^c\text{ is open }
\end{equation*}
For each $a\in A$, collect one $O_a$ into the collection $\mathcal{O}=\set{O_a:a\inA}$.\\

Then, because $A$ is countable, we see $\mathcal{O}\cup \set{A^c}$ is an countable cover for $X$, and every finite open sub-cover contain only finite amount of point of $A$ when  $A$ is infinite. \CaC $\vdone$\\

If $A$ is uncountable, then  $A$ contain a countable set  $B$. We know  $B$ has a limit point  $p$, and we know  $p$ is also a limit point for $A$.
\end{proof}
\begin{corollary}
\label{2.6.8}
\textbf{(Compact Implies Limit Point Compact)} If $X$ is compact, then $X$ is limit point compact.
\end{corollary}
\fbox{\begin{minipage}{39em}
Lastly, we give a theorem involving countable base.
\end{minipage}}
\begin{theorem}
\label{2.6.9}
\textbf{(Existence of Countable Base Implies Existence of Countable Sub-Cover)} 
\begin{equation*}
X\text{ has a countable base }\implies\text{every open cover for $X$ has a countable sub-cover}
\end{equation*}
\end{theorem}
\begin{proof}
Let $\mathcal{O}$ be a countable base for $X$ and let $\mathcal{G}$ be an open cover of $X$. We wish to find an countable open sub-cover of $\mathcal{G}$.\\

Because $\mathcal{O}$ is a base, for each $G\in \mathcal{G}$, there exists basic open set $O\in \mathcal{O}$ contained by $G$. Then because  $\mathcal{G}$ is an open cover, for each $p\in X$, there exists $G_p,O_p$ such that
\begin{equation*}
p\in O_p\subseteq G_p
\end{equation*}
Collect all $G_p$ from above so we have a sub-cover $\mathcal{G}'$. Because $\mathcal{O}$ is countable, we know $\mathcal{G}'$ is also countable.
\end{proof}
\fbox{\begin{minipage}{39em}
Now, if we want to draw a conclusion for this section by drawing a Venn diagram, we should first draw compact inside countably compact inside limit point compact. 
\end{minipage}}
\chapter{Important Notions in Metric Space}
\section{Basic Fact of Sequence}
\begin{definition}
\label{3.1.1}
\textbf{(Definition of Metric)} We say a real-valued function $d:X^2\rightarrow \R$ is a metric if
\begin{equation*}
d(x,x)=0 
\end{equation*}
\begin{equation*}
x\neq y\longrightarrow d(x,y)>0\text{ (Positive Definitness) }
\end{equation*}
\begin{equation*}
d(x,y)=d(y,x)\text{ (Commutative) }
\end{equation*}
\begin{equation*}
d(x,z)\leq d(x,y)+d(y,z)\text{ (Triangle Inequality) }
\end{equation*}
\end{definition}
\begin{definition}
\label{3.1.2}
\textbf{(Definition of Sequence)} In this chapter, by a sequence in $X$, we mean a function from  $\N$ to  $X$
\end{definition}
\fbox{\begin{minipage}{39em}
Above is specification.
\end{minipage}}
\begin{definition}
\label{3.1.3}
\textbf{(Definition of Convergence of Reals)} Let $\set{a_n}$ be a sequence of reals. By $\set{a_n}$ converges to $a$ 
\begin{equation*}
\lim_{n\to\infty} a_n=a
\end{equation*}
We mean for each $\epsilon \inr^+$, there exists great enough $N_\epsilon \inn$  such that for each $n\inn$ 
\begin{equation*}
n>N_\epsilon \implies \abso{a_n-a}<\epsilon 
\end{equation*}
\end{definition}
\begin{definition}
\label{3.1.4}
\textbf{(Definition of Convergence)} Let $\set{a_n}$ be a sequence in metric space $X$. By $\set{a_n}$ converge to $a$
\begin{equation*}
\lim_{n\to\infty}a_n=a
\end{equation*}
We mean for each $\epsilon \inr^+$, there exists great enough $N\epsilon \inn$ such that for each $n\inn$
 \begin{equation*}
n>N_\epsilon \implies d(a_n,a)<\epsilon 
\end{equation*}
\end{definition}
\fbox{\begin{minipage}{39em}
Above is definition.
\end{minipage}}
\begin{theorem}
\label{3.1.5}
\textbf{(Other Definition for Convergence)} Let $\set{a_n}$ be a sequence in metric space $X$. We have
 \begin{gather*}
\lim_{n\to \infty}a_n=a\iff \lim_{n\to\infty}d(a_n,a)=0\\
\liff (\forall \epsilon ,\exists N_\epsilon, n\geq N_\epsilon \iff  d(a_n,a)<\epsilon) \\
\liff (\forall \epsilon, \exists N_\epsilon , a_n\in B_\epsilon (a)\iff  n\geq N_\epsilon )
\end{gather*}
\end{theorem}
\begin{theorem}
\label{3.1.6}
\textbf{(Limit of Sequence is Unique)} Let $\set{a_n}$ be a sequence in metric space $X$. We have
 \begin{equation*}
\lim_{n\to\infty}a_n=a\text{ and }\lim_{n\to\infty}a_n=a'\implies a'=a
\end{equation*}
\end{theorem}
\begin{proof}
Assume not unique and use $\epsilon=\frac{d(a',a)}{2}$ to cause a contradiction.
\end{proof}
\fbox{\begin{minipage}{39em}
Below discuss sub-sequence.
\end{minipage}}
\begin{definition}
\label{3.1.7}
\textbf{(Definition of Sub-Sequence)} We use
\begin{equation*}
\set{a_{n_k}}
\end{equation*}
To denote sub-sequence of $\set{a_n}$, so $n_k\inn$ must satisfy
 \begin{equation*}
n_k> \max \set{n_1,\dots, n_{k-1}}\text{ and }n_k\geq k 
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.1.8}
\textbf{(Sequence Converge if and only if All Sub-sequence Converge)} 
\begin{equation*}
\text{ Sequence $\set{a_n}$ converge to $a$ $\iff $ all sub-sequence $\set{a_{n_k}}$ converge to $a$ }
\end{equation*}
\end{theorem}
\begin{proof}
From right to left is trivial. From left to right, notice $k_n\geq n$. 
\end{proof}
\begin{theorem}
\label{3.1.9}
\textbf{(Existence of a Convenient sub-Sequence)} 
\begin{gather*}
\set{a_n}\text{ converge to }a\\
\implies  \text{ there exists a sub-sequence }\set{a_{n_k}}  \text{ such that }\forall k\inn, d(a_{n_k},a)<\frac{1}{k}
\end{gather*}
\end{theorem}
\begin{proof}
The construction is done by recursion of selecting a point in $B_{\frac{1}{k+1}}(a)\cap \set{a_n}$ after $a_{n_k}$. Such point must exists, otherwise all point after $a_{n_k}$ is of distance to $a$ greater than  $\frac{1}{k+1}$.
\end{proof}
\fbox{\begin{minipage}{39em}
Below introduce Cauchy.
\end{minipage}}
\begin{definition}
\label{3.1.10}
\textbf{(Definition of Cauchy Sequence)} We say 
 \begin{equation*}
\set{a_n}\text{ is a Cauchy sequence }
\end{equation*}
If for each $\epsilon \inr^+$, there exists great enough $N_\epsilon \inn$ such that for each $n,k\inn$
\begin{equation*}
n,k>N_\epsilon \implies d(a_n,a_k)<\epsilon 
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.1.11}
\textbf{(Every Convergent Sequence is Cauchy)}
\begin{equation*}
\set{a_n}\text{ converge }\implies \set{a_n}\text{ is Cauchy }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{a_n}$ converge to $a$. We know for each $\epsilon $ there exists $N_{\frac{\epsilon }{2} }$ such that $n>N_{\frac{\epsilon }{2}}\implies d(a_n,a)<\frac{\epsilon}{2}$. Observe $m,k>N_\frac{\epsilon }{2}\implies d(a_m,a_k)\leq d(a_m,a)+d(a,a_k)<\epsilon $
\end{proof}
\fbox{\begin{minipage}{39em}
Lastly, we close this section with example of uncompleted metric space.
\end{minipage}}
\begin{theorem}
\label{3.1.12}
\textbf{(Not Every Cauchy Sequence Converge)} 
\begin{equation*}
\set{a_n}\text{ is Cauchy NOT}\implies \set{a_n}\text{ converge }
\end{equation*}
\end{theorem}
\begin{proof}
Let $X=\R \setminus \set{p}$ be standard. Observe $\set{a_n=p+\frac{1}{n}}$ is in $X$ and for each $\epsilon \inr^+$, we have
\begin{equation*}
m\geq k>\frac{1}{\epsilon } \implies d(a_m,a_k)= a_k-a_m<\frac{1}{k}-\frac{1}{m}<\frac{1}{k}<\epsilon 
\end{equation*}
Let $q\in X$. We know $q\neq p$. Let $\epsilon =\frac{\abso{p-q}}{2}$. Let $k>\frac{1}{\epsilon }$. Observe
\begin{equation*}
\frac{1}{k}<\frac{\abso{p-q}}{2}\implies \frac{q-p}{2}<\frac{1}{k}<\frac{p-q}{2}
\end{equation*}
And observe
\begin{align*}
  d(a_k,q)=\abso{p+\frac{1}{k}-q}&>p-q+\frac{1}{k}>\frac{p-q}{2}\\
  &>q-p-\frac{1}{k}>\frac{q-p}{2}
\end{align*}
So we have
\begin{equation*}
d(a_k,q)>\frac{\abso{p-q}}{2}=\epsilon 
\end{equation*}
We have found an open ball around $q$ that contain only finite amount of  $\set{a_n}$, showing $\set{a_n}$ does not converge to $q$ when $q$ arbitrarily picked from  $X$.

\end{proof}
\section{Closed}
\begin{definition}
\label{3.2.1}
\textbf{(Definition of Closure and Closed in Metric Space)} Let $E$ be a subset of  $(X,d)$. By closure $\overline{E}$, we mean the set containing precisely all possible points to which the sequence in $E$ may converge. We say $E$ is closed if every convergent sequence in $E$ converge to some point in $E$, in other words:
 \begin{equation*}
\overline{E}=E
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.2.2}
\textbf{(Another Definition of Closure)}
\begin{equation*}
\overline{E}\text{ is the smallest closed set containing }E
\end{equation*}
\end{theorem}
\begin{proof}
To see $E\subseteq \overline{E}$ is simple, for each $x\in E$, the trivial sequence $\set{x_n=x}$ is a sequence in $E$ that converge to $x$.\\

We first prove \vi{$\overline{E}$ is closed}. Let $\set{x_n}$ be a convergent sequence in $\overline{E}$, and let
\begin{equation*}
\lim_{n\to\infty} x_n=x
\end{equation*}
We wish to prove $x\in \overline{E}$. If $\set{x_n}$ contain infinite terms in $E$, then we know $\set{x_n}$ has a sub-sequence $\set{x_{n_k}}$ in $E$ which converge to $x$ by  \myref{Theorem}{3.1.8}. By definition of closure, we know $x\in \overline{E}$ and we are done.\\

Now, we only have to consider when $\set{x_n}$ contain finite terms in $E$, which tell us $\set{x_n}$ contain infinite terms (a sub-sequence that converge to $x$) $\set{x_{n_k}}$ in $\overline{E}\setminus E$.\\

We wish to find a sequence in $E$ that converge to  $x$, then we know  $x$ is in $\overline{E}$. By definition of closure, we know each term $x_{n_k}$ of the sub-sequence $\set{x_{n_k}}$ is a limit of a sequence $\set{x^k_m}$ in $E$. Then for each $x_{n_k}$, we can pick a point $x^k_{m_k}$ in $E$ being $d(x,x_{n_k})$ closed to $x_{n_k}$ 
\begin{equation*}
x_{m_k}^k\in E\text{ and }d(x^k_{m_k},x_{n_k})<d(x,x_{n_k})
\end{equation*}
This also give us
\begin{equation*}
d(x_{m_k}^k,x)\leq d(x_{m_k}^k,x_{n_k})+d(x_{n_k},x)<2d(x,x_{n_k})
\end{equation*}
Now, we wish to prove 
\begin{equation*}
\lim_{k\to\infty}x^k_{m_k}=x
\end{equation*}
Because
\begin{equation*}
\lim_{k\to\infty}x_{n_k}=x
\end{equation*}
For each small $\epsilon $, we know there exists $N$ such that
\begin{equation*}
k>N_\epsilon \implies d(x,x_{n_k})<\frac{\epsilon}{2} 
\end{equation*}
Then we have
\begin{equation*}
k>N_\epsilon \implies d(x^k_{m_k},x)<2d(x,x_{n_k})<\epsilon \vdone
\end{equation*}
We now prove \blue{$\overline{E}$ is the smallest closed set containing $E$}.\\ 

Let $K$ be a closed set containing  $E$. We wish to show all $x\in \overline{E}$ is also in $K$. By definition, we know  $x$ is a limit of a sequence  $\set{x_n}$ in $E\subseteq K$, then because $K$ is closed, we know $x$ is also in $K\bdone$  
\end{proof}
\begin{theorem}
\label{3.2.3}
\textbf{(Manipulation of Closure)} We have
\begin{gather*}
\overline{X}=X\text{ and }\overline{\varnothing}=\varnothing\\
\bigcup_{n=1}^\infty \overline{A_n}\subseteq \overline{\bigcup_{n=1}^\infty A_n}\text{ and }\overline{\bigcap_{n=1}^\infty A_n}\subseteq \bigcap_{n=1}^\infty \overline{A_n} 
\end{gather*}
where the $\subseteq$ may be proper. Notice that $\infty$ does not need to be countable. 
\end{theorem}
\begin{proof}
$\overline{X}=X\text{ and }\overline{\varnothing}=\varnothing$ are clear. We now prove  \vi{$\bigcup_{n=1}^\infty \overline{A_n}\subseteq \overline{\bigcup_{n=1}^\infty A_n}$}\\

Let $x\in \bigcup_{n=1}^\infty \overline{A_n}$. We know $x$ is a limit of some sequence $\set{x_k}$ in some $A_i$. Notice that the sequence $\set{x_k}$ is also in $\bigcup_{n=1}^\infty A_n$, so we also have $x\in \overline{\bigcup_{n=1}^\infty A_n}$. $\vdone$\\

For a case where $\subseteq$ is proper, well order $\Q$ into $\set{a_n}_{n\inn}$. We know $\overline{\set{a_n}}=\set{a_n}$, so we have
\begin{equation*}
\bigcup _{n=1}^\infty \overline{A_n}=\Q 
\end{equation*}
We also have
\begin{equation*}
\overline{\bigcup_{n=1}^\infty A_n}=\overline{\Q}=\R
\end{equation*}
For a rigorous proof of $\overline{\Q}=\R$, notice for each $x\inr$, the supremum of the set $\set{q\inq: q<x}$ is $x$.\\

We now prove \blue{$\overline{\bigcap_{n=1}^\infty A_n}\subseteq \bigcap _{n=1}^\infty \overline{A_n}$}\\

Let $x\in \overline{\bigcap _{n=1}^\infty A_n}$. We know there is a sequence $\set{x_n}$ in $\bigcap _{n=1}^\infty A_n$ that converge to $x$. Notice that $\set{x_n}$ is a sequence in $A_i$ for all  $i$, so by definition of closure, the limit  $x$ is in  $\overline{A_i}$ for all $i$. $\bdone$\\

For a case where $\subseteq$ is proper, see the below proof for \myref{Theorem}{3.2.4}.
\end{proof}
\fbox{\begin{minipage}{39em}
If the above are said to be a "general" statements of how closure act on infinite union and intersection. The below shows a stronger property of closure, where one and only one of the inclusion become equality.
\end{minipage}}
\begin{theorem}
\label{3.2.4}
\textbf{(Manipulation of Closure)} We have
\begin{equation*}
\overline{E}\cup \overline{F}=\overline{E\cup F}\text{ and }\overline{E\cap F}\subseteq \overline{E}\cap \overline{F}
\end{equation*}
\end{theorem}
\begin{proof}
We first prove \vi{$\overline{E}\cup \overline{F}=\overline{E\cup F}$}.\\

$\overline{E}\cup \overline{F}\subseteq \overline{E\cup F}$ is clear, since a convergent sequence in $E$ is a convergent sequence in $E\cup F$. To see $\overline{E\cup F}\subseteq \overline{E}\cup \overline{F}$, let $x$ be a limit of a sequence  $\set{x_n}$ in $E\cup F$. We know $\set{x_n}$ has infinite terms (a sub-sequence converging to $x$) $\set{x_{n_k}}$ either in $E$, in $F$ or in both. Then by definition of closure, we know $x$ is either in $\overline{E}$, $\overline{F}$ or both. $\vdone$\\


We now prove \blue{$\overline{E\cap F}\subseteq \overline{E}\cap \overline{F}$}.\\

Notice that if a sequence $\set{x_n}$ is in $E\cap F$, then the sequence $\set{x_n}$ is both in $E$ and in $F$. Then we know the limit is both in  $\overline{E}$ and $\overline{F}$. $\bdone$\\

For a case where $\subseteq $ is proper, observe
\begin{equation*}
E=(0,1)\text{ and }F=(1,2)
\end{equation*}
We have
\begin{equation*}
\overline{E\cap F}=\overline{\varnothing}=\varnothing \subset \set{1}=[0,1]\cap [1,2]=\overline{E}\cap \overline{F}
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.2.5}
\textbf{(Manipulation of Closed Sets)} Let $\mathfrak{F}$ be the family of closed sets. We have
\begin{gather*}
\varnothing\text{ and }X\text{ are closed. }\\
\text{ Intersection of closed sets are closed. }\\
\text{ Union of finite closed sets are closed, but union of infinite closed set may not be closed. }
\end{gather*}
\end{theorem}
\begin{proof}
In \myref{Theorem}{3.2.3}, we have proved $\varnothing$ and $X$ are closed. Let $\set{A_\ld}$ be an infinite collection of closed sets. Because $A_\ld $ are closed, by \myref{Theorem}{3.2.3}, we have
\begin{equation*}
\overline{\bigcap_{\ld  \in \Lambda }A_\ld }\subseteq \bigcap _{\ld \in\Lambda }\overline{A_\ld }=\bigcap _{\ld \in\Lambda }A_\ld 
\end{equation*}
And notice the other side is trivial.\\

Finite union of closed sets is closed is an immediate consequence of \myref{Theorem}{3.2.4} by induction. To see $\bigcup_{\ld \in \Lambda }A_\ld $ need not always be closed. Notice that if $\bigcup _{\ld \in\Lambda }A_\ld$ are closed, then we have
 \begin{equation*}
   \overline{\bigcup_{\ld \in \Lambda }A_\ld}=\bigcup_{\ld \in\Lambda }A_\ld=\bigcup_{\ld \in\Lambda }\overline{A_\ld }
\end{equation*}
but \myref{Theorem}{3.2.3} and \myref{Theorem}{3.2.4} tell us the equality between $\overline{\bigcup_{\ld \in\Lambda }A_\ld }=\bigcup_{\ld \in \Lambda }\overline{A_\ld }$ need not hold true. For a constructive counter-example, let $A_n=[\frac{1}{n},1]$, and observe
\begin{equation*}
  \bigcup_{n=1}^\infty A_n=(0,1]\not\in \mathfrak{F}
\end{equation*}
Notice if we let $A_n=(\frac{1}{n},1]$, we have a counter-example for $\bigcup_{\ld \in \Lambda }\overline{A_\ld }=\overline{\bigcup _{\ld \in\Lambda }A_\ld }$
\begin{equation*}
  \bigcup_{n=1}^\infty A_n=\bigcup_{n=1}^\infty [\frac{1}{n},1]=(0,1]\subset [0,1]=\overline{(0,1]}=\overline{\bigcup_{n=1}^\infty(\frac{1}{n},1]}=\overline{\bigcup _{n=1}^\infty A_n}
\end{equation*}
\fbox{\begin{minipage}{39em}
We now introduce an important idea hidden behind our above proofs for each theorems. 
\end{minipage}}
\begin{definition}
\label{3.2.6}
\textbf{(Definition of Limit Point)} We say $x$ is a limit point of  $E$ if there is a sequence $\set{x_n}$ in $E$ such that
\begin{equation*}
\lim_{n\to\infty} x_n=x\text{ and }\forall n, x_n\neq x
\end{equation*}
We denote the set of limit points of $E$ by  $E'$. We say a point in $E$ that isn't a limit point is an isolated point. Also, we say a set is perfect if $E'=E$. 
\end{definition}
\begin{theorem}
\label{3.2.7}
\textbf{(Property of Limit Points)} We have
\begin{equation*}
\overline{E}=E'\cup E
\end{equation*}
\end{theorem}
\begin{proof}
The fact $E\cup E'\subseteq \overline{E}$ is clear, as we notice for each $x$ in $E$, the sequence $\set{x_n=x}$ is a sequence in $E$ that converges to $x$. Let $x\in \overline{E}\setminus E$. We wish to show $x\in E'$. Let $\set{x_n}$ be a sequence in $E$ that converge to  $x$. Because $\set{x_n}$ is a sequence in $E$ and $x$ is not in $E$, we know  $\set{x_n}$ must not contain $x$, and we are done.
\end{proof}
\begin{corollary}
\label{3.2.8}
\textbf{(Property of Limit Points)} We have
\begin{equation*}
E\text{ is closed }\iff  E'\subseteq E
\end{equation*}
\end{corollary}
\begin{proof}
If $E$ is closed, then  $E'\cup E=\overline{E}=E$ tell us $E'\subseteq E$. If $E'\subseteq E$, then $\overline{E}=E'\cup E=E$
\end{proof}
\begin{theorem}
\label{3.2.9}
\textbf{(Property of Limit Points)} 
\begin{equation*}
p\in E'\iff \forall \epsilon, B_\epsilon (p)\cap E\text{ is infinite }
\end{equation*}
\end{theorem}
\begin{proof}
  From left to right, let $\set{x_n}$ be a sequence in $E$ that converge to $p$ and does not contain  $p$. For all $\epsilon$, we know there exists $N_\epsilon $ such that $n>N_\epsilon \implies x_n\in B_\epsilon (p)$. \As{the terms after $N_\epsilon $ is finitely distinct}. We can let $\epsilon=\min \set{x_k:k>N_\epsilon }$ and $\tCaC$\\

From right to left, we wish to construct a sequence $\set{x_n}$ in $E$ that converge to $p$. For all $n$, by premise we know there exists a point in  $B_{\frac{1}{n}}(p)\cap E$ that isn't $p$  (because $B_{\epsilon }(p)\cap E$ is infinite). Select that point as $x_n$. Then the sequence $\set{x_n}$ does converge to $p$ and does not contain $p$.
\end{proof}
\begin{theorem}
\label{3.2.10}
\textbf{(Property of Limit Points)} We have
\begin{gather*}
(E\cap F)'\subseteq E'\cap F'\\
E'\cup F'=(E\cup F)'
\end{gather*}
Where the $\subseteq$ may be proper. We also have
\begin{equation*}
E\subseteq F\implies E'\subseteq F'
\end{equation*}
Where the converse may not hold true. We also have
\begin{equation*}
E'\text{ is closed. }
\end{equation*}
\end{theorem}
\begin{proof}
For the first two statements, use the fact that a sequence in $E\cap F$ is in $E$ and a sequence in  $E$ is in $E\cup F$, and that a sequence in $E\cup F$ must has a sub-sequence in $E$ or $F$.\\

For an example of $\subseteq$ being proper, let $E=(0,1)$ and $F=(1,2)$. We have 
\begin{equation*}
  (E\cap F)'=\varnothing \subset \set{1}=E'\cap F'
\end{equation*}
For the third statements, use the fact a sequence in $E$ is in $F$. To see the converse may not hold true, let $E=(0,1)\cup \set{2}$ and $F=(0,1)$\\

We now prove \vi{$E'$ is closed}. Let $\set{x_n}_{n\inn}$ be a convergent sequence in $E'$. Let $x=\lim_{n\to\infty} x_n$. We wish to prove $x\in E'$. Notice that we can construct a sub-sequence $\set{x_{n_m}}_{m \inn}$ such that for all $m$, we have 
 \begin{equation*}
d(x_{n_m},x)<\frac{1}{m}
\end{equation*}
Remember $\set{x_{n_m}}$ converge to $x$, and $\set{x_{n_m}}\subseteq \set{x_n}$ is in $E'$. From now, we use $\set{x_u}$ to denote $\set{x_{n_m}}$ where $u=m$.\\ 

Because $\set{x_u}$ is in $E'$, so if  $x\in \set{x_u}$, our proof is trivially done. We only have to consider when $x\not\in \set{x_u}$.\\

Because $\set{x_u}_{u\inn}$ is in $E'$, we know for each  $u\inn$, there exists a sequence  $\set{x^u_k}_{k\inn}$ in $E$ that converges to $x_u$. Using the infinite amount of sequence $\set{\set{x^u_k}_{k\inn}:u\inn}$ in $E$, we wish to construct a sequence in $E$ that converge to  $x$ and doesn't contain  $x$.\\

Fix $u$. Because 
\begin{equation*}
\lim_{k\to\infty} x_k^u=x_u
\end{equation*}
We know there exists $N_{\frac{1}{u}}$ such that
\begin{equation*}
k>N_{\frac{1}{u}}\implies d(x_k^u,x_u)<\frac{1}{u}
\end{equation*}
Notice that because the limit is unique and we are under the consideration $x\not\in \set{x_n}$, it is impossible to happen that 
\begin{equation*}
\forall k>N_{\frac{1}{u}}, x_k^u=x
\end{equation*}
Otherwise we can see $\set{x^n_k}$ converge to $x$, not  $x_n$.\\

Then, we know there exists some  $k_u$ such that  $d(x_{k_u}^u,x_u)<\frac{1}{u}$ and $x_{k_u}^u<\frac{1}{u}$. Collect all such $x^u_{k_u}$ for all $u$, and we have a sequence in  $E$ that doesn't contain $x$. We now prove $\set{x^u_{k_u}}_{u\inn}$ converge to $x$.\\

Fix $\epsilon $. Recall how we construct $\set{x_u}$. We have
\begin{equation*}
\forall u\inn, d(x_u,x)<\frac{1}{u}
\end{equation*}
Observe  
\begin{equation*}
u>\frac{2}{\epsilon }\implies d(x^u_{k_u},x)\leq d(x^u_{k_u},x_u)+d(x_u,x)<\frac{1}{u}+\frac{1}{u}<\frac{2}{u}<\epsilon \vdone
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
Lastly, we verify the idea of subspace topology. (need to formulate for limit point and closure)
\end{minipage}}
\begin{theorem}
\label{3.2.11}
\textbf{(Subspace Topology)} Let $Y$ be a subspace of  $(X,d)$, let $E\subseteq X$, and let $p\in Y$. We have
\begin{gather*}
p\text{ is a limit point of $E\cap Y$ in }Y\implies p\text{ is a limit point of $E$ in  $X$ }\\
\text{ The closure of $E\cap Y$  in $Y$ is a subset of the closure of $E$ in $X$ }\\
E\text{ is closed in $X$ }\implies  E\cap Y\text{ is closed in $Y$ }
\end{gather*}
where the converse of the first and the third statement may not hold true, and the subset relation in the second statement may be proper. 
\end{theorem}
\begin{proof}
For the first statement, use the fact a sequence is in $E\cap Y$ is in $E$.\\

For a non-trivial example of of the converse in first statement mat not hold true, let $X=\R$, let $Y=\set{1}\cup [2,3]$ and let $E=(0,3)$. We see $1$ and $0$ are both limit points of $E$ in $X$, and neither of them is a limit point of  $E\cap Y$ in $Y$, as $1\in Y$ is an isolated point and $0$ is not even in $Y$.\\

We now prove the second statement. Let $p$ be in the closure of $E\cap Y$, and let $\set{x_n}$ be a sequence in $E\cap Y$ that converge to $p$. Notice that the sequence $\set{x_n}$ is in $E$ and we are done.\\

We now prove the last statement. From left to right, let $\set{x_n}$ be a sequence in $E\cap Y$ that converge to some $p$ in $Y$. We wish to prove  $p$ is in  $E\cap Y$. Notice that $\set{x_n}$ is also in $E$, so we know  $p$ is in $E$. Then because $p$ is in $Y$, we know  $p\in  E\cap Y$.\\

For an non-trivial example of the converse in last statement may not hold true. Let $X=\R$, let $Y=(0,2)$ and $E=[1,2)$. Clearly $E$ is not closed, but  $E\cap Y=[1,2)$ is closed in $Y$ (One can even observe that $E$ in  $Y$ is even "informally" unbounded).\\
\end{proof}
\section{Open}
\begin{definition}
\label{3.3.1}
\textbf{(Definition of Interior Points and Open)} Let $E$ be a subset of $(X,d)$. By an interior point $p$ of $E$, we mean a point such every sequence  $\set{x_n}$ converge to $p$ must eventually be in  $E$:
 \begin{equation*}
\exists N, \forall n>N, x_n\in E
\end{equation*}
We call the set of interior points of $E$  interior, and denote interior by  $E^\circ $. We say $E$ is open if every sequence that converge to a point in  $E$ must eventually be in  $E$, in other words, every point of $E$ is an interior point of $E$:
 \begin{equation*}
E^\circ =E
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.3.2}
\textbf{(Another Definition of Interior)} 
\begin{equation*}
E^\circ\text{ is the greatest open set in $E$ }
\end{equation*}
\end{theorem}
\begin{proof}
We first prove \vi{$E^\circ \subseteq E$}.\\

Let $p$ be an interior point of $E$. Verify for the trivial sequence $\set{x_n=p}_{n\inn}$. $\vdone$\\

We now prove \blue{$E^\circ $ is open}.\\

Let $x$ be a point in $E^\circ $, and let $\set{x_n}$ be a sequence converge to $x$. We wish to show  $\set{x_n}$ must eventually be in $E^\circ $. \As{$\set{x_n}$ doesn't end in $E^\circ $}. That is, there exists a sub-sequence $\set{x_u}$ of $\set{x_n}$ in $E\setminus E^\circ $.\\

Because $\set{x_u}$ is a sub-sequence of $\set{x_n}$, we know $\set{x_u}$ converge to $x$. Because of such, by  \myref{Theorem}{3.1.9}, we know there exists a sub-sequence $\set{x_r}$ of $\set{x_u}$ such that
\begin{equation*}
\forall r\inn, d(x_r,x)<\frac{1}{r}
\end{equation*}
Fix $r$. Notice that $x_r\in \set{x_u}$, so we know $x_r$ is not in  $E^\circ $. Then we know there exists a sequence $\set{x^r_m}$ converge to $x_r$ that does not end in $E$.\\

Then by \myref{Theorem}{3.1.9}, we know there exists a sub-sequence $\set{x^r_k}\subseteq \set{x^r_m}$ such that
\begin{equation*}
\forall k\inn, d(x^r_k,x_r)<\frac{1}{k}
\end{equation*}
Now for each $r$, notice that  $k=r$ implies
\begin{equation*}
d(x_k^r,x)<d(x^r_k,x_r)+d(x_r,x)<\frac{1}{k}+\frac{1}{r}=\frac{2}{r}
\end{equation*}
Collect all such $x_r^r$ as a sequence  $\set{x_r^r}$. Notice that every term $x_r^r$ is in the sequence $\set{x_m^r}$ which are not in $E$, so we know the sequence is not in $E$. Observe
 \begin{equation*}
\forall \epsilon, r>\frac{2}{\epsilon }\implies d(x_r^r,x)<\frac{2}{r}<\epsilon 
\end{equation*}
We have constructed a sequence $\set{x_r^r}$ that converge to $x$ but completely not in  $E\tCaC$ to $x\in E^\circ $. $\bdone$\\

Lastly, we prove \vi{$E^\circ $ is the greatest open set contained by $E$}. \As{There exists an open set $O\subseteq E$ such that $O\not\subseteq E^\circ $}. Let $x\in O\setminus E^\circ $. Because $x\not\in E^\circ $, we know there exists a sequence $\set{x_n}$ converge to $x$ and have a sub-sequence  $\set{x_k}\subseteq \set{x_n}$ not in $E$. Then because $x\in O$, we know the sub-sequence $\set{x_k}$ must eventually end in $O\subseteq E\tCaC$. $\vdone$
\end{proof}
\begin{theorem}
\label{3.3.3}
\textbf{(Manipulation of Interior)} We have
\begin{gather*}
X=X^\circ\text{ and }\varnothing^\circ =\varnothing\\
\bigcup_{n=1}^\infty A_n^\circ \subseteq (\bigcup_{n=1}^\infty A_n)^\circ\\
(\bigcap _{n=1}^\infty A_n)^\circ \subseteq \bigcap _{n=1}^\infty A_n^\circ 
\end{gather*}
where the $\subseteq$ can be proper.
\end{theorem}
\begin{proof}
The first statement is trivial. For the second and the third, use the fact a sequence that ends in  some $A_n$ will end in  $\bigcup A_n$ and the fact a sequence that ends in $\bigcap A_n$ will end in each $A_n$.\\

For an example of the second statement may be proper, well order all numbers in $(0,1)$, and let $A_n$ contain exactly one number. Then
\begin{equation*}
\bigcup_{n=1}^\infty A_n^\circ=\bigcup_{n=1}^\infty \varnothing=\varnothing \subset (0,1)=(\bigcup_{n=1}^\infty A_n)^\circ 
\end{equation*}
For an example of the third statement may be proper, let $A_n:=(1-\frac{1}{n},1+\frac{1}{n})$. Then
\begin{equation*}
  (\bigcap _{n=1}^\infty A_n)^\circ =\set{1}^\circ =\varnothing \subset \set{1}=\bigcap_{n=1}^\infty A_n^\circ 
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.3.4}
\textbf{(Manipulation of Interior)} We have
\begin{gather*}
E^\circ \cup F^\circ\subseteq (E\cup F)^\circ\\
(E\cap F)^\circ = E^\circ \cap F^\circ 
\end{gather*}
And if $E\subseteq F$, we have
\begin{equation*}
E^\circ \subseteq F^\circ 
\end{equation*}
\end{theorem}
\begin{proof}
For the first statement, use the fact a sequence that ends in $E$ is a sequence that ends in $E\cup F$. Notice that a sequence end in $E\cup F$ need not end in neither $E$ nor  $F$.\\

To see the $\subseteq$ may be proper, let $E=[0,1]$ and $F=[1,2]$. We have
\begin{equation*}
E^\circ \cup F^\circ= (0,1)\cup (1,2)\neq (0,2)=[0,2]^\circ =(E\cup F)^\circ 
\end{equation*}
For $(E\cap F)^\circ \subseteq E^\circ \cap F^\circ $, use the fact a sequence that ends in $E\cap F$ will end in both $E$ and  $F$.\\

For $E^\circ \cap F^\circ \subseteq (E\cap F)^\circ $, use the fact a sequence that end in both $E$ and  $F$ will end in  $E\cap F$. In more rigor, let $N_E$ satisfy
 \begin{equation*}
n>N_E\implies x_n\in E
\end{equation*}
and let $N_F$ satisfy
\begin{equation*}
n>N_F\implies x_n\in F
\end{equation*}
Then we see
\begin{equation*}
n>\max \set{N_E,N_F}\implies x_n\in E\cap F
\end{equation*}
For the last statement, use the fact a sequence that ends in $E$ is a sequence that ends in  $F$
\end{proof}
\begin{theorem}
\label{3.3.5}
\textbf{(Manipulation of Open Sets)} We have
\begin{gather*}
\text{ $X$ and  $\varnothing$ are open. }\\
\text{ Union of open sets are open. }\\
\text{ Finite intersection of open sets are open}\\
\text{ Infinite intersection of open sets may not be open. }
\end{gather*}
\end{theorem}
\begin{proof}
In \myref{Theorem}{3.3.3}, we have proved $X$ and $\varnothing$ are open. Let $\set{A_\ld :\ld \in\Lambda }$ be a collection of open sets. By \myref{Theorem}{3.3.3}, we know
\begin{equation*}
\bigcup_{\ld \in \Lambda } A_\ld= \bigcup_{\ld  \in \Lambda }A_\ld ^\circ \subseteq (\bigcup_{\ld  \in \Lambda } A_\ld )^\circ 
\end{equation*}
The other side follows from \myref{Theorem}{3.3.2}.\\

Finite intersection of open sets are open follows from \myref{Theorem}{3.3.4}:
\begin{equation*}
  (E\cap F)^\circ =E^\circ \cap F^\circ =E\cap F
\end{equation*}
For an example of infinite intersection of open sets may not be open, let $A_n=(1-\frac{1}{n},1+\frac{1}{n})$. We have $\bigcap_{n=1}^\infty A_n=\set{1}$
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce some interaction between interior and closure operator.
\end{minipage}}
\begin{theorem}
\label{3.3.6}
\textbf{(Interaction of Interior and Closure)} We have
\begin{equation*}
  (E^\circ )^c=\overline{E^c}
\end{equation*}
\end{theorem}
\begin{proof}
$x\in \overline{E^c}$ means there exists a sequence in $E^c$ converge to  $x$. That sequence has no term in $E$, so clearly does not end in  $E$. This show  $x\not \in E^\circ $, which means $x\in (E^\circ )^c$.\\

$x\not \in E^\circ $ means there exists a sequence $\set{x_n}$ that converges to $x$ and does not end in  $E$. We know $\set{x_n}$ has a sub-sequence $\set{x_k}$ not in $E$ that converges to $x$. The sequence $\set{x_k}$ is in $E^c$, and converge to $x$, so we know  $x\in \overline{E^c}$
\end{proof}
\begin{theorem}
\label{3.3.7}
\textbf{(Non-relation)}  
\begin{equation*}
  \overline{E^\circ }\subseteq \overline{E}\text{ and }E^\circ \subseteq (\overline{E})^\circ 
\end{equation*}
Are true, but the $\subseteq$ can be proper. Also, $\overline{E^\circ }$ and $(\overline{E})^\circ $ are incomparable.
\end{theorem}
\begin{proof}
For $\overline{E^\circ }\subset \overline{E}$ and $E^\circ \subset (\overline{E})^\circ $, let $E=\Q$. We have
 \begin{equation*}
\overline{E^\circ }=\overline{\varnothing}=\varnothing \subset \R=\overline{\Q}=\overline{E}
\end{equation*}
And we have
\begin{equation*}
E^\circ = \varnothing \subset \R=(\R)^\circ  = (\overline{\Q})^\circ = (\overline{E})^\circ 
\end{equation*}
\end{proof}
For $\overline{E^\circ }\not \subseteq (\overline{E})^\circ $, let $E=[0,1]$. We have
\begin{equation*}
\overline{E^\circ }=[0,1]\not\subseteq (0,1)=(\overline{E})^\circ 
\end{equation*}
For $(\overline{E})^\circ \not\subseteq \overline{E^\circ }$, let $E=\Q$ . We have
\begin{equation*}
  (\overline{E})^\circ =\R \not\subseteq \varnothing= \overline{E^\circ }
\end{equation*}
\fbox{\begin{minipage}{39em}
We now shed light on subspace topology of sequential space. 
\end{minipage}}
\begin{theorem}
\label{8.3.2}
\textbf{(Subspace Topology)} Let $Y$ be a subspace of $(X,d)$, let $E\subseteq X$, and let $p\in  Y$. We have
\begin{gather*}
p\text{ is an interior point of $E$ in $X$ }\implies p\text{ is an interior point of $E\cap Y$ in $Y$ } \\
\text{ $Y\cap E^\circ $ in $X$ is a subset of the interior of $E\cap Y$ in $Y$ }\\
E\text{ is open in $X$ }\implies E\cap Y\text{ is open in $Y$ }
\end{gather*}
where the converse may not hold true.
\end{theorem}
\begin{proof}
We first prove the first statement. Let $\set{x_n}$ be a sequence in $Y$ that converge to $p$. Because  $p$ is an interior point of  $E$ in  $X$, and  $\set{x_n}$ is in $X$, as  $Y\subseteq X$, we know there exists $N$ such that 
 \begin{equation*}
n>N\implies x_n\in E
\end{equation*}
Notice $x_n\in Y$, and we are done.\\

For a nontrivial example of the converse of the first statement may not hold true, let $E=(0,2)$, let $Y=\set{1}\cup (2,3)$. We see $1$ is an interior point of  $E$ in  $\R$, but  $1$ isn't an interior point of  $\set{1}=E\cap Y$ in $Y$.

The second and the third statement follows from the first statement. 
\end{proof}
\fbox{\begin{minipage}{39em}
We now show that metric space is sequential.
\end{minipage}}
\begin{theorem}
\label{3.3.9}
  \textbf{(Open Balls are Sequentially Open)} 
  \begin{equation*}
  B_r(p)\text{ is open }
  \end{equation*}
\end{theorem}
\begin{proof}
Let $x\in B_r(p)$. We wish to show every sequence $\set{x_n}$ that converge to $x$ ends in  $B_r(p)$. Let $u=r-d(p,x)$. Notice that $B_u(x)\subseteq B_r(p)$ and that $\set{x_n}$ ends in $B_u(x)$, and we are done.
\end{proof}
\begin{theorem}
\label{3.3.10}
\textbf{(Metric Spaces are Sequential)} 
\begin{gather*}
\text{ $p$ is an interior point of $E$ }\\
\liff \text{ there exists an small enough open ball $B_r(p)$ contained by $E$}
\end{gather*}
\end{theorem}
\begin{proof}
We first prove from left to right. \As{every open ball $B_r(p)$ is too big to be contained by $E$}. That is:
\begin{equation*}
\forall \epsilon, B_\epsilon (p)\setminus E\neq \varnothing
\end{equation*}
Then for each $n$, we let  $x_n\in B_{\frac{1}{n}}(p)\setminus E$. Then $\set{x_n}$ is a sequence outside of $E$ that converge to  $p\tCaC$ to $p\in E^\circ $.\\

We now prove from right to left. Let $\set{x_n}$ be a sequence converge to $p$. We know there exists $N$ such that
\begin{equation*}
n>N\implies x_n\in B_r(p)\subseteq E
\end{equation*}
This finish our proof.
\end{proof}
\begin{corollary}
\label{3.3.11}
\textbf{(Metric Spaces are Sequential)} 
\begin{gather*}
\text{ $E$ is open  }\\
\liff \text{ for each point  $p$ in  $E$ there exists an small enough open ball $B_r(p)$ contained by $E$ }
\end{gather*}
\end{corollary}
\fbox{\begin{minipage}{39em}
Lastly, the most important.
\end{minipage}}
\begin{theorem}
\label{3.3.12}
\textbf{(Topology Axiom)}
\begin{equation*}
E\text{ is open  }\iff E^c\text{ is closed }
\end{equation*}
\end{theorem}
\begin{proof}
We first prove from left to right. Let $\set{x_n}$ be a sequence in $E^c$ that converge to $x$. We wish to show  $x\in E^c$. \As{$x\in E$}. Because $E$ is open, we know $\set{x_n}$ eventually ends in $E\tCaC$.\\

We now prove from right to left. Let $x\in E$, and let $\set{x_n}$ be a sequence converge to $x$. We wish to show  $\set{x_n}$ eventually ends in $E$. Because  $x\not \in E^c$, and $E^c$ is closed, we know  $\set{x_n}$ has no sub-sequence in  $E^c$, otherwise we can see that sub-sequence in $E^c$ converge to  $x\not\in E^c$.\\

Then we know there exists only finite terms in $E^c$. Let $N$ be the maximal index of all finite terms in  $E^c$. We see
 \begin{equation*}
n>N\implies x_n\not\in E^c\implies x_n\in E
\end{equation*}
\end{proof}
\section{Connected and Convex}
\begin{theorem}
\label{3.4.1}
\textbf{(Interval is Connected)} 
\begin{equation*}
[0,1],(0,1),[0,1),(0,1]\text{ are all connected }
\end{equation*}
\end{theorem}
\begin{proof}
Arbitrarily partition $[0,1]$ into $A\cup B$. We wish to prove
\begin{equation*}
  \vi{A,B\text{ are not separated }}
\end{equation*}
If $\sup A=1=\sup B$, notice that $1$ is, WOLG, in $A$, and we can construct sequence in $B$ that converge to  $1$. This give us $1\in A\cap \overline{B}$. WOLG, let  $\sup A\neq 1$.\\


If $\sup A\in A$. We see the tail of
 \begin{equation*}
\sup A+0.1, \sup A+0.01, \sup A+0.001,\dots
\end{equation*}
is a sequence in $B$ that converge to  $A$. This give us $\sup A\in A\cap \overline{B}$\\

If $\sup A\not\in A$. Then $\sup A\in B$, and we can construct a sequence in $A$ that converge to  $\sup A$. This give us $\sup A=\overline{A}\cap B\vdone$\\

Arbitrarily partition $(0,1)$ into $A\cup B$. We wish to prove 
\begin{equation*}
\blue{A,B\text{ are not separated }}
\end{equation*}
Let $a\in A$. There are two possibilities:
\begin{equation*}
\begin{cases}
  \set{b\in B: b<a}\text{ is empty }\\
  \set{b\in B:b<a}\text{ is non-empty }
\end{cases}
\end{equation*}
If $\set{b\in B:b<a}$ is empty, we see
\begin{equation*}
0<a\leq \inf B
\end{equation*}
Then there are two possibility: $\inf B\in B\text{ or } \inf B\not\in B$. The former give us an sequence in $A$ converge to  $\inf B$ and the latter give us a sequence in $B$ converge to  $\inf B\in A$.\\

If $\set{b\in B:b<a}$ is non-empty. Let
\begin{equation*}
b'=\sup \set{b\in B:b<a}
\end{equation*}
If $b'\in B$ we construct a sequence in $A$ that converge to  $b'$. If  $b'\in A$, we construct a sequence in $B$ that converge to  $b'\bdone$\\

For $[0,1)$ and $(0,1]$ use the same argument for \vi{violet} part with infimum when dealing with $[0,1)$.
\end{proof}
\begin{theorem}
\label{3.4.2}
\textbf{(Non-Singleton Connected Set In $\R^k$ Must be Uncountable)} In $\R^k$
\begin{equation*}
  S\text{ is connected and $\abso{S}>1$ }\implies S\text{ is uncountable }
\end{equation*}
\end{theorem}
\begin{proof}

\end{proof}
\begin{theorem}
\label{3.4.3}
\textbf{(Cantor Set is Uncountable and totally disconnected)}
\end{theorem}
\fbox{\begin{minipage}{39em}
We now discuss convex set. 
\end{minipage}}
\section{Bounded}
\begin{definition}
\label{3.5.1}
\textbf{(Definition of Bounded and Totally Bounded)} We say $E$ is bounded if there exists $(x\in X,r\inr^+)$ such that 
\begin{equation*}
E\subseteq B_r(x)
\end{equation*}
We say $E$ is totally bounded if for all $\epsilon $ there exists a finite set 
\begin{equation*}
\set{x_1,x_2,\dots ,x_m}
\end{equation*}
such that 
\begin{equation*}
E\subseteq \bigcup_{i=1}^m B_{\epsilon }(x_i)
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.5.2}
\textbf{(Totally Bounded Implies Bounded)} 
\begin{equation*}
E\text{ is totally bounded }\implies E\text{ is bounded }
\end{equation*}
\end{theorem}
\begin{proof}
Because $E$ is totally bounded, we know there exists a finite set $\set{x_1,\dots ,x_m}$ such that 
\begin{equation*}
E\subseteq \bigcup_{i=1}^m B_1(x_i)
\end{equation*}
Let
\begin{equation*}
r:=\max_{2\leq i\leq m} d(a,x_i)+1
\end{equation*}
We prove such $(x_1,r)$ works. That is  
\begin{equation*}
E\subseteq B_r(x_1)
\end{equation*}
Let $a\in E$. Because $E$ is covered by $\set{B_1(x_i):1\leq i\leq m}$, we know there exists $k\in \set{1,\dots ,m}$ such that 
\begin{equation*}
a \in B_1(x_k)
\end{equation*}
If $k=1$, the proof is trivial. If  $k>1$, observe
\begin{equation*}
  d(a,x_1)\leq d(a,x_k)+d(x_k,x_1)<1+d(x_k,x_1)\leq r
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.5.3}
\textbf{(Closure of Bounded Set is Bounded)} 
\begin{equation*}
E\text{ is bounded }\implies \overline{E}\text{ is bounded }
\end{equation*}
\end{theorem}
\begin{proof}
Suppose
\begin{equation*}
E\subseteq B_r(y)
\end{equation*}
We prove 
\begin{equation*}
  \vi{\overline{E}\subseteq B_{r+1}(y)}
\end{equation*}
Let $x\in \overline{E}\setminus E$. Because $x\in \overline{E}\setminus E\implies x\in E'$, we know there exists $z\in E$ such that $d(x,z)<1$. Then we see 
\begin{equation*}
d(x,y)\leq d(x,z)+d(y,z)< 1+r \vdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.5.4}
\textbf{(In $\R^n$, Bounded implies Totally Bounded)} Given $X\subseteq \R^n$
\begin{equation*}
X\text{ is bounded }\implies X\text{ is totally bounded }
\end{equation*}
\end{theorem}
\begin{proof}
Suppose 
\begin{equation*}
X\subseteq B_r(x)
\end{equation*}
Fix $\epsilon $. By \myref{Theorem}{3.5.3}, we know $\overline{X}$ is close and bounded. Then we know $\overline{X}$ is compact. Then we know $\overline{X}$ is totally bounded. Then we see
\begin{equation*}
X\subseteq \overline{X}\subseteq \bigcup_{i=1}^m B_{\epsilon }(x_i)
\end{equation*}
\end{proof}
\begin{corollary}
\label{3.5.5}
\textbf{(In $\R^n$, Totally Bounded is Equivalent to Bounded)}  Given $X\subseteq \R^n$ 
\begin{equation*}
X\text{ is bounded }\iff X\text{ is totally bounded }
\end{equation*}
\end{corollary}
\fbox{\begin{minipage}{39em}
Notice that bounded doesn't implies totally bounded. See Discrete Metric.
\end{minipage}}

\section{Compactness}
\fbox{\begin{minipage}{39em}
There are tools we need to prove the MEGA TFAE. 
\end{minipage}}
\begin{theorem}
\label{3.6.1}
\textbf{(In Metric Space, Existence of Countable Base is Equivalent to Separable)} Let $(X,d)$ be a metric space. 
\begin{equation*}
X\text{ has a countable base }\iff  X\text{ is separable }
\end{equation*}
\end{theorem}
\begin{proof}
Because by \myref{Corollary}{2.6.5}, $X$ is separable if  $X$ has a countable base, we only have to prove separable implies existence of countable base. Let $X$ be separable. Then we have a countable dense subset $E$ of $X$. We now prove
\vi{
 \begin{equation*}
\mathcal{O}:=\set{B_r(p):p\in E\text{ and }r\inq} 
\end{equation*}
is a countable base}.\\

Notice $\mathcal{O}=\bigcup \set{\set{B_r(p):r\inq}:p\in E}$ is a countable union of countable sets, so $\mathcal{O}$ is countable. We only have to prove $\mathcal{O}$ is a base.\\

For each open neighborhood $O$ around  $x\in X$, by definition of open in metric space, we know there exists an open ball $B_r(x)$ contained by $O$. Because $E$ is dense, we know  $x$ either in $E$ or in  $E'$. If $x$ is in  $E$, we can pick a positive rational  $n$ smaller than  $r$, so we have  
\begin{equation*}
x\in B_n(x)\subseteq B_r(x)\subseteq O\text{ and }B_n(x)\in \mathcal{O} 
\end{equation*}
and we are done. If $x\in E'$, we know there exists a point $q$ in  $E\cap B_{\frac{r}{2}}(x)$. Then we can pick a rational $n$ between $d(q,x)$ and $\frac{r}{2}$. Now, observe
\begin{equation*}
x\in B_n(q)\subseteq B_r(x)\subseteq O\text{ and }B_n(q)\in \mathcal{O}
\end{equation*}
and we are done. $\vdone$
\end{proof}
\begin{lemma}
\label{3.6.2}
\textbf{(Existence of Intuitively Small Enough Open Ball)} Let $p\in  B_r(q)$. There exists an open ball $B_{r'}(p)$ small enough to be contained by $B_r(q)$ and not containing $q$ 
\end{lemma}
\begin{proof}
If $d(p,q)\leq \frac{r}{2}$, we can let $r'=d(p,q)$. Clearly $B_{r'}(p)$ doesn't contain $q$, and for each  $x\in B_{r'}(p)$ we have
\begin{equation*}
d(x,q)\leq d(x,p)+d(p,q)\leq r'+\frac{r}{2}\leq r
\end{equation*}
If $d(p,q)\geq \frac{r}{2}$, we can let $r'=r-d(p,q)$. Clearly $B_{r'}(p)\subseteq B_r(q)$, and we can deduce
\begin{equation*}
2d(p,q)\geq r\implies d(p,q)\leq r-d(p,q)=r'
\end{equation*}
so we have $q\in B_{r'}(p)$. In short, we use the open ball  $B_{\min \set{d(p,q),r-d(p,q)}}(p)$
\end{proof}
\begin{theorem}
\label{3.6.3}
\textbf{(Limit Point Compact Implies Totally Bounded)} Let $X$ be a metric space, and let $K\subseteq X$.
\begin{equation*}
K\text{ is limit point compact }\implies K\text{ is totally bounded }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. Let $a_1\in K$. We only have to construct a finite set of open balls of radius $\epsilon $ and centering some points in $K$
\begin{equation*}
\set{B_{\epsilon}(a_1),B_{\epsilon }(a_2),\dots ,B_{\epsilon }(a_n)}\text{ where }\set{a_1,\dots ,a_n}\subseteq K
\end{equation*}
That cover whole $K$.\\

We construct such set by adding  $B_{\epsilon }(a_j)$ such that 
\begin{equation*}
\forall i<j,a_i \not\in B_\epsilon (a_j)
\end{equation*}
\As{this procedure never ends}. In other words, we assume our method of adding open balls can go on infinitely. Then we have sets 
\begin{equation*}
A=\set{a_i}_{\inn}\subseteq K\text{ and }B_\epsilon (A):=\set{B_\epsilon (a_i)}_{\inn}
\end{equation*}
Such that
\begin{equation*}
\forall j\inn,\forall i<j\inn, a_i\not\in B_{\epsilon }(a_j) 
\end{equation*}
Now, we wish to cause a contradiction to the premise that $K$ is limit point compact by showing no point in $K$ is limit point for $A$.\\

It is clear that no $\set{a_i}$ is a limit point of $A$. Let $x\in K\setminus A$. We wish to find an open ball $B_r(x)$ small enough so that $B_r(x)$ contain no $a_i$.\\

If $x$ is not contained by no  $B_\epsilon (a_i)$, then we see $\epsilon $ is small enough to be our $r$.\\

If $x$ is contained by some  $B_\epsilon (a_i)$, use \myref{Lemma}{3.9.2}, and we are done. \CaC
\end{proof}
\begin{theorem}
\label{3.6.4}
\textbf{(Limit Point Compact Implies Separable)} Let $K$ be a metric space.
\begin{equation*}
K\text{ is limit point compact }\implies K\text{ is separable }
\end{equation*}
\end{theorem}
\begin{proof}
 By \myref{Theorem}{3.9.3} (limit point compact implies totally bounded), we know $K$ is totally bounded. This tell us, for each $n\inn$, there exists a finite set $A_{\frac{1}{n}}$ 
\begin{equation*}
  A_{\frac{1}{n}}=\set{a_1,a_2,\dots ,a_{m_n}}\text{ s.t. }\forall k\in K,\exists i\leq m_n\inn, k\in B_{\frac{1}{n}}(a_i)
\end{equation*}
Define
\begin{equation*}
A:=\bigcup_{n\inn}A_{\frac{1}{n}}
\end{equation*}
Because $A$ is a countable union of finite sets, we know $A$ is countable. We now prove \blue{$A$ is dense}.\\


Arbitrarily pick $p\in K\setminus A$. For each open ball $B_r(p)$, there exists $\frac{1}{n}$ small enough to be smaller than $r$ to give us $p\in B_{\frac{1}{n}}(a)$ for some $a\in A_{\frac{1}{n}}$, and give us $a\in B_{\frac{1}{n}}(p)\subseteq B_r(p)\bdone$
\end{proof}
\begin{theorem}
\label{3.6.5}
\textbf{(Limit Point Compact Implies Compact)} 
\begin{equation*}
K\text{ is limit point compact }\implies K\text{ is compact }
\end{equation*}
\end{theorem}
\begin{proof}
  Let $\mathcal{G}$ be an open cover for $K$. By \myref{Theorem}{3.8.4}, we know $K$ is separable. Then by  \myref{Theorem}{3.9.1}, we know $K$ has a countable base. Then by \myref{Theorem}{2.6.9}, we know $\mathcal{G}$ contain a countable sub-cover
  \begin{equation*}
  \set{G_i}_{i\inn}
  \end{equation*}
We now prove \vi{there exists $m\inn$ such that $\set{G_i}_{i=1}^m$ cover $K$}.\\

For each $n\inn$, denote
\begin{equation*}
E_n=\bigcup \set{G_i}_{i=1}^n\
\end{equation*}
Obviously we have 
\begin{equation*}
E_1\subseteq E_2\subseteq E_3\subseteq \cdots 
\end{equation*}
We know $E_n$ converge to  $K$, that is
 \begin{equation*}
\bigcup \set{E_i}_{i=1}^\infty=K
\end{equation*}
\As{$\forall m \inn, E_m\neq K$}. Then we can construct an infinite sequence
\begin{equation*}
\set{x_i:x_i\in E_i^c}_{i=1}^\infty
\end{equation*}
Because $K$ is limit point compact, we know $\set{x_i}_{i=1}^\infty$ has a limit point $p$.\\

Because $\set{G_i}_{i=1}^\infty$ is an open cover, we know $p\in G_n$ for some $n$. Then we know there exist open ball $B_r(p)$ small enough to be contained by $G_n$. Notice  $B_r(p)$ is contained by $G_n$ implies $B_r(p)$ is contained by $E_n$, so we know  $B_r(p)$ at best contain all $x_j$ where  $j\leq n\tCaC$ $\vdone$
\end{proof}
\begin{theorem}
\label{3.6.6}
\textbf{(Limit Point Compact is Equivalent to Sequentially Compact)}
\begin{equation*}
K\text{ is limit point compact }\iff K\text{ is sequentially compact }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{x_n}$ be a sequence in $K$. \\

$(\longrightarrow)$\\

We wish to prove the existence of a convergent sub-sequence for $\set{x_n}$. If some point appears infinite times in $\set{x_n}$, then the proof is trivial. If not, we see the range $\set{x_n:n\inn}$ must be infinite. Then because $K$ is limit point compact, we know there exists a limit point $x$ of $\set{x_n}$.\\

By definition of limit point, we know there exists some $x_{n_1}$ in $B_1(x)$. Because $B_{\frac{1}{2}}(x)$ contain infinite point of $\set{x_n}$, again, we know there exists some $n_2$ greater than  $n_1$ and  $x_{n_2}\in B_{\frac{1}{2}}(x)$.\\

Again because $B_{\frac{1}{3}}(x)$ contain infinite points of $\set{x_n}$, we know there exists some $n_3$ greater than  $n_2$ and $x_{n_3}\in B_{\frac{1}{3}}(x)$. Proceeding the procedure infinitely, we get a sub-sequence $\set{x_{n_k}}$ converge to $x$.\\

$(\longleftarrow)$\\

Let $Y\subseteq K$ be infinite. We wish to prove the existence of some limit point of $Y$.\\

Let $\set{x_n}$ be a sequence in $Y$ such that no term repeats. Because $K$ is sequentially compact, we know there exists a sub-sequence $\set{x_{n_k}}$ converge to some point $x$ in  $K$. \\

If $x$ is in $\set{x_{n_k}}$, we can delete that term, and see that this sequence still converge to $x$.
\end{proof}
\fbox{\begin{minipage}{39em}
We now prove the MEGA TFAE.
\end{minipage}}
\begin{theorem}
\label{3.6.7}
\textbf{(In Metric Space, Limit Point Compact is Equivalent to Compact)} For metric space $K$, the followings three are equivalent
\begin{enumerate}[label=(\alph*)]
  \item $K$ is limit point compact 
  \item $K$ is compact 
  \item $K$ is countably compact 
  \item $K$ is sequentially compact
\end{enumerate}
Above three implies the following two equivalent statement ($\R$ is an example the below statements are stronger)
\begin{enumerate}[label=(\arabic*)]
  \item $K$ is separable
  \item $K$ has a countable base
\end{enumerate}
and all above implies
\begin{enumerate}[label=(\roman*)]
  \item every open cover for $K$ has a countable sub-cover.
\end{enumerate}
\end{theorem}
\begin{proof}
In \myref{Theorem}{3.9.5}, we prove that if $K$ has a count base then $K$ is separable by collecting one point from each basic open set, and in \myref{Theorem}{3.9.1}, we construct a countable base by collecting all open balls of rational radius around each point in the countable dense subset.\\

In \myref{Theorem}{3.9.3}, we prove if $K$ is limit point compact then  $K$ is separable. We proved such by covering $K$ with open ball of radius $\frac{1}{n}$ for each $n\inn$, and argue the set of centers $A$ is countable by proving to cover $K$ always require only finite amount of point (which is easy to see if one know $K$ is bounded, but our method here is to show if require infinite centers, then the set of infinite centers has no limit point, which is also easy to see if one use geometric intuition), and argue that each point $p$ that isn't in $A$ is a limit point of $A$ by arguing for each distance $r$, there exists an  $a\in A$ closed enough to be closer to $p$ than $r$.\\

One may observe, at the end of the proof for \myref{Theorem}{3.9.3}, we also construct a countable base for $K$.\\

In  \myref{Theorem}{2.4.7}, we prove if  $K$ has a countable base then every open cover for  $K$ has a countable sub-cover by filtering the open cover with countable base.\\

Notice that we have proved that if $K$ is limit point compact, then  $K$ is separable, has a countable base and every open cover for  $K$ has a countable sub-cover.\\

Then, in \myref{Theorem}{3.7.7}, we proved that if $K$ is limit point compact, the countable sub-cover $\set{G_i}_{i\inn}$ for $K$ must has a finite sub-cover $\set{G_i}_{i=1}^m$, otherwise we can construct an infinite set $\set{x_i}_{i=1}^\infty$ by picking a point from area that isn't covered by $\set{G_i}_{i=1}^n$ for each natural $n$, and show that the limit point of $\set{x_i}_{i=1}^\infty$, being in some $G_n$, must contain at most of  $n$ amount of points of  $\set{x_i}_{i=1}^\infty$, which is impossible.\\

It is clear that $K$ is compact implies $K$ is countably compact. In  \myref{Theorem}{3.6.11}, we proved if $K$ is countably compact, then $K$ is limit point compact by showing if no point is a limit point of a countably infinite set $A$, then for each point $a\in A$, we can find an open neighborhood around $a$ small enough to contain only one point in $A$, i.e.  $a$ itself, and the collection of such small enough open neighborhood and  $A^c$ form an open cover that has no finite sub-cover, as any finite sub-cover contain only finite amount of point in  $A$, and observe every uncountable set must contain some countable set thus also have a limit point.\\

In summary 
\begin{gather*}
\text{(2)$\implies $(1) by \myref{Corollary}{3.9.5}}\\
\text{(1)$\implies $(2) by \myref{Theorem}{3.9.1}}\\
\text{(a)$\implies $(1) and (2) by \myref{Theorem}{3.9.3}}\\
\text{(2)$\implies $(i) by \myref{Theorem}{2.4.7}}\\
\text{(a)$\implies $(b) by \myref{Theorem}{3.7.7}, which use  (i)}\\
\text{(b)$\implies$(c) by \myref{Theorem}{3.6.10}, which is trivial}\\
\text{(c)$\implies $(a) by \myref{Theorem}{3.6.11}}
\end{gather*}
\end{proof}
\fbox{\begin{minipage}{39em}
Above is a very important result. One must know how to prove such.
\end{minipage}}
\section{Metric Space: Some Properties Implied by Compact}
\fbox{\begin{minipage}{39em}
Notice there is a difference between compact space and compact set
\end{minipage}}
\begin{definition}
\label{3.7.1}
\textbf{(Definition of Compact Subset)} Let $X$ be a metric space, we say  $K\subseteq X$ is a compact subset if
\begin{equation*}
\text{ every open cover for $K$ has a finite sub-cover}
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.7.2}
\textbf{(Compact Subset is itself a Compact Space)} 
\begin{equation*}
\text{ A compact subset regarded as a metric space is compact }
\end{equation*}
\end{theorem}
\begin{proof}
Let $K$ be a compact subset, and regard  $K$ as a metric space. Let $\set{G_\ld :\ld \in \Lambda }$ be an open cover for $K$ with respect to $K$. We know there exists a collection of open set $\set{G_i:i\in I}$ such that 
\begin{equation*}
\set{G_\ld: \ld  \in \Lambda}=\set{G_i\cap K: i\in I}
\end{equation*}
Because $\set{G_\ld }$ is an open cover for $K$ with respect to  $K$, we know  $\set{G_i}$ is an open cover for $K$.  Then because $K$ is compact, there exists a finite sub-cover  $\set{G_j}$. Notice $\set{G_j\cap K}$ is a finite sub-cover for $K$ with respect to  $K$.
\end{proof}
\begin{theorem}
\label{3.7.3}
\textbf{(Compact Sets Are Closed)} Let $X$ be a metric space, and let $K\subseteq X$
\begin{equation*}
K\text{ is compact }\implies K\text{ is closed }
\end{equation*}
\end{theorem}
\begin{proof}
We prove  \vi{$K^c$ is open}. Let $p\in K^c$. For each $k\in K$, collect the open balls $B_{\frac{d(k,p)}{2}}(k)$ as $\mathcal{O}$, so we have an open cover $\mathcal{O}$ for $K$. Because  $K$ is a compact subset, we know $\mathcal{O}$ has a finite sub-cover
\begin{equation*}
\mathcal{O}':=\set{B_{\frac{d(k_i,p)}{2}}(k_i)}_{i=1}^m
\end{equation*}
Let $r=\min \set{\frac{d(k_i,p)}{2}:1\leq i\leq m}$. We only have to prove 
\begin{equation*}
  \text{ the open neighborhood $B_r(p)$ is disjoint to $\bigcup \mathcal{O}'$.}
\end{equation*}
This isn't difficult to prove if one have geometric intuition.  \As{$q\in  B_r(p)\cap \bigcup \mathcal{O'}$}. Because $q\in \bigcup \mathcal{O}'$, we know there exists $B_{\frac{d(k_i,p)}{2}}(k_i)$ that contain $q$, so we have
 \begin{equation*}
d(k_i,q)\leq \frac{d(k_i,p)}{2}
\end{equation*}
Then we have
\begin{equation*}
d(p,q)+d(q,k_i)< r+\frac{d(k_i,p)}{2}\leq d(k_i,p)\tCaC\vdone
\end{equation*}
\begin{definition}
\label{3.7.4}
\textbf{(Definition of Bounded)} Let $X$ be a metric space, we say $E\subseteq X$ is bounded if there exists an open ball $B_r(p)$ centering some $p$ in $E$, that contained $E$
\end{definition}
\begin{theorem}
\label{3.7.5}
\textbf{(Compact Sets are Bounded)} Let $X$ be a metric space and $K\subseteq X$. We have
\begin{equation*}
K\text{ is compact }\implies K\text{ is bounded }
\end{equation*}
\end{theorem}
\begin{proof}
\As{$K$ is unbounded}. Arbitrarily pick $q$ from  $K$. We know for each $n\inn$, the open ball $B_n(q)$ doesn't cover $K$  
\begin{equation*}
K\setminus B_n(q)\neq \varnothing
\end{equation*}
Then we can construct a sequence $\set{x_n}_{n\inn}$ by arbitrarily picking $x_n$ from $K\setminus B_n(q)$ for each $n\inn$. Because  $K$ is compact implies $K$ is limit point compact, we know $\set{x_n}_{n\inn}$ has a limit point  $p$ in  $K$. Let $r=d(p,q)$, and let $m$ be the smallest natural greater than  $r$. Because $B_{m-r}(p)$ is contained by $B_m(q)$, we know $B_{m-r}(p)$ contain at most first $m$ points in  $\set{x_n}_{n=1}^\infty\tCaC$ to every open ball centering limit point contain infinite point.
\end{proof}
\begin{corollary}
\label{3.7.6}
\textbf{(Compact Sets are Closed and Bounded)}
\begin{equation*}
K\text{ is compact }\implies K\text{ is closed and bounded }
\end{equation*}
\end{corollary}
\fbox{\begin{minipage}{39em}
We close this section with a method to generate new compact sets.
\end{minipage}}
\end{proof}
\begin{theorem}
\label{3.7.7}
\textbf{(Closed Subsets of Compact Sets Are Compact)} 
\begin{equation*}
\text{$K$ is compact and  $F\subseteq K$ is closed}\implies E\text{ is compact }
\end{equation*}
\end{theorem}
\begin{proof}
Arbitrarily pick an open cover $\mathcal{G}$ for $F$. We wish to prove there is a finite sub-cover for $F$. Because $F$ is closed, we know $F^c$ is open, so we know  $\mathcal{G}\cup \set{F^c}$ is also an open cover, and moreover, open cover for $X$ and $K$. Then because $K$ is compact, we know $\mathcal{G}\cup \set{F^c}$ has a finite sub-cover $\mathcal{G}'$ for $K$, and apparently, for $F$. If  $\mathcal{G}'\subseteq \mathcal{G}$, then we are done. If $F^c\subseteq \mathcal{G}'$, then we can see $\mathcal{G}'\setminus \set{F^c}$ is a finite sub-cover of $\mathcal{G}$ for $F$.
\end{proof}
\begin{corollary}
\label{3.7.8}
Let $F$ be closed and  $K$ be compact. We have $F\cap K$ is compact.
\end{corollary}
\begin{proof} 
The fact that  $K$ is compact tell us $K$ is closed. Then we know $F\cap K$ is closed and is a subset to $K$
\end{proof}
\section{Complete}
\fbox{\begin{minipage}{39em}
We now introduce two equivalent definition of bounded.
\end{minipage}}
\begin{definition}
\label{3.8.1}
\textbf{(Definition of Bounded)} We say a set $E$ is bounded if there exists some open ball $B_r(p)$ that contain $E$.
\end{definition}
\begin{theorem}
\label{3.8.2}
\textbf{(Equivalent Definition of Bounded)} 
\begin{equation*}
E\text{ is bounded }\iff \exists r\inr^+, \forall p,q\in E, d(p,q)<r 
\end{equation*}
\end{theorem}
\begin{proof}
From left to right, notice that every two points in $B_r(p)$ is of distance smaller than $2r$. From right to left, verify $B_r(p)$. 
\end{proof}
\begin{theorem}
\label{3.8.3}
\textbf{(Converge Implies Bounded)}
\begin{equation*}
\lim_{n\to\infty} a_n=a\implies \set{a_n}\text{ is bounded }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $r $. Because $\set{a_n}$ converge to $a$, there exists $N_\epsilon \inn$ such that $n> N_r \implies a_n\in B_r (a)$. We know the ball $B_r(a)$ contain all but finitely many points of $\set{a_n}$. Then we know there exists a point in $\set{a_n}$ that is most far from $a$. Denote the most far point from $a$ by $a_k$. Observe that  $B_{d(a,a_k)+1}(a)$ contain $\set{a_n}$. 
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce definition of totally bounded.
\end{minipage}}
\begin{definition}
\label{3.8.4}
\textbf{(Definition of Totally Bounded)} 
\end{definition}
\section{Euclidean Metric}
\begin{definition}
\label{3.9.1}
\textbf{(Definition of k-cell)} In $(\R^k,d(\vecta{x},\vecta{y})=\abso{\vecta{x}-\vecta{y}})$,  we say $I$ is a $k$-cell, when there exists  $k$ intervals  $\set{[a_1,b_1],\dots , [a_k,b_k]}$ such that
 \begin{equation*}
   I=\prod_{n=1}^k [a_n,b_n]
\end{equation*}
\end{definition}
\begin{lemma}
\label{3.9.2}
\textbf{(Shrinking k-cell Sequence Convergence)} Let $\set{I_n}_{n=1}^\infty$ be a sequence of  k-cell such that 
\begin{equation*}
\forall n\inn, I_{n+1}\subseteq I_n
\end{equation*}
Then we have
\begin{equation*}
\bigcap \set{I_n}_{n=1}^\infty\neq \varnothing
\end{equation*}
\end{lemma}
\begin{proof}
For each $n\inn$, denote the first component of $I_n$ by $[a_n,b_n]$. We know $\set{a_n}_{n=1}^\infty$ is an monotonically increasing sequence  and bounded above by every $b_n$. Notice that the supremum of a set $A$ is exactly the infimum of the set of upper bounds of $A$. Applying this fact into our situation, we know for each $n$, $\sup \set{a_n}_{n=1}^\infty$ is greater than or equal to $a_n$ and smaller than or equal to  $b_n$. Them we know $\sup \set{a_n}_{n=1}^\infty$ must be contained by every $[a_n,b_n]$. In other words,
\begin{equation*}
\sup \set{a_k}_{k=1}^\infty\in \bigcap_{n=1}^\infty [a_n,b_n]
\end{equation*}
Use the same argument in every other component to get a point in $\bigcap \set{I_n}_{n=1}^\infty$. 
\end{proof}
\begin{lemma}
\label{3.9.3}
\textbf{(Maximum Distance of Two Points In k-cell is the length of Diagonal Line)} Let $I=[a_1,b_1]\times\cdots\times[a_k,b_k]$. Let $\vecta{a}=(a_1,\dots, a_k)\text{ and }\vecta{b}=(b_1,\dots, b_k)$. The maximum distance of two points in k-cell are 
\begin{equation*}
 \abso{\vecta{b}-\vecta{a}}= (\sum_{i=1}^k (b_i-a_i)^2)^\frac{1}{2}
\end{equation*}
\end{lemma}
\begin{proof}
Let $\vecta{x},\vecta{y}\in I$. For each $n\inn$ smaller then or equal to $k$,  because $x_n,y_n\in [a_n,b_n]$, we have $\abso{x_n-y_n}\leq b_n-a_n$. Then we can deduce
\begin{equation*}
\abso{\vecta{x}-\vecta{y}}=(\sum_{i=1}^k \abso{x_i-y_i}^2)^{\frac{1}{2}}\leq (\sum_{i=1} (b_i-a_i)^2)^{\frac{1}{2}}=\abso{\vecta{b}-\vecta{a}}
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.9.4}
\textbf{(k-Cell is Compact)} 
\begin{equation*}
\text{ Every k-cell $I$ is compact. }
\end{equation*}
\end{theorem}
\begin{proof}
Let $I_0$ be $[a_1,b_1]\times \cdots \times [a_k,b_k]$. \As{$\set{G_\ld:\ld \in\Lambda }$ is an open cover for $I_0$ that has no finite sub-cover}. For each $n\inn: 1\leq n\leq k$, let 
\begin{equation*}
c_n=\frac{a_n+b_n}{2}
\end{equation*}
Now, we can break $I_0$ into  $2^k$ amount of  $k$-cell of same size, where the $n$-th component of each of them is either  $[a_n,c_n]$ or $[c_n,b_n]$. Notice that at least one of them can not be covered by finite amount of $G_\ld$, since if all of them can be covered by a finite subset of $\set{G_\ld}$ the union of finite sub-covers for each small k-cell is a finite sub-cover for $I_0$, where we have assumed such finite sub-cover does not exist.\\

Denote the smaller k-cell that can not be covered by finite amount of $G_\ld$ by $I_1$. Notice that $\set{G_\ld:\ld\in\Lambda  }$ is an open cover for $I_1$, and there exists no finite sub-cover of $\set{G_\ld :\ld\in \Lambda }$ for $I_1$, just like  $I_0$.\\

Let $I_1=[x_1,y_1]\times\cdots\times[x_k,y_k]$. For each $n\inn:1\leq n\leq k$, let
\begin{equation*}
z_n=\frac{x_n+y_n}{2}
\end{equation*}
Again, break  $I_1$ into  $2^k$ amount of k-cell of even smaller size using $z_n$. And, again, argue that one of the even smaller k-cell has can not be covered by a finite subset of $\set{G_\ld }$.\\

Let $I_2$ be one of the even smaller k-cell that can not be covered by finite amount of  $G_\ld $, and use the same technique to construct a even even smaller k-cell $I_3$, which can not be covered by finite amount of $G_\ld $. This procedure can go on infinitely, so we have a sequence
\begin{equation*}
\cdots \subseteq I_3 \subseteq I_2\subseteq I_1\subseteq I_0
\end{equation*}
Denote this sequence $\set{I_n}_{n=1}^\infty$. By \myref{Lemma}{3.7.7}, we know 
\begin{equation*}
\bigcap \set{I_n}_{n=0}^\infty \neq \varnothing
\end{equation*}
Then, we can let
\begin{equation*}
\vecta{x}\in \bigcap \set{I_n}_{n=0}^\infty
\end{equation*}
Because for each non-negative integer $n$, 
\begin{equation*}
\vecta{x}\in I_n\text{ and }\set{G_\ld :\ld \in\Lambda }\text{ is an open cover for $I_n$ }
\end{equation*}
We know there exists $G_\ld $ that contain $\vecta{x}$. By definition of open, we know there exists an open ball $B_r(\vecta{x})$ small enough to be contained by $G_\ld $
\begin{equation*}
\vecta{x}\in B_r(\vecta{x})\subseteq G_\ld 
\end{equation*}
Now, we wish to pick a even smaller $I_m \in \set{I_n}_{n=0}^\infty$ contained by $B_r(\vecta{x})$, so we have
\begin{equation*}
\vecta{x}\in I_m \subseteq B_r(\vecta{x})\subseteq G_\ld 
\end{equation*}
to show $\set{G_\ld }$ is a cardinality-one sub-cover for $I_m$, which is impossible as we construct $\set{I_n}_{n=0}^\infty$ to satisfy that each of them can not be covered by finite subset of $\set{G_\ld :\ld \in \Lambda }$.\\

Because of the way we break each k-cell to smaller k-cell is to cut the k-cell right in half, it is simple to use geometric intuition to see such small enough $I_m$ should exist.\\

Let
\begin{equation*}
\delta=(\sum_{j=1}^k (b_j-a_j)^2)^{\frac{1}{2}}=\abso{\vecta{b}-\vecta{a}}
\end{equation*}
We can use mathematical induction to rigorously prove that the length of diagonal line for each $I_n$ is exactly $2^{-n}\delta$.\\

Now, using \myref{Lemma}{3.9.3}, it is easy to verify when  $m>\log_2 \frac{\delta}{r}$, the k-cell $I_m$ is small enough to be contained by $B_r(\vecta{x})\tCaC$.
\end{proof}
\begin{corollary}
\label{3.9.5}
\textbf{(Heine-Borel Theorem)} In $(\R^k, d(\vecta{x},\vecta{y})=\abso{\vecta{x}-\vecta{y}})$, we have
\begin{equation*}
K\text{ is closed and bounded}\iff K\text{ is compact }
\end{equation*}
\end{corollary}
\begin{proof}
  We have proved from right to left in \myref{Theorem}{3.9.3} and  \myref{Theorem}{3.9.5}. To prove from left to right, just observe $K$ is bounded implies that $K$ is contained by some k-cell. Then because closed subset of compact set is compact, we can deduce $K$ is compact.
\end{proof}
\begin{definition}
\label{3.9.6}
\textbf{(Definition of Sequentially Compact)} Let $E\subseteq X$ be a topological space. We say $E$ is sequentially compact if 
 \begin{equation*}
\text{ for each sequence $\set{a_n}$ there exists a converging sub-sequence $\set{a'_n}$  }
\end{equation*}
\end{definition}
\fbox{\begin{minipage}{39em}
Notice that in general topology, a sequence can converge to multiple point, the trivial topology of multiple points is an example.
\end{minipage}}
\fbox{\begin{minipage}{39em}
Notice that in the proof above, we actually show that it is possible that it is impossible for $N_U$ and  $N_\epsilon $ to be equal.
\end{minipage}}
\begin{theorem}
\label{3.9.7}
\textbf{(In Metric Space, Limit Point Compact Implies Sequentially Compact)} 
\begin{equation*}
K\text{ is limit point compact }\implies K\text{ is sequentially compact }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{p_n}$ be a sequence in $K$. If the range of $\set{p_n}$ is finite, then there exists a point $p_m$ appears infinitely many time in the sequence, otherwise the sequence end at a finite term. Then, we can pick the sub-sequence that contain only $p_m$ and we are done. \\

If the range of $\set{p_n}$ is infinite, then because $K$ is limit point compact, we know  the range of  $\set{p_n:n\inn}$ has a limit point in $K$. Denote this point  by $p'$, and pick the first term $p'_1=p_{k_1}$ of our sub-sequence by arbitrarily picking  $p_{k_1}$ in  $B_1(p)$. Now, we wish to find points in $B_{\frac{1}{2}}(p)\cap \set{p_n:k_1<n\inn}$. Notice the set
\begin{equation*}
B_{\frac{1}{2}}(p)\cap \set{p_n:n\inn}
\end{equation*}
is infinite, so  it is impossible
\begin{equation*}
B_{\frac{1}{2}}(p)\cap \set{p_n:n\inn}\subseteq \set{p_n:p\leq k_1\inn}
\end{equation*}
then we know 
\begin{equation*}
\text{ $B_{\frac{1}{2}}(p)\cap \set{p_n:n>k_1\inn}\neq \varnothing$  }
\end{equation*}
So we can let $p'_2=p_{k_2}\in B_{\frac{1}{2}}(p)$. By axiom of choice, we can do this infinite amount of time and have a sub-sequence $\set{p'_n}$ that converge to $p$.
\end{proof}
\begin{theorem}
\label{3.9.8}
\textbf{(In Euclidean Space, Bounded Implies Existence of Convergent Sub-sequence)} In $\R^n$, we have
 \begin{equation*}
\set{p_n}\text{ is bounded }\implies \set{p_n}\text{ has a convergent sub-sequence }
\end{equation*}
\end{theorem}
\begin{proof}
In Euclidean space, if $\set{p_n}$ is bounded, we can find a k-cell $I$ that contain  $\set{p_n}$. We know k-cell $I$ is compact, so we know it is sequentially compact. Then we know  $\set{p_n}$ has a convergent sub-sequence in $I$. 
\end{proof}
\begin{theorem}
\label{3.9.9}
\textbf{(Characterization of Connected in $\R^1$ is Interval)} Let $I\subseteq \R$. 
\begin{equation*}
I\text{ is connected }\implies I\text{ is an interval and its endpoint is  $\inf I,\sup I$ }
\end{equation*}
\end{theorem}
\begin{proof}
WOLG, suppose $\inf I,\sup I \not\in I$. We need to prove
\begin{equation*}
  \vi{I=(\inf I,\sup  I)}
\end{equation*}
Because $\inf I,\sup I\not \in I$, it is clear that 
\begin{equation*}
I\subseteq (\inf I,\sup I)
\end{equation*}
At this point, one can tell why we can use WOLG. \As{$(\inf I,\sup I)\setminus I\neq \varnothing$}. Define $R:=(\inf I,\sup I)\setminus I$. Let $x\in R$. Consider 
\begin{equation*}
\alpha :=\sup \set{y\in I:y<x}\text{ and }\beta :=\inf \set{y \in I:y>x}
\end{equation*}
We see 
\begin{equation*}
\alpha \leq x\leq \beta 
\end{equation*}
This tell us 
\begin{equation*}
\set{y\in I:y<x}\text{ and }\set{y\in I:y>x}\text{ are separated }\tCaC
\end{equation*}


\end{proof}
\section{Cauchy Sequence}
\begin{definition}
\label{3.10.1}
\textbf{(Definition of Cauchy Sequence)} Let $X$ be a metric space. We say $\set{p_n}$ is a Cauchy sequence if 
\begin{equation*}
\forall \epsilon\inr^+,\exists N\inn, \forall k,m>N, d(p_m,p_k)<\epsilon 
\end{equation*}
\end{definition}
\begin{definition}
\label{3.10.2}
\textbf{(Definition of Diameter)} Let $E\subseteq X$. We say the diameter of $E$ is
 \begin{equation*}
\diam{E}=\sup \set{d(p,q):p,q\in E}
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.10.3}
\textbf{(Convergent Implies Cauchy)} 
\begin{equation*}
\set{p_n}\text{ converge }\implies \set{p_n}\text{ is a Cauchy sequence }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{p_n}$ converge to $p$. Fix $\epsilon $, we know there exists $N_{\frac{\epsilon }{2}}$ such that
\begin{equation*}
k,m>N_{\frac{\epsilon}{2}}\implies d(p_k,p),d(p_m,p)<\frac{\epsilon}{2}\implies d(p_k,p_m)<d(p_k,p)+d(p_m,p)=\epsilon 
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.10.4}
\textbf{(Diameter of Closure Equals to Original Diameter)} Let $E\subseteq X$. We have
\begin{equation*}
\diam{\overline{E}}=\diam{E}
\end{equation*}
\end{theorem}
\begin{proof}
We wish to prove
\begin{equation*}
  \vi{\sup \set{d(p,q):p,q\in \overline{E}}=\sup \set{d(p,q):p,q\in E}}
\end{equation*}
Because $E\subseteq \overline{E}$, we have
\begin{equation*}
\sup \set{d(p,q):p,q\in \overline{E}}\geq \sup \set{d(p,q):p,q\in E}
\end{equation*}
\As{$\sup \set{d(p,q):p,q\in \overline{E}}>\sup \set{d(p,q):p,q\in E}$}. We know there exists $p',q'\in \overline{E}$ such that
\begin{equation*}
d(p',q')>\diam{E}
\end{equation*}
Clearly, $p',q'$ can not both be in  $E$. Let $p'\in \overline{E}\setminus E$. We know 
\begin{equation*}
p'\in E'
\end{equation*}
If $q'\in E$, denote $q'$ by  $q$. We see
\begin{equation*}
\forall p\in  E,d(p',q)\leq d(p,p')+d(p,q)
\end{equation*}
where $d(p,p')>0$ can be arbitrarily small. So if 
\begin{equation*}
d(p,q)<\diam{E}
\end{equation*}
We have
\begin{equation*}
d(p',q)\leq d(p,p')+d(p,q)<\diam{E}
\end{equation*}
Then we have
\begin{equation*}
\forall p\in E, d(p,q)=\diam{E}
\end{equation*}
Which give us 
\begin{equation*}
E=\set{q}\tCaC
\end{equation*}
Let $q'\not\in E$. We know
\begin{equation*}
q'\in E'
\end{equation*}
We see 
\begin{equation*}
\forall p,q\in E, d(p',q')\leq d(p',p)+d(p,q)+d(q,q')
\end{equation*}
where $d(p',p)$ and $d(q,q')$ can be arbitrarily small.\\

Let $d(p,q)$ satisfy 
\begin{equation*}
d(p,q)<\diam{E}
\end{equation*}
Let $d(p',p)$ and $d(q,q')$ small enough to have 
\begin{equation*}
d(p',q')\leq d(p',p)+d(p,q)+d(q,q')<\diam{E}
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.10.5}
\textbf{(Sufficient Condition for Infinite Intersection of Compact Sets being Non-empty)} Let $\set{K_\ld :\ld \in \Lambda }$ be an infinite collection of compact sets.
\begin{equation*}
\text{ every finite sub-collection of $\set{K_\ld:\ld \in\Lambda }$ is non-empty }\implies \text{ $\bigcap_{\ld\in \Lambda }  K_\ld $ is non-empty }
\end{equation*}
\end{theorem}
\begin{proof}
\As{$\bigcap_{\ld \in \Lambda }K_\ld =\varnothing$}. Fix $K_{\alpha }$. Because 
\begin{equation*}
K_{\alpha }\cap (\bigcap_{\ld \in \Lambda ,\ld \neq \alpha }K_{\ld  })=\varnothing
\end{equation*}
We know no point of $K_{\alpha }$ belong to every $K_\ld $, otherwise that point is in $K_{\alpha }\cap (\bigcap_{\ld \in \Lambda ,\ld \neq \alpha }K_\ld )$.\\

Then we see
\begin{equation*}
\set{K_{\ld}^c: \ld  \in\Lambda }
\end{equation*}
Is an open cover for $K_\alpha $.\\

Because $K_\alpha $ is compact, there exists a finite sub-cover
\begin{equation*}
\set{K^c_1,\dots, K^c_m}
\end{equation*}
Notice that for all $x\in K_\alpha $, we have
\begin{equation*}
\exists n:1\leq n\leq m, x\in K^c_n 
\end{equation*}
Where $x\in K_n^c\implies x\not\in K_n$. This tell us 
\begin{equation*}
\text{ no points in $K_\alpha $ belong to all }K_1,\dots ,K_m
\end{equation*}
Meaning
\begin{equation*}
  K_{\alpha }\cap \bigcap \set{K_1,\dots, K_m}=\varnothing\tCaC
\end{equation*}
\end{proof}
\begin{corollary}
\label{3.10.6}
\textbf{(Single Point)} Let $\set{K_n}$ be a sequence of compact sets such that 
 \begin{equation*}
   \forall n,K_{n+1}\subseteq K_n\text{ and }\lim_{n\to\infty}\diam{K_n}=0
\end{equation*}
We have
\begin{equation*}
\bigcap_{n=1}^\infty K_n\text{ consists of exact one point }
\end{equation*}
\end{corollary}
\begin{proof}
Notice if $\bigcap_{n=1}^\infty K_n$ has more than one point, we can pick two different points in $\bigcap_{n=1}^\infty K_n$ to show
\begin{equation*}
  \diam{\bigcap_{n=1}^\infty K_n}>0
\end{equation*}
However, because for all $m$, we have
 \begin{equation*}
\bigcap_{n=1}^\infty K_n\subseteq K_m
\end{equation*}
We know
\begin{equation*}
\forall m, 0<\diam{\bigcap_{n=1}^\infty K_n}\leq \diam{K_m}
\end{equation*}
\CaC to
\begin{equation*}
\lim_{n\to\infty}\diam{K_n}=0
\end{equation*}
\end{proof}
\begin{definition}
\label{3.10.7}
\textbf{(Definition of Completed)} We say a subset $X$ of a metric space is complete if 
\begin{equation*}
\text{ every Cauchy sequence in $X$ converge to some point in $X$}
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.10.8}
\textbf{(Compact Implies Complete)} 
\begin{equation*}
K\text{ is compact }\implies K\text{ is complete }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{p_n}$ be a Cauchy sequence in $K$. For each $n$ let
 \begin{equation*}
E_n:=\set{p_k:k> n}
\end{equation*}
We first prove 
\begin{equation*}
  \vi{\lim_{n\to\infty}\diam \overline{E_n}=0}
\end{equation*}
Fix $\epsilon $. We know there exists $N$ such that
 \begin{equation*}
m,k>N\implies d(p_m,p_k)<\epsilon 
\end{equation*}
This implies
\begin{equation*}
\epsilon \geq \diam{E_N}=\diam \overline{E_N}\vdone
\end{equation*}
Notice that
\begin{equation*}
\overline{E_n}\subseteq K\text{ and }\overline{E_n}\text{ is closed }\implies \overline{E_n}\text{ is compact }
\end{equation*}
Then by \myref{Corollary}{3.10.6}, we know there exists a unique $p$ such that
 \begin{equation*}
\forall n, p\in  \overline{E_n}
\end{equation*}
Now, we wish to prove
\begin{equation*}
  \blue{\lim_{n\to\infty}p_n=p}
\end{equation*}
Fix $\epsilon $. Because
\begin{equation*} \lim_{n\to\infty}\diam{\overline{E_n}=0}
\end{equation*}
We know there exists $N$ such that
 \begin{equation*}
\forall n>N, \diam{\overline{E_n}}<\epsilon 
\end{equation*}
Let $N_1>N$. We have
 \begin{equation*}
\diam{\overline{E_{N_1}}}<\epsilon 
\end{equation*}
Notice
\begin{equation*}
E_{N_1}=\set{p_{N_1+1},p_{N_1+2},\dots }\subseteq \overline{E_{N_1}}
\end{equation*}
This implies 
\begin{equation*}
\forall n>N_1, d(p_n,p)\leq \diam{\overline{E_n}}<\epsilon \bdone
\end{equation*}
\end{proof}
\begin{corollary}
\label{3.10.9}
\textbf{($\R^n$ is complete)}
\begin{equation*}
\R^n\text{ is complete }
\end{equation*}
\end{corollary}
\begin{proof}
Let $\set{p_n}$ be a Cauchy sequence in $\R^n$. We first prove \vi{$\set{p_n}$ is bounded}.\\

Again, let
\begin{equation*}
E_n:=\set{p_k:k>n}
\end{equation*}
Because 
\begin{equation*}
\lim_{n\to\infty}\diam{\overline{E_n}}=0
\end{equation*}
We know there exists $n$ such that
 \begin{equation*}
\diam{E_n}=\diam{\overline{E_n}}\inr^+
\end{equation*}
This implies $\set{p_n}$ is bounded. $\vdone$\\

Pick a k-cell that contain $\set{p_n}$. We know k-cell is compact. By \myref{Theorem}{3.10.8}, we see $\set{p_n}$ must converge to some point in that k-cell.
\end{proof}
\begin{corollary}
\label{3.10.10}
\textbf{(Cauchy Criterion for Series)}  
\begin{equation*}
\sum a_n\text{ converge }\iff \forall \epsilon ,\exists N, \forall m>u>N, \abso{\sum_{n=u}^m a_n}\leq \epsilon 
\end{equation*}
\end{corollary}
\begin{proof}
Let 
 \begin{equation*}
s_n=\sum_{k=1}^n a_k
\end{equation*}
We see 
\begin{equation*}
  \abso{s_m-s_u}=\abso{\sum_{n=u}^m a_n}
\end{equation*}
Because $\R$ is complete, we know 
 \begin{equation*}
s_n\text{ converge }\iff  s_n\text{ is Cauchy }
\end{equation*}
Notice $s_n$ is Cauchy means 
 \begin{equation*}
\forall \epsilon, \exists N, \forall m,u>N, \abso{s_m-s_u}<\epsilon 
\end{equation*}
\end{proof}
\section{Equivalent Definition of Continuity and Two Kinds of Discontinuity for $\R\rightarrow X$}
\begin{definition}
\label{3.11.1}
\textbf{(Definition of Limit of Function)} Let $f:X\rightarrow Y$. We write 
\begin{equation*}
\lim_{x\to p}f(x)=q
\end{equation*}
If for all $\epsilon $, there exists $\delta$ such that 
\begin{equation*}
x \in B_\delta(p)\implies  f(x)\in B_{\epsilon }(q)
\end{equation*}
Let $f:\R\rightarrow \R$. The definition is extended to 
\begin{equation*}
\lim_{x\to \infty}f(x)=\infty \iff  \forall \alpha ,\exists M, x>M\implies f(x)>\alpha 
\end{equation*}
Let $f:\R\rightarrow Y$. The definition is extended to 
\begin{equation*}
\lim_{x\to \infty}f(x)=q\iff \forall \epsilon, \exists M, x>M\implies f(x)\in B_{\epsilon }(q)
\end{equation*}
Let $f:X\rightarrow \R$. The definition is extended to 
\begin{equation*}
\lim_{x\to p}f(x)=\infty \iff \forall \alpha , \exists \delta, x\in B_\delta(p)\implies f(x)>\alpha 
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.11.2}
\textbf{(Sequential Definition of Limit of Function)} The following are equivalent
\begin{enumerate}[label=(\alph*)]
  \item $\lim_{x\to p}f(x)=q$
  \item $p_n\to p \implies f(p_n)\to q$
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\begin{equation*}
\vi{\lim_{x\to p}f(x)=q\implies (b)}
\end{equation*}
Arbitrarily pick a sequence $\set{p_n}$ in $X$ that converge to $p$. Fix $\epsilon $. Because $\lim_{x\to p}f(x)=q$, we know there exists $\delta$ such that 
\begin{equation*}
p_n \in B_{\delta}(p)\implies f(p_n)\in B_\epsilon (q)
\end{equation*}
Then because $p_n$ converge to $p$, we know there exists  $N$ such that 
\begin{equation*}
\forall n>N, p_n\in B_{\delta }(p)
\end{equation*}
Then we have 
\begin{equation*}
\forall n>N,f(p_n)\in B_\epsilon (q)\vdone
\end{equation*}
We now prove 
\begin{equation*}
\blue{(b)\implies \lim_{x\to p}f(x)=q}
\end{equation*}
\As{$\exists \epsilon , \forall \delta, \exists x_\delta\in E\cap B_{\delta}(p),f(x_\delta)\not\in  B_\epsilon (q)$}. Consider the sequence 
\begin{equation*}
\set{x_{\frac{1}{n}}}_{n\inn}
\end{equation*}
This sequence converge to $p$ but we don't have 
\begin{equation*}
\lim_{n\to\infty}f(x_{\frac{1}{n}})=q
\end{equation*}
since $\forall n, f(x_{\frac{1}{n}})\not\in B_\epsilon (q)\tCaC\bdone$ 
\end{proof}
\begin{corollary}
\label{3.11.3}
\textbf{(Function is Continuous at Limit Point)} If $p\in X'$, we have
\begin{equation*}
\lim_{x\to p}f(x)=f(p)
\end{equation*}
\end{corollary}
\begin{theorem}
\label{3.11.4}
\textbf{(Limit of Function is Unique)} 
\begin{equation*}
\lim_{x\to p}f(x)=q\text{ and }\lim_{x \to p}f(x)=q'\implies q=q'
\end{equation*}
\end{theorem}
\begin{proof}
\As{$q\neq q'$}. Let $\alpha =d(q,q')$. We know $\alpha >0$. Then we know there exists $\delta_q$ and $\delta_{q'}$ such that 
\begin{align*}
&x\in B_{\delta_q}(p)\implies f(x)\in B_{\frac{\alpha}{2}}(q)\\
\text{ and }&x\in B_{\delta_{q'}}(p)\implies f(x)\in B_{\frac{\alpha}{2}} (q')
\end{align*}
Because $p$ is an limit point of $X$, we know there must exists $x\neq p$ such that $x\in B_{\min \set{\delta_q,\delta_{q'}}}(p)$, and that $x$ is in both $B_{\frac{\alpha }{2}}(q)$ and $B_{\frac{\alpha}{2}}(q')$, which is impossible. \CaC
\end{proof}
\begin{theorem}
\label{3.11.5}
\textbf{(Arithmetic of Limit of $\R^n$ Valued Function)} Suppose  $f:X\rightarrow \R^n$ and $g:X\rightarrow \R^n$ satisfy
\begin{equation*}
\lim_{x\to p}f(x)=A\text{ and }\lim_{x\to p}g(x)=B
\end{equation*}
We have
\begin{enumerate}[label=(\alph*)]
  \item $\lim_{x\to p}(f+g)(x)=A+B$ 
  \item $\lim_{x\to p}(fg)(x)=AB$  
  \item $\lim_{x\to p}(\textbf{f}\cdot \textbf{g})(x)=A\cdot B$
\end{enumerate}
\end{theorem}
\begin{proof}
Arbitrarily pick a sequence $\set{p_n}$ in $X$ that converge to  $p$. We have
 \begin{equation*}
\lim_{n\to\infty}f(p_n)=A\text{ and }\lim_{n\to\infty}g(p_n)=B
\end{equation*}
Then the result follows from showing each component of the L.H.S converge to R.H.S, which is proved in \myref{Theorem}{4.1.6}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce continuous. 
\end{minipage}}
\begin{definition}
\label{3.11.6}
\textbf{(Definition of Function being Continuous at point $p$ in Metric Space)} We say a function $f:X\rightarrow Y$ is continuous at $p$ 
\begin{equation*}
\text{ If }\lim_{x\to p}f(x)=f(p)
\end{equation*}
\end{definition}
\begin{definition}
\label{3.11.7}
\textbf{(Definition of Continuous)} We say a function $f:X\rightarrow Y$ is continuous 
\begin{equation*}
\text{ If }f\text{ is continuous at all $x\in X$ }
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.11.8}
\textbf{(Equivalent Definition)} The following are equivalent
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous 
  \item For all open $V\subseteq Y$, the set $f^{-1}[V]\subseteq X$ is open
\item For all closed $V\subseteq Y$, the set $f^{-1}[V]\subseteq X$ is closed
\item For all $A\subseteq X$, we have $f[\overline{A}]\subseteq \overline{f[A]}$
\end{enumerate}
\end{theorem}
\begin{proof}
\myref{Theorem}{3.11.8} is the special case of \myref{Theorem}{2.4.4}. The only thing we have to prove is 
\begin{equation*}
f\text{ is continuous }\iff  f\text{ is continuous at all $x\in X$ }
\end{equation*}
where the term "continuous" at R.H.S. is in the sense of neighborhood.\\

$(\longrightarrow)$\\

Let $W$ be a neighborhood around  $f(x)$. By definition of neighborhood and open in metric space, there exists  an open ball $B_{\epsilon }(f(x))$ centering $f(x)$ small enough to be contained by $W$. Because $f$ is continuous at  $x$ in the sense of metric space, we know either $x$ is an isolated point or we have 
\begin{equation*}
\lim_{p\to x}f(p)=f(x)
\end{equation*}
If $x$ is an isolated point, then we can pick an neighborhood $V$ (open ball if you want) around $x$  small enough so that $V\cap X=\set{x}$. Then we see 
\begin{equation*}
f[V]\subseteq W
\end{equation*}
If we have 
\begin{equation*}
\lim_{p\to x}f(p)=f(x)
\end{equation*}
Then we know there exists $\delta$ such that 
\begin{equation*}
p\in B_{\delta}(x)\implies f(p)\in B_{\epsilon }(f(x))\subseteq W
\end{equation*}
Then we see $B_{\delta}(x)\cap X$ is the neighborhood we want.\\

$(\longleftarrow)$\\

Let $p\in  X$. We wish to prove $f$ is continuous at $p$ in the sense of metric space. We don't have to consider when $p$ is an isolated point, so we only need to prove
\begin{equation*}
\lim_{x\to p}f(x)=f(p)
\end{equation*}
Notice that $B_{\epsilon }(f(p))$ is an neighborhood around $p$, so we know there exists an neighborhood $V$ around $p$ such that 
\begin{equation*}
f[V]\subseteq B_{\epsilon }(f(p))
\end{equation*}
By definition of neighborhood and open in metric space, we know there exists $\delta$ small enough so that 
\begin{equation*}
B_{\delta}(p)\subseteq V
\end{equation*}
which give us
\begin{equation*}
f[B_{\delta}(p)]\subseteq f[V]\subseteq B_{\epsilon }(f(p))
\end{equation*}
Then 
\begin{equation*}
\forall x\in B_{\delta}(p), f(x)\in f[B_{\delta}(p)]\subseteq B_{\epsilon }(f(p))
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
An immediate usage: Intermediate Value Theorem.
\end{minipage}}
\begin{theorem}
\label{3.11.9}
\textbf{(Intermediate Value Theorem)} Suppose $f:X\rightarrow \R$ is continuous and $X$ is connected. 
\begin{equation*}
y\in (\inf f[\partial X],\sup f[\partial X])\implies \exists x\in X, f(x)=y
\end{equation*}
\end{theorem}
\begin{proof}
By \myref{Theorem}{2.4.6} (Continuity map connected set to connected set), and \myref{Theorem}{3.9.9}, we know 
\begin{equation*}
f[X]\text{ is an interval }
\end{equation*}
Then because 
\begin{equation*}
\inf f[X] \leq \inf f[\partial  X]\leq \sup f[\partial X]\leq \sup f[X]
\end{equation*}
from $y\in (\inf f[\partial X],\sup f[\partial X])$, we can deduce
\begin{equation*}
 \inf f[X]<y<\sup f[X]\implies y \in f[X]
\end{equation*}
and we are done.
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce discontinuity.
\end{minipage}}
\begin{definition}
\label{3.11.10}
\textbf{(Definition of Removable Discontinuity)} Given $f:I\subseteq \R\rightarrow Y$. We say $f$ has a removable discontinuity at $p$ if there exists $L\neq f(p)\in Y$ such that 
\begin{equation*}
\lim_{x\to p}f(x)=L
\end{equation*}
\end{definition}
\begin{definition}
\label{3.11.11}
\textbf{(Definition of Jump Discontinuity)} Suppose $p\in I^\circ $. We say $f$ has a jump discontinuity at  $p$ if
 \begin{equation*}
\lim_{x\to p^-}f(x)\text{ and }\lim_{x\to p^+}f(x)\text{ both exists and are not equal }
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.11.12}
\textbf{(Real Function is Not Jump Discontinuous at Boundary)}  
\begin{equation*}
  p \in I\setminus I^\circ \implies f\text{ has no jump discontinuity at $p$}
\end{equation*}
\end{theorem}
\begin{definition}
\label{3.11.13}
\textbf{(Definition of Essential Discontinuity)} Suppose $p \in I^\circ $. We say $f$ has an essential discontinuity at $p$ if 
\begin{equation*}
\text{ one or both of }\lim_{x\to p^-}f(x),\lim_{x\to p^+}f(x)\text{ does not exists }
\end{equation*}
Suppose $p\in I \setminus  I^\circ $ and, WOLG, $\exists \delta, (p-\delta,p)\subseteq I$. We say $f$ has an essential discontinuity at  $p$ if 
\begin{equation*}
\lim_{x\to p^-}f(x)\text{ does not exists }
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.11.14}
\textbf{(Classification of Discontinuity)} Let $p\in  I$. Exactly one of the following is true
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous at $p$ 
  \item $f$ has a removable discontinuity at $p$ 
  \item $f$ has a jump discontinuity at $p$ 
  \item $f$ has an essential discontinuity at $p$
\end{enumerate}
\end{theorem}
\begin{proof}
Recall 
\begin{equation*}
f\text{ is continuous at  }p \iff \lim_{x\to p}f(x)=f(p)
\end{equation*}
It is clear that $f$ is discontinuous implies the threes below. Suppose $f$ is discontinuous at $p$. If $p\in I\setminus I^\circ $, because $f$ is discontinuous at  $p$, we know  $p$ is not an isolated point, then there must be one side of limit we can discuss. The limit either exists or not, and clearly not both. If $p \in I^\circ $. Either both  side of limit exists or not. If the latter, essential. If the former, either they are equal or not. If not equal, jump. If they are equal, notice they are not $f(p)$ because $f$ is not continuous at $p$,  then removable.
\end{proof}
\fbox{\begin{minipage}{39em}
We now do some exercises.
\end{minipage}}
\begin{theorem}
\label{3.11.15}
\textbf{(Example of Discontinuity)} 
\begin{enumerate}[label=(\alph*)]
  \item Define $f(x)=\begin{cases}
    1& \text{ if $x$ is rational }\\
    0& \text{ if $x$ is irrational }
  \end{cases}$. Then $f$ is essentially discontinuous every where 
  \item Define $f(x)=\begin{cases}
    x& \text{ if $x$ is rational }\\
    0& \text{ if $x$ is irrational }
  \end{cases}$. Then $f$ is essentially discontinuous every where but  $x=0$.
\item Define $f(x)=\begin{cases}
  x+2& \text{ if $-3<x<-2$ }\\
  -x-2& \text{ if $-2\leq x<0$ }\\
  x+2& \text{ if $0\leq x<1$ }
\end{cases}$. Then $f$ is continuous every where except when $x=0$, which has a jump discontinuity 
\item Define $f(x)=\begin{cases}
  \sin \frac{1}{x}& \text{ if $x\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}$. Then $f$ is continuous every where except when  $x=0$, which has an essential discontinuity.
\end{enumerate}
\end{theorem}
\begin{proof}
For $(a)$, arbitrarily pick $x\inr$. Let 
\begin{equation*}
x_n \in \Q \cap (x,x+\frac{1}{n})\text{ and }y_n \in (\R \setminus \Q)\cap (x,x+\frac{1}{n})
\end{equation*}
It is easy to see $x_n \to x^+\text{ and }y_n\to x^+\text{ and }f(x_n)\to 1\text{ and }f(y_n)\to 0$. This show 
\begin{equation*}
\lim_{z \to x^+}f(z)\text{ D.N.E. }
\end{equation*}
For $(b)$, we first prove $f$ is continuous at $x=0$. Let  $x_n\to 0$. Fix $\epsilon $. Let $N$ satisfy 
\begin{equation*}
\forall n>N, \abso{x_n-0}<\epsilon 
\end{equation*}
We see 
\begin{equation*}
\forall n>N, \abso{f(x_n)}=\begin{cases}
  \abso{x_n}& \text{ if $x_n$ is rational }\\
  0& \text{ if $x_n$ is irrational }
\end{cases}<\epsilon 
\end{equation*}
Let $x\neq 0$. We now prove $f$ is discontinuous at $x$. Fix $\epsilon =\abso{x}$. For all $\delta$, clearly there exists $y\in B_{\delta}(x)$ such that $y$ is an irrational. Then we see 
 \begin{equation*}
y \in B_{\delta}(x)\text{ and }f(y)=0\not\in B_{\abso{x}}(x)=B_{\epsilon }(x)=B_{\epsilon }(f(x))
\end{equation*}
For $(c)$, we first prove $f$ has a jump discontinuity at $x=0$. For all $\epsilon $, we see 
\begin{align*}
  y\in B_{\min \set{2,\epsilon }}(0)\cap (-\infty,0)&\implies f(y)=-y-2\text{ and } 0<-y<\abso{\min \set{2,\epsilon }}\leq \epsilon \\
  &\implies f(y)\in B_{\epsilon }(-2)
\end{align*}
By using $\delta=\min \set{2,\epsilon }$, we have proved 
\begin{equation*}
\lim_{y\to 0^-}f(y)=-2 
\end{equation*}
With similar method $\delta=\min \set{\epsilon ,1}$,  we can prove 
\begin{equation*}
\lim_{y\to 0^+}f(y)=2\neq -2
\end{equation*}
Notice that at $x=-2$, we can use similar method  (both sides use $\min {\epsilon ,1}$) to show $\lim_{y\to -2}f(y)=0=f(-2)$. At other points, just observe the restrict (-3,-2),[-2,0),[0,1) is continuous.\\

For $(d)$, first notice that the composition of continuous functions is continuous (\myref{Theorem}{2.4.5}). We only have to prove $f$ has an essential discontinuous at $x=0$.\\

Observe that  $x_n=\frac{1}{\frac{\pi}{2}+n \pi}$ and $y_n=\frac{1}{\frac{3\pi}{2}+n \pi}$ satisfy
\begin{equation*}
x_n\to 0\text{ and }y_n\to 0\text{ and }\forall n,x_n>y_n>0
\end{equation*}
Yet 
\begin{equation*}
f(x_n)\to 1\text{ and }f(y_n)\to -1
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
Lastly, we do some characterization of monotone function.
\end{minipage}}
\begin{theorem}
\label{3.11.16}
\textbf{(Monotone Function Has Only Jump Discontinuity)} Let $f:\R\rightarrow \R$ be monotone.
\begin{equation*}
f\text{ is discontinuous at $x$ }\implies f\text{ has jump discontinuity at $x$ }
\end{equation*}
\end{theorem}
\begin{proof}
WOLG, let $f$ be increasing. We prove 
\begin{equation*}
\vi{\lim_{p\to x^-}f(p)=\sup_{p<x} f(p)\leq f(x)\leq \inf_{p>x}f(p)=\lim_{p\to x^+}f(p)}
\end{equation*}
Because $p<x<q\implies f(p)\leq f(x)\leq f(q)$, the inequality follows from definition of supremum and infimum. Suppose
\begin{equation*}
p_n\to x\text{ and }\forall n,p_n<x
\end{equation*}
We wish to prove 
\begin{equation*}
f(p_n)\to \sup_{p<x}f(p)
\end{equation*}
Fix $\epsilon $. By definition of supremum, we know there exists $q<x$ such that 
 \begin{equation*}
   f(q)\in (\sup_{p<x}f(p)-\epsilon ,\sup_{p<x}f(p)]
\end{equation*}
Because $p_n\to x$, we know there exists $N$ such that 
 \begin{equation*}
\forall n>N, q<p_n<x
\end{equation*}
Then we have
\begin{equation*}
\forall n>N, \abso{f(p_n)-\sup_{p<x}f(p)}<\epsilon \vdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.11.17}
\textbf{(Monotone Function Has Countable Discontinuity)} Let $f:\R\rightarrow \R$ be monotone. 
\begin{equation*}
\text{ $f$ has countable discontinuity }
\end{equation*}
\end{theorem}
\begin{proof}
Let $E$ be the set of points that $f$ has a discontinuity. Suppose $p<q \in E$.We see 
\begin{equation*}
  \lim_{x\to p^+}f(x)\leq \lim_{x\to q^-}f(x)
\end{equation*}
Then we see the set of interval
\begin{equation*}
\set{(\lim_{x\to p^- }f(x),\lim_{x\to p^+}f(x)):p \in E}\text{ is pairwise disjoint }
\end{equation*}
Because $\Q$ is dense in $\R$.  For each $p\in  E$, we can associate it with 
a rational number $q_p$ such that 
 \begin{equation*}
q_p \in (\lim_{x\to p^-}f(x),\lim_{x\to p^+}f(x))
\end{equation*}
Because the set of intervals is pairwise disjoint, we know the function  $q_p$ is one-to-one. The inverse function of $q_p$ show that  $E$ is countable.
\end{proof}
\begin{theorem}
\label{3.11.18}
\textbf{(Inverse Function of a Continuous Function on Compact Domain is Continuous)} Let $X$ be compact. Given bijective continuous  $f:X\rightarrow Y$
\begin{equation*}
f^{-1}:Y\rightarrow X\text{ is continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Let $V$ be open in $X$. We have to show 
 \begin{equation*}
   (f^{-1})^{-1}[V]\text{ is open }
\end{equation*}
We know $V^c$ is closed. Then because $V^c\subseteq X$, we know $V^c$ is compact  (\myref{Theorem}{2.4.8}). Then by \myref{Theorem}{2.4.7}, we know $f[V^c]$ is compact. Then by \myref{Theorem}{3.9.3}, we know $f[V^c]$ is closed. This tell us 
\begin{equation*}
  (f^{-1})^{-1}[V]=f[V]=(f[V^c])^c\text{ is open }
\end{equation*}
\end{proof}
\chapter{Numerical Sequence and Series}
\section{Special Real Sequence and Series}
\begin{definition}
\label{4.1.1}
\textbf{(Definition of Extended Real Numbers)} We define
\begin{gather*}
\forall x\inr, -\infty <x<\infty\\
\forall x\inr, x+\infty=\infty \text{ and }x-\infty=-\infty\\
\infty+\infty=\infty \text{ and }-\infty-\infty=-\infty\\
\forall x\inr^+, x(\infty)=\infty \text{ and }x(-\infty)=-\infty\\
0(\infty)=0=0(-\infty)\\
\forall x\inr^-, x(\infty)=-\infty\text{ and }x(-\infty)=\infty\\
\forall x\inr, \frac{x}{\infty}=0=\frac{x}{-\infty}\\
  (\infty)(\infty)=\infty=(-\infty)(-\infty)\text{ and }(\infty)(-\infty)=-\infty\\
  \forall r>0, \sqrt[r]{\infty}=\infty=\abso{-\infty}= \abso{\infty}
\end{gather*}
Notice that we don't define  $\frac{\infty}{\infty}$ and $\infty-\infty$ and $\sqrt[r]{-\infty} $
\end{definition}
\begin{definition}
\label{4.1.2}
\textbf{(Definition of Converge to Infinity)} We say a sequence $\set{x_n}$ converge to positive infinity $\infty$:
\begin{equation*}
\lim_{n\to\infty} x_n=\infty
\end{equation*}
If the sequence $\set{x_n}$ get sufficiently big after some point:
 \begin{equation*}
\forall M, \exists N, \forall n>N, x_n>M
\end{equation*}
Similarly, we define
\begin{equation*}
\lim_{n\to\infty}x_n=-\infty\iff \forall M, \exists N, \forall n>N, x_n<M
\end{equation*}
\end{definition}
\begin{theorem}
\label{4.1.3}
\textbf{(Convergent Implies Bounded)} 
\begin{equation*}
\set{x_n}\text{ converge to a real number $x$}\implies \set{x_n}\text{ is bounded }
\end{equation*}
\end{theorem}
\begin{corollary}
\label{4.1.4}
\textbf{(Unbounded Implies Existence of a sub-sequence Converge to Infinity)}
\begin{equation*}
\set{x_n}\text{ is unbounded above }\implies \exists \set{x_{f(n)}}, \lim_{n\to\infty}x_{f(n)}=\infty
\end{equation*}
\end{corollary}
\begin{proof}
\As{$\set{x_n}$ is unbounded}. WOLG, let $\sup \set{x_n}=\infty$. Observe for all $R$, there exists infinite $n$ such that  $x_n>R$. Then for each $\epsilon $, it is impossible $x_n<x+\epsilon $ after some finite $N$.  \CaC
\end{proof}
\fbox{\begin{minipage}{39em}
Notice that divergent doesn't imply unbounded.\\

Notice that $\lim_{n\to\infty}x_n=\infty\implies \set{x_n}$ is unbounded above, 

An example for sequence converge to infinity is $x_n=n-\frac{1}{n}$. A non-example is 
\begin{equation*}
x_n=\begin{cases}
  n& \text{ if $n$ is odd }\\
  0& \text{ if $n$ is even }
\end{cases}
\end{equation*}
Notice that we seldom say a series converge to infinity.\\


Now we introduce some special sequence. 
\end{minipage}}
\begin{definition}
\label{4.1.5}
\textbf{(Definition of Little-o and Equivalent)} We say $b_n=o(a_n)$ if
\begin{equation*}
\lim_{n\to\infty}\frac{b_n}{a_n}=0
\end{equation*}
And say $a_n\sim b_n$ if
\begin{equation*}
\lim_{n\to\infty} \frac{b_n}{a_n}=1
\end{equation*}
\end{definition}
\fbox{\begin{minipage}{39em}
Notice that interestingly, 
\begin{align*}
a_n\sim b_n \iff  \lim_{n\to\infty}b_n-a_n=0
\end{align*}
\end{minipage}}
\begin{theorem}
\label{4.1.6}
\textbf{(Sequence Arithmetic)} Suppose 
\begin{align*}
x_n \to x \in \R\cup \set{\infty,-\infty}\text{ and }y\inr
\end{align*}
We have 
\begin{enumerate}[label=(\alph*)]
  \item $-x_n \to -x$ 
  \item $x_n+y_n\to x+y$
  \item $cx_n\to cx$
  \item $c+x_n \to c+x$
  \item $x_ny_n \to xy$
  \item  $x_n^{-1}\to x^{-1}$
\end{enumerate}
If the arithmetic on right hand side is possible. 
\end{theorem}
\begin{proof}
Fix $\epsilon $. Respectively use 
\begin{enumerate}[label=(\alph*)]
  \item $N(\epsilon )$
  \item $N(\frac{\epsilon}{2})$
  \item  $N(\frac{\epsilon}{\abso{c}})$ 
  \item $N(\epsilon )$
  \item  $\max \set{N(\sqrt{\frac{\epsilon}{3}} ),N (\frac{\epsilon }{3\abso{x}}),N(\frac{\epsilon }{3\abso{y}})}$
  \item $\max \set{N(\frac{\abso{x}}{2}),N(\frac{\epsilon \abso{x}^2}{2})}$
\end{enumerate}
For $\lim_{n\to\infty} x_ny_n=xy$, notice
\begin{align*}
  \abso{x_ny_n-xy}&=\abso{(x_n-x)(y_n-y)+x(y_n-y)+y(x_n-x)}\\
  &\leq \abso{x_n-x}\times\abso{y_n-y}+\abso{x}\times\abso{y_n-y}+\abso{y}\times\abso{x_n-x}
\end{align*}
For $\lim_{n\to\infty}x_n^{-1}=x^{-1}$, notice 
\begin{equation*}
  \abso{x-x_n}<\frac{\abso{x}}{2}\implies  \abso{x_n}\geq \frac{\abso{x}}{2}
\end{equation*}
Then notice
\begin{equation*}
\abso{\frac{1}{x_n}-\frac{1}{x}}=\abso{\frac{x-x_n}{x\times x_n}}=\abso{\frac{1}{x}}\times \abso{\frac{1}{x_n}}\times \abso{x-x_n}\leq \frac{1}{\abso{x}}\times \frac{2}{\abso{x}}\times \abso{x-x_n}
\end{equation*}
\end{proof}
\begin{corollary}
\label{4.1.7}
\textbf{(Equivalent)} 
\begin{equation*}
\sim\text{ is an equivalent relation. }
\end{equation*}
\end{corollary}
\begin{corollary}
\label{4.1.8}
\textbf{(Sequence Arithmetic)} Let $p\inz,q\inn$
\begin{gather*}
\lim_{n\to\infty} x_n=x\implies \lim_{n\to\infty} x_n^p=x^p\\
\lim_{n\to\infty} x_n=x\text{ and }x_n\text{ is positive }\implies \lim_{n\to\infty} x_n^{\frac{p}{q}}=x^{\frac{p}{q}}
\end{gather*}
\end{corollary}
\begin{proof}
The first statement follows from \myref{Theorem}{4.1.6}.\\

To prove the second statement, we only have to prove
\begin{equation*}
\lim_{n\to\infty} \sqrt[q]{x_n} =\sqrt[q]{x} 
\end{equation*}
Notice that we have
\begin{equation*}
\abso{\sqrt[q]{x_n}-\sqrt[q]{x}}=\frac{\abso{x_n-x}}{\sum_{i=0}^{q-1}x_n^{\frac{q-1-i}{q}}\times x^{\frac{i}{q}}}=\frac{\abso{x_n-x}}{\sum_{i=0}^{q-1}(x_n^{q-1-i}\times x^i)^{\frac{1}{q}}}
\end{equation*}
We wish to find a lower bound for 
\begin{equation*}
\sum_{i=0}^{q-1}(x_n^{q-1-i}\times x^i)^{\frac{1}{q}}
\end{equation*}
Notice that for sufficiently large $n$, we have $x_n>\frac{x}{2}$. Then we know
 \begin{equation*}
\sum_{i=0}^{q-1}(x_n^{q-1-i}\times x^i)^{\frac{1}{q}}>\sum_{i=0}^{q-1}((\frac{1}{2})^{q-1-i}\times x^{q-1} )^{\frac{1}{q}}
\end{equation*}
Then we know for sufficiently large $n$, we have
 \begin{equation*}
\abso{\sqrt[q]{x_n}-\sqrt[q]{x} }=\frac{\abso{x_n-x}}{\sum_{i=0}^{q-1}(x_n^{q-1-i}\times x^i)^{\frac{1}{q}}}<\frac{\abso{x_n-x}}{\sum_{i=0}^{q-1}((\frac{1}{2})^{q-1-i}\times x^{q-1})^{\frac{1}{q}}}
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.1.9}
\textbf{(Special Sequence)} Let $p>0$ and $q>1$ and $\alpha \inr$. We have
\begin{gather*}
\lim_{n\to\infty}\frac{1}{n^p}=0\\
\lim_{n\to\infty}\sqrt[n]{p}=1\\
\lim_{n\to\infty}\sqrt[n]{n}=1\\
\lim_{n\to\infty}\frac{1}{q^n}=0\\
\lim_{n\to\infty}\frac{n^{\alpha }}{q^n}=0
\end{gather*}
\end{theorem}
\begin{proof}
For $\lim_{n\to\infty}\frac{1}{n^p}=0$, observe
\begin{equation*}
\frac{1}{n^p}<\epsilon \iff n>\sqrt[p]{\frac{1}{\epsilon }}  
\end{equation*}
For $\lim_{n\to\infty}\sqrt[n]{p}=1$, observe
\begin{equation*}
\sqrt[n]{p}-1<\epsilon  \iff p<(1+\epsilon )^n\text{ and }1-\sqrt[n]{p}<\epsilon \iff (1-\epsilon )^n<p  
\end{equation*}
For $\lim_{n\to\infty}\sqrt[n]{n} =1$, observe
\begin{equation*}
 (\sqrt[n]{n}-1+1)^n=n\implies \binom{n}{2} (\sqrt[n]{n}-1)^2<n\implies \sqrt[n]{n}-1<\sqrt{\frac{2}{n-1}}  
\end{equation*}
And observe
\begin{equation*}
\sqrt{\frac{2}{n-1}}<\epsilon  \iff n>\frac{2}{\epsilon^2}+1  
\end{equation*}
For $\lim_{n\to\infty}\frac{1}{q^n}=0$, observe
\begin{equation*}
\frac{1}{q^n}<\epsilon  \iff  q^n>\frac{1}{\epsilon }
\end{equation*}
For $\lim_{n\to\infty}\frac{n^\alpha }{q^n}=0$. Let $q=1+p$. We have
 \begin{equation*}
\forall k\leq n\inn, q^n>\binom{n}{k}p^k=n(n-1)\cdots (n-k+1) \frac{p^k}{k!}
\end{equation*}
Notice that if $k<\frac{n}{2}$, we have
 \begin{equation*}
n-k+1>\frac{n}{2}\text{ and }n(n-1)\cdots (n-k+1) \frac{p^k}{k!}>(\frac{n}{2})^k \frac{p^k}{k!}
\end{equation*}
Then if we pick $k>\alpha $, for large  $n$  ($n>2k$), we have 
\begin{equation*}
\frac{n^{\alpha }}{q^n}<\frac{n^{\alpha }}{(\frac{n}{2})^k \frac{p^k}{k!}}=n^{\alpha -k}\frac{2^kk!}{p^k}
\end{equation*}
We see the right hand side approach to $0$.
\end{proof}
\fbox{\begin{minipage}{39em}
Remark: to prove $\lim_{n\to\infty} (f(n))^{g(n)}$ converge to sth. One trick is to use binomial theorem so that $\binom{\alpha }{k}(f(n))<h(n) $, then we get an upper bound $h(n)$ that may be easier to handle.
\end{minipage}}
\section{Sequence in Normed Space} 
\begin{theorem}
\label{4.2.1}
\textbf{(Limit Arithmetic In Normed Space)} Let $x,y$ be in an normed space. Suppose 
\begin{align*}
x_n \to x \text{ and }y_n \to y
\end{align*}
We have 
\begin{enumerate}[label=(\alph*)]
  \item $x_n+y_n \to x+y$ 
  \item  $cx_n \to cx$
\end{enumerate}
\end{theorem}
\begin{proof}
Use $N(\frac{\epsilon }{2})$ and $N(\frac{\epsilon }{\abso{c}})$. 
\end{proof}
\section{Comparison Test, Geometric Series and p-Series}

\fbox{\begin{minipage}{39em}
We first give an easier way to determine if a series converge
\end{minipage}}
\begin{theorem}
\label{4.3.1}
\textbf{(Cauchy Criterion For Series)} 
\begin{equation*}
  \sum_{k=1}^\infty a_k\text{ converge }\iff \forall \epsilon , \exists N, \forall n>m>N, \abso{\sum_{k=m}^n a_k}<\epsilon   
\end{equation*}
\end{theorem}
\begin{proof}
Notice 
\begin{equation*}
\sum_{k=1}^\infty a_k=\lim_{n\to\infty} (\sum_{k=1}^n a_k)
\end{equation*}
Because $\R$ is complete, we know  
 \begin{gather*}
\lim_{n\to\infty}\sum_{k=1}^n a_k\inr\\
\liff \set{\sum_{k=1}^n a_k}_{n\inn}\text{ is Cauchy }\\
\liff \forall \epsilon ,\exists N,\forall n>m>N, \abso{\sum_{k=m}^n a_k}=\abso{\sum_{k=1}^n a_k- \sum_{k=1}^m a_k}<\epsilon 
\end{gather*}
\end{proof}
\begin{corollary}
\label{4.3.2}
\textbf{(Cauchy Criterion For Series)} 
\begin{equation*}
  \sum_{k=1}^\infty a_k\text{ diverge }\iff \exists \epsilon, \forall N, \exists  n>m \inn, \abso{\sum_{k=m}^n a_k}>\epsilon 
\end{equation*}
\end{corollary}
\fbox{\begin{minipage}{39em}
We now introduce absolutely converge.
\end{minipage}}
\begin{definition}
\label{4.3.3}
\textbf{(Definition of Absolutely Convergent)} We say $\sum_{k=1}^\infty a_k$ absolutely converge if 
\begin{equation*}
 \sum_{k=1}^\infty \abso{a_k} \text{ converge to a real number }
\end{equation*}
\end{definition}
\begin{theorem}
\label{4.3.4}
\textbf{(Non-negative Series Converge Implies Absolutely Converge)} Let $\set{a_n}$ be a sequence that eventually become non-negative. We have
\begin{equation*}
\sum_{k=1}^\infty a_k\text{ converge }\implies \sum_{k=1}^\infty a_k\text{ absolutely converge }
\end{equation*}
\end{theorem}
\begin{theorem}
\label{4.3.5}
\textbf{(Absolutely Convergent Implies Convergent)} 
\begin{equation*}
\sum_{k=1}^\infty a_k\text{ absolutely converge }\implies \sum_{k=1}^\infty a_k\text{ converge }
\end{equation*}
\end{theorem}
\begin{proof}
Notice 
\begin{equation*}
\forall n>m \inn, \abso{\sum_{k=m}^n a_k}\leq \sum_{k=m}^n \abso{a_k}=\abso{\sum_{k=m}^n \abso{a_k}}
\end{equation*}
Then from $\sum_{k=1}^\infty \abso{a_k}$ satisfy Cauchy criterion, we can deduce $\sum_{k=1}^\infyt a_k$ satisfy Cauchy Criterion. 
\end{proof}
\fbox{\begin{minipage}{39em}
    We now introduce comparison test.Notice that in comparison test, the $\set{b_n},\set{c_n}$ must eventually be non-negative.
\end{minipage}}
\begin{theorem}
\label{4.3.6}
\textbf{(Squeeze Theorem)} If for large $n$, we have  $x_n<y_n<z_n$, then we have
 \begin{equation*}
\lim_{n\to\infty}x_n=\alpha =\lim_{n\to\infty}z_n\implies \lim_{n\to\infty}y_n=\alpha 
\end{equation*}
\end{theorem}
\begin{proof}
For large $n$, 
 \begin{equation*}
\alpha -\epsilon <x_n<y_n<z_n<\alpha +\epsilon 
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.3.7}
\textbf{(Comparison Test)} Let $\set{c_n}$ be eventually non-negative. We have
\begin{gather*}
  \sum_{k=1}^\infty c_k\text{ converge and for large }n, \abso{b_n}\leq c_n\implies \sum_{k=1}^\infty b_k\text{ absolutely converge }\\
  \sum_{k=1}^\infty a_k\text{ diverge and for large $n$}, b_n>\abso{a_n}\implies \sum_{k=1}^\infty b_k\text{ diverge }
\end{gather*}
\end{theorem}
\begin{proof}
Observe for all large $n$ and $m$,
\begin{equation*}
\abso{\sum_{k=m}^n \abso{b_k}}=\sum_{k=m}^n \abso{b_k}\leq \sum_{k=m}^n c_k\leq \abso{\sum_{k=m}^n c_k}
\end{equation*}
Observe for all $n,m$
\begin{equation*}
\abso{\sum_{k=m}^n a_k}<\sum_{k=m}^n \abso{a_k}<\sum_{k=m}^n b_k=\abso{\sum_{k=m}^n b_k}
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce geometric series.
\end{minipage}}
\begin{theorem}
\label{4.3.8}
\textbf{(Geometric Series)} 
\begin{align*}
x\in (-1,1)\implies \sum_{n=0}^\infty x^n=\frac{1}{1-x}\\
x\not\in (-1,1)\implies \sum_{n=0}^\infty x^n\text{ diverge }
\end{align*}
\end{theorem}
\begin{proof}
Let 
\begin{equation*}
s_n:=\sum_{k=1}^n x^k
\end{equation*}
Notice
\begin{equation*}
  (1-x)s_n=\sum_{k=0}^n x^k-\sum_{k=0}^n x^{k+1}=1-x^{n+1}
\end{equation*}
This give us
\begin{equation*}
s_n=\frac{1-x^{n+1}}{1-x}
\end{equation*}
Then we have
\begin{equation*}
x\in (-1,1)\implies \lim_{n\to\infty}x^{n+1}=0\implies \lim_{n\to\infty}s_n=\frac{1}{1-x}
\end{equation*}
And have
\begin{equation*}
x\not\in (-1,1)\implies \lim_{n\to\infty}x^{n+1}=\infty\text{ or }-\infty\implies \lim_{n\to\infty}s_n= \frac{-\infty}{1-x}\text{ or }\frac{\infty}{1-x}
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce monotonic series and important \myref{Theorem}{4.3.10}
\end{minipage}}
\begin{definition}
\label{4.3.9}
\textbf{(Monotonic)} We say a real sequence $\set{x_n}$ is monotonic if $\set{x_n}$ monotonically increase or monotonically decrease.
\begin{equation*}
\set{x_n}\text{ monotonically increase if }\forall n, x_{n+1}\geq x_n
\end{equation*}
\begin{equation*}
\set{x_n}\text{ monotonically decrease if }\forall n, x_{n+1}\leq x_n
\end{equation*}
\end{definition}
\begin{theorem}
\label{4.3.10}
\textbf{(Monotonic Sequence Converge if and only if Bounded)} Let $\set{x_n}$ be a monotonic sequence. We have
\begin{equation*}
\set{x_n}\text{ converge }\iff \set{x_n}\text{ is bounded }
\end{equation*}
\end{theorem}
\begin{proof}
Let $\set{x_n}$ monotonically increase. We know $\set{x_n}$ is bounded below by $x_1$. We first prove
\begin{equation*}
  \vi{\set{x_n}\text{ is bounded above }\implies \set{x_n}\text{ converge }}
\end{equation*}
Notice that
\begin{equation*}
\set{x_n}\text{ is bounded above }\implies \sup \set{x_n}\inr
\end{equation*}
Let $x:=\sup \set{x_n}$. We wish to prove
\begin{equation*}
\lim_{n\to\infty}x_n=x
\end{equation*}
Fix $\epsilon $. We know there exists some $N$ such that
 \begin{equation*}
x_N>x-\epsilon 
\end{equation*}
Notice that
\begin{equation*}
\forall m>u>N, x_m,x_u\in (x-\epsilon ,x)\text{ and }\abso{x_m-x_u}<\epsilon 
\end{equation*}
We have proved $\set{x_n}$ is Cauchy. $\vdone$\\


For $\set{x_n}\text{ converge }\implies \set{x_n}\text{ is bounded}$, look at \myref{Theorem}{2.1.5}.\\

Let $\set{x_n}$ monotonically decrease. We know $\set{-x_n}$ monotonically increase. If $\set{x_n}$ is bounded, then $\set{-x_n}$ is bounded, which implies $\set{-x_n}$ converges and $\set{x_n}$ converges.\\

If $\set{x_n}$ converge, then $\set{-x_n}$ converge, which implies $\set{-x_n}$ is bounded and $\set{x_n}$ is bounded.
\end{proof}
\fbox{\begin{minipage}{39em}
Lastly, we introduce p-series.
\end{minipage}}
\begin{lemma}
\label{4.3.11}
Let $\set{a_n}$ be a positive monotonically decreasing sequence. We have
 \begin{equation*}
\sum_{n=1}^\infty a_n\text{ converge }\iff \sum_{n=0}^\infty 2^na_{2^n}\text{ converge }
\end{equation*}
\end{lemma}
\begin{proof}
Notice that $\sum_{n=1}^\infty a_n$ and $\sum_{n=0}^\infty 2^n a_{2^n}$ both monotonically increase. We have introduce the question into proving 
\begin{equation*}
\sum_{n=1}^\infty a_n\text{ is bounded above }\iff \sum_{n=0}^\infty 2^na_{2^n}\text{ is bounded above }
\end{equation*}
We first prove
\begin{equation*}
\vi{\forall n,\sum_{k=0}^n 2^ka_{2^k}\leq M\implies \forall n,\sum_{k=1}^n a_k\leq M}
\end{equation*}
Fix $n$. Let  $u$ be the smallest natural such that  $2^{u+1}-1\geq n$.\\

Observe
\begin{equation*}
M\geq \sum_{k=0}^u 2^k a_{2^k}\geq \sum_{k=0}^u \sum_{r=1}^{2^k} a_{2^{k}+r-1}=\sum_{k=1}^{2^{u+1}-1} a_k\geq \sum_{k=1}^n a_k\vdone
\end{equation*}
We now prove
\begin{equation*}
  \blue{\forall n,\sum_{k=1}^n a_k<M\implies \forall n, \sum_{k=0}^n 2^ka_{2^k}<2M}
\end{equation*}
Fix $n$. Observe
\begin{align*}
  2M\geq 2\sum_{k=1}^{2^{n+2}-1} a_k=2\sum_{k=0}^{n+1}\sum_{r=1}^{2^k}a_{2^k+r-1}&\geq 2\sum_{k=0}^{n+1} \sum_{r=1}^{2^k}a_{2^{k+1}}\\
  &=2\sum_{k=0}^{n+1} 2^ka_{2^{k+1}}\\
  &=\sum_{k=0}^{n+1} 2^{k+1}a_{2^{k+1}}\\
  &\geq \sum_{k=0}^n 2^{k}a_{2^k}\bdone
\end{align*}
\end{proof}
\begin{corollary}
\label{4.3.12}
\textbf{(p-series)}
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{n^p}\text{ converge }\iff  p>1
\end{equation*}
\end{corollary}
\begin{proof}
If $p\leq 0$, observe
\begin{equation*}
\lim_{n\to\infty}\frac{1}{n^p}=\infty\text{ or }1\implies \sum_{n=1}^\infty \frac{1}{n^p}=\infty
\end{equation*}
If $p>0$, observe
 \begin{equation*}
\sum_{n=0}^\infty 2^n \frac{1}{(2^n)^p}=\sum_{n=0}^\infty 2^{n(1-p)}=\sum_{n=0}^\infty (2^{1-p})^n\text{ converge }\iff 2^{1-p}<1\iff p>1
\end{equation*}
\end{proof}

\section{Sub-Sequential Limit}
\begin{definition}
\label{4.4.1}
\textbf{(Definition of Limit Superior)} 
\begin{equation*}
\limsup_{n\to\infty} x_n:= \lim_{n\to\infty} \sup \set{x_k:k\geq n}
\end{equation*}
\end{definition}
\fbox{\begin{minipage}{39em}
Notice that as extended real numbers, $\limsup_{n\to\infty} x_n$ always exists, since each term exists and the sequence decrease.\\

The following suggest that if the first term  $\sup \set{x_k:k\geq 1}$ equals to infinity, then every term equals to infinity, in spite of the fact the sequence decrease. 
\end{minipage}}
\begin{theorem}
\label{4.4.2}
\textbf{(Investigation of the Behavior of The Sequence)} 
\begin{equation*}
\sup_{n\inn} x_n=\infty \iff \forall n, \sup \set{x_k:k\geq n}=\infty \iff  \limsup_{n\to\infty} x_n=\infty
\end{equation*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

\As{$\exists n,\sup \set{x_k:k\geq n}<\infty$}. We see 
\begin{equation*}
\sup_{k\inn} x_k= \max \set{x_1,\dots , x_{n-1}, \sup \set{x_k:k\geq n}}<\infty\tCaC
\end{equation*}
$(\longleftarrow)$\\

Notice that 
\begin{equation*}
\set{\sup_{k\geq n}x_k}_{n\inn}\text{ decrease }
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.4.3}
\textbf{(Common Sense)} 
\begin{equation*}
x_n \to \alpha \iff  \alpha =\limsup_{n\to\infty} x_n=\liminf_{n\to\infty} x_n
\end{equation*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

\As{$\limsup_{n\to\infty} x_n<\alpha $}. Because $\set{\sup \set{x_k:k\geq n}}_{n\inn}$ decrease, there exists $m$ such that 
\begin{equation*}
\sup_{n>m} x_n<\alpha 
\end{equation*}
Because $x_n \to \alpha $, we can let $N$ satisfy 
\begin{equation*}
\forall n>N, x_n>\sup_{k\geq m} x_k 
\end{equation*}
Let $n>\max \set{N,m}$. We see 
\begin{equation*}
\sup_{k\geq m} x_k\geq x_n>\sup_{k\geq m} x_k\tCaC
\end{equation*}
$(\longleftarrow)$\\

Observe that 
\begin{equation*}
\inf_{k\geq n}x_k\leq x_n\leq \sup_{k\geq n}x_k
\end{equation*}
The result follows from Squeeze Theorem.
\end{proof}
\begin{corollary}
\label{4.4.4}
\textbf{(Common Sense)}
\begin{equation*}
\limsup_{n\to\infty} x_n=-\infty \implies x_n \to -\infty
\end{equation*}
\end{corollary}
\fbox{\begin{minipage}{39em}
\myref{Theorem}{4.4.3} is a very basic fact that we shall give another proof later on.    
\end{minipage}}
\begin{theorem}
\label{4.4.5}
\textbf{(Main Property of Limit Superior)} Suppose 
\begin{equation*}
\liminf_{n\to\infty}  x_n=\alpha<\infty \text{ and }\limsup_{n\to\infty}  x_n=\beta>-\infty \end{equation*}
For all $\epsilon $, there exists $N$ such that 
\begin{equation*}
\forall n>N, x_n\in (\alpha -\epsilon ,\beta +\epsilon )
\end{equation*}
\end{theorem}
\begin{proof}
We first consider when $\alpha ,\beta \inr$. Because of monotone of $\set{\inf_{k\geq n} x_k}_{n\inn}$ and $\set{\sup_{k\geq n} x_k}_{n\inn}$, we know there exists $m$ such that 
 \begin{equation*}
\alpha -\epsilon <\inf_{k\geq m}x_k \leq \sup_{k\geq m} x_k <\beta +\epsilon 
\end{equation*}
We now see $m$ work as $N$, as 
 \begin{equation*}
n>m \implies \alpha -\epsilon <x_n<\beta +\epsilon 
\end{equation*}
If $\alpha =-\infty$ or $\beta =\infty$, the theorem hold true by same argument. 
\end{proof}
\fbox{\begin{minipage}{39em}
Notice in the proof for \myref{Theorem}{4.4.5}, the constraint allow us to exclude the situation $x_n\to \infty$ or $x_n \to -\infty$. One can observe that if $\alpha =\infty$, \myref{Theorem}{4.4.5} become false.\\

Interestingly, \myref{Theorem}{4.4.5} can be restated in the form 
\begin{equation*}
x_n\in (\alpha -\epsilon ,\beta +\epsilon )\text{ for large }n
\end{equation*}
\end{minipage}}
\begin{theorem}
\label{4.4.6}
\textbf{(Main Property of Limit Superior)} Suppose 
\begin{equation*}
\limsup_{n\to\infty} x_n=\beta\inr
\end{equation*}
For all $\epsilon $, there exists sub-sequences $x_{n_u}$ such that 
\begin{equation*}
\forall u,x_{n_u}>\beta - \epsilon 
\end{equation*}
\end{theorem}
\begin{proof}
Because $\set{\sup_{k\geq n}x_k}_{n\inn}\searrow \beta $, for all $m\inn$, we have 
 \begin{equation*}
\sup_{k\geq m} x_k\geq \beta >\beta -\epsilon 
\end{equation*}
Such sequence $x_{n_u}$ can then be recursively constructed by picking 
\begin{equation*}
n_u=\min \set{k\inn:k\geq n_{u-1}\text{ and }x_{k}>\beta -\epsilon }
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.4.7}
\textbf{(Main Property of Limit Superior)} Suppose 
\begin{equation*}
\limsup_{n\to\infty} x_n=\infty
\end{equation*}
Then there exists a sub-sequence $x_{n_k}$ such that 
\begin{equation*}
x_{n_k}\nearrow \infty
\end{equation*}
\end{theorem}
\begin{proof}
Let 
\begin{equation*}
n_k=\min \set{n\inn:n_k>n_{k-1}\text{ and }x_{n_k}>x_{n_{k-1}}+1}
\end{equation*}
This is possible because 
\begin{equation*}
\sup_{u\geq n_{k-1}} x_u=\infty
\end{equation*}
from \myref{Theorem}{4.4.2}.
\end{proof}
\fbox{\begin{minipage}{39em}
Notice that like \myref{Theorem}{4.4.5}, both \myref{Theorem}{4.4.6} and  \myref{Theorem}{4.4.15} avoid the null discussion of $\limsup_{n\to\infty} x_n=-\infty$.\\

Also, notice that in \myref{Theorem}{4.4.6}, we can not restate it in the form of "for large $n$", as we are getting a sub-sequence instead of a tail.
\end{minipage}}

\begin{theorem}
\label{4.4.8}
\textbf{(Useful Property of Limit Superior)} 
Given 
 \begin{equation*}
x_n\leq y_n\text{ for large $n$ }
\end{equation*}
We have 
\begin{equation*}
\limsup_{n\to\infty} x_n\leq \limsup_{n\to\infty} y_n\text{ and }\liminf_{n\to\infty} x_n\leq \liminf_{n\to\infty} y_n 
\end{equation*}
\end{theorem}
\begin{proof}
Let $N$ be large enough such that 
 \begin{equation*}
\forall n>N, x_n\leq y_n
\end{equation*}
We then see 
\begin{equation*}
\forall n>N, \sup_{k\geq n} x_k\leq \sup_{k\geq n}y_k
\end{equation*}
This give us 
\begin{equation*}
  \limsup_{n\to\infty} x_k\leq \limsup_{n\to\infty} y_k
\end{equation*}
The other side is done with similar method.
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce another equivalent definition of limit superior.
\end{minipage}}
\begin{theorem}
\label{4.4.9}
\textbf{(Set of sub-Sequential Limit is Closed)} Let $x_n\in X$. Suppose 
\begin{equation*}
E\text{ is the set of sub-sequential limit of $x_n$}
\end{equation*}
Then 
\begin{equation*}
E\text{ is closed }
\end{equation*}
\end{theorem}
\begin{proof}
Let $y_n$ be a sequence in $E$ and $y_n \to y$. We have to prove 
 \begin{equation*}
y \in E
\end{equation*}
by finding a sub-sequence $x_{n_k}$ such that 
\begin{equation*}
x_{n_k}\to y
\end{equation*}
By \myref{Theorem}{3.1.9}, we can, WOLG, suppose 
\begin{equation*}
\forall n,d(y_n,y)<\frac{1}{n}
\end{equation*}
Because $y_n\in E$, for each $m \inn$, we can define a sub-sequence  
 \begin{equation*}
x_{n_{m,k}}\to y_m\text{ as $k\to \infty$ }
\end{equation*}
Notice that in our definition, for each fixed $m$, we have specified $n_{m,k}\nearrow \infty$.\\

Also, again by \myref{Theorem}{3.1.9}, we can, WOLG, suppose 
\begin{equation*}
\forall k,d(x_{n_{m,k}},y_m)<\frac{1}{k}
\end{equation*}
Now, for each $u\inn$, we recursively define $k_u$ by
\begin{equation*}
k_u:=\min \set{k\inn:k>u\text{ and }n_{u,k}>n_{u-1,k_{u-1}}}
\end{equation*}
This is possible because for each fixed  $u$, we have 
\begin{equation*}
n_{u,k}\nearrow \infty\text{ and }x_{n_{u,k}}\to y_u\text{ as }k\to\infty
\end{equation*}
It is easy to see $n_{u,k_u}\nearrow \infty$. We see that $x_{n_{u,k_u}}$ is a sub-sequence. We now prove 
\begin{equation*}
  \vi{x_{n_{u,k_u}}\to y\text{ as }u\to \infty} 
\end{equation*}
Fix $\epsilon $. Let $N>2\epsilon $. Notice that by definition of $k_u$, we have  $k_u>u$. Now we see 
\begin{equation*}
u>N\implies d(x_{n_{u,k_u}},y)\leq d(x_{n_{u,k_u}},y_u)+d(y_u,y)<\frac{1}{k_u}+\frac{1}{u}<\frac{2}{u}<\frac{2}{N}=\epsilon \vdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.4.10}
\textbf{(Equivalent Definition of Limit Superior)} Suppose 
\begin{equation*}
E\text{ is the set of sub-sequential limit of $x_n$ }
\end{equation*}
We have 
\begin{equation*}
\limsup_{n\to\infty} x_n=\sup E
\end{equation*}
\end{theorem}
\begin{proof}
Let $\alpha =\limsup_{n\to\infty} x_n$. If $\alpha \inr$, because $E$ is closed, we have to first
  \begin{equation*}
  \text{ \vi{find a sub-sequence $x_{n_k}$ that converge to $\alpha $} }
  \end{equation*}
and show that 
\begin{equation*}
  \blue{\text{ every convergent sub-sequence $x_{n_u}\to x$ satisfy $x\leq \alpha $ }}
\end{equation*}
Let 
\begin{equation*}
n_k=\min \set{n\inn:n>n_{k-1}\text{ and }x_{n}<\alpha +\frac{1}{k}}
\end{equation*}
This is possible because of the main property of limit superior \myref{Theorem}{4.4.5}. Then we see $x_{n_k}\searrow \alpha \vdone$\\

\As{$x_{n_u}\to x\text{ and }x>\alpha $}. This $\tCaC$ to \myref{Theorem}{4.4.5}. $\bdone$\\

If $\alpha =-\infty$, we see $x_n\to -\infty$, then the result follows from \myref{Theorem}{3.1.8}, as the only sub-sequential limit is $-\infty$. If $\alpha =\infty$, result follows from main property of limit superior \myref{Theorem}{4.4.15}.
\end{proof}
\begin{theorem}
\label{4.4.11}
\textbf{(Limit Points Set of Range Are Exactly Set of sub-Sequential Limits)} Let $E$ be the set of sub-sequence limits of  $\set{x_n}$. We have
\begin{equation*}
\set{x_n:n\inn}'\subseteq E
\end{equation*}
\end{theorem}
\begin{proof}
Let $y\in \set{x_n:n\inn}'$. We know there exists a sequence $\set{x_{f(n)}}_{n\inn}$ converge to $y$, where we are not guarantee $a<b\implies f(a)<f(b)$.\\

We wish to construct a sub-sequence of $\set{x_{f(n)}}$ that is also a sub-sequence of $\set{x_n}$.\\

Notice  $\forall n, \exists m>n, f(m)>f(n)$ and we are done.
\end{proof}
\begin{theorem}
\label{4.4.12}
\textbf{(Topological Properties of sub-Sequential Limits)} Let $E$ be the set of sub-sequential limits of $\set{x_n}$, and let $Y$ be the set of points that appears infinite times in  $\set{x_n}$, i.e.
\begin{equation*}
  Y=\set{y:\exists \set{x_{n_k}},\forall k, y=x_{n_k}}
\end{equation*}
We have
\begin{equation*}
E=\set{x_n:n\inn}' \cup Y
\end{equation*}
\end{theorem}
\begin{proof}
By \myref{Theorem}{4.4.15}, it is clear 
\begin{equation*}
\set{x_n:n\inn}' \cup Y\subseteq E
\end{equation*}
Arbitrarily pick $z\in E$. We wish to prove
\begin{equation*}
z\in \set{x_n:n\inn}'\cup Y
\end{equation*}
In other words, we wish to prove
\begin{equation*}
  \vi{z\not\in Y\implies z\in \set{x_n:n\inn}'}
\end{equation*}
We know there exist a sub-sequence $\set{x_{f(n)}}$ such that 
\begin{equation*}
\lim_{n\to\infty}x_{f(n)}=z
\end{equation*}
Because $z\not\in Y$. We know there a sub-sequence $\set{x_{f(g(n))}}$ such that
\begin{equation*}
\forall n,x_{f(g(n))}\neq z
\end{equation*}
Notice that
 \begin{equation*}
z=\lim_{n\to\infty}x_{f(n)}=\lim_{n\to\infty}x_{f(g(n))} 
\end{equation*}
And we are done.
\end{proof}
\fbox{\begin{minipage}{39em}

\end{minipage}}
\begin{theorem}
\label{4.4.13}
\textbf{(Useful Lemma)} Suppose $x\inr$
 \begin{equation*}
x_n \to x \implies \text{ existence of  } x_{n_k} \nearrow x\text{ or }x_{n_k}\searrow x
\end{equation*}
\end{theorem}
\begin{proof}
If it happens infinite times that $x_n=x$. The proof is trivial. WOLG, suppose there exists a sub-sequence $x_{n_j}$ such that  $\forall j,x_{n_j}<x$. Clearly we have $x_{n_j}\to x$. Then we can define 
\begin{equation*}
j_1=1\text{ and }\forall k>1,  j_k:=\min \set{s \inn:x_{n_s}> \max_{u<k}x_{n_{j_u}}} 
\end{equation*}
We now prove 
\begin{equation*}
  \vi{x_{n_{j_k}}\nearrow x}
\end{equation*}
It is clear that $x_{n_{j_k}}\to x$. We first prove well definiteness.\\

Suppose $j_k$ exists, we need to prove $j_{k+1}$ exists. In other words, we need to prove there exists $s \inn$ such that 
\begin{equation*}
x_{n_s}>\max_{u<k}x_{n_{j_u}}
\end{equation*}
This is true because $x_{n_{j_k}}\to x$ and $\max_{u<k}x_{n_{j_u}}<x$.\\

We now prove 
\begin{equation*}
x_{n_{j_{k-1}}}<x_{n_{j_k}}
\end{equation*}
Notice that $j_k$ satisfy
 \begin{equation*}
x_{n_{j_k}}>\max_{u<k}x_{n_{j_u}}\geq x_{n_{j_{k-1}}}\vdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.4.14}
\textbf{(Useful Lemma)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $\abso{a_n}\leq \abso{b_n}+c$
  \item $b_n\to 0$
\end{enumerate}
We have 
\begin{equation*}
\limsup_{n\to\infty} \abso{a_n}\leq c
\end{equation*}
\end{theorem}
\begin{proof}
\As{$\limsup_{n\to\infty} \abso{a_n}>c$}. Let 
\begin{equation*}
\alpha  = \frac{\limsup_{n\to\infty}  \abso{a_n}+c}{2}
\end{equation*}
We have 
\begin{equation*}
c<\alpha <\limsup_{n\to\infty} \abso{a_n}
\end{equation*}
We then know there exists a sub-sequence $a_{n_k}$ such that 
\begin{equation*}
  \forall k, \abso{a_{n_k}} > \alpha  
\end{equation*}
Yet if we let $\epsilon =\alpha -c$. Because $b_{n_k} \to 0$, we know there exists $N$ such that 
\begin{equation*}
\forall k>N, \abso{b_{n_k}}<\epsilon 
\end{equation*}
We now see 
\begin{equation*}
\forall k, \alpha <\abso{a_{n_k}}\leq \abso{b_{n_k}}+c< \epsilon +c=\alpha \tCaC
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.4.15}
\textbf{(Useful Lemma)} Let $c\inr^+$. If for all $\epsilon $, we have
\begin{equation*}
\limsup_{n\to\infty} \abso{a_n}\leq \epsilon c
\end{equation*}
Then 
\begin{equation*}
a_n \to 0
\end{equation*}
\end{theorem}
\begin{proof}
It is clear that 
\begin{equation*}
\limsup_{n\to\infty} \abso{a_n}=0
\end{equation*}
Notice that  
\begin{equation*}
\forall n,\abso{a_n}\geq 0\implies \liminf_{n\to\infty} \abso{a_n}\geq 0
\end{equation*}
We now have 
\begin{equation*}
\liminf_{n\to\infty} \abso{a_n}=0=\limsup_{n\to\infty} \abso{a_n}
\end{equation*}
\end{proof}





\section{Euler Number}
\begin{lemma}
\label{4.5.1}
\textbf{(Bernoulli's Inequality)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $x\geq -1$
  \item $r\inn$
\end{enumerate}
Then
\begin{equation*}
  (1+x)^r\geq 1+rx
\end{equation*}
\end{lemma}
\begin{proof}
We prove by induction. Notice that base case is trivial.\\

Suppose 
\begin{equation*}
  (1+x)^r\geq 1+rx
\end{equation*} 
Observe
\begin{equation*}
  \frac{(1+x)^{r+1}}{1+rx+x}=\frac{(1+x)^r}{1+rx}\times \frac{(1+x)(1+rx)}{1+rx+x}=\frac{(1+x)^r}{1+rx}\times (1+\frac{rx^2}{1+rx+x})
\end{equation*}
Notice that $1+x\geq 0$. If $1+rx+x>0$, then the result follows from seeing 
\begin{equation*}
\frac{(1+x)^{r+1}}{1+rx+x}\geq 1
\end{equation*}
If $1+rx+x<0$. Then the proof is trivial, since $(1+x)^r\geq 0$ 
\end{proof}
\begin{theorem}
\label{4.5.2}
\textbf{(Existence of Euler Number)} 
\begin{equation*}
\sum_{n=0}^\infty \frac{1}{n!}\text{ converge }
\end{equation*}
\end{theorem}
\begin{proof}
Notice 
\begin{equation*}
\forall n, n!\geq 2^{n-1}\implies \forall n,\frac{1}{n!}\leq \frac{1}{2^{n-1}}
\end{equation*}
Observe 
\begin{equation*}
\sum_{n=0}^\infty  \frac{1}{2^{n-1}}=\frac{2}{\frac{1}{2}}=4
\end{equation*}
Then by comparison test, we are done. 
\end{proof}
\begin{definition}
\label{4.5.3}
\textbf{(Definition of Euler Number)}
\begin{equation*}
e:=\sum_{n=0}^\infty \frac{1}{n!}
\end{equation*}
\end{definition}
\begin{theorem}
\label{4.5.4}
\textbf{(Existence of Euler Number)}
\begin{equation*}
\lim_{n\to\infty} (1+\frac{1}{n})^n\text{ converge }
\end{equation*}
\end{theorem}
\begin{proof}
We first prove
\begin{equation*}
\vi{(1+\frac{1}{n})^n\text{ monotonically increase }}
\end{equation*}
We wish to prove
\begin{equation*}
\frac{(1+\frac{1}{n+1})^{n+1}}{(1+\frac{1}{n})^n}\geq 1
\end{equation*}
Observe
\begin{align*}
\frac{(1+\frac{1}{n+1})^{n+1}}{(1+\frac{1}{n})^n}&=\frac{(\frac{n+2}{n+1})^{n+1}}{(\frac{n+1}{n})^n}\\
&=(\frac{n(n+2)}{(n+1)^2})^n(1+\frac{1}{n+1})\\
&=(1-\frac{1}{(n+1)^2})^n (1+\frac{1}{n+1})\\
&\geq (1-\frac{n}{(n+1)^2})(1+\frac{1}{n+1})\\
&=\frac{(n^2+n+1)(n+2)}{(n+1)^3}\\
&=\frac{n^3+3n^2+3n+3}{n^3+3n^2+3n+1}> 1\vdone
\end{align*}
We now prove
\begin{equation*}
  \blue{\forall n,(1+\frac{1}{n})^n<e}
\end{equation*}
Observe
\begin{align*}
  (1+\frac{1}{n})^n&= \sum_{k=0}^n \binom{n}{k}\frac{1}{n^k}\\
  &=\sum_{k=0}^n \frac{n(n-1)\cdots (n-k+1)}{n^kk!}\\
  &=\sum_{k=0}^n \frac{1}{k!}(\frac{n}{n})(\frac{n-1}{n})(\frac{n-2}{n})\cdots (\frac{n-k+1}{n})\leq \sum_{k=0}^n \frac{1}{k!}<\sum_{n=0}^\infty \frac{1}{n!}=e\bdone
\end{align*}
\end{proof}
\begin{theorem}
\label{4.5.5}
\textbf{(Existence of Euler Number)} 
\begin{equation*}
e=\lim_{n\to\infty}(1+\frac{1}{n})^n
\end{equation*}
\end{theorem}
\begin{proof}
Define 
\begin{equation*}
t_n=(1+\frac{1}{n})^n
\end{equation*}
In \myref{Theorem}{4.5.4}, we have proved
\begin{equation*}
t_n\text{ is increasing and }\forall n, t_n\leq e
\end{equation*}
Fix $\epsilon $. Now, we only have to find $n$ such that 
 \begin{equation*}
   \vi{t_n>e-\epsilon}
\end{equation*}
Let
 \begin{equation*}
m=\min \set{n\inn: \sum_{k=0}^n \frac{1}{k!}>e-\epsilon }
\end{equation*}
Let $n\geq m$. Observe
\begin{align*}
  (1+\frac{1}{n})^n&= \sum_{k=0}^n \binom{n}{k}\frac{1}{n^k}\\
  &\geq \sum_{k=0}^m \binom{n}{k}\frac{1}{n^k}\\
  &=\sum_{k=0}^m \frac{n(n-1)\cdots (n-k+1)}{n^k k!}
\end{align*}
This give us  
\begin{equation*}
  (1+\frac{1}{n})^n\geq \sum_{k=0}^m \frac{n(n-1)\cdots (n-k+1)}{n^kk!}\to \sum_{k=0}^m \frac{1}{k!}>e-\epsilon \text{ as }n\to \infty \vdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.5.6}
\textbf{(Property of Exponential Function)} Given $r\inr$
\begin{equation*}
 e^r=\lim_{n\to \infty}(1+\frac{r}{n})^n 
\end{equation*}
\end{theorem}
\begin{proof}
Observe 
\begin{align*}
  (1+\frac{r}{n})^n&= ((1+\frac{1}{ \frac{n}{r}})^\frac{n}{r})^r\\
\end{align*}
\end{proof}
Then 
\begin{align*}
\lim_{n \to \infty} (1+\frac{r}{n})^n &= \lim_{n\to \infty}((1+\frac{1}{\frac{n}{r}})^{\frac{n}{r}})^r\\
&=\lim_{u \to \infty}((1+\frac{1}{u})^u)^r\\
&=e^r
\end{align*}



\section{Tests for Convergence of Series}
\begin{theorem}
\label{4.6.1}
\textbf{(Root Test)} Given $\sum a_n$. Let
\begin{equation*}
\alpha:= \limsup_{n\to\infty} \sqrt[n]{\abso{a_n}} 
\end{equation*}
Then 
\begin{enumerate}[label=(\alph*)]
  \item $\alpha <1\implies \sum a_n$ absolutely converge\\
  \item $\alpha >1\implies \sum a_n$ diverge
\end{enumerate}
\end{theorem}
\begin{proof}
Let 
\begin{equation*}
\limsup_{n\to\infty} \sqrt[n]{\abso{a_n}} =\alpha <1
\end{equation*}
We know there exists $N$ such that
 \begin{equation*}
\forall n>N, \sqrt[n]{\abso{a_n}}\leq \alpha  <1 
\end{equation*}
Then we have
\begin{equation*}
\forall n>N, a_n\leq \abso{a_n}<\alpha  ^n
\end{equation*}
Because $\alpha <1$, we know 
\begin{equation*}
\sum_{n=1}^\infty \alpha ^n=\frac{\alpha }{1-\alpha  }
\end{equation*}
Then by Comparison Test, we know $\sum a_n$ converge. If
\begin{equation*}
\alpha =\limsup_{n\to\infty} \sqrt[n]{\abso{a_n}}>1
\end{equation*}
Let $\set{a_{f(n)}}$ be a sub-sequence such that
\begin{equation*}
  \forall n,\sqrt[f(n)]{\abso{a_{f(n)}}}>1 
\end{equation*}
Which means
\begin{equation*}
\forall n, \abso{a_{f(n)}}>1^{f(n)}=1
\end{equation*}
Then it is impossible $\lim_{n\to\infty}a_n=0$, let alone $\sum_{n=1}^\infty a_n$ converge.
\end{proof}
\begin{theorem}
\label{4.6.2}
\textbf{(Ratio Test)} Given series $\sum a_n$. We have
\begin{enumerate}[label=(\alph*)]
  \item  $\limsup_{n\to\infty} \abso{\frac{a_{n+1}}{a_n}}<1\implies \sum a_n$ converge\\
  \item $\liminf_{n\to\infty} \abso{\frac{a_{n+1}}{a_n}}> 1\implies \sum a_n$ diverge
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\vi{
\begin{equation*}
\limsup_{n\to\infty} \abso{\frac{a_{n+1}}{a_n}}<1\implies \sum a_n\text{ converge }
\end{equation*}}
Let 
\begin{equation*}
\limsup_{n\to\infty} \abso{\frac{a_{n+1}}{a_n}}=\alpha <1
\end{equation*}
We know there exists $N$ that satisfy
 \begin{equation*}
\forall n\geq N, \abso{\frac{a_{n+1}}{a_n}}\leq \alpha <1
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \abso{a_n}\leq \alpha ^{n-N}\abso{a_N}
\end{equation*}
In other words,
\begin{equation*}
\forall k, \abso{a_{N+k}}\leq \alpha ^{k}\abso{a_N}
\end{equation*}
Then because
\begin{equation*}
\alpha <1\implies \sum_{k=1}^\infty \beta^{k} \abso{a_{N+k}}=\frac{\abso{a_N}\alpha  }{1-\alpha  }
\end{equation*}
By comparison test we know the series
\begin{equation*}
\sum_{k=1}^\infty a_{N+k}\text{ converges }
\end{equation*}
This tell us
\begin{equation*}
\sum a_n=\sum_{k=1}^N a_k+ \sum_{k=1}^\infty a_{N+k}\text{ converge }\vdone
\end{equation*}
We now prove 
\blue{
\begin{equation*}
\liminf_{n\to\infty} \abso{\frac{a_{n+1}}{a_n}}>1\implies \sum a_n\text{ diverge }
\end{equation*}}
Let 
\begin{equation*}
\liminf_{n\to\infty} \abso{\frac{a_{n+1}}{a_n}}=\beta >1
\end{equation*}
Then we know there exists $N$ that satisfy
 \begin{equation*}
\forall n\geq N, \abso{\frac{a_{n+1}}{a_n}}\geq \beta >1
\end{equation*}
This give us
\begin{equation*}
\forall k, \abso{a_{N+k}}\geq \beta^k \abso{a_N}
\end{equation*}
Then as 
\begin{equation*}
\beta >1\implies \lim_{k\to\infty}\beta ^k\abso{a_N}=\infty
\end{equation*}
We can see it is impossible $\lim_{n\to\infty}a_n=0$, let alone $\sum a_n$ converge. $\bdone$
\begin{theorem}
\label{4.6.3}
\textbf{(Root Test is Stronger than Ratio Test)} For positive sequence $\set{a_n}$, we have
\begin{equation*}
\liminf_{n\to\infty}\frac{a_{n+1}}{a_n}\leq \liminf_{n\to\infty} \sqrt[n]{a_n} \leq \limsup_{n\to\infty} \sqrt[n]{a_n} \leq \limsup_{n\to\infty} \frac{a_{n+1}}{a_n}
\end{equation*}
\end{theorem}
\begin{proof}
Clearly, 
\begin{equation*}
\liminf_{n\to\infty} \sqrt[n]{a_n}\leq \limsup_{n\to\infty} \sqrt[n]{a_n}  
\end{equation*}
We first prove
\begin{equation*}
  \vi{\liminf_{n\to\infty} \frac{a_{n+1}}{a_n}\leq \liminf_{n\to\infty} \sqrt[n]{a_n}}
\end{equation*}
Let 
\begin{equation*}
\alpha =\liminf_{n\to\infty} \frac{a_{n+1}}{a_n}
\end{equation*}
If $\alpha =-\infty$, the theorem hold true trivially. We only have to consider when $\alpha >-\infty$. Arbitrarily pick positive $\beta $ smaller than $\alpha $:
\begin{equation*}
\beta <\alpha=\liminf_{n\to\infty} \frac{a_{n+1}}{a_n}
\end{equation*}
Then we know there exists $N$ such that
 \begin{equation*}
\forall n\geq N, \frac{a_{n+1}}{a_n}>\beta 
\end{equation*}
This implies 
\begin{equation*}
\forall k, a_{N+k}>\beta^k a_N
\end{equation*}
Then for all $n>N$, we have
 \begin{equation*}
   \sqrt[n]{a_n}>\sqrt[n]{ \beta^{n-N}a_{N}}=\beta \sqrt[n]{\beta^{-N}a_N} 
\end{equation*}
Because 
\begin{equation*}
\lim_{n\to\infty}\beta \sqrt[n]{\beta^{-N}a_N}=\beta 
\end{equation*}
We see
\begin{equation*}
\liminf_{n\to\infty} \sqrt[n]{a_n} \geq \beta 
\end{equation*}
Then because $\beta<\alpha $ can be arbitrarily close to $\alpha $, we see
\begin{equation*}
\liminf_{n\to\infty} \sqrt[n]{a_n}  \geq \alpha \vdone
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce Dirichlet's Test
\end{minipage}}
\begin{lemma}
\label{4.6.4}
\textbf{(Abel's Formula)} Let
\begin{equation*}
A_n=\sum_{k=0}^n a_k\text{ and }q\geq p \geq 0
\end{equation*}
We have
\begin{equation*}
\sum_{n=p}^q a_nb_n=\sum_{n=p}^{q-1}A_n(b_n-b_{n+1})+A_qb_q-A_{p-1}b_p
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
  \sum_{n=p}^q a_nb_n&=\sum_{n=p}^q (A_{n}-A_{n-1})b_n\\
  &=\sum_{n=p}^q A_nb_n-\sum_{n=p-1}^{q-1}A_nb_{n+1}\\
  &=\sum_{n=p}^{q-1}A_n(b_n-b_{n+1})+A_qb_q-A_{p-1}b_p
\end{align*}
\end{proof}
\begin{theorem}
\label{4.6.5}
\textbf{(Dirichlet's Test)} 
Let $A_n=\sum_{k=0}^n a_k$. Given
\begin{gather*}
b_1\geq b_2\geq b_3\cdots\\
\forall n, b_n\geq 0\\
\set{A_n}\text{ is bounded  }
\end{gather*}
We have
\begin{gather*}
\sum_{n=0}^\infty a_nb_n\text{ converge }
\end{gather*}
\end{theorem}
\begin{proof}
Given $\epsilon $, there exists $N$ such that for all $n>N$, we have  $b_n<\frac{\epsilon }{2M}$. Let 
\begin{equation*}
q\geq p\geq N
\end{equation*}
We have
\begin{align*}
\abso{\sum_{n=p}^q a_nb_n}&=\abso{\sum_{n=p}^{q-1}A_n(b_n-b_{n+1})+A_qb_q-A_{p-1}b_p}\\
&\leq \sum_{n=p}^{q-1}\abso{A_n(b_n-b_{n+1})}+\abso{A_qb_q}+\abso{A_{p-1}b_p}\\
&=\sum_{n=p}^{q-1} \abso{A_n}\abso{b_n-b_{n+1}}+\abso{A_qb_q}+\abso{A_{p-1}b_p}\\
&=\sum_{n=p}^{q-1}\abso{A_n}(b_n-b_{n+1})+\abso{A_q}\abso{b_q}+\abso{A_{p-1}}\abso{b_p}\\
&\leq M(\sum_{n=p}^{q-1}(b_n-b_{n+1})+b_q+b_p)\\
&=M(b_p-b_q+b_q+b_p)=2Mb_p<2M \frac{\epsilon }{2M}=\epsilon 
\end{align*}
\end{proof}
\begin{theorem}
\label{4.6.6}
\textbf{(Abel's Test)}
Given 
\begin{gather*}
\sum_{n=1}^\infty a_n\text{ converges }\\
\set{b_n}\text{ is monotone and bounded }
\end{gather*}
We have
\begin{equation*}
\sum_{k=1}^\infty a_kb_k\text{ converges }
\end{equation*}
\end{theorem}
\begin{proof}
Define $b'_n:=b_n-b$. We have
 \begin{align*}
\sum_{k=1}^\infty &=\sum_{k=1}^\infty a_k(b+b'_k)\\
&=b\sum_{k=1}^\infty a_k+\sum_{k=1}^\infty a_kb'_k
\end{align*}
Where by  \myref{Theorem}{4.6.5} (Dirichlet's test), the series converge.
\end{proof}
\section{Cauchy Product and Mertens' Theorem}
\begin{definition}
\label{4.7.1}
\textbf{(Definition of Cauchy Product)} Given two power series $\sum_{n=0}^\infty a_nz^n$ and $\sum_{n=0}^\infty b_nz^n$. We say the Cauchy product of these two power series is 
\begin{equation*}
\sum_{n=0}^\infty c_nz^n\text{ where $c_n:=\sum_{k=0}^n a_kb_{n-k}$}
\end{equation*}
\end{definition}
\begin{theorem}
\label{4.7.2}
\textbf{(Example of $\sum_{n=0}^\infty c_n$ Diverge When  $\sum_{n=0}^\infty a_n,\sum_{n=0}^\infty b_n$ Both Converge)} Let 
\begin{equation*}
a_n:=\frac{(-1)^n}{\sqrt{n+1} }\text{ and }b_n:=\frac{(-1)^n}{\sqrt{n+1} }
\end{equation*}
We see 
\begin{equation*}
\sum_{n=0}^\infty a_n\text{ and }\sum_{n=0}^\infty b_n\text{ converge }
\end{equation*}
Yet 
\begin{equation*}
\sum_{n=0}^\infty c_n\text{ diverge }
\end{equation*}
\end{theorem}
\begin{proof}
It is easy to see $\sum_{n=0}^\infty a_n$ converge by Alternating Series Test. We prove 
\begin{equation*}
\vi{\abso{c_n}\not\to 0}
\end{equation*}
We now compute $c_n$ 
\begin{align*}
c_n&=\sum_{k=0}^n a_kb_{n-k}\\
&=\sum_{k=0}^n \frac{(-1)^k}{\sqrt{k+1} } \times \frac{(-1)^{n-k}}{\sqrt{ n-k+1} }\\
&=(-1)^n \sum_{k=0}^n \frac{1}{\sqrt{(k+1)(n-k+1)} }
\end{align*}
Observe
\begin{equation*}
  (n-k+1)(k+1)=(\frac{n}{2}+1)^2-(\frac{n}{2}-k)^2\leq (\frac{n}{2}+1)^2
\end{equation*}
Notice that one can verify $(\frac{n}{2}+1)^2-(\frac{n}{2}-k)^2$ is always positive. The observation give us 
\begin{equation*}
\sqrt{(n-k+1)(k+1)} < (\frac{n}{2}+1)
\end{equation*}
We now have
\begin{equation*}
\abso{c_n}=\sum_{k=0}^n \frac{1}{\sqrt{(k+1)(n-k+1)} }\geq \sum_{k=0}^n \frac{1}{\frac{n}{2}+1}=\frac{2n+2}{n+2}\vdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{4.7.3}
\textbf{(Merten's Theorem)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $\sum_{n=0}^\infty a_n$ converges absolutely.
  \item $\sum_{n=0}^\infty a_n=A$ 
  \item $\sum_{n=0}^\infty b_n=B$
\end{enumerate}
When $z=1$,  the Cauchy product of the two power series $\sum_{n=0}^\infty a_nz^n$ and $\sum_{n=0}^\infty b_nz^n$  converge to $AB$
 \begin{equation*}
\sum_{n=0}^\infty c_n=AB
\end{equation*}
\end{theorem}
\begin{proof}
Define 
\begin{enumerate}[label=(\alph*)]
  \item $A_n:=\sum_{k=0}^n a_k$ 
  \item $B_n:=\sum_{k=0}^n b_k$
  \item $C_n:=\sum_{k=0}^n c_k$
  \item $\gamma_n := C_n-A_nB$
  \item $\beta_n:=B_n-B$
\end{enumerate}
In this setting, because $A_n \to A$, to show $C_n\to AB$, we only have to show 
\begin{equation*}
  \vi{ \gamma_n \to 0}
\end{equation*}
We first compute 
\begin{align*}
C_n&=\sum_{u=0}^n c_u\\
&= \sum_{u=0}^n \sum_{k=0}^u a_k b_{u-k}\\
&= \sum_{0\leq k\leq u\leq n} a_kb_{u-k}\\
&= \sum_{k=0}^n a_k \sum_{u=k}^n b_{u-k}\\
&=\sum_{k=0}^n a_k \sum_{s=0}^{n-k} b_s\\
&=\sum_{k=0}^n a_k B_{n-k}\\
&=\sum_{k=0}^n a_k (B+\beta_{n-k})\\
&=A_nB+\sum_{k=0}^n a_k\beta_{n-k}
\end{align*}
Then we have 
\begin{equation*}
\gamma_n=\sum_{k=0}^n \beta_k a_{n-k}
\end{equation*}
Let 
\begin{equation*}
\alpha =\sum_{n=0}^\infty \abso{a_n}
\end{equation*}
By \myref{Theorem}{4.4.15}, 
Fix $\epsilon $. It remains to show
\begin{equation*}
\limsup_{n\to\infty} \abso{\gamma_n}\leq \epsilon \alpha 
\end{equation*}
Notice that $\beta_k \to 0$. Then we can choose $N$ such that  $\forall k>N, \abso{\beta _k}\leq \epsilon $. Let $n>N$. We now have 
 \begin{align*}
\abso{\gamma_n}:&=\abso{\sum_{k=0}^n  \beta_k a_{n-k}}\\
&\leq \abso{\sum_{k=0}^N \beta_k a_{n-k}}+\abso{\sum_{k=N+1}^n \beta_k a_{n-k}}\\
&\leq \abso{\sum_{k=0}^N \beta _k a_{n-k}}+ \sum_{k=N+1}^n \abso{\beta_k} \abso{a_{n-k}}\\
&\leq \abso{\sum_{k=0}^N \beta_k a_{n-k}}+\sum_{k=N+1}^n \epsilon \abso{a_{n-k}}\\
&\leq \abso{\sum_{k=0}^N \beta_k a_{n-k}}+\epsilon \alpha 
\end{align*}
We only have to prove 
\begin{equation*}
\sum_{k=0}^N \beta_k a_{n-k} \to 0\text{ as $n\to \infty$ }
\end{equation*}
This is obvious, since there are only $N$ terms and we have  $a_n \to 0\vdone$. 
\end{proof}
\section{Reimann Rearrangement Theorem} 
\begin{definition}
\label{4.8.1}
\textbf{(Permutation)} We say $\sigma$ is a permutation of $X$ if  $\sigma:X\rightarrow X$ is bijective.
\end{definition}
\begin{definition}
\label{4.8.2}
\textbf{(Converge Conditionally)} We say a series $\sum_{n=0}^\infty a_n$ converge conditionally if 
\begin{equation*}
\sum_{n=0}^\infty a_n\text{ converge }\text{ and }\sum_{n=0}^\infty \abso{a_n}\text{ diverge }
\end{equation*}
\end{definition}
\begin{theorem}
\label{4.8.3}
\textbf{(Reimann Rearrangement Theorem)} If  $\sum_{n=0}^\infty a_n$ converge conditionally, then for all $M\inr$, there exists a permutation $\sigma_M$ such that 
\begin{equation*}
\sum_{n=0}^\infty a_{\sigma_M(n)}=M
\end{equation*}
There also exists permutation $\sigma_\infty,\sigma_{-\infty},\sigma'$ such that 
\begin{equation*}
\sum_{n=0}^\infty a_{\sigma_\infty (n)}=\infty\text{ and }\sum_{n=0}^\infty a_{\sigma_{-\infty}(n)}=-\infty\text{ and }\sum_{n=0}^\infty a_{\sigma'(n)}\text{ diverge }
\end{equation*}
\end{theorem}
\begin{proof}
Define 
\begin{equation*}
a_n^+:=\max \set{a_n,0}\text{ and }a_n^-:=\min \set{a_n,0}
\end{equation*}
Notice that 
\begin{equation*}
\sum_{n=0}^\infty a_n=\sum_{n=0}^\infty a_n^++a_n^-\text{ and }\sum_{n=0}^\infty \abso{a_n}=\sum_{n=0}^\infty a_n^+-a_n^-
\end{equation*}
Then because $\sum_{n=0}^\infty a_n$ converge conditionally, we know  
\begin{equation*}
\sum_{n=0}^\infty a_n^+\text{ diverge }\text{ and }\sum_{n=0}^\infty a_n^-\text{ diverge }
\end{equation*}
Then because their positiveness, we know 
\begin{equation*}
\sum_{n=0}^\infty a_n^+=\infty\text{ and }\sum_{n=0}^\infty a_n^-=-\infty
\end{equation*}
Let $p_1$ be the smallest natural such that 
 \begin{equation*}
\sum_{n=0}^{p_1} a_n^+>M
\end{equation*}
Let $p_2$ be the smallest natural such that 
 \begin{equation*}
\sum_{n=0}^{p_1}a_n^+ +\sum _{n=0}^{q_1}a_n^- <M
\end{equation*}
Let $p_2$ be the smallest natural such that 
 \begin{equation*}
\sum_{n=0}^{p_2} a_n^+ + \sum_{n=0}^{q_1} a_n^- =\sum_{n=0}^{p_1} a_n^+ + \sum_{n=0}^{q_1} a_n^- +\sum_{n=p_1+1}^{p-2} a_n^+ >M 
\end{equation*}
This is possible because $\sum_{n=p_1+1}^{\infty}a_n^+=\infty$. Recursively do such. Define $p_0=-1=q_0$.  Define 
\begin{equation*}
\alpha_n:=\abso{\set{k\inn: p_{n-1}<k\leq p_n,a_k> 0}}
\end{equation*}
and define 
\begin{equation*}
\beta _n:=\abso{\set{k\inn: q_{n-1}<k\leq q_n,a_k<0}}
\end{equation*}
Notice that we don't have to consider when $a_k=0$, since we can add finite term or a sub-sequence of $0$ to make  $\sigma$ actually a permutation.\\

We now recursively define $\sigma$ by 
\begin{equation*}
  a_0^+,\dots ,a_{p_1}^+, a_{0}^-,\dots ,a_{q_1}
\end{equation*}


\end{proof}

\chapter{Calculus and Relevant Notions}
\section{Derivative}
\begin{definition}
\label{5.1.1}
\textbf{(Definition of Frchet derivative)} Given two normed vector space $V,W$, an open set $S\subseteq V$ and a function  $f:S\subseteq V\rightarrow W$, we say $f$ is differentiable at  $\vecta{x}\in S$ if there exists a bounded linear operator $A:V\rightarrow W$ such that 
\begin{align*}
 \frac{\norm{f(\vecta{x}+\vecta{h})-f(\vecta{x})-A(\vecta{h})}_W}{\norm{\vecta{h}}_V} \to 0 \text{ as  }\norm{\vecta{h}}_V \to 0
\end{align*}
We say $A$ is the differential or derivative of  $f$ at  $\vecta{x}$
\end{definition}
\fbox{\begin{minipage}{39em}
Normally, we write the derivative of $f$ at  $x$ by  $\partial f(x)$
\end{minipage}}
\begin{theorem}
\label{5.1.2}
\textbf{(Equivalent Definition of Frchet derivative)} The following are equivalent 
\begin{enumerate}[label=(\alph*)]
  \item $A$ is the derivative of  $f$ at  $x$
  \item  $\frac{f(x+h)-f(x)}{h}=f'(x)+o(h)$ 
\end{enumerate}
\end{theorem}
\begin{proof}
Notice 
\begin{align*}
f(x+h)-f(x)-Ah =\norm{h}_V o(h)
\end{align*}
\end{proof}
\begin{theorem}
\label{5.1.3}
\textbf{(Uniqueness of Frchet derivative)} 
\begin{align*}
A,B\text{ are both derivative of $f$ at $\vecta{x}$}\implies A=B
\end{align*}
\end{theorem}
\begin{proof}
Notice that the definition can be re written into 
\begin{align*}
\frac{f(\vecta{x}+\vecta{h})-f(\vecta{x})-A(\vecta{h})}{\norm{\vecta{h}}_V} \to \vecta{0}
\end{align*}
Then by \myref{Theorem}{4.2.1}, we have 
\begin{align*}
\frac{(A-B)\vecta{h}}{\norm{\vecta{h}}_V}\to 0
\end{align*}
\As{$\exists \vecta{v}, A\vecta{v}\neq B\vecta{v}$}. Notice 
\begin{align*}
  \frac{(A-B) \frac{\vecta{v}}{c}}{\norm{\frac{\vecta{v}}{c}}_V}\to \frac{(A-B)(\vecta{v})}{\norm{\vecta{v}}_V} \neq 0 \text{ as $\frac{\vecta{v}}{c} \to 0$ }\tCaC
\end{align*}
\end{proof}
\begin{theorem}
\label{5.1.4}
\textbf{(Addition of Frchet derivative)} Given two functions $f,g:S\subseteq V\rightarrow W$ that is differentiable at $\vecta{x}$ with derivative $A$ and  $B$ 
 \begin{align*}
\text{ The derivative of $f+g$ at  $\vecta{x}$ is }A+B
\end{align*}
\end{theorem}
\begin{proof}
Observe 
\begin{align*}
  &\frac{\norm{(f+g)(\vecta{x}+\vecta{h})-(f+g)\vecta{x}-(A+B)\vecta{h}}_W}{\norm{\vecta{h}}_V}\\
  &\leq \frac{\norm{f(\vecta{x}+\vecta{h})-f(\vecta{x})-A \vecta{h}}_W}{\norm{\vecta{h}}_V}+\frac{\norm{g(\vecta{x}+\vecta{h})-g(\vecta{x})-B\vecta{h}}_W}{\norm{\vecta{h}}_V} \to 0 
\end{align*}
\end{proof}
\begin{theorem}
\label{5.1.5}
\textbf{(Chain Rule in Normed Space Space)}  
\begin{align*}
\partial (g\circ f)(x_0)=\partial g(f(x_0)) \circ \partial f(x_0)
\end{align*}
\end{theorem}
\begin{proof}
Denote 
\begin{enumerate}[label=(\alph*)]
  \item $y=f(x)$ 
  \item  $\tilde{f}(x)=f(x+x_0)$ 
  \item $\hat{f}(x)=\tilde{f}(x)-\tilde{f}(0)$
  \item $\tilde{g}(y)=g(y+\tilde{f}(0))$
\end{enumerate}
  We first prove that \vi{if the chain rule hold for $x=0$, it hold true for all $x\in X$}.\\

Because 
\begin{align*}
\frac{g \circ f (x_0+h)-g \circ f(x_0)-\partial (g \circ f)x_0}{\norm{h}_V}\to 0
\end{align*}
and 
\begin{align*}
\frac{g \circ  \tilde {f} (h)- g \circ  \tilde{f}(0)-\partial (g\circ  \tilde{f})(0)}{\norm{h}_V}\to 0
\end{align*}
and 
\begin{align*}
\tilde{f}(h)=f(x_0+h)\text{ and }\tilde{f}(0)=f(x_0)
\end{align*}
we know 
\begin{align*}
\partial (g \circ  f)(x_0)= \partial (g\circ \tilde{f} )(0)
\end{align*}
Then we can compute 
\begin{align*}
\partial (g \circ f)(x_0)=\partial (g\circ  \tilde{f} )(0)
\end{align*}

\end{proof}
\begin{theorem}
\label{5.1.6}
\textbf{(Differentiable Implies Continuous)} Given $f:\R\rightarrow \R$
\begin{align*}
f\text{ is differentiable at $x$ }\implies f\text{ is continuous at $x$ }
\end{align*}
\end{theorem}
\begin{proof}
\As{$x_n \to x$ and $f(x_n)\to y\neq x$}.
\begin{align*}
\frac{f(x_n)-f(x)}{x_n-x}\to \pm \infty \tCaC
\end{align*}
\end{proof}



\section{Discontinuity and Convex}
\fbox{\begin{minipage}{39em}
In this section, $\ld ^c$ is defined to be $1-\ld $. 
\end{minipage}}
\begin{definition}
\label{5.2.1}
\textbf{(Definition of Line Segment)} Suppose $\vecta{a},\vecta{b}\in \R^n$. We define the line segment $\overline{AB}$ by 
\begin{align*}
  \overline{AB}=\set{\ld  \vecta{a}+\ld ^c\vecta{b}\inr^n:\ld  \in [0,1]}
\end{align*}
\end{definition}
\begin{definition}
\label{5.2.2}
\textbf{(Definition of Convex Set)} We say a set $X$ in  $\R^n$ is convex if
\begin{align*}
\forall \vecta{a},\vecta{b}\in X,\overline{AB}\subseteq X
\end{align*}
\end{definition}
\begin{theorem}
\label{5.2.3}
\textbf{(Convex Set is Path Connected)} 
\begin{align*}
X\text{ is convex }\implies X\text{ is path-connected }
\end{align*}
\end{theorem}
\begin{proof}
The function $f:[0,1]\rightarrow X$ defined by
\begin{align*}
f(\ld )=\ld \vecta{a}+\ld ^c\vecta{b}
\end{align*}
is clearly a path from $a$ to  $b$, as we can prove it is continuous at each component. 
\end{proof}
\begin{definition}
\label{5.2.4}
\textbf{(Definition of Convex Function)} Let $X\subseteq \R^n$ be convex. We say a function  $f:X\rightarrow \R$ is convex if 
\begin{align*}
\forall \vecta{a},\vecta{b}\in X, \forall \ld \in [0,1], f(\ld \vecta{a}+ \ld ^c\vecta{b})\leq \ld  f(\vecta{a})+\ld ^cf(\vecta{b})
\end{align*}
\end{definition}
\begin{definition}
\label{5.2.5}
\textbf{(Definition of Mid-Point Convex)} Let $X\subseteq \R^n$ be convex. We say $f:X\rightarrow \R$ is mid-point convex if 
\begin{align*}
\forall \vecta{a},\vecta{b}\in X, f(\frac{\vecta{a}+\vecta{b}}{2})\leq \frac{f(\vecta{a})+f(\vecta{b})}{2}
\end{align*}
\end{definition}
\begin{theorem}
\label{5.2.6}
\textbf{(Equivalent Definition of Convex Function)} Given a function $f:\R^n\rightarrow \R$. The following are equivalent 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is convex
  \item The epigraph $\set{(\vecta{x},y)\inr^{n+1}:\vecta{x}\in X\text{ and }y\geq f(\vecta{x})}$ is convex 
\end{enumerate}
\end{theorem}
\begin{proof}
We now prove  \vi{$(a)\implies (b)$}.\\
 
Denote 
\begin{align*}
Y=\set{(\vecta{x},y)\inr^{n+1}:\vecta{x}\in X\text{ and }y\geq f(\vecta{x})}
\end{align*} 
\As{$Y$ is not convex}. That is, there exists two points $(\vecta{x},y_x),(\vecta{z},y_z)\in Y$ such that 
\begin{align*}
\set{\ld (\vecta{x},y_\vecta{x})+\ld ^c(\vecta{z},y_\vecta{z})\inr^{n+1}:\ld  \in [0,1]}\not \subseteq Y
\end{align*}
Fix the $\ld +\ld ^c=1$ such that 
\begin{align*}
  (\ld \vecta{x}+\ld ^c\vecta{z}, \ld y_{\vecta{x}}+\ld ^cy_{\vecta{z}})=\ld (\vecta{x},y_{\vecta{x}})+\ld ^c(\vecta{z},y_{\vecta{z}})\not\in Y
\end{align*}
Because $X$ is convex, we know 
 \begin{align*}
\ld \vecta{x}+\ld ^c\vecta{z} \in X
\end{align*}
Then because $(\ld \vecta{x}+\ld ^c\vecta{z},\ld y_{\vecta{x}}+\ld ^cy_{\vecta{z}})\not\in Y$, by definition of $Y$, we can deduce 
\begin{align*}
\ld f(\vecta{x})+\ld ^c f(y_{\vecta{z}})\leq \ld  y_{\vecta{x}}+\ld ^cy_{\vecta{z}}<f(\ld \vecta{x}+\ld ^c\vecta{z})\tCaC f\text{ is convex }\vdone
\end{align*}
We now prove  \blue{$(b)\implies (a)$}. We first prove  


\end{proof}
\fbox{\begin{minipage}{39em}
Look at the following figure. 
\end{minipage}}

\begin{theorem}
\label{5.2.7}
\textbf{(Mid Point Convex and Continuity Implies Convex)} 
\begin{align*}
f\text{ is mid point convex and continuous }\implies f\text{ is convex }
\end{align*}
\end{theorem}
\begin{proof}
\As{ $f$ is not convex}. Then there exists $\vecta{x},\vecta{y}$ such that if we define $g:[0,1]\rightarrow \R$ by 
\begin{align*}
g(\ld ):=f(\ld \vecta{x}+\ld ^c \vecta{y})- \big(\ld  f(\vecta{x})+\ld ^c f(\vecta{y})\big)
\end{align*}
there exists some $\ld$ such that $g(\ld )>0$. Notice that $g$ is continuous and  $[0,1]$ is compact, so we know there exists maximum 
\begin{align*}
\max_{\ld \in [0,1]} g(\ld )=M
\end{align*}
Again, because $g$ is continuous, we then know there exists a minimum $\ld_0$ such that 
\begin{align*}
\ld_0 =\min \set{\ld \in [0,1]:g(\ld )=M}
\end{align*}
Let $\delta$ be small enough such that 
\begin{align*}
\ld _0 \in (\ld _0-\delta,\ld _0+\delta)\subseteq (0,1)
\end{align*}
Define 
\begin{align*}
  \overline{\vecta{x}}:&=(\ld _0-\delta)\vecta{x}+ (\ld _0^c+\delta)\vecta{y}\\
\overline{\vecta{y}}:&= (\ld _0 +\delta)\vecta{x}+(\ld _0^c-\delta)\vecta{y}
\end{align*}
It is easy to check $\overline{\vecta{x}},\overline{\vecta{y}}$ are in the geodesic from $\vecta{x}$ to $\vecta{y}$. Then because $f$ is mid point convex, we can deduce 
 \begin{align*}
f(\ld_0 ^c \vecta{x}+\ld_0 \vecta{y})&= f(\frac{\overline{\vecta{x}}+\overline{\vecta{y}}}{2})\leq \frac{f(\overline{\vecta{x}})+f(\overline{\vecta{y}})}{2}
\end{align*}
Observe 
\begin{align*}
f(\overline{\vecta{x}})&=f((\ld _0-\delta)\vecta{x}+(\ld _0^c +\delta)\vecta{y})\\
&=g(\ld_0-\delta)+(\ld _0-\delta) f(\vecta{x})+(\ld _0^c+\delta) f(\vecta{y})
\end{align*}
and 
\begin{align*}
f(\overline{\vecta{y}})&=f((\ld _0+\delta) \vecta{x}+(\ld _0^c-\delta)\vecta{y})\\
&=g(\ld _0+\delta)+(\ld _0+\delta)f(\vecta{x})+(\ld _0^c-\delta)f(\vecta{y})
\end{align*}
This tell us 
\begin{align*}
\frac{f(\overline{\vecta{x}})+f(\overline{\vecta{y}})}{2}=\frac{g(\ld _0-\delta)+g(\ld _0+\delta)}{2}+ \ld_0 f(\vecta{x})+\ld _0^c f(\vecta{y})
\end{align*}
Also we compute 
\begin{align*}
f(\frac{\overline{\vecta{x}}+\overline{\vecta{y}}}{2})&=f(\ld _0\vecta{x}+\ld _0^c \vecta{y})\\
&=g(\ld _0)+ \ld _0 f(\vecta{x})+\ld _0^c f(\vecta{y})
\end{align*}
Then because $f$ is mid point convex, we can deduce 
\begin{align*}
g(\ld _0)+\ld _0 f(\vecta{x})+\ld _0^c f(\vecta{y})&=f(\frac{\overline{\vecta{x}}+\overline{\vecta{y}}}{2})\\
&\leq \frac{f(\overline{\vecta{x}})+f(\overline{\vecta{y}})}{2}=\frac{g(\ld _0-\delta)+g(\ld _0+\delta)}{2}+ \ld _0 f(\vecta{x})+\ld _0^c f(\vecta{y})
\end{align*}
This give us 
\begin{align*}
M=g(\ld _0)\leq \frac{g(\ld _0-\delta)+g(\ld _0+\delta)}{2}
\end{align*}
which is impossible $\tCaC$, since $M$ is the greatest of  $g(\ld )$, where $g(\ld _0-\delta)<M$ and $g$ is continuous.
\end{proof}
\begin{lemma}
\label{5.2.8}
Given 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is convex on $(a,b)$ 
  \item $a<s<t<u<b$
\end{enumerate}
We have 
\begin{align*}
\frac{f(t)-f(s)}{t-s}\leq \frac{f(u)-f(s)}{u-s}\leq \frac{f(u)-f(t)}{u-t}
\end{align*}
\end{lemma}
\begin{proof}
We first prove 
\begin{align*}
\vi{\frac{f(t)-f(s)}{t-s}\leq \frac{f(u)-f(s)}{u-s}}
\end{align*}
Observe 
\begin{align*}
  \frac{f(t)-f(s)}{t-s}\leq \frac{f(u)-f(s)}{u-s}&\iff  f(t)-f(s)\leq \frac{t-s}{u-s}f(u)-\frac{t-s}{u-s}f(s)\\
&\iff f(t)\leq \frac{t-s}{u-s}f(u)+\frac{u-t}{t-s}f(s)
\end{align*}
Notice that 
\begin{align*}
\frac{t-s}{u-s}+\frac{u-t}{t-s}=\frac{u-s}{u-s}=1\text{ and }\frac{t-s}{u-s}\in (0,1)
\end{align*}
The result then follow from definition of convex  $\vdone$\\

We now prove 
\begin{align*}
\blue{\frac{f(u)-f(s)}{u-s}\leq \frac{f(u)-f(t)}{u-t}}
\end{align*}
Observe 
\begin{align*}
  \frac{f(u)-f(s)}{u-s}\leq \frac{f(u)-f(t)}{u-t}&\iff \frac{u-t}{u-s}f(u)-\frac{u-t}{u-s}f(s)\leq f(u)-f(t)\\
  &\iff f(t)\leq (1-\frac{u-t}{u-s})f(u)+\frac{u-t}{u-s}f(s)
\end{align*}
Then because $\frac{u-t}{u-s}\in (0,1)$, the result again follows from definition of convex $\bdone$
\end{proof}
\begin{theorem}
\label{5.2.9}
\textbf{(Convex Function is Continuous on Open Interval)} Given $f:I\subseteq \R\rightarrow \R$
\begin{align*}
f\text{ is convex on $(a,b)$}\implies f\text{ is continuous on $(a,b)$ }
\end{align*}
\end{theorem}
\begin{proof}
Let $y\in (a,b)$. We wish to prove 
\begin{align*}
\vi{\lim_{x\to y^-}f(x)=f(y)=\lim_{x\to y^+}f(x)}
\end{align*}
Let 
\begin{align*}
0<x<y<z<1
\end{align*}
For small $\delta$, by \myref{Lemma}{5.2.8}, we have 
\begin{align*}
\frac{f(y)-f(x)}{y-x}\leq \frac{f(y+\delta)-f(y)}{\delta}\leq \frac{f(z)-f(y)}{z-y}
\end{align*}
Then 
\begin{align*}
f(y+\delta)-f(y)\in [\frac{f(y)-f(x)}{y-x}\delta,\frac{f(z)-f(y)}{z-y}\delta]\to [0,0]\text{ as $\delta \to 0$ }
\end{align*}
This shows that $\lim_{x\to y^+}f(x)=f(y)$. Again, for samll $\delta$, by \myref{Lemma}{5.2.8}, we have 
\begin{align*}
\frac{f(y)-f(x)}{y-x} \leq \frac{f(y)-f(y-\delta)}{\delta} \leq \frac{f(z)-f(y)}{z-y}
\end{align*}
Then again for the same reason, $f(y)-f(y-\delta)\to 0\vdone$.
\end{proof}
\section{Uniformly Continuous}

\begin{definition}
\label{5.3.1}
\textbf{(Uniformly Continuous)} We say a function $f:X\rightarrow Y$ is uniformly continuous if 
\begin{equation*}
\forall \epsilon, \exists \delta, d(p,q)<\delta \implies d(f(p),f(q))<\epsilon 
\end{equation*}
\end{definition}
\begin{theorem}
\label{5.3.2}
\textbf{(Sequential Definition of Uniform Continuous)} The following are equivalent
\begin{enumerate}[label=(\alph*)]
  \item $f$ is uniformly continuous
  \item $\lim_{n\to \infty}d(p_n,q_n)=0 \implies \lim_{n\to \infty}d(f(p_n),f(q_n))=0$ 
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove
\begin{equation*}
  \vi{f\text{  is uniformly continuous }\implies \big(\lim_{n\to\infty}d(p_n,q_n)=0\implies \lim_{n\to\infty}d(f(p_n),f(q_n))=0\big)}
\end{equation*}
Fix $\epsilon $. Let  $\delta$ be from uniform continuity of $f$. Because $\lim_{n\to\infty}d(p_n,q_n)=0$, we know there exists $N$ such that 
 \begin{equation*}
\forall n>N, d(p_n,q_n)<\delta
\end{equation*}
This implies 
\begin{equation*}
\forall n>N, d(f(p_n),f(q_n))<\epsilon \vdone
\end{equation*}
We now prove
\begin{equation*}
\blue{\big( \lim_{n\to\infty} d(p_n,q_n)=0 \implies \lim_{n\to\infty}d(f(p_n),f(q_n))=0 \big) \implies f\text{ is uniformly continuous }}
\end{equation*}
\As{$f$ is not uniformly continuous}. Fix $\epsilon $. We know there exists a sequence 
\begin{equation*}
\lim_{n\to \infty}d(p_n,q_n)
\end{equation*}
such that 
\begin{equation*}
\forall n, d(f(p_n),f(q_n))>\epsilon \tCaC \bdone
\end{equation*}





\end{proof}
\begin{theorem}
\label{5.3.3}
\textbf{(Uniformly Continuous is Stronger than Continuous)} Given $f:X\rightarrow Y$
\begin{equation*}
f\text{ is uniformly continuous }\implies f\text{ is continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Arbitrarily pick $x\in X$ and $\epsilon \inr^+$. We wish to prove there exists $\delta$ such that 
\begin{equation*}
p \in B_{\delta}(x)\implies f(p)\in B_{\epsilon }(f(x))
\end{equation*}
Because $f$ is uniformly continuous, we know there exists $\delta'$ such that 
\begin{equation*}
d(p,q)<\delta'\implies d(f(p),f(q))<\epsilon 
\end{equation*}
Then we can deduce 
\begin{equation*}
p\in B_{\delta'}(x)\implies d(p,x)<\delta ' \implies d(f(p),f(x))<\epsilon  \implies f(p)\in B_{\epsilon }(f(x))
\end{equation*}
We have proved $\delta'$ is the desired $\delta$. 
\end{proof}
\begin{theorem}
\label{5.3.4}
\textbf{(Uniformly Continuous is Strictly Stronger than Continuous)} Define $f:\R\rightarrow \R$ by
\begin{equation*}
f(x)=x^2
\end{equation*}
We have 
\begin{equation*}
f\text{ is continuous but not uniformly continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Let $a\inr$. We wish to prove $f$ is continuous at  $a$. Fix  $\epsilon $. Observe 
\begin{align*}
  \abso{x^2-a^2}&=\abso{x^2-ax+ax-a^2}\\
  &\leq \abso{x^2-ax}+\abso{ax-a^2}\\
  &=\abso{x}\times\abso{x-a}+\abso{a}\times \abso{x-a}\\
  &=(\abso{x}+\abso{a})\abso{x-a}\\
  &=(\abso{x-a+a}+\abso{a})\abso{x-a}\\
  &\leq (\abso{x-a}+2\abso{a})\abso{x-a}\\
  &=\abso{x-a}^2 +2 \abso{a}\times \abso{x-a}
\end{align*}
Then if we let
\begin{equation*}
  \delta< \min \set{\frac{\epsilon }{4\abso{a}},\sqrt{\frac{\epsilon}{2}} }
\end{equation*}
we see 
\begin{equation*}
\abso{x-a}<\delta \implies \abso{x^2-a^2}\leq \abso{x-a}^2 + 2\abso{a}\times \abso{x-a}\leq \frac{\epsilon}{2}+ \frac{\epsilon}{2}=\epsilon 
\end{equation*}
We now prove $f$ is not uniformly continuous. Arbitrarily pick $\delta\inr^+$, we wish to find a pair of real number $b,c$ such that $d(b,c)<\delta$ but $d(f(b),f(c))>1$.\\

Observe
\begin{equation*}
d(f(b),f(c))=\abso{f(c)-f(b)}=\abso{c^2-b^2}=\abso{c-b}\times \abso{c+b}
\end{equation*}
Then  we can let $b=\frac{2}{\delta}$ and $c=\frac{2}{\delta}+\frac{\delta}{2}$, and we have
\begin{equation*}
d(b,c)=\frac{\delta}{2}<\delta
\end{equation*}
and
\begin{equation*}
d(f(b),f(c))=\abso{c-b}\times \abso{c+b}=\frac{\delta}{2}\times (\frac{4}{\delta}+\frac{\delta}{2})=2+\frac{\delta^2}{4}>1
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now prove Heine-Cantor Theorem.
\end{minipage}}
\begin{lemma}
\label{5.3.5}
  \textbf{(Lebesgue Number Covering Lemma)}
\end{lemma}
\begin{theorem}
\label{5.3.6}
\textbf{(Linear Function is Uniformly Continuous)} Define $f:\R\rightarrow \R$ by 
\begin{equation*}
f(x)=ax+b
\end{equation*}
Then 
\begin{equation*}
f\text{ is uniformly continuous on every subset of $\R$}
\end{equation*}
\end{theorem}
\begin{proof}
It is cleat that $\delta=\frac{\epsilon}{a}$ works. 
\end{proof}
\begin{theorem}
\label{5.3.7}
\textbf{(Uniformly Continuous Function Need not be Differentiable)} Define $f:\R\rightarrow \R$ by 
\begin{equation*}
f(x)=\abso{x}
\end{equation*}
Then 
\begin{equation*}
f\text{ is uniformly continuous but not differentaible everywhere}
\end{equation*}
\end{theorem}
\begin{proof}
Clearly, $f$ is not differentiable at $x=0$. We now prove $\delta=\epsilon $ works. Let $0<h<\delta$. If $a\geq 0$, we see 
\begin{equation*}
\abso{f(a+h)-f(a)}=\abso{\abso{a+h}-\abso{a}}=h<\delta=\epsilon 
\end{equation*}
If $a\leq -\delta$, we see 
\begin{equation*}
\abso{f(a+h)-f(a)}=\abso{(-a-h)-(-a)}=h<\delta=\epsilon 
\end{equation*}
If $-\delta<a<0$, there are two possibilities $a+h>0$ or  $a+h\leq 0$. It the former
\begin{equation*}
\abso{f(a+h)-f(a)}=\abso{\abso{a+h}+a}=\abso{2a+h}=2a+h<h<\delta=\epsilon 
\end{equation*}
If the latter 
\begin{equation*}
\abso{f(a+h)-f(a)}=\abso{\abso{a+h}-\abso{a}}=\abso{-h}=h<\delta=\epsilon 
\end{equation*}
\end{proof}
\begin{theorem}
\label{5.3.8}
\textbf{(Addition of Uniform Continuous Functions is Uniform Continuous)} Given $f:X\rightarrow \R$ and $g:X\rightarrow \R$
\begin{equation*}
f,g\text{ are both uniform continuous }\implies f+g\text{ is uniform continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. Let 
\begin{equation*}
\delta= \min \set{\delta_f(\frac{\epsilon}{2}),\delta_g(\frac{\epsilon}{2})}
\end{equation*}
We now prove such $\delta$ works. Suppose $d(p,q)<\delta$. We have 
\begin{equation*}
d(p,q)<\delta_f(\frac{\epsilon}{2})\text{ and }\delta_g(\frac{\epsilon}{2})
\end{equation*}
which give us 
\begin{equation*}
\abso{f(p)-f(q)}< \frac{\epsilon}{2}\text{ and }\abso{g(p)-g(q)}<\frac{\epsilon}{2}
\end{equation*}
Then we see 
\begin{align*}
\abso{(f+g)(p)-(f+g)(q)}&=\abso{f(p)-f(q)+g(p)-g(q)}\\
&\leq \abso{f(p)-f(q)}+\abso{g(p)-g(q)}\\
&<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon 
\end{align*}




\end{proof}
\fbox{\begin{minipage}{39em}
Lastly, we introduce Cauchy continuous. 
\end{minipage}}
\begin{theorem}
\label{5.3.9}
\textbf{(Cauchy Continuous)} Given $f:A\subseteq X\rightarrow Y$. Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is uniformly continuous
  \item $A$ is dense in  $X$ 
  \item $Y$ is complete
\end{enumerate}
Then
\begin{equation*}
\text{ There exists a unique continuous extension $\overline{f}$ on $X$ }
\end{equation*}
\end{theorem}
\begin{proof}
Define 
\begin{equation*}
\overline{f}(x)=\lim_{n\to \infty}f(x_n)\text{ where $x_n\to x$}
\end{equation*}
We first prove  \vi{$\overline{f}$ is well defined}.\\

\As{$x_n\to x\text{ and }y_n\to x\text{ and }\lim_{n\to\infty}f(x_n)\neq \lim_{n\to \infty}f(y_n)$}. Let 
 \begin{equation*}
p=\lim_{n\to\infty}f(x_n)\text{ and }q=\lim_{n\to\infty}f(y_n)\text{ and }\alpha =d(p,q)
\end{equation*}
Let $N$ satisfy 
 \begin{equation*}
\forall n>N, f(x_n)\in B_{\frac{\alpha}{3}}(p)\text{ and }f(y_n)\in B_{\frac{\alpha}{3}}(q)
\end{equation*}
Let $\delta=\delta(\frac{\alpha}{3})$, and let $N_1$ satisfy 
 \begin{equation*}
n>N_1\implies x_n,y_n\in B_{\frac{\delta}{2}}(x)
\end{equation*}
Let $m>\max \set{N,N_1}$. We now have
\begin{enumerate}[label=(\alph*)]
  \item $d(f(x_m),p)<\frac{\alpha}{3}$ 
  \item $d(f(y_m),q)<\frac{\alpha}{3}$ 
  \item $d(f(x_m),f(y_m))<\frac{\alpha}{3}$
\end{enumerate}
Then 
\begin{equation*}
\alpha =d(p,q)<\alpha \tCaC \vdone
\end{equation*}
We now prove \blue{$\overline{f}$ is defined every where on  $X$}.\\

It is clear that $\overline{f}$ is defined every where on  $A$, since if  $a\in A$, we can let $a_n=a$ and see 
\begin{equation*}
\overline{f}(a)=\lim_{n\to\infty}f(a_n)=f(a)
\end{equation*}
This also show that $\overline{f}$ is indeed an extension of $f$.\\

Suppose $x\in X\setminus A$. We know $x\in A'$. Let $a_n\to x$. Because $Y$ is complete, we only have to prove 
\begin{equation*}
\set{f(a_n)}\text{ is Cauchy }
\end{equation*}
Fix $\epsilon $. Let $\delta=\delta(\epsilon )$. Let $N$ satisfy 
 \begin{equation*}
\forall n>N, a_n\in B_{\frac{\delta}{2}}(x)
\end{equation*}
We see for all $p,q>N$
 \begin{equation*}
a_p,a_q\in B_{\frac{\delta}{2}}(x)\implies d(a_p,a_q)<\delta \implies d(f(a_p),f(a_q))<\epsilon \bdone
\end{equation*}
We now show $\vi{\overline{f}\text{ is continuous on $X$ }}$.\\

Because $f$ is uniformly continuous on $A$. We know  $\overline{f}$ is continuous on $A$. Then we see by  \myref{Theorem}{3.11.2}, $\overline{f}$ is continuous on $X$. $\vdone$\\

Lastly, we prove  \blue{such completion is unique}.\\

Let $g$ be a completion of  $f$.  It is clear that for $a\in A$, we have $g(a)=\overline{f}(a)$. Suppose $x\in X\setminus A$. \As{$g(x)\neq \overline{f}(x)$}. Let 
\begin{equation*}
a_n \to x
\end{equation*}
We now have 
\begin{equation*}
\lim_{n\to \infty}f(a_n)=\overline{f}(x)\neq g(x)
\end{equation*}
By \myref{Theorem}{3.11.2}, we see $g$ is not continuous  $\tCaC\bdone$ 
\end{proof}
\section{Tests for Uniform Continuity}

\begin{theorem}
\label{5.4.1}
\textbf{(Heine-Cantor Theorem)} Given $f:X\rightarrow Y$
\begin{equation*}
X\text{ is compact and  }f\text{ is continuous }\implies f\text{ is uniformly continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish to find $\delta$ such that 
\begin{equation*}
d(x,y)<\delta \implies d(f(x),f(y))<\epsilon 
\end{equation*}
Because $f$ is continuous, for each $x\in X$ we can associate it with a positive number $\delta_x$ such that 
\begin{equation*}
\forall y\in B_{\delta_x}(x), f(y) \in B_{\frac{\epsilon}{2}}(x)
\end{equation*}
The set $\set{B_{\frac{\delta_x}{2}}(x):x\in X}$ is an open cover of $X$. Because  $X$ is compact, we know there exists a finite sub-cover
\begin{equation*}
\set{B_{\frac{\delta_{x_1}}{2}}(x_1),\dots ,B_{\frac{\delta_{x_m}}{2}}(x_m)}
\end{equation*}
Let 
\begin{equation*}
\delta=\min \set{\frac{\delta_{x_1}}{2},\dots , \frac{\delta_{x_m}}{2}}
\end{equation*}
Suppose $d(x,y)<\delta$. We have to prove
\begin{equation*}
d(f(x),f(y))<\epsilon 
\end{equation*}
Because 
\begin{equation*}
\set{B_{\frac{\delta_{x_1}}{2}}(x_1),\dots ,B_{\frac{\delta_{x_m}}{2}}(x_m)}\text{ is an sub-cover }
\end{equation*}
We know there exists $r\in \set{1,\dots ,m}$ such that 
\begin{equation*}
x\in B_{\frac{\delta_{x_r}}{2}}(x_r)
\end{equation*}
Which means 
\begin{equation*}
d(x,x_r)<\frac{\delta_{x_r}}{2}
\end{equation*}
Then because 
 \begin{equation*}
d(x,y)<\delta \leq \frac{\delta_{x_r}}{2}
\end{equation*}
We have 
\begin{equation*}
d(y,x_r)\leq d(y,x)+d(x,x_r)<\frac{\delta_{x_r}}{2}+\frac{\delta_{x_r}}{2}=\delta_{x_r}
\end{equation*}
Then because 
\begin{equation*}
d(x,x_r)<\frac{\delta_{x_r}}{2}<\delta_{x_r}
\end{equation*}
We now have
\begin{equation*}
x\in B_{\delta_{x_r}}(x_r)\text{ and }y\in B_{\delta_{x_r}}(x_r)
\end{equation*}
By triangular inequality, we now have
\begin{equation*}
d(f(x),f(y))\leq d(f(x),f(x_r))+d(f(x_r),f(y))<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon 
\end{equation*}



\end{proof}
\begin{theorem}
\label{5.4.2}
\textbf{(Bounded Derivative and Open Interval Domain Implies Uniformly Continuous)} Let $I$ be an open interval. Given differentaible $f:I\rightarrow \R$ 
\begin{equation*}
f'[I]\text{ is bounded }\implies f\text{ is uniformly continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Let $M$ satisfy
 \begin{equation*}
\forall x, \abso{f'(x)}<M
\end{equation*}
Fix $\epsilon $. We prove $\delta:=\frac{\epsilon}{M}$ works. Let $h$ satisfy
\begin{equation*}
0<h<\delta
\end{equation*}
We see 
\begin{equation*}
  \abso{f(a+h)-f(a)}=\abso{\int_a^{a+h}f'(x)dx} \leq hM<\delta M=\epsilon 
\end{equation*}
\end{proof}
\begin{theorem}
\label{5.4.3}
\textbf{(Continuous and Horizontal Asymptote Implies Uniformly Continuity)} 
Given $f:[0,\infty)\rightarrow \R$. Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous.
  \item $f(x)\to L$ as $x\to \infty$
\end{enumerate}
Then
\begin{equation*}
f\text{ is uniformly continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. 
Because $f(x)\to L$ as $x\to \infty$, we know there exists $R\inr^+$ such that 
\begin{equation*}
\forall x>R, \abso{f(x)-L}<\frac{\epsilon}{2}
\end{equation*}
Also, because $[0,R+1]$ is compact and $f$ is continuous, we know $f$ is uniformly continuous on  $[0,R+1]$, then we know there exists $\delta'$ such that 
\begin{equation*}
\forall y',z'\in [0,R+1], \abso{y'-z'}<\delta'\implies \abso{f(y')-f(z')}<\epsilon 
\end{equation*}
We now prove 
\begin{equation*}
\delta:=\min \set{\delta',1}\text{ works }
\end{equation*}
WOLG, let $z+\delta>y>z\geq 0$. If $z<R$, then  $y<z+\delta<R+\delta\leq R+1$. This tell us $z,y\in [0,R+1]$. Then because $y-z<\delta<\delta'$, we have $\abso{f(y')-f(z')}<\epsilon $. If $z\geq R$. We have $y>z>R$. Then we have 
 \begin{equation*}
\abso{f(y)-f(z)}\leq \abso{f(y)-L}+\abso{f(z)-L}<\epsilon 
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
\myref{Theorem}{5.5.1}, \myref{Theorem}{5.5.2} and \myref{Theorem}{5.5.3} are three easy positive criteria to check for elementary function. We now proceed to give other three negative criteria.
\end{minipage}}
\begin{theorem}
\label{5.4.4}
\textbf{(Derivative Converge to Infinity Implies Not Uniformly Continuous)} Given differentiable $f:\R\rightarrow \R$
\begin{equation*}
\lim_{x\to \infty}f'(x)=\infty \implies f\text{ is not uniformly continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $ and $\delta$. We need to find a pair $(a,h)$ such that  
\begin{equation*}
h<\delta \text{ and }\abso{f(a+h)-f(a)}>\epsilon 
\end{equation*}
Let $h<\delta$. Because  $f'(x)\to \infty$ as $x \to \infty$, we know there exists $r$ such that 
 \begin{equation*}
\forall x>r, f'(x)>\frac{\epsilon +1}{h}
\end{equation*}
Let $a>r$. By Fundamental Theorem of Calculus, we know 
 \begin{equation*}
f(a+h)=\int_a^{a+h}f'(x)dx + f(a)
\end{equation*}
Then 
\begin{equation*}
\abso{f(a+h)-f(a)}=\int_a^{a+h}f'(x)dx\geq h\times \frac{\epsilon +1}{h}>\epsilon 
\end{equation*}

\end{proof}

\begin{theorem}
\label{5.4.5}
\textbf{(Uniformly Continuous Map Totally Bounded Set to Totally Bounded Set)} Let $X$ be a totally bounded set.
 \begin{equation*}
f:X\rightarrow Y\text{ is uniformly continuous }\implies f[X]\text{ is totally bounded }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. Let $\delta$ be from uniform continuity of $f$. Because $X$ is totally bounded, we can cover $X$ with 
 \begin{equation*}
X\subseteq \bigcup_{i=1}^m B_{\delta}(x_i)
\end{equation*}
We now prove 
\begin{equation*}
  \vi{f[X]\subseteq \bigcup_{i=1}^m B_\epsilon (f(x_i))}
\end{equation*}
Arbitrarily pick $y\in f[X]$. We wish to prove it is in some ball $B_\epsilon (f(x_i))$. We know there exists $x$ such that 
 \begin{equation*}
y=f(x)
\end{equation*}
Because $X$ is covered by the finite  $m $ balls, we know there exists $n\in \set{1,\dots ,m}$ such that 
\begin{equation*}
x\in B_{\delta}(x_n)
\end{equation*}
Then because $d(x,x_n)<\delta$, we see $d(f(x),f(x_n))<\epsilon $. Then we have 
\begin{equation*}
y=f(x)\in B_{\epsilon }(f(x_n))\vdone
\end{equation*}
\end{proof}
\begin{theorem}
\label{5.4.6}
\textbf{(Uniform Continuous has No Vertical Asymptote)} Given $f:X\rightarrow \R$
\begin{equation*}
\lim_{x \to p} f(x)=\infty \implies f\text{ is not uniformly continuous }
\end{equation*}
\end{theorem}
\begin{proof}
We know there exists a strict sequence $p_n \to p$ such that 
\begin{equation*}
f(p_n)\to \infty
\end{equation*}
Fix $\epsilon =1\text{ and }\delta$. We wish to find $(p_m,p_u)$ such that 
\begin{equation*}
d(p_m,p_u)<\delta\text{ and }d(f(p_m),f(p_u))>1
\end{equation*}
Let $p_m \in  B_{\frac{\delta}{2}}(p)$. We know there exists $N$ such that 
\begin{equation*}
n>N \implies f(p_n)>f(p_m)+1
\end{equation*}
Let $u>\min \set{N,m}$. Because $p_n \to p$ is strict, $u>m$ and $p_m \in B_{\frac{\delta}{2}}(p)$. We see 
\begin{equation*}
p_u\in B_{\frac{\delta}{2}}(p)
\end{equation*}
This tell us 
\begin{equation*}
d(p_m,p_u)<\delta
\end{equation*}
Then because $u>N$, we see 
 \begin{equation*}
f(p_n)>f(p_m)+1
\end{equation*}
\end{proof}

\begin{theorem}
\label{5.4.7}
\textbf{(Tests for Uniform Continuity)} Given $f:X\rightarrow Y$. The following suggest $f$ is uniform continuous 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous and $X$ is compact 
  \item $X\subseteq \R$ is an open interval, $Y\subseteq \R$, $f$ is differentiable and $f'[X]$ is bounded 
  \item $X$ is of the form  $[a,\infty)$, $Y\subseteq \R$, $f$ is continuous and  $f(x)\to L$ as $x\to \infty$ 
\end{enumerate}
The following suggest $f$ is NOT uniform continuous
\begin{enumerate}[label=(\alph*)]
  \item $\lim_{x\to \infty}f'(x)=\infty$
  \item $X$ is totally bounded while $f[X]$ isn't 
  \item $\lim_{x\to p}f(x)=\infty$
\end{enumerate}
\end{theorem}
\begin{proof}
For the three positive suggestion, see \myref{Theorem}{5.5.1}, \myref{Theorem}{5.5.2} and \myref{Theorem}{5.5.3}. For the three negative suggestion, see \myref{Theorem}{5.5.4}, \myref{Theorem}{5.5.5} and \myref{Theorem}{5.5.6}
\end{proof}
\fbox{\begin{minipage}{39em}
Notice that it is possible $\lim_{x\to p}f'(x)=\infty$, but $f$ still be uniform continuous. See $\sqrt{x}$. 
\end{minipage}}
\begin{theorem}
\label{5.4.8}
\textbf{(Example and Non-Example)}
\begin{enumerate}[label=(\alph*)]
  \item $\sin x$ is uniformly continuous on $\R$ 
  \item $\frac{\cos x^3}{x}$ is uniformly continuous on $[1,\infty)$
  \item $\frac{\cos x^3}{x}$ is Not uniformly continuous on $(0,\infty)$
  \item $x \sin x$ is Not uniformly continuous on $\R$ 
  \item $\tan x$ is not uniform continuous on  $(\frac{-\pi}{2},\frac{\pi}{2})$
\end{enumerate}
\end{theorem}
\begin{proof}
$\sin x$ is differentiable and the derivative is bounded by $2$.\\

$\frac{\cos x^3}{x}$ is uniformly continuous on $[1,\infty)$ because $\frac{\cos x^3}{x}\to 0$ as $x\to \infty$.\\

$\frac{\cos x^3}{x}$ is not uniformly continuous on $(0,\infty)$ because $\frac{\cos x^3}{x}\to \infty$ as $x\to 0$.\\

Observe 
\begin{align*}
  (a+h)\sin (a+h)- a\sin a &=(a+h)(\cos h \sin a +\sin h \cos a)-a \sin a\\
  &=(a+h)\sin h \cos a + (\sin a)(a(\cos h-1)+h \cos h)
\end{align*}
If we let $a$ be the of form $a=2n\pi $. We have 
\begin{align*}
  (a+h)\sin (a+h)-a \sin a&=(h+2n\pi)\sin h+ 2n\pi (\cos h-1)+h \cos h\\
  &=2n \pi (\sin h +\cos h-1)+h (\sin h+\cos h)
\end{align*}
Fix $0<h<\delta$. It is now easy to see 
 \begin{equation*}
   \abso{(a+h)\sin (a+h)-a \sin a}\to \infty\text{ as $n \to \infty$ }
\end{equation*}
$\tan x$ is not uniformly continuous on  $(\frac{-\pi}{2},\frac{\pi}{2})$ because $\tan x \to \infty$ as $x\to \frac{\pi}{2}$
\end{proof}



\section{Absolute Continuous and Lipschitz Continuous}
\fbox{\begin{minipage}{39em}
Absolutely continuous is about induced measure and Lebesgue measure. I should keep that in note and study this years after.
\end{minipage}}
\begin{definition}
\label{5.5.1}
\textbf{(Absolute Continuous)} Let $I$ be an interval in $\R$. We say a function $f: I\rightarrow \R$ is absolute continuous if for all $\epsilon $ there exists $\delta$ such that whenever a finite sequence of pairwise disjoint sub-intervals $(x_k,y_k)\subseteq I$ satisfy
\begin{equation*}
\sum_{k=1}^m (y_k-x_k)< \delta
\end{equation*}
we have 
\begin{equation*}
\sum_{k=1}^m \abso{f(y_k)-f(x_k)}<\epsilon 
\end{equation*}
\end{definition}
\begin{theorem}
\label{5.5.2}
\textbf{(Absolute Continuous is Stronger than Uniformly Continuous)} Let $I$ be an interval of $\R$. Given  $f:I\rightarrow \R$, we have 
\begin{equation*}
f\text{ is absolute continuous }\implies f\text{ is uniformly continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. Because $f$ is absolutely continuous, we know there exists  $\delta$ such that for all finite sequence of pairwise disjoint sub-interval $(x_k,y_k)\subseteq I$ that satisfy
\begin{equation*}
\sum_{k=1}^m (y_k-x_k)<\delta
\end{equation*}
we have 
\begin{equation*}
\sum_{k=1}^m \abso{f(y_k)-f(x_k)}<\epsilon 
\end{equation*}
Then for all pair of real number $p,q$ that satisfy 
\begin{equation*}
d(p,q)=\abso{p-q}<\delta
\end{equation*}
we can WOLG, let $a_1=\min  \set{p,q}$  and $b_1=\max  \set{p,q}$, and see
\begin{equation*}
  \sum_{k=1}^1 b_1-a_1=\abso{p-q}<\delta
\end{equation*}
which give us 
\begin{equation*}
d(f(p),f(q))=\abso{f(p)-f(q)}=\abso{f(b_1)-f(a_1)}=\sum_{k=1}^1 \abso{f(b_k)-f(a_k)}<\epsilon 
\end{equation*}
as we wish
\end{proof}
\begin{theorem}
\label{5.5.3}
\textbf{(Absolutely Continuous is Strictly Stronger Than Uniformly Continuous)} Define
\begin{equation*}
f(x)=\begin{cases}
  x \cos \frac{1}{x}& \text{ if $x \in (0,1]$ }\\
  0& \text{ if $x=0$ }
\end{cases}
\end{equation*}
We have 
\begin{equation*}
f\text{ is uniformly continuous but not absolutely continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Because $f$ is defined on $[0,1]$ and $f$ is clearly continuous, by  \myref{Theorem}{5.5.1}, we know $f$ is uniformly continuous. We now prove $f$ is not absolutely continuous. Fix $\epsilon =1$. It suffice to show that for all $\delta$, there exists a finite sequence $\set{(a_k,b_k)}_{k=1}^n$ of pairwise disjoint sub-interval such that 
\begin{equation*}
\sum_{k=1}^n b_k-a_k<\delta\text{ and }\sum_{k=1}^n f(b_k)-f(a_k)>1=\epsilon 
\end{equation*}
Let $m$ be an odd natural. For all $k\inn$, define 
\begin{equation*}
  (a_k,b_k):=(\frac{1}{(2k+1)m\pi},\frac{1}{2km\pi})
\end{equation*}
We have 
\begin{equation*}
b_k-a_k=\frac{1}{ (4k^2+2k)m \pi}
\end{equation*}
and because $m$ is odd, we have 
\begin{equation*}
f(b_k)=\frac{\cos (2km\pi)}{2km \pi}=\frac{1}{2km\pi}\text{ and }f(a_k)=\frac{\cos ((2k+1 )m\pi)}{(2k+1)m\pi }=\frac{-1}{(2k+1)m \pi}
\end{equation*}
This give us 
\begin{equation*}
\abso{f(b_k)-f(a_k)}=\frac{4k+1}{(4k^2+2k)m \pi }
\end{equation*}
Notice that by limit comparison test, we know $\sum _{k=1}^\infty \frac{1}{4k^2+2k}$ converge and $\sum_{k=1}^\infty \frac{4k+1}{4k^2+2k}$ diverge. Define
\begin{equation*}
\alpha :=\sum_{k=1}^\infty \frac{1}{4k^2+2k}=m\pi \sum_{k=1}^\infty b_k-a_k
\end{equation*}
We have 
\begin{equation*}
\sum_{k=1}^\infty b_k-a_k=\frac{\alpha }{m \pi}
\end{equation*}
Then for all $\delta$, we see if we require $m>\frac{\alpha}{\pi \delta}$. No matter how large is $n$, we have 
 \begin{equation*}
\sum_{k=1}^n b_k-a_k<\sum_{k=1}^\infty b_k-a_k=\frac{\alpha }{m \pi}<\delta
\end{equation*}
And because 
\begin{equation*}
\sum_{k=1}^\infty \abso{f(b_k)-f(a_k)}=\frac{1}{m \pi }\sum_{k=1}^\infty \frac{4k+1}{4k^2+2k}=\infty
\end{equation*}
We know there exists $n$ such that 
 \begin{equation*}
\sum_{k=1}^n \abso{f(b_k)-f(a_k)}>1=\epsilon 
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce Lipschitz continuous. 
\end{minipage}}
\begin{definition}
\label{5.5.4}
\textbf{(Lipschitz Continuous)} We say a function $f:X\rightarrow Y$ is Lipschitz continuous if there exists a non-negative real constant $K$ such that 
\begin{equation*}
\forall p,q\in X, d(f(p),f(q))\leq Kd(p,q)
\end{equation*}
\end{definition}
\begin{theorem}
\label{5.5.5}
\textbf{(Lipschitz Continuous is Stronger than Uniform Continuous)} Given $f:X\rightarrow Y$
\begin{equation*}
f\text{ is Lipschitz continuous }\implies f\text{ is uniform continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. If $K=0$, we see 
 \begin{equation*}
\forall p,q\in X, d(f(p),f(q))\leq (0)d(p,q)=0
\end{equation*}
Then clearly $f$ is uniform continuous as every positive real can be our $\delta$.\\


If $K>0$, we see 
\begin{equation*}
d(p,q)<\frac{\epsilon}{K}\implies d(f(p),f(q))<Kd(p,q)=\epsilon 
\end{equation*}
In this case, $\frac{\epsilon}{K}$ can be our $\delta$. 
\end{proof}
\begin{theorem}
\label{5.5.6}
\textbf{(Lipschitz Continuous is Stronger than Absolutely Continuous)} Given $f:I\subseteq \R\rightarrow \R$
\begin{equation*}
f\text{ is Lipschitz continuous }\implies f\text{ is absolutely continuous }
\end{equation*}
\end{theorem}
\begin{proof}
Because $f$ is Lipschitz continuous, we know there exists non-negative $K$ such that 
\begin{equation*}
\forall x,y \in I, \abso{f(y)-f(x)}\leq K\abso{y-x}
\end{equation*}
If $K=0$, then $f$ is a constant function. Every positive real can work as $\delta$. We only have to consider when $K>0$. Let $\delta=\frac{\epsilon}{K}$. We proceed to $\delta=\frac{\epsilon}{K}$ works.\\

Arbitrarily pick a finite sequence $\set{(a_k,b_k)}_{k=1}^n $ of pairwise disjoint sub-interval of $I$ such that 
 \begin{equation*}
\sum_{k=1}^n b_k-a_k<\delta
\end{equation*}
It remains to show 
\begin{equation*}
\sum_{k=1}^n \abso{f(b_k)-f(a_k)}<\epsilon 
\end{equation*}
Observe
\begin{align*}
  \sum_{k=1}^n \abso{f(b_k)-f(a_k)}&\leq \sum_{k=1}^n K \abso{b_k-a_k}\\
  &=K \sum_{k=1}^n b_k-a_k\\
  &<K\delta\\
  &=\epsilon 
\end{align*}
\end{proof}
\begin{lemma}
\label{5.5.7}
\begin{equation*}
0\leq h<a<b\implies \sqrt{b}-\sqrt{a}\leq \sqrt{b-h}-\sqrt{a-h} 
\end{equation*}
\end{lemma}
\begin{proof}
Define 
\begin{equation*}
f(h)=\sqrt{b-h} -\sqrt{a-h} -(\sqrt{b} -\sqrt{a} )   
\end{equation*}
We have to show 
\begin{equation*}
\forall h\in [0,a), f(h)>0
\end{equation*}
Because $b>a$, we can compute
\begin{equation*}
f'(h)=\frac{\sqrt{b-h} -\sqrt{a-h} }{2\sqrt{(a-h)(b-h)} }>0
\end{equation*}
Observe that $f(0)=0$ and we are done.
\end{proof}
\begin{theorem}
\label{5.5.8}
\textbf{(Lipschitz Continuous is Strictly Stronger Than Absolutely Continuous)} Define 
 \begin{equation*}
f:[0,1]\rightarrow \R, x\mapsto \sqrt{x} 
\end{equation*}
Then
\begin{equation*}
f\text{ is absolutely continuous but not Lipschitz continuous}
\end{equation*}
\end{theorem}
\begin{proof}
Notice that given $y\neq x$, we have 
\begin{equation*}
\abso{\sqrt{y}  -\sqrt{x} }\leq K \abso{y-x}\iff  \frac{1}{\sqrt{y} +\sqrt{x} }\leq K 
\end{equation*}
Then for all $K$ we can use  $x=(\frac{2}{K})^2$ and $y=(\frac{1}{K})^2$ to have $\abso{\sqrt{y} -\sqrt{x} }> K \abso{y-x}$. We have proved $f$ is not Lipschitz continuous.\\

We now prove \vi{$f$ is absolutely continuous}. Fix $\epsilon $. We proceed to show 
\begin{equation*}
\delta=\epsilon ^2\text{ works }
\end{equation*}
Let $\set{(a_k,b_k)}_{k=1}^n$ be a finite sequence of pairwise disjoint sub-interval of $[0,1]$ such that 
\begin{equation*}
\sum_{k=1}^n b_k-a_k<\delta
\end{equation*}
Foo all $k\in \set{1,\dots ,n}$, define 
\begin{equation*}
h_k=b_k-a_k\text{ and }c_k=\sum_{u=1}^{k-1} h_u \text{ and }d_k=c_k+h_k
\end{equation*}
We have
\begin{equation*}
\sum_{k=1}^n d_k-c_k<\sum_{k=1}^n h_k<\delta
\end{equation*}
It is easy to see $c_{k+1}=d_k$, then we have
\begin{equation*}
\sum_{k=1}^n \sqrt{d_k}-\sqrt{c_k}=\sqrt{d_n}=\sqrt{\sum_{k=1}^n h_k}=\sqrt{\sum_{k=1}^n b_k-a_k}<\sqrt{\delta} =\epsilon  
\end{equation*}
Then, it suffice to show 
\begin{equation*}
\sum_{k=1}^n \sqrt{b_k} -\sqrt{a_k} \leq \sum_{k=1}^n \sqrt{d_k} -\sqrt{c_k} 
\end{equation*}
Notice that $d_k-c_k=b_k-a_k$, so to satisfy the hypothesis of \myref{Lemma}{5.5.7}, we only have to show 
\begin{equation*}
\forall k, c_k\leq a_k
\end{equation*}
We prove this by induction. Base case is trivial, as $c_1=0$. Suppose $c_m\leq a_m$. We have to show $c_{m+1}\leq a_{m+1}$.\\

Observe 
\begin{equation*}
c_{m+1}=c_m+b_m-a_m\leq b_m\leq a_{m+1}
\end{equation*}
We can now use \myref{Lemma}{5.5.7} to see 
\begin{equation*}
\forall k\in \set{1,\dots ,n}, \sqrt{b_k} -\sqrt{a_k} \leq \sqrt{d_k} -\sqrt{c_k}  
\end{equation*}
Then 
\begin{equation*}
\sum_{k=1}^n \sqrt{b_k} -\sqrt{a_k}\leq \sum_{k=1}^n \sqrt{d_k} -\sqrt{c_k}<\epsilon \vdone
\end{equation*}
\end{proof}
\begin{corollary}
\label{5.5.9}
 \textbf{(Lipschitz Continuous is Strictly Stronger than Uniformly Continuous)} 
\end{corollary}
\begin{proof}
The $f$ in \myref{Lemma}{5.5.8} is uniformly continuous, as \myref{Theorem}{5.5.2} suggest.
\end{proof}
\section{Test Problems for Uniformly Continuous}
\begin{question}{}{}
Suppose that $f(x)=\frac{4}{x^2+2x+2}$ and $g(x)=2e^{-x^2}$. Prove 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is uniformly continuous on  $\R$ 
   \item $g$ is uniformly continuous on  $\R$
\end{enumerate}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Prove that $f(x)=\begin{cases} \sin \frac{1}{x} & \text{ if $x\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}$ is discontinuous at $x\neq 0$
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Prove that $g(x)=\begin{cases}
  x \sin \frac{1}{x}& \text{ if $x=\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}$ is uniformly continuous on $\R$
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Prove that $h(x)=\begin{cases}
  x^2 \sin \frac{1}{x} & \text{ if $x\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}$ is uniformly continuous on $\R$
\end{question}
\begin{proof}

\end{proof}
\chapter{Rudin: Differentiation}
\section{Derivative}
\begin{definition}
\label{6.1.1}
\textbf{(Definition of Derivative)} Given $f:I\subseteq \R\rightarrow \R$. Suppose $x$ is a limit point of  $I$, we say  $f$ is differentiable at $x$ if 
 \begin{align*}
\frac{f(x+h)-f(x)}{h}\text{ converge as $h \to 0$ }
\end{align*}
and we say 
\begin{align*}
\lim_{h \to 0} \frac{f(x+h)-f(x)}{h}\text{ is the derivative of $f$ }
\end{align*}
The derivative function is then defined 
\begin{align*}
f'(x)=\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}
\end{align*}
\end{definition}
\fbox{\begin{minipage}{39em}
Notice that if $f'(x)=\infty$, then at that point it is not even continuous?
\end{minipage}}
\begin{theorem}
\label{6.1.2}
\textbf{(Equivalent Writing of Derivative)}
\begin{align*}
f(x+h)-f(x)=h(f'(x)+o(h))
\end{align*}
\end{theorem}
\begin{proof}
Deduce
\begin{align*}
&\frac{f(x+h)-f(x)}{h}\to f'(x)\\
\implies & \frac{f(x+h)-f(x)}{h}-f'(x) = o(h)\\
\implies &f(x+h)-f(x)=h(f'(x)+o(h))
\end{align*}
\end{proof}
\begin{theorem}
\label{6.1.3}
\textbf{(Arithmetic of Derivative of Real to Real Function)} Suppose $f,g:I\subseteq \R\rightarrow \R$ are both differentiable at $x$, we have 
\begin{enumerate}[label=(\alph*)]
  \item $ (f+g)'(x)=f'(x)+g'(x)$
  \item $(fg)'(x)=f'(x)g(x)+f(x)g'(x)$ 
  \item $(\frac{f}{g})'(x)=\frac{g(x)f'(x)-g'(x)f(x)}{g^2(x)}$
\end{enumerate}
\end{theorem}
\begin{proof}
$(a)$ is a special case of \myref{Theorem}{5.1.4}. We now prove 
\begin{align*}
  \vi{(fg)'(x)=f'(x)g(x)+f(x)g'(x)}
\end{align*}
Observe 
\begin{align*}
fg(x+h)-fg(x)&=f(x+h)g(x+h)-f(x)g(x)\\
&=f(x+h)[g(x+h)-g(x)]+g(x)[f(x+h)-f(x)]
\end{align*}
Then 
\begin{align*}
\frac{fg(x+h)-fg(x)}{h}=f(x+h) \times \frac{g(x+h)-g(x)}{h}+ g(x) \times \frac{f(x+h)-f(x)}{h} \to 0 \vdone
\end{align*}
We now prove 
\begin{align*}
  \blue{(\frac{f}{g})'(x)=\frac{g(x)f'(x)-g'(x)f(x)}{g^2(x)}}
\end{align*}
Observe 
\begin{align*}
\frac{\frac{f}{g}(x+h)-\frac{f}{g}x}{h}&=\frac{f(x+h)g(x)-f(x)g(x+h)}{g(x+h)g(x)h}\\
&=\frac{1}{g(x+h)g(x)} \times \frac{f(x+h)g(x)-f(x)g(x+h)}{h}\\
&=\frac{1}{g(x+h)g(x)}\times \frac{g(x) [f(x+h)-f(x)]+f(x)[g(x)-g(x+h)]}{h} \to 0 \bdone
\end{align*}
\end{proof}
\begin{theorem}
\label{6.1.4}
\textbf{(Chain Rule of Real to Real Function)} Given $f,g:\R\rightarrow \R$. Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable at  $x$ 
  \item  $g$ is differentaible at  $f(x)$
\end{enumerate}
we have 
\begin{align*}
  (g \circ  f)'(x)=g'(f(x)) f'(x)
\end{align*}
\end{theorem}
\begin{proof}
We are given 
\begin{align*}
f(h+x)-f(x)&=h[f'(x)+o(h)]\\
g(h+x)-g(x)&=h[g'(x)+o(h)]
\end{align*}
Then because $f(x+h)\to f(x)$ 
\begin{align*}
  (g\circ f)(x+h)- (g\circ f)(x)&= [f(x+h)-f(x)]\times [g'(f(x))+o(f(x+h)-f(x))]\\
  &=h[f'(x)+o(h)]\times [g'(f(x))+o (f(x+h)-f(x))]
\end{align*}
Notice that because $f(x+h)-f(x)\to 0$ as $h \to 0$, then 
\begin{align*}
\frac{g \circ  f(x+h)-g \circ  f(x)}{h}&=[f'(x)+o(h)]\times [g'\circ  f(x)+ o (f(x+h)-f(x))]\\
&\to f'(x) \times g' \circ  f(x) \text{ as  }h\to 0
\end{align*}
\end{proof}
\section{Mean Value Theorem}
\begin{definition}
\label{6.2.1}
\textbf{(Definition of Local Maximum)} Given $f:X\rightarrow \R$, we say $f$ has a local maximum at  $x$ if there exists  $\delta$ such that 
\begin{align*}
y \in B_{\delta}(x)\implies f(y)\leq f(x)
\end{align*}
\end{definition}
\begin{theorem}
\label{6.2.2}
\textbf{(For $f:\R\rightarrow \R$, Local Maximum is Either Isolated or Has 0 Derivative)} Given $f:\R\rightarrow \R$, if $f$ has a local maximum at  $x$, then either
\begin{enumerate}[label=(\alph*)]
  \item $x$ is an isolated point of domain of  $f$
   \item  $f$ is not differentiable at  $x$ 
  \item $f'(x)=0$ 
\end{enumerate}
\end{theorem}
\begin{proof}
Observe left side limit 
\begin{align*}
\lim_{h \to 0^-}\frac{f(x+h)-f(x)}{h}\geq 0
\end{align*}
Observe right side limit 
\begin{align*}
\lim_{h \to 0^+}\frac{f(x+h)-f(x)}{h}\leq 0
\end{align*}
Then we see $f'(x)$ must equals to $0$ if possible.
\end{proof}
\begin{theorem}
\label{6.2.3}
\textbf{(Generalized Mean Value Theorem)}
Given 
\begin{enumerate}[label=(\alph*)]
  \item $f,g$ are continuous on  $[a,b]$ 
  \item  $f,g$ are differnetiable on  $(a,b)$
\end{enumerate}
Then there exists a point $x\in (a,b)$ such that 
\begin{align*}
[f(b)-f(a)]g'(x)=[g(b)-g(a)]f'(x)
\end{align*}
\end{theorem}
\begin{proof}
Define $v:[a,b]\to \R$ by 
\begin{align*}
v(t)=[f(b)-f(a)]g(t)-[g(b)-g(a)]f(t)
\end{align*}
We know $v$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Notice 
\begin{align*}
v'(t)=[f(b)-f(a)]g'(t)-[g(b)-g(a)]f'(t)
\end{align*}
Then, we reduce the problem into proving the existence of $x\in (a,b)$ such that 
\begin{align*}
v'(x)=0
\end{align*}
Observe  
\begin{align*}
v(a)&=[f(b)-f(a)]g(a)-[g(b)-g(a)]f(a)\\
&=f(b)g(a)-g(b)f(a)\\
&=[f(b)-f(a)]g(b)-[g(b)-g(a)]f(b)=v(b)
\end{align*}
Notice that $v$ is continuous, then because the domain is $[a,b]$, we know $v$ has maximum and minimum. If one of maximum and minimum is in $(a,b)$, the result follows from \myref{Theorem}{6.2.2}. If not, notice that $v$ is a constant so the derivative is  $0$ every where.  
\end{proof}
\begin{corollary}
\label{6.2.4}
\textbf{(Useful Form of Cauchy Mean Value Theorem)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $f,g$ are continuous on $[a,b]$ 
  \item $f,g$ are differentiable  $(a,b)$ 
  \item $g'$ does not equals to $0$ in  $(a,b)$
\end{enumerate}
Then there exists a point $x\in (a,b)$ such that 
\begin{align*}
\frac{f'(x)}{g'(x)}=\frac{f(b)-f(a)}{g(b)-g(a)}
\end{align*}
\end{corollary}
\begin{corollary}
\label{6.2.5}
\textbf{(Mean Value Theorem)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous on $[a,b]$ 
  \item $f$ is differentiable on  $(a,b)$
\end{enumerate}
there exists $x\in (a,b)$ such that 
\begin{align*}
f(b)-f(a)=f'(x)(b-a)
\end{align*}
\end{corollary}
\begin{proof}
Define $g$ by 
 \begin{align*}
g(x)=x
\end{align*}
Notice that $g'(x)=1$ and $g(b)-g(a)=b-a$, then by \myref{Theorem}{6.2.3}, we know there exists $x$ such that 
\begin{align*}
f(b)-f(a)=(b-a)f'(x)
\end{align*}
\end{proof}
\begin{corollary}
\label{6.2.6}
\textbf{(Derivative to Increasing without FTC)} 
\begin{enumerate}[label=(\alph*)]
  \item $f'(x)\geq 0$ on $(a,b)\implies f$ increase on $(a,b)$   
  \item  $f'(x)=0$ on $(a,b)\implies f$ is constant on $(a,b)$ 
  \item  $f'(x)\leq 0$ on $(a,b)\implies f$ decrease on $(a,b)$
\end{enumerate}
\end{corollary}
\begin{proof}
\As{the opposite} and use \myref{corollary}{6.2.5}
\end{proof}
\section{The Continuity of Derivative} 
\begin{theorem}
\label{6.3.1}
\textbf{(Derivative Function Satisfy Intermediate Value Property)} Given $f:[a,b]\rightarrow \R$ 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differnetiable
  \item $f'(a)<\ld  < f'(b)$
\end{enumerate}
then there exists a point $x\in (a,b)$ such that 
\begin{align*}
f'(x)=\ld 
\end{align*}
\end{theorem}
\begin{proof}
Define $g:[a,b]\rightarrow \R$ by 
\begin{align*}
g(t)=f(t)-\ld t
\end{align*}
We see $g$ is differnetiable and 
\begin{align*}
g'(t)=f'(t)-\ld 
\end{align*}
We have reduced the problem into finding $x\in (a,b)$ such that 
\begin{align*}
g'(x)=0
\end{align*}
Notice that because $f$ is differentiable, we know $g$ is continuous. Then because $[a,b]$ is compact, we know $g$ has a maximum, at which by \myref{Theorem}{6.2.2}, has $0$ derivative.
\end{proof}
\fbox{\begin{minipage}{39em}
Notice that if $f'(a)=\ld = f'(b)$, such $x$ may not exists in  $(a,b)$ 
\end{minipage}}
\begin{corollary}
\label{6.3.2}
\begin{align*}
f\text{ is differentiable on $[a,b]$ }\implies f'\text{ has only essential discontinuity on $[a,b]$ }
\end{align*}
\end{corollary}
\begin{proof}
  \As{$f$ has a removable or jump discontinuity at  $x\in [a,b]$}, WOLG, it must happens 
\begin{align*}
f'(a)\neq \lim_{h \to 0^+}
\end{align*}
\begin{align*}
f'(a)< \lim_{h \to 0}f'(a+h)=\lim_{h\to 0^+} f'(a+h)=\alpha 
\end{align*}
We can find $\delta$ small enough such that 
\begin{align*}
x\in B_{\delta}(a)\implies f'(x)>\frac{\alpha +f'(a)}{2}>f'(a)
\end{align*}
We now see 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable on $[a,a+\frac{\delta}{2}]$ 
  \item  $f'(a)<\frac{\alpha +f'(a)}{2}<f'(a+\frac{\delta}{2})$ 
\end{enumerate}
From \myref{Theorem}{6.3.1}, we now obtain an contradiction. \CaC\\
\section{L'Hospital Rule}
\begin{theorem}
\label{6.4.1}
\textbf{(L'Hospital Rule)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $f(x),g(x) \to 0$ or $\abso{f(x)},\abso{g(x)}\to \infty$ as $x \to c$
  \item $f,g$ are differentiable at an interval  $I$ containing  $c$  except possibly at $c$ 
  \item $g'(x)\neq 0$ for all $x\in I \setminus \set{c}$ 
  \item $\lim_{x \to c}\frac{f'(x)}{g'(x)}=\alpha \in\R$
\end{enumerate}
We have 
\begin{align*}
\lim_{x \to c} \frac{f(x)}{g(x)}=\alpha 
\end{align*}
\end{theorem}
\begin{proof}
We first prove when 
 \begin{align*}
 \vi{f(x),g(x)\to c\text{ as $x \to c$ and $c\inr$} }
 \end{align*}
Let $x_n$ be a sequence converge to $c$ that does not contain $c$. WOLG, We only have to prove 
\begin{align*}
\frac{f(x_n)}{g(x_n)}\to \alpha \text{ as $x \to c$ }
\end{align*}
Let $y_{n,k}$ be a double sequence satisfy 
\begin{align*}
y_{n,k} \text{ is between }x_n\text{ and }c\text{ and }y_{n,k}\to c\text{ as $k\to \infty$ }
\end{align*}
By \myref{Theorem}{6.2.3}, we can construct a sequence $t_{n,k}$ such that 
 \begin{align*}
t_{n,k} \text{ is between $x_n$ and  $y_{n,k}$ and }\frac{f'(t_{n,k})}{g'(t_{n,k})}=\frac{f(x_{n,k})-f(y_{n,k})}{g(x_{n,k})-g(y_{n,k})}
\end{align*}
Observe 
\begin{align*}
\frac{f'(t_{n,k})}{g'(t_{n,k})}&=\frac{f(x_{n})-f(y_{n,k})}{g(x_n)-g(y_{n,k})}\\
&=\frac{\frac{f(x_n)}{g(x_n)}-\frac{f(y_{n,k})}{g(x_n)}}{1-\frac{g(y_{n,k})}{g(x_n)}}
\end{align*}
Then because 
\begin{align*}
f(y_{n,k}),g(y_{n,k})\to 0\text{ as $k \to \infty$ }
\end{align*}
On right hand side, we have 
\begin{align*}
\lim_{k\to \infty} \frac{\frac{f(x_n)}{g(x_n)}-\frac{f(y_{n,k})}{g(x_n)}}{1-\frac{g(y_{n,k})}{g(x_n)}}=\frac{f(x_n)}{g(x_n)}
\end{align*}
Now, we have 
\begin{align*}
\lim_{k \to \infty}\frac{f'(t_{n,k})}{g'(t_{n,k})}=\frac{f(x_n)}{g(x_n)}
\end{align*}
This give us 
\begin{align*}
\lim_{n \to \infty }\frac{f(x_n)}{g(x_n)}=\lim_{n \to \infty}\lim_{k \to \infty}\frac{f'(t_{n,k})}{g'(t_{n,k})}
\end{align*}
For each $n$, we can select a  $m_n$ such that 
\begin{align*}
d(\frac{f'(t_{n,m_n})}{g'(t_{n,m_n})},\lim_{k \to \infty}\frac{f'(t_{n,k})}{g'(t_{n,k})})<\frac{1}{n}
\end{align*}
Then because $t_{n,m_n}$ is bounded by $x_n,c$, we see 
 \begin{align*}
\lim_{x \to c} \frac{f'(x)}{g'(x)}&=\lim_{n \to \infty}\frac{f'(t_{n,m_n})}{g'(t_{n,m_n})}\\
&=\lim_{n \to \infty}\lim_{k \to\infty}\frac{f'(t_{n,k})}{g'(t_{n,k})}\\
&=\lim_{n\to \infty} \frac{f(x_n)}{g(x_n)}\vdone
\end{align*}







\end{proof}
\section{Taylor's Theorem}
\begin{theorem}
\label{6.5.1}
\textbf{(Taylor's Theorem)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $f\text{ is $n$ time continuously differentiable on }I$, which contain $a$
\end{enumerate}
We have
\begin{align*}
f(x)=\sum_{k=0}^n \frac{f^{(k)}(a)(x-a)^k}{k!}+o(x-a)(x-a)^n
\end{align*}
\end{theorem}
\begin{proof}
Let 
 \begin{align*}
P(x)=\sum_{k=0}^n \frac{f^{(k)}(a)(x-a)^k}{k!}
\end{align*}
We wish to prove 
\begin{align*}
\vi{\frac{f(x)-P(x)}{(x-a)^n}\to 0\text{ as $x\to a$ }}
\end{align*}
Notice that 
\begin{align*}
P(x)=\sum_{k=0}^n \frac{f^{(k)}(a)(x-a)^k}{k!}\to f(a)=\frac{f(a)(x-a)^0}{0!}\text{ as $x\to a$ }
\end{align*}
We now can recursively applies L'Hospital Rule
\begin{align*}
\lim_{x \to a} \frac{f(x)-P(x)}{(x-a)^n}&= \lim_{x \to a} \frac{f'(x)-P'(x)}{n (x-a)^{n-1}}\\
&=\lim_{x \to a}\frac{f''(x)-P''(x)}{n(n-1)(x-a)^{n-2}}\\
&=\lim_{x \to a}\frac{f^{(n)}(x)-P^{(n)}(x)}{n!}\\
&=\lim_{x\to a}\frac{f^{(n)}(x)-f^{(n)}(x)}{n!}=0
\end{align*}
\end{proof}
\begin{theorem}
\label{6.5.2}
\textbf{(Mean Value Form Remainder of Taylor's Theorem)} Given 
\begin{align*}
f:I\subseteq \R\rightarrow \R\text{ is $n$ time continuously differentiable at $a\in I$}
\end{align*}
Define 
\begin{enumerate}[label=(\alph*)]
  \item $P_n(x)=\sum_{k=0}^n \frac{f^{(k)}(a) (x-a)^k}{k!}$ 
  \item  $R_n(x)=f(x)-P_n(x)$
\end{enumerate}
If 
\begin{enumerate}[label=(\alph*)]
  \item $G$ is continuous on  $[a,x]$ 
  \item  $G'$ exists and not equals to  $0$ on  $(a,x)$
\end{enumerate}
We have 
\begin{align*}
\exists \xi ,\in (a,x) R_n(x)=\frac{f^{(n+1)}(\xi)(x-\xi )^n}{n!}  \frac{G(x)-G(a)}{G'(\xi )}
\end{align*}
\end{theorem}
\begin{proof}
Define $F:(a,x)\to \R$ by 
\begin{align*}
F(t)=\sum_{k=0}^n \frac{f^{(k)}(t)(x-t)^k}{k!}
\end{align*}
By \myref{Corollary}{6.2.5}, we now have  
\begin{align*}
\frac{F'(\xi)}{G'(\xi)}=\frac{F(x)-F(a)}{G(x)-G(a)}
\end{align*}
Compute 
\begin{align*}
F(x)=f(x)
\end{align*}
Compute 
\begin{align*}
F(a)=\sum_{k=0}^n \frac{f^{(k)}(a)(x-a)^k}{k!}=P_n(x)
\end{align*}
Compute 
\begin{align*}
F'(\xi)&=\sum_{k=0}^n \frac{f^{(k+1)}(\xi)(x-\xi)^k-k f^{(k)}(\xi)(x-\xi)^{k-1}}{k!}\\
&=\frac{f^{(n+1)}(\xi)(x-\xi)^n}{n!} 
\end{align*}
We now have 
\begin{align*}
\frac{\frac{f^{(n+1)}(\xi)(x-\xi)^{n}}{n!}}{G'(\xi)}=\frac{R_n(x)}{G(x)-G(a)}
\end{align*}
Then we can deduce 
\begin{align*}
R_n(x)=\frac{f^{(n+1)}(\xi)(x-\xi)^n}{n!}\frac{G(x)-G(a)}{G'(\xi)}
\end{align*}
\end{proof}
\begin{corollary}
\label{6.5.3}
\textbf{(Lagarange Form of Remainders in Taylor's Theorem)} Let 
\begin{align*}
G(t)=(x-t)^{n+1}
\end{align*}
We have
\begin{align*}
\exists \xi \in (a,x), R_n(x)=\frac{f^{(n+1)}(\xi)(x-a)^{n+1}}{(n+1)!} 
\end{align*}
\end{corollary}
\begin{proof}
Compute 
\begin{align*}
G'(\xi)&=-(n+1)(x-\xi)^{n}\\
G(x)&=0\\
G(a)&=(x-a)^{n+1}
\end{align*}
The result now follows from \myref{Theorem}{6.5.2}. 
\end{proof}
\chapter{Rudin: Riemann-Stieltjes Integral}
\section{Definition and Existence of The Integral}
\fbox{\begin{minipage}{39em}
In this section, we shall assume $f$ is defined and bounded on $[a,b]$ and $\alpha $ is defined and increasing on $[a,b]$.
\end{minipage}}
\begin{definition}
\label{7.1.1}
\textbf{(Definition of Partition)} By a Partition $P$ of a compact interval  $[a,b]$, we mean a finite set of points 
\begin{align*}
P=\set{x_1,\dots ,x_n}
\end{align*}
such that 
\begin{align*}
a=x_1\leq x_2 \leq  \cdots \leq x_n=b
\end{align*}
\end{definition}
\begin{definition}
\label{7.1.2}
\textbf{(Definition of Riemman Upper Sum)} Given a partition $P$ of $[a,b]$, the Riemann Upper and Lower Sum are respectively defined by 
\begin{align*}
U(P,f)&=\sum_{i=1}^n M_i \Delta x_i\\
L(P,f)&=\sum_{i=1}^n m_i \Delta x_i
\end{align*}
where 
\begin{align*}
M_i&=\sup_{x\in [x_{i-1},x_i]}f(x)\\
m_i&=\inf_{x\in [x_{i-1},x_i]}f(x)\\
\Delta x_i&=x_i-x_{i-1}
\end{align*}
\end{definition}
\begin{definition}
\label{7.1.3}
\textbf{(Definition of Upper Riemann Integral)} The Upper and Lower Riemann Integral is defined 
\begin{align*}
\overline{\int_a^b} fdx&=\inf_P U(P,f)\\
\underline{\int_a^b}fdx&=\sup_P L(P,f)
\end{align*}
\end{definition}
\begin{theorem}
\label{7.1.4}
\textbf{(Existence of Upper and Lower Riemann Integral)} 
\begin{align*}
\overline{\int_a^b}fdx\text{ and }\underline{\int_a^b}fdx\text{ exist }
\end{align*}
\end{theorem}
\begin{proof}
Let 
 \begin{align*}
m =\inf_{[a,b]}f \text{ and }M =\sup_{[a,b]}f
\end{align*}
Because $f$ is bounded, we know both $\alpha,\beta $ are finite. We only wish to show 
\begin{align*}
  &\vi{M  (b-a)\text{ is an upper bound of  }\set{L(P,f)\inr:P\text{ is a partition of $[a,b]$ }}}\\
  &\vi{m  (b-a)\text{ is an lower bound of }\set{U(P,f)\inr: P\text{ is a partition of $[a,b]$ }}}
\end{align*}
Arbitrarily pick a partition $P$ of $[a,b]$. Observe that 
\begin{align*}
\forall i\inn, m \leq m_i \leq M_i\leq M 
\end{align*}
This is true because $[x_{i-1},x_i]\subseteq [a,b]$. Now, we can deduce 
\begin{align*}
m (b-a)=\sum_{i=1}^n m \Delta x_i\leq \sum_{i=1}^n m_i \Delta x_i \leq \sum_{i=1}^n M_i \Delta x_i \leq \sum_{i=1}^n M\Delta x_i =M (b-a)
\end{align*}
Then we can just write 
\begin{align*}
m (b-a)\leq L(P,f)\leq U(P,f) \leq M (b-a)
\end{align*}
Because $P$ is an arbitrary partition, our proof is done. $\vdone$
\end{proof}
\begin{definition}
\label{7.1.5}
\textbf{(Definition of Riemann Integrable)} We say bounded $f$ is integrable on $[a,b]$ and write $f\in \mathscr{R}$ if 
\begin{align*}
\overline{\int_a^b}fdx=\underline{\int_a^b}fdx
\end{align*}
\end{definition}
\fbox{\begin{minipage}{39em}
Now, we consider integral with respect to an increasing function. 
\end{minipage}}
\begin{definition}
\label{7.1.6}
\textbf{(Definition of Riemman-Stieltjes Upper Sum)} The Riemann-Stieltjes Upper and Lower Sum are respectively defined by 
\begin{align*}
U(P,f,\alpha )&= \sum_{i=1}^n M_i \Delta \alpha (x_i)\\
L(P,f,\alpha )&=\sum_{i=1}^n m_i \Delta \alpha (x_i)
\end{align*}
where 
\begin{align*}
  M_i&=\sup_{x\in [x_{i-1},x_i]}f(x)\\
m_i&=\inf_{x\in [x_{i-1},x_i]}f(x)\\
\Delta \alpha (x_i)&=\alpha (x_i)-\alpha (x_{i-1})
\end{align*}
\end{definition}
\begin{theorem}
\label{7.1.7}
\textbf{(Useful Lemma)} 
\begin{align*}
\sup_{[x,y]\subseteq [x_{i-1},x_i]}\abso{f(x)-f(y)}= M_i-m_i
\end{align*}
\end{theorem}
\begin{proof}
\As{$\sup_{[x,y]\subseteq [x_{i-1},x_i]}\abso{f(x)-f(y)}<M_i-m_i$}. Then there exists $z_1\in [x_{i-1},x_i]$ such that 
\begin{align*}
f(z_1)>\sup_{[x,y]\subseteq [x_{i-1},x_i]}\abso{f(x)-f(y)}+m_i
\end{align*}
Also, there exists $z_2\in [x_{i-1,x_i}]$
\begin{align*}
f(z_1)-\sup_{[x,y]\subseteq [x_{i-1},x_i]}\abso{f(x)-f(y)}>f(z_2)
\end{align*}
Then we see 
\begin{align*}
\abso{f(z_1)-f(z_2)}>\sup_{[x,y]\subseteq [x_{i-1},x_i]}\abso{f(x)-f(y)}\tCaC
\end{align*}
\As{$\sup_{[x,y]\subseteq [x_{i-1},x_i]}\abso{f(x)-f(y)}>M_i-m_i$}. Then there exists $[x,y]\subseteq [x_{i-1},x_i]$ such that 
\begin{align*}
\abso{f(x)-f(y)}> M_i-m_i
\end{align*}
Then we see 
\begin{align*}
\forall z_1\in [x_{i-1},x_i], \abso{f(x)-f(y)}+m_i>f(z_1)
\end{align*}
and 
\begin{align*}
\forall z_1\in [x_{i-1},x_i],\forall z_2\in [x_{i-1},x_i], f(z_1)>f(z_2)-\abso{f(x)-f(y)}
\end{align*}
Then we have 
\begin{align*}
\forall z_1,z_2, \abso{f(x)-f(y)}>f(z_2)-f(z_1)\tCaC
\end{align*}
\end{proof}
\begin{definition}
\label{7.1.8}
\textbf{(Definition of Riemman-Stieltjes Upper Integral)} The Riemann-Stieltjes Upper and Lower Integral of $f$ with respect to $\alpha $ is 
\begin{align*}
\overline{\int_a^b} fdg&=\inf_P U(P,f,g)\\
\underline{\int_a^b}fdg&=\sup_P L(P,f,g)
\end{align*}
\end{definition}
\begin{theorem}
\label{7.1.9}
\textbf{(Existence of Riemman-Stieltjes Upper Integral)} 
\begin{align*}
\overline{\int_a^b}fd\alpha, \underline{\int_a^b}fd\alpha \text{ both exists }
\end{align*}
\end{theorem}
\begin{proof}
The proof is similar to that of \myref{Theorem}{7.1.4}
\end{proof}
\begin{definition}
\label{7.1.10}
\textbf{(Definition of Riemman-Stieltjes Integral)} We say $f$ is integrable on $[a,b]$ with respect to $\alpha $ if 
\begin{align*}
\overline{\int_a^b}fd\alpha =\underline{\int_a^b}fd\alpha 
\end{align*}
\end{definition}
\fbox{\begin{minipage}{39em}
We now consider the difference of upper and lower sums when different partition is given.
\end{minipage}}
\begin{definition}
\label{7.1.11}
\textbf{(Definition of Refinement of Partition)} We say $P^*$ is a  refinement of $P$ if 
 \begin{align*}
P\subseteq P^*
\end{align*}
We say  $P^*$ is a common refinement of $P_1,P_2$ if 
 \begin{align*}
P_1\cup P_2 \subseteq P^*
\end{align*}
\end{definition}
\begin{theorem}
\label{7.1.12}
\textbf{(Upper and Lower Sum is Closer to the Value if Partition is more Refined)} 
\begin{align*}
  P^*\text{ is a refinement of }P\implies L(P^*,f,g)\geq L(P,f,g)\text{ and }U(P^*,f,g)\leq U(P,f,g)
\end{align*}
\end{theorem}
\begin{proof}
Notice that $P^*\setminus P$ is minus. We then complete the proof by induction, proving the result only when $P^*$ is of one point larger than $P$. Let $x^*$ be the extra point in  $P^*$, and let  $x_j$ be the smallest point greater than  $x^*$.  We see 
\begin{align*}
L(P,f,g)&=\sum_{i=1}^n m_i \Delta g(x_i)\\
L(P^*,f,g)&=\sum_{i=1}^{j-1}m_i\Delta g(x_i)+\sum_{i=j+1}^n m_i\Delta g(x_i)\\
&+(\inf_{x\in [x_{j-1},x^*]}f(x))(g(x^*)-g(x_j-1))+(\inf_{x\in [x^*,x_j]} f(x))(g(x_j)-g(x^*))
\end{align*}
This give us 
\begin{align*}
L(P^*,f,g)-L(P,f,g)&=m_j\Delta g(x_j)\\
&-(\inf_{x\in [x_{j-1},x^*]} f(x)) (g(x^*)-g(x_{j-1}))-(\inf_{x\in[x^*,x_j]}f(x))(g(x_j)-g(x^*))
\end{align*}
Because $[x_{j-1},x^*],[x^*,x_j]\subseteq [x_{j-1},x_j]$, we know 
\begin{align*}
  m_j\leq \min \set{\inf_{x\in [x_{j-1},x^*]}f,\inf_{x\in [x^*,x_j]}f}
\end{align*}
Then we have 
\begin{align*}
L(P^*,f,g)-L(P,f,g)&\leq m_j \Delta g(x_j)-m_j (g(x^*)-g(x_{j-1}))-m_j (g(x_j)-g(x^*))\\
&=0 \vdone
\end{align*}
\end{proof}
\begin{theorem}
\label{7.1.13}
\textbf{(Upper Sum is Greater than Lower Sum)} Given a function $f$ defined almost everywhere and bounded on $[a,b]$, a function $g$ defined and increase on $[a,b]$, we have 
\begin{align*}
\underline{\int_a^b}fdg\leq \overline{\int_a^b}fdg
\end{align*}
\end{theorem}
\begin{proof}
\As{$\underline{\int_a^b}fdg>\overline{\int_a^b}fdg$}. Then we know there exists $P_1$ and  $P_2$ such that 
\begin{align*}
U(P_2,f,g)<L(P_1,f,g)
\end{align*}
Let $P^*=P_1\cup P_2$. We have
\begin{align*}
L(P_1,f,g)\leq L(P^*,f,g)\leq U(P^*,f,g)\leq U(P_2,f,g)\tCaC
\end{align*}
\end{proof}
\begin{theorem}
\label{7.1.14}
\textbf{(Iff for $f$ being Integrable on $[a,b]$ W.R.T. $\alpha $)}
\begin{align*}
f\text{ is integrable on $[a,b]$ W.R.T $\alpha $ }\iff  \forall \epsilon ,\exists P, U(P,f,\alpha )-L(P,f,\alpha )<\epsilon 
\end{align*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

Let $P_1,P_2$ satisfy 
 \begin{align*}
L(P_1,f,\alpha )>\int_a^b fd\alpha -\frac{\epsilon}{2}\text{ and }U(P_2,g,\alpha )<\int_a^b fd\alpha +\frac{\epsilon}{2}
\end{align*}
Let $P^*=P_1\cup  P_2$, we see 
\begin{align*}
[L(P^*,f,\alpha ),U(P^*,f,\alpha )] \subseteq [\int_a^b fd\alpha -\frac{\epsilon}{2},\int_a^b fd\alpha +\frac{\epsilon}{2}]
\end{align*}
$(\longleftarrow)$\\

\As{$\underline{\int_a^b }fd\alpha <\overline{\int_a^b}fd\alpha $}. Let $\epsilon = \frac{\overline{\int_a^b}fd\alpha - \underline{\int_a^b}fd\alpha }{2}$. We see for all $P$, 
 \begin{align*}
U(P,f,\alpha )\geq \overline{\int_a^b}fd\alpha=\underline{\int_a^b}fd\alpha +2\epsilon \geq L(P,f,\alpha )+2\epsilon 
\end{align*}
Then 
\begin{align*}
\forall P,U(P,f,\alpha )-L(P,f,\alpha )\geq 2\epsilon \geq \epsilon \tCaC
\end{align*}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce some theorem regard to more computation.
\end{minipage}}
\begin{theorem}
\label{7.1.15}
\textbf{(Continuous on Compact Interval Implies Integrable)} 
\begin{align*}
f\text{ is continuous on }[a,b]\implies f\in \mathscr{R}(\alpha )\text{ on $[a,b]$ }
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish to  
\begin{align*}
  \vi{\text{ find a partition $P$ such that }U(P,f,\alpha )-L(P,f,\alpha )<\epsilon}
\end{align*}
We know $f$ is uniformly continuous on $[a,b]$. Let  $\delta=\delta( \frac{\epsilon }{2(\alpha (b)-\alpha (a))})$. Let $P$ be a partition such that 
\begin{align*}
\forall i, \Delta x_i<\delta
\end{align*}
This is clearly possible as $b-a$ is finite. Let $[x,y]\subseteq [x_{i-1},x_i]$. We see 
\begin{align*}
d(x,y)<x_i-x_{i-1}=\Delta x_i<\delta
\end{align*}
Then we know  
\begin{align*}
\abso{f(x)-f(y)}\leq \frac{\epsilon }{2(\alpha (b)-\alpha (a))}
\end{align*}
Because 
 \begin{align*}
M_i-m_i=\sup_{[x,y]\subseteq [x_{i-1},x_i]} \abso{f(x)-f(y)}
\end{align*}
We have 
\begin{align*}
M_i-m_i\leq \frac{\epsilon }{2(\alpha (b)-\alpha (a))}
\end{align*}


Then we see  
\begin{align*}
U(P,f,\alpha )-L(P,f,\alpha )&=\sum_{i=1}^n (M_i-m_i) \Delta\alpha (x_i)\\
&\leq \sum_{i=1}^n  \frac{\epsilon}{2(\alpha (b)-\alpha (a))}\Delta \alpha (x_i)=\frac{\epsilon }{2}<\epsilon \vdone
\end{align*}
\end{proof}
\begin{theorem}
\label{7.1.16}
\textbf{(Monotonic Function $f$ is Integrable W.R.T. Continuous $\alpha$)} 
\begin{align*}
\begin{cases}
  f\text{ is monotone on $[a,b]$ }\\
  \alpha \text{ is continuous on }[a,b]
\end{cases}\implies f \in \mathscr{R}(\alpha )\text{ on $[a,b]$ }
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish  
\begin{align*}
\vi{\text{ to find $P$ such that }U(P,f,\alpha )-L(P,f,\alpha )<\epsilon } 
\end{align*}
WOLG, suppose $f$ increase on $[a,b]$. Let 
\begin{align*}
m>\frac{(\alpha (b)-\alpha (a))(f(b)-f(a))}{\epsilon }
\end{align*}
Because $\alpha $ is continuous, we can find a partition $P$ such that 
 \begin{align*}
\alpha (x_j)=\sum_{i=1}^j \frac{i(\alpha (b)-\alpha (a))}{m} +\alpha (a)
\end{align*}
Because $f$ increase, we see 
\begin{align*}
M_i=f(x_i)\text{ and }m_i=f(x_{i-1})
\end{align*}
Then 
\begin{align*}
U(P,f,\alpha )-L(P,f,\alpha )&=\sum_{i=1}^n (f(x_i)-f(x_{i-1})) (\alpha (x_i)-\alpha (x_{i-1}))\\
&=\frac{\alpha (b)-\alpha (a)}{m}\sum_{i=1}^n f(x_i)-f(x_{i-1})\\
&=\frac{(\alpha (b)-\alpha (a))(f(b)-f(a))}{m}<\epsilon \vdone
\end{align*}



\end{proof}
\begin{theorem}
\label{7.1.17}
\textbf{(Integrable Functions Are Closed Under Continuous Composition)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $f\in \mathscr{R}(\alpha )\text{ on $[a,b]$ }$
  \item $h:\R\rightarrow \R$ is continuous on $f[[a,b]]$
\end{enumerate}
We have 
\begin{align*}
h\circ f\in\mathscr{R}(\alpha )\text{ on $[a,b]$ }
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish to find a partition $P$ of $[a,b]$ such that 
\begin{align*}
U(P,h\circ f,\alpha )-L(P,h\circ f,\alpha )<\epsilon 
\end{align*}
Because $f$ is bounded, we can Let $f[[a,b]]\subseteq [m,M]$. Also, let $K=\sup_{x\in [m,M]} \abso{h}$.\\


Notice $h$ is uniform continuous on $[m,M]$. Let $\epsilon_0=\frac{\epsilon }{\alpha (b)-\alpha (a)+2K}$. Let $\delta=\frac{\min  \set{\epsilon_0, \delta(\epsilon_0)}}{2}$.\\

Because $f\in \mathscr{R}(\alpha )$ on $[a,b]$, we can find a partition $P=\set{x_0,\dots ,x_n}$ of $[a,b]$ such that 
\begin{align*}
U(P,f,\alpha )-L(P,f,\alpha )<\delta^2
\end{align*}
We now prove 
\begin{align*}
\vi{\text{ such $P$ works}}
\end{align*}
Define 
\begin{align*}
  \begin{cases}
    M_i=\sup_{x\in [x_{i-1},x_i]} f(x)\text{ and }M_i^*=\sup_{x\in [x_{i-1},x_i]}h \circ f(x)\\
    m_i=\inf_{x\in [x_{i-1},x_i]} f(x)\text{ and }m_i^*=\inf_{x\in [x_{i-1},x_i]}h \circ f(x)
  \end{cases}
\end{align*}
Divide $\set{(m_i^*,M_i^*)\inr^2:1\leq i\leq n}$ into two classes
\begin{align*}
 A= \set{i\inn: i\leq n,M_i - m_i <\delta}\text{ and }B=\set{i\inn:i\leq n,M_i-m_i \geq \delta}
\end{align*}
Because $\delta<\delta(\epsilon_0 )$, we have
\begin{align*}
\sum_{i \in A}(M_i^*-m_i^*)\Delta \alpha (x_i) \leq \sum_{i \in A}\epsilon_0 \Delta \alpha (x_i)\leq \epsilon_0 \sum_{i=1}^n \Delta \alpha (x_i)=\epsilon_0(\alpha (b)-\alpha (a))
\end{align*}
Notice that because $U(P,f,\alpha )-L(P,f,\alpha )<\delta^2$, we see 
\begin{align*}
i \in B \implies M_i-m_i\geq \delta \implies \Delta \alpha (x_i)\leq \delta
\end{align*}
Then because $\delta<\epsilon_0 $, we have 
\begin{align*}
\sum_{i \in B}(M_i^*-m_i^*)\Delta \alpha (x_i)\leq \delta \sum_{i \in B} (M_i^*-m_i^*)\leq \delta 2K\leq \epsilon_0  2K
\end{align*}
We now see 
\begin{align*}
  \sum_{i=1}^n (M_i^*-m_i^*)\Delta \alpha (x_i)\leq \epsilon_0(\alpha (b)-\alpha (a)+2K)<\epsilon \vdone
\end{align*}
\end{proof}
\begin{theorem}
\label{7.1.18}
\textbf{(Very Useful Theorem)} If there exists a sequence of partition $\set{P_n}$ of $[a,b]$ such that 
\begin{align*}
U(P_n,f,\alpha ) \searrow Z \text{ and } L(P_n,f,\alpha )\nearrow Z
\end{align*}
Then 
\begin{align*}
f \in \mathscr{R}(\alpha )\text{ on $[a,b]$ and }\int_a^b f d\alpha =Z
\end{align*}
\end{theorem}
\begin{proof}
For each $\epsilon $, let $n$ satisfy 
 \begin{align*}
U(P_n,f,\alpha )<Z+\frac{\epsilon }{2}\text{ and }L(P_n,f,\alpha )>Z-\frac{\epsilon }{2}
\end{align*}
Then 
\begin{align*}
U(P_n,f,\alpha )-L(P_n,f,\alpha )<\epsilon 
\end{align*}
Because $\epsilon $ is arbitrary, this give us $f\in \mathscr{R}(\alpha )$.\\

We now prove $Z=\int_a^b fd\alpha $ by first assuming $Z$ is larger or smaller and then rejecting this assumption.\\ 

\As{$Z<\int_a^b fd\alpha =\overline{\int_a^b}fd\alpha $}. Because $U(P_n,f,\alpha )\searrow Z$, we then know there exists $n$ such that 
 \begin{align*}
U(P_n,f,\alpha )<\overline{\int_a^b}fd\alpha = \inf_P U(P,f,\alpha )\tCaC
\end{align*}

\As{$Z>\int_a^b fd\alpha =\underline{\int_a^b}fd\alpha $}. Because $L(P_n,f,\alpha )\nearrow Z$, we then know there exists $n$ such that 
 \begin{align*}
L(P_n,f,\alpha )>\underline{\int_a^b}fd\alpha = \sup_P L(P,f,\alpha )\tCaC
\end{align*}
\end{proof}
\section{Properties of Integral}
\begin{theorem}
\label{7.2.1}
\textbf{(Arithmetic of Integrals)} Given $f_1,f_2 \in \mathscr{R}(\alpha )$ on $[a,b]$ and $\alpha $ increase, we have 
\begin{enumerate}[label=(\alph*)]
  \item $f_1+f_2 \in \mathscr{R}(\alpha )$ and $\int_a^b f_1+f_2 d\alpha = \int_a^b f_1 d\alpha  + \int_a^b f_2 d\alpha $
  \item $cf_1 \in \mathscr{R}(\alpha ) $ and $\int_a^b cf_1 d\alpha =c \int_a^b f_1 d\alpha $
  \item  $\int_a^b f_1 d \alpha = \int_a^c f_1 d\alpha +\int_c^b f_1 d\alpha $ 
  \item $\abso{f}\leq M \implies \abso{\int_a^b fd\alpha } \leq M(\alpha (b)-\alpha (a)) $ 
  \item $\int_a^b fd(\alpha _1+\alpha _2)=\int_a^b fd\alpha _1 +\int_a^b fd\alpha_2$ 
  \item $f_1 \leq f_2 \implies \int_a^b f_1 d\alpha  \leq \int_a^b f_2 d\alpha $ 
  \item $f_1f_2 \in \mathscr{R}(\alpha )$
  \item $\abso{f}\in \mathscr{R}(\alpha )$ and $\abso{\int_a^b fd\alpha }\leq  \int_a^b \abso{f}d\alpha $
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\begin{align*}
\vi{\int_a^b f_1 + f_2 d\alpha =\int_a^b f_1 d\alpha +\int_a^b f_2 d\alpha }
\end{align*}
Let $\set{P_n}$ and $\set{Q_n}$ be two sequences of partitions of $[a,b]$ such that 
\begin{align*}
\begin{cases}
  U(P_n,f_1,\alpha ) \searrow   \int_a^b f_1 d\alpha \text{ and } U(Q_n,f_2,\alpha ) \searrow \int_a^b f_2 d\alpha \\
  L(P_n,f_1,\alpha )\nearrow \int_a^b f_1 d\alpha \text{ and }L(Q_n,f_2, \alpha ) \nearrow \int_a^b f_2 d\alpha 
\end{cases}
\end{align*}
Such sequence $\set{P_n}$ that satisfy the two properties exists because we can find two sequence  $\set{P_n^*}$ and $\set{P_{*n}}$ that each satisfy one of the two properties, and let $P_n$ be the common  refinement of $P_n^*$ and  $P_{*n}$ for all $n$.\\

Define sequence $\set{E_n}$ of partitions of $[a,b]$ by 
\begin{align*}
\forall n\inn,E_n=P_n \cup Q_n
\end{align*}
Notice 
\begin{align*}
U(P_n,f_1,\alpha )+U(Q_n,f_2,\alpha )\geq U(E_n,f_1,\alpha )+U(E_n,f_2,\alpha )=U(E_n,f_1+f_2,\alpha )
\end{align*}
Then we have 
\begin{align*}
\limsup_{n\to\infty}  U(E_n,f_1+f_2,\alpha )  \leq  \int_a^b f_1 d\alpha +\int_a^b f_2 d\alpha 
\end{align*}
Also notice 
\begin{align*}
 L(P_n,f_1,\alpha )+L (Q_n,f_2,\alpha ) \leq  L(E_n,f_1,\alpha )+L(E_n,f_2,\alpha )= L(E_n,f_1+f_2,\alpha )
\end{align*}
Then we have 
\begin{align*}
\liminf_{n\to\infty} L(E_n,f_1+f_2,\alpha )\geq \int_a^b f_1 d\alpha  +\int_a^b f_2 d\alpha 
\end{align*}
Then because $U(E_n,f_1+f_2,\alpha )\geq L(E_n,f_1+f_2,\alpha )$, we now have 
\begin{enumerate}[label=(\alph*)]
  \item $\limsup_{n\to\infty} U(E_n,f_1+f_2,\alpha )\leq \int_a^b f_1 d\alpha + \int_a^b f_2 d\alpha $ 
  \item  $\liminf_{n\to\infty} U(E_n,f_1+f_2,\alpha )\geq \liminf_{n\to\infty} L(E_n,f_1+f_2,\alpha )\geq \int_a^b f_1d\alpha +\int_a^b f_2 d\alpha $
\end{enumerate}
Combining above, we now have 
\begin{align*}
\lim_{n \to \infty} U(E_n,f_1+f_2,\alpha )=\int_a^b f_1d\alpha +\int_a^b f_2 d\alpha 
\end{align*}
using similar method, we have 
\begin{align*}
\lim_{n \to \infty}L(E_n,f_1+f_2,\alpha )=\int_a^b f_1d\alpha + \int_a^b f_2 d\alpha 
\end{align*}
Then for each $\epsilon $, we can find $n$ such that 
 \begin{align*}
U(E_n,f_1+f_2,\alpha )-L(E_n,f_1+f_2,\alpha )<\epsilon 
\end{align*}
This give $f_1+f_2 \in \mathscr{R}(\alpha )$. Notice that above give 
\begin{align*}
\overline{\int_a^b} f_1+f_2 d\alpha  \leq \int_a^b f_1 d\alpha +\int_a^b f_2 d\alpha \leq \overline{\int_a^b}f_1+f_2 d\alpha 
\end{align*}
This give 
\begin{align*}
\int_a^b f_1+f_2 d\alpha =\int_a^b f_1 d\alpha +\int_a^b f_2 d\alpha \vdone
\end{align*}
We now prove 
\begin{align*}
\blue{\int_a^b f d\alpha =\int_a^b fd\alpha +\int_c^b fd\alpha }
\end{align*}
Let $\set{R_n}$ and $\set{T_n}$ respectively be partition of $[a,c]$ and $[c,b]$ such that the upper and lower darboux sum approach to $\int_a^c fd\alpha $ and $\int_c^b fd\alpha $. For all $n\inn$, define 
\begin{align*}
E_n=R_n \cup  T_n
\end{align*}
We have 
\begin{align*}
\begin{cases}
U(E_n,f,\alpha )=U(R_n,f,\alpha )+U(T_n,f,\alpha ) \\
L(E_n,f,\alpha )=L(R_n,f,\alpha )+L(T_n,f,\alpha )
\end{cases}
\end{align*}
This give us 
\begin{align*}
U(E_n,f,\alpha )\searrow \int_a^c fd\alpha +\int_c^b fd\alpha \text{ and }L (E_n,f,\alpha )\nearrow \int_a^c fd\alpha +\int_c^b fd\alpha 
\end{align*}
The result follows from \myref{Theorem}{7.1.17}. $\bdone$\\

We now prove 
\begin{align*}
\vi{f_1f_2 \in \mathscr{R}(\alpha )}
\end{align*}
Because Integrable function is closed under continuous composition (\myref{Theorem}{7.1.17}), we know 
\begin{align*}
f_1^2 \in \mathscr{R}(\alpha )\text{ and }f_2^2 \in \mathscr{R}(\alpha )
\end{align*}
Then because 
\begin{align*}
f_1f_2=\frac{(f_1+f_1)^2 -f_1^2 -f_2^2}{2}
\end{align*}
We see $f_1f_2\in \mathscr{R}(\alpha )$. $\vdone$\\


We now prove 
\begin{align*}
\blue{\abso{\int_a^b fd\alpha }\leq \int_a^b \abso{f}d\alpha }
\end{align*}
If $\int_a^b fd\alpha \geq 0$, the proof follows from observing $f\leq \abso{f}$. If $\int_a^b d\alpha <0$, the proof follows from observing $-f\leq \abso{f}$. $\bdone$\\

We now prove 
\begin{align*}
\vi{\int_a^b fd(\alpha _1+\alpha _2)=\int_a^b fd\alpha _1+\int_a^b fd\alpha_2}
\end{align*}
Let $\set{P_n}$ be a sequence of partitions such that the darboux upper and lower sum converge to the integral. Because 
\begin{align*}
&U(P_n,f,\alpha _1+\alpha_2 )=U(P_n,f,\alpha _1)+U(P_n,f,\alpha _2)\\
&L(P_n,f,\alpha _1+\alpha _2)=L(P_n,f,\alpha _1)+L(P_n,f,\alpha _2)
\end{align*}
We have 
\begin{align*}
&U(P_n,f,\alpha _1+\alpha _2)\searrow \int_a^b fd\alpha_1 +\int_a^b fd\alpha_2\\
&L(P_n,f,\alpha _1+\alpha _2)\nearrow \int_a^b fd\alpha _1+\int_a^b fd\alpha _2 \vdone
\end{align*}
\end{proof}
\begin{definition}
\label{7.2.2}
\textbf{(Definition of Unit Step Function)} 
\begin{align*}
I(x):=\begin{cases}
  1& \text{ if  }x> 0\\
 0& \text{ if $x\leq 0$ } 
\end{cases}
\end{align*}
\end{definition}
\begin{theorem}
\label{7.2.3}
\textbf{(Integrate W.R.T. to Unit Step Function)} Let $s \in [a,b]$. Given 
\begin{enumerate}[label=(\alph*)]
  \item $\alpha (x)=I(x-s)$ 
  \item $f$ is bounded on  $[a,b]$ and continuous at $s$
\end{enumerate}
We have 
\begin{align*}
\int_a^b fd\alpha =f(s)
\end{align*}
\end{theorem}
\begin{proof}
For each $n\inn$,define a four points partitions $P_n$ by 
\begin{align*}
P_n=\set{a,s-\frac{1}{n},s+\frac{1}{n},b}
\end{align*}
We see 
\begin{align*}
U(P_n,f,\alpha )=\sum_{i=1}^4 M_i \Delta \alpha (x_i)= \sup_{x \in [s-\frac{1}{n},s+\frac{1}{n}]} f(x) 
\end{align*}
We now prove 
\begin{align*}
  \vi{\sup_{x \in [s-\frac{1}{n},s+\frac{1}{n}]} f(x) \searrow f(s)\text{ as $n\to \infty$ }}
\end{align*}
Because $f$ is continuous at  $s$, for each  $\epsilon $, we can find $n$ such that 
 \begin{align*}
\sup_{x \in [s-\frac{1}{n},s+\frac{1}{n}]}f(x)<f(s)+\epsilon \vdone
\end{align*}
We have proved 
\begin{align*}
U(P_n,f,\alpha ) \searrow f(s)
\end{align*}
With similar method, we can prove 
\begin{align*}
L(P_n,f,\alpha )\nearrow f(s)
\end{align*}

\end{proof}
\begin{theorem}
\label{7.2.4}
\textbf{(Integrate W.R.T to Pure Step Function)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $c_n\geq 0$ 
  \item $\sum c_n$ converge 
  $\set{s_n}_{n\inn} \subseteq (a,b)$
\end{enumerate}
And define 
\begin{align*}
\alpha (x):=\sum_{n=1}^\infty c_n f(s_n)
\end{align*}
We have 
\begin{align*}
f\text{ is bounded and continuous on } [a,b]\implies \int_a^b fd\alpha =\sum_{n=1}^\infty c_n f(s_n)
\end{align*}
\end{theorem}
\begin{proof}
For each $n\inn$, define 
\begin{enumerate}[label=(\alph*)]
  \item $g_n(x):=c_nI(x-s_n)$ 
  \item  $\alpha _n(x):=\sum_{k=1}^n g_n(x)$
\end{enumerate}
We can deduce
\begin{align*}
\int_a^b fd\alpha_n = \sum_{k=1}^n c_k f(s_k) \implies \lim_{n \to \infty}\int_a^b fd\alpha_n =\sum_{n=1}^\infty c_n f(s_n)
\end{align*}
Because of such, we only have to prove 
\begin{align*}
  \vi{\int_a^b fd\alpha =\lim_{n \to \infty} \int_a^b fd\alpha_n}
\end{align*}
Notice that $\alpha -\alpha _n =\sum_{k=n+1}^\infty g(k)$ is an increasing function, so by \myref{Theorem}{7.2.1} 
\begin{align*}
  \abso{\int_a^b fd\alpha_n -\int_a^bfd\alpha }\leq \abso{\int_a^b fd (\alpha -\alpha_n)}\leq \int_a^b \abso{f}d(\alpha -\alpha _n)
\end{align*}
Let $M\geq \sup \abso{f}$. We now have 
\begin{align*}
\int_a^b \abso{f}d(\alpha -\alpha _n) \leq  M (\alpha (b)-\alpha _n(b)-\alpha (a)+\alpha _n(a)) \searrow 0\text{ as $n \to \infty\vdone$ }
\end{align*}


\end{proof}

\fbox{\begin{minipage}{39em}
One can notice that $\alpha $ is increasing in \myref{Theorem}{7.2.4}. Next, we will introduce an integral theorem much better than that of Rudin's. 
\end{minipage}}

\section{Computation of Riemann-Stieltjes Integral}
\begin{theorem}
\label{7.3.1}
\textbf{(Riemann Stieltjes Integral can be Reduced to Riemann Integral if $\alpha $ is good enough)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $\alpha $ increase 
  \item $\alpha '\in \mathscr{R}$ on $[a,b]$ 
  \item $f$ is bounded
\end{enumerate}
We have 
\begin{align*}
 f\in \mathscr{R}(\alpha )\iff  f\alpha ' \in \mathscr{R}
\end{align*}
In which case 
\begin{align*}
\int_a^b fd\alpha =\int_a^b f \alpha ' dx
\end{align*}
\end{theorem}
\begin{proof}
We only have to prove 
\begin{align*}
\overline{\int_a^b} fd\alpha = \overline{\int_a^b}f\alpha ' dx 
\end{align*}
Let $M\geq \sup \abso{f}$. We first prove 
\begin{align*}
  \vi{U(P,\alpha ')-L(P,\alpha ')<\frac{\epsilon}{M}\implies \abso{U(P,f,\alpha )-U(P,f\alpha ')}\leq \epsilon }
\end{align*}
By Mean Value Theorem, we know there exists $x_i^* \in [x_{i-1},x_i]$ such that 
\begin{align*}
\alpha' (x_i^*)= \frac{\alpha (x_i)-\alpha (x_{i-1})}{x_i-x_{i-1}}=\frac{\Delta \alpha (x_i)}{\Delta x_i}
\end{align*}
Then because $U(P,\alpha ')-L(P,\alpha ')<\frac{\epsilon}{M}$, we see if $s_i \in [x_{i-1},x_i]$ then 
\begin{align*}
\sum_{i=1}^n \abso{\alpha '(s_i)-\alpha '(x_i^*)} \Delta x_i <\frac{\epsilon}{M}
\end{align*}
Then because 
\begin{align*}
\sum_{i=1}^n f(s_i)\Delta \alpha (x_i)= \sum_{i=1}^n f(s_i) \alpha '(x_i^*)\Delta x_i
\end{align*}
We now have 
\begin{align*}
\abso{\sum_{i=1}^n f(s_i)\Delta \alpha (x_i)-\sum_{i=1}^n f(s_i)\alpha '(s_i)\Delta x_i}&= \abso{\sum_{i=1}^n f(s_i)\alpha '(x_i^*)\Delta x_i -\sum_{i=1}^n f(s_i)\alpha '(s_i)\Delta x_i}\\
&=\abso{\sum_{i=1}^n f(s_i) (\alpha '(x_i^*)-\alpha '(s_i))\Delta x_i}\\
&=\sum_{i=1}^n \abso{f(s_i)(\alpha '(x_i^*)-\alpha '(s_i))\Delta x_i} \\
&=\sum_{i=1}^n \abso{f(s_i)}\cdot \abso{(\alpha '(x_i^*)-\alpha '(s_i))\Delta x_i}\\
&\leq \sum_{i=1}^n M \cdot \abso{(\alpha '(x_i^*)-\alpha '(s_i))\Delta x_i} < \epsilon  
\end{align*}
This give us 
\begin{align*}
\forall s_i,\sum_{i=1}^n f\alpha ' (s_i)\Delta x_i< \sum_{i=1}^n f(s_i)\Delta \alpha (x_i) + \epsilon \leq U (P,f,\alpha )+\epsilon 
\end{align*}
Then we have 
\begin{align*}
U(P,f\alpha ')\leq U(P,f,\alpha )+\epsilon 
\end{align*}
With similar method, we have 
\begin{align*}
U(P,f,\alpha )\leq U(P,f\alpha ')+\epsilon 
\end{align*}
which give us 
\begin{align*}
\abso{U(P,f,\alpha )-U(P,f\alpha ')}\leq \epsilon \vdone
\end{align*}
\As{$\overline{\int_a^b} fd\alpha <\overline{\int_a^b}f\alpha 'dx$}. Then there exists partition $W$ such that 
\begin{align*}
  U(W,f,\alpha )< \frac{\overline{\int_a^b} f\alpha 'dx+\overline{\int_a^b}fd\alpha }{2}
\end{align*}
Let $\epsilon = \frac{\overline{\int_a^b}f\alpha 'dx-\overline{\int_a^b }fd\alpha }{2}$, and let $P$ satisfy the hypothesis of \vi{violet lemma}. We see $W\cup P$ must also satisfy the \vi{violet lemma}, which give us 
\begin{align*}
U(W\cup  P,f,\alpha )\leq U(W,f,\alpha )<\overline{\int_a^b}f\alpha 'dx - \epsilon 
\end{align*}
Then from $U(W\cup  P,f\alpha ')\geq \overline{\int_a^b}f\alpha 'dx$, we have 
\begin{align*}
U(W\cup  P,f\alpha ')-U(W\cup P,f,\alpha )>\epsilon \tCaC \text{ to }\vi{\text{violet lemma}}
\end{align*}
Other side of assumption is rejected similarly with \vi{violet lemma}.\\

With similar method from above, we have 
\begin{align*}
\underline{\int_a^b}f\alpha 'dx=\underline{\int_a^b}fd\alpha 
\end{align*}
Then 
\begin{align*}
\overline{\int_a^b}f\alpha 'dx=\underline{\int_a^b}fd\alpha \iff  \overline{\int_a^b}f\alpha 'dx=\underline{\int_a^b}fd\alpha 
\end{align*}
And from $\overline{\int_a^b}f\alpha 'dx=\overline{\int_a^b}fd\alpha $, we have $\int_a^b f\alpha 'dx =\int_a^b fd\alpha $. 
\end{proof}
\begin{theorem}
\label{7.3.2}
\textbf{(Change of Variable)} Given that 
\begin{align*}
\int_a^b fd\alpha \text{ exists }
\end{align*}
Let $\phi:[A,B]\to [a,b]$ be continuous, increasing and bijective, and define 
\begin{align*}
\beta = \alpha \circ  \phi \text{ and }g = f \circ  \phi 
\end{align*}
Then we have 
\begin{align*}
\int_A^B g d \beta =\int_a^b fd\alpha \text{ if L.H.S. exists }
\end{align*}
\end{theorem}
\begin{proof}
For each partition $Q=\set{x_1,\dots ,x_n}$ of  $[A,B]$, we can associate $Q$ with a partition $P_Q$ defined by 
 \begin{align*}
P_Q=\set{\phi (x_1) ,\dots ,\phi (x_n)}
\end{align*}
Notice that because $\phi$ is bijective, such an association is also bijective from the set of partition of $[A,B]$ to the set of those of $[a,b]$.\\ 

Because $\phi$ is continuous and increasing, we have  
\begin{align*}
  g[[x_{i-1},x_i]]=f [[\phi (x_{i-1}), \phi (x_i)]]
\end{align*}
Same with 
\begin{align*}
\Delta \beta (x_i)=\beta (x_i)-\beta  (x_{i-1})= \alpha \phi (x_i)-\alpha \phi (x_{i-1})
\end{align*}
Now we have 
\begin{align*}
U(Q,g,\beta )= U(P_Q,f,\alpha )
\end{align*}
The proof follows form noticing the association of $Q$ and  $P_Q$ is bijective. 

\end{proof}
\section{Integration and Differentiation}
\begin{theorem}
\label{7.4.1}
  \textbf{(Antiderivative)} Define
  \begin{align*}
  F:[a,b]\rightarrow \R, x \mapsto  \int_a^x fdt
  \end{align*}
We have 
\begin{align*}
F\text{ is continuous and $F'(x_0)=f(x_0)$ if $f$ is continuous at $x_0$}
\end{align*}
Notice that for $F$ to be defined on  $[a,b]$, we must be provided $f\in \mathscr{R}$ on $[a,b]$ 
\end{theorem}
\begin{proof}
We first prove 
\begin{align*}
\vi{F\text{ is continuous }}
\end{align*}
Because $f\in \mathscr{R}$, we know $\abso{f}$ is bounded by some positive number $M$.\\

Notice that 
\begin{align*}
  \abso{F(x+h)-F(x)}=\abso{\int_x^{x+h} f dt} \leq \int_x^{x+h} \abso{f}dt \leq hM \to 0 \text{ as $h \to 0$ }\vdone
\end{align*}
We now prove 
\begin{align*}
  \blue{f\text{ is continuous at $x_0$ }\implies F'(x_0)=f(x_0)}
\end{align*}
Consider the trivial partition $\set{x,x+h}$ of $[x,x+h]$, we have 
\begin{align*}
\int_x^{x+h}fdt \leq \sup_{[x,x+h]}f(t)h
\end{align*}
This give us 
\begin{align*}
  \frac{\int_x^{x+h}fdt}{h}\leq \sup_{[x,x+h]}f(t) \searrow f(x) \text{ as $h \to 0$ }
\end{align*}
The reason $\sup_{[x,x+h]}f(t) \searrow f(x)$ is that $f$ is continuous at  $x$. We now have 
\begin{align*}
  \limsup_{h \to 0}  \frac{\int_x^{x+h}fdt}{h} \leq f(x) 
\end{align*}
Also, 
\begin{align*}
\int_{x}^{x+h} fdt \geq \inf_{[x,x+h]}f(t)h
\end{align*}
This give us 
\begin{align*}
\frac{\int_x^{x+h}fdt}{h}\geq \inf_{[x,x+h]}f(t)\nearrow f(x)\text{ as }h \to 0
\end{align*}
We now have 
\begin{align*}
\frac{\int_x^{x+h}fdt}{h} \to f(x) \text{ as $h\to 0$ }
\end{align*}
The other side $\int_{x-h}^x$ is done exactly the same way. $\bdone$

\end{proof}
\fbox{\begin{minipage}{39em}
Give an example of $F$ is differentiable on $[a,b]$ yet  $\int_a^b fdx$ does not exist? See $x^2 \sin \frac{1}{x^2}$.\\

Give an example of $F$ is differentiable on  $[a,b]$, $f$ is bounded on $[a,b]$, yet $\int_a^b fdx$ does not exists?  
\end{minipage}}
\begin{theorem}
\label{7.4.2}
\textbf{(Fundamental Theorem of Calculus)} Given 
\begin{align*}
F\text{ is differentible on $[a,b]$ and $f=F'$ }
\end{align*}
We have 
\begin{align*}
\int_a^b fdx= F(b)-F(a)\text{ if $\int_a^b fdx$ exists }
\end{align*}
\end{theorem}
\begin{proof}
For each partition $\set{x_0,\dots ,x_n}$ of $[a,b]$, by M.V.T, we know there exists $\set{t_1,\dots, t_n}$ such that 
\begin{align*}
t_k \in [x_{k-1},x_k]\text{ and } F(x_k)-F(x_{k-1})=f(t_k)\Delta x_k
\end{align*}
This give us 
\begin{align*}
F(b)-F(a)=\sum_{k=1}^n F(x_k)-F(x_{k-1}) =\sum_{k=1}^n f(t_k)\Delta x_k 
\end{align*}
where 
\begin{align*}
L(P,f)\leq \sum_{k=1}^n f(t_k)\Delta x_k \leq U(P,f)
\end{align*}
We have proved, for all partition $P$, 
 \begin{align*}
L(P,f)\leq F(b)-F(a) \leq U(P,f)
\end{align*}
Then we have $\int_a^b fdt =F(b)-F(a)$. 
\end{proof}
\begin{theorem}
\label{7.4.3}
\textbf{(Integral by Part)} Suppose $F,G$ are differentiable on $[a,b]$ and  $f,g \in \mathscr{R}$ on $[a,b]$. We have 
\begin{align*}
\int_a^b Fgdx=FG\at_{a}^b-\int_a^b fGdx
\end{align*}
\end{theorem}
\begin{proof}
We have 
\begin{align*}
D(FG)=Fg+fG
\end{align*}
Then 
\begin{align*}
\int_a^b Fgdx+\int_a^b fGdx&= \int_a^b Fg+fG dx\\
&=FG\at_{a}^b
\end{align*}
\end{proof}







\chapter{Advanced Calculus HW}
\section{HW1}
\begin{question}{}{}
\begin{enumerate}
    \item Prove that the following statements are equivalent: for a given sequence $\{x_n\}$,
    \begin{enumerate}
        \item for every $0 < \epsilon \in \mathbb{Q}$, there exists $N \in \mathbb{N}$ such that $|x_n - x| < \epsilon$ whenever $n \geq N$.
        \item for every $0 < \epsilon \in \mathbb{R}$, there exists $N \in \mathbb{N}$ such that $|x_n - x| < \epsilon$ whenever $n \geq N$.
    \end{enumerate}
\end{enumerate}
\end{question}
\begin{proof}
From $(b)$ to $(a)$, just observe  $\Q \subseteq \R$ and we are done.Now we prove from $(a)$ to $(b)$.\\

Because $\Q$ is dense in  $\R$,  for all $\epsilon \in \R^+$, we can pick $\epsilon' \in \Q$ such that $0<\epsilon '< \epsilon $. By $(a)$,  we know there exists $N\in \N$ such that $\abso{x_n-x}<\epsilon ' <\epsilon $ whenever $n\geq N$. This finish the proof.
\end{proof}
\begin{question}{}{}
2. Let $\{x_n\}_{n=1}^{\infty}$ be a monotone increasing sequence such that 
\[x_{n+1} - x_n \leq \frac{1}{n}.\]
Determine whether the sequence converges. (If yes, prove it; if not, disprove it or give a counterexample.)
\end{question}
\begin{proof}
No, consider $p$-series. The sequence $\set{x_n}_{n=1}^{\infty}$ defined by $x_i:=\sum_{j=1}^i \frac{1}{j}$ is monotone increasing, satisfying the desired property, and from our knowledge, diverge. 
\end{proof}
\begin{question}{}{}
Let $M_{n \times m}$ be the collection of all $n \times m$ matrices with real entries. Define a function $\| \cdot \|: M_{n \times m} \to \mathbb{R}$ by
\[
\| A \| = \sup \left\{ \frac{\|Ax\|_2}{\|x\|_2} : x \in \mathbb{R}^m, x \neq 0 \right\},
\]
where we recall that $\| \cdot \|_2$ is the 2-norm on Euclidean space given by
\[
\| Ax \|_2 = \left( \sum_{i=1}^{k} x_i^2 \right)^{1/2} \quad \text{if } x \in \mathbb{R}^k.
\]
Show that:
\begin{enumerate}
    \item $\| A \| = \sup \left\{ \|Ax\|_2 : x \in \mathbb{R}^m, \norm{x}_2 =1 \right\} = \inf \left\{ M \in \mathbb{R} : \|Ax\|_2 \leq M \|x\|_2 \, \forall x \in \mathbb{R}^m \right\}$.
    \item $\| Ax \|_2 \leq \| A \| \| x \|_2$ for all $x \in \mathbb{R}^m$.
    \item $\| \cdot \|$ defines a norm on $M_{n \times m}$.
\end{enumerate}
\end{question}
\begin{proof}
In this proof, we use $\abso{\cdot}$ to denote $\norm{\cdot}_2$, and if we write $x$ without specification, it belong to  $\R^m$\\

We first show \vi{$\norm{A}= \sup \set{ \abso{Ax}: \abso{x}=1}$}\\

\As{$\sup \set{\frac{\abso{Ax}}{\abso{x}}:x\neq 0}=\norm{A}>  \sup \set{\abso{Ax} : \abso{x}=1} $}.Then we know $ \sup \set{\abso{Ax}:\abso{x}=1} $ is not an upper bound of $ \set{\frac{\abso{Ax}}{\abso{x}}:x\neq 0} $, so we know there exists $x\in \R^m$ such that $\frac{\abso{Ax}}{\abso{x}}>\sup \set{\abso{Ay}:\abso{y}=1}$.\\

Define $\hat{x}:=\frac{x}{\abso{x}}$. We have $\frac{\abso{Ax}}{\abso{x}}=\abso{\frac{Ax}{\abso{x}}}=\abso{A \hat{x}}\leq \sup \set{\abso{Ay}:\abso{y}=1}
$, since $\abso{\hat{x}}=\abso{\frac{x}{\abso{x}}}=\frac{\abso{x}}{\abso{x}}=1\implies \abso{A \hat{x}}\in \set{\abso{Ay}:\abso{y}=1}$. This \CaC.\\

\As{$\sup \set{\frac{\abso{Ax}}{\abso{x}}:x\neq 0}=\norm{A}<\sup \set{\abso{Ax} : \abso{x}=1} $}. Then we know $\sup \set{\frac{\abso{Ax}}{\abso{x}}:x\neq 0}$ is not an upper bound of $\sup \set{\abso{Ax}:\abso{x}=1}$, so we know there exists $\hat{x} \in \R^m:\abso{\hat{x}}=1$ such that $\abso{A \hat{x}}>\sup \set{\frac{\abso{Ay}}{\abso{y}}:y\neq 0}$.\\

We see $\abso{A \hat{x}}>\sup \set{\frac{\abso{Ay}}{\abso{y}}:y\neq 0}\geq \frac{\abso{A \hat{x}}}{\abso{\hat{x}}}=\frac{\abso{A \hat{x}}}{1}=\abso{A \hat{x}}\tCaC \vdone$\\

Observe $  \inf \left\{ M \in \mathbb{R} : \|Ax\|_2 \leq M \|x\|_2 \, \forall x \in \mathbb{R}^m \right\}= \inf \set{c \in \R: \forall x\neq 0, c\geq \frac{\abso{Ax}}{\abso{x}}}$, since $\forall M, \abso{A\vecta{0}}\leq M\abso{\vecta{0}}$.\\

Observe that $   \set{c \in \R: \forall x\neq 0, c\geq \frac{\abso{Ax}}{\abso{x}}}$ is the set of upper bound of $\set{\frac{\abso{Ax}}{\abso{x}}:\abso{x}\neq 0}$, so $\inf   \set{c \in \R: \forall x, c\geq \frac{\abso{Ax}}{\abso{x}}}=\norm{A}=\sup \set{\frac{\abso{Ax}}{\abso{x}}:\abso{x}\neq 0}$.\\
\end{proof}
\begin{proof}
In this proof, we use $\abso{\cdot}$ to denote $\norm{\cdot}_2$, and if we write $x$ without specification, it belong to  $\R^m$\\

If $x=0$, then we trivially have  $\abso{Ax}=\abso{0}=0\leq \norm{A}\abso{x}=0$, so from now, we only have to consider $x\neq 0$.\\

If $x\neq 0$, we have $\abso{Ax}\leq \norm{A}\abso{x}\iff \frac{\abso{Ax}}{\abso{x}}\leq \norm{A}=\sup \set{\frac{\abso{Ax}}{\abso{x}}:x\neq 0}$, trivially true.
\end{proof}
\begin{proof}
In this proof, we use $\abso{\cdot}$ to denote $\norm{\cdot}_2$, and if we write $x$ without specification, it belong to  $\R^m$\\

For non-negativity, observe $\forall x\neq 0,\frac{\abso{Ax}}{\abso{x}}\geq 0 \implies \norm{A}=\sup \set{\frac{\abso{Ax}}{\abso{x}}:x \in \R^m, x\neq 0}\geq 0$.\\

For definite-positive, observe $A=0\implies \forall x\neq 0, \frac{\abso{Ax}}{\abso{x}}=\frac{\abso{0}}{\abso{x}}=0\implies \norm{A}=0$. Also, if $A\neq 0$, we can pick a column, say $p$-th column, that contain non-zero entry. We see the vector $e\in \R^m$ where the only non-zero entry is the  $p$-th entry being $1$ satisfy $\abso{Ae}>0$, thus $\frac{\abso{Ae}}{\abso{e}}>0$. Because $e\neq 0$, we see $\norm{A}=\sup \set{\frac{\abso{Ax}}{\abso{x}}:x\in\R^m,x\neq 0}\geq \frac{\abso{Ae}}{\abso{e}}>0$.\\

For absolute-homogenity, let $c \in \R\text{ and }A\in M_{n\times m}$. We wish to prove \vi{$\norm{cA}=\sup \set{\frac{\abso{cAx}}{\abso{x}}:x\neq 0}=\abso{c}\sup \set{\frac{\abso{Ax}}{\abso{x}}:x\neq 0}=\abso{c}\norm{A}$}. Notice that $\frac{\abso{cAx}}{\abso{x}}=\frac{\abso{c}\abso{Ax}}{\abso{x}}$, so we only have to prove the more general statement : $c>0 \implies c \sup X= \sup \set{cx:x\in X}$. Notice that $\forall x \in X, c \sup X\geq cx$, so we have $c\sup X\geq \sup \set{cx:x\in X}$. If $c\sup X$ is not the smallest upper bound, we see there exists $cx$ such that $c\sup X<cx$, and we see $\sup X<x$, causing a contradiction, so we do have $c\sup X=\sup \set{cx:x\in X}\vdone$   \\

For triangle-inequality, first observe $\frac{\abso{(A+B)x}}{\abso{x}}=\frac{\abso{Ax+Bx}}{\abso{x}}\leq \frac{\abso{Ax}+\abso{Bx}}{\abso{x}}=\frac{\abso{Ax}}{\abso{x}}+\frac{\abso{Bx}}{\abso{x}}$. \As{$\norm{A+B}>\norm{A}+\norm{B}$}.\\

Because $\norm{A}+\norm{B}$ is not an upper bound of $\set{\frac{\abso{(A+B)x}}{\abso{x}}:x\neq 0}$, we know there exists $x'$ such that  $\frac{\abso{(A+B)x'}}{\abso{x'}}>\norm{A}+\norm{B}$. Further, by definition of $\norm{A},\norm{B}$, we have
\begin{equation*}
\frac{\abso{(A+B)x'}}{\abso{x'}}>\frac{\abso{Ax'}}{\abso{x'}}+\frac{\abso{Bx'}}{\abso{x'}}\tCaC
\end{equation*}
\end{proof}

\begin{question}{}{}
Suppose that \(S_1, S_2, \ldots, S_n\) are sets in \(\mathbb{R}\) and 
\[ S = \bigcup_{i=1}^{n} S_i. \]
Define \(B_i = \sup S_i\) for \(i = 1, \ldots, n\).

\begin{enumerate}
    \item Show that \(\sup S = \max\{B_1, B_2, \ldots, B_n\} \).
    \item If \(S\) is the union of an infinite collection of \(S_i\), find the relation between \(\sup S\) and \(B_i\).
\end{enumerate}
\end{question}
\begin{proof}
Let $\sup S_j=B_j=\max \set{B_1,\dots, B_n}$. We first show \vi{$\sup S_j$ is an upper bound of $S$}.\\

By definition, we have $\forall x\in S_j,x\leq \sup S_j$ and have $\forall i \neq j, \forall x \in S_i, x\leq \sup S_i\leq \sup S_j $, so we have $\forall x\in S, \exists k \in \set{1,\dots, n}, x \in S_k \implies x\leq \sup S_k\leq \sup S_j\vdone$\\

We now show  \blue{$\sup S_j$ is the least upper bound of $S$}.\\

\As{there exists an upper bound of $S$ smaller than $\sup S_j$}. Denote that upper bound $y$. Because $y$ is smaller than $\sup S_j$, we know $y$ is not an upper bound of $S_j$, so we know there is a number  $z \in S_j$ greater than $y$. Observe that the fact $y$ is an upper bound of $S$ implies  $y$ is greater than or equal to $z \in S_j\subseteq S\tCaC\bdone$
\end{proof}
\begin{proof}
  We prove \vi{$\sup S=\sup \set{B_i}$}.\\

  Notice $\sup S$ is an upper bound of $S_i$, so we have $\forall i, \sup S>\sup S_i=B_i$. This means $\sup S$ is an upper bound of $\set{B_i}$. We have proved $\sup S\geq \sup \set{B_i}$. \As{$\sup S>\sup \set{B_i}$}. Then because $\sup \set{B_i}$ is not an upper bound of $S$, we know there exists  $s \in S$ such that $s>\sup \set{B_i}$. But because $S=\bigcup \set{S_i}$, we know $\exists S_j,s \in S_j$, which give us $s\leq \sup S_j=B_j\leq \sup \set{B_i}\tCaC\vdone$
\end{proof}
\begin{question}{}{}
Let \( A \) be a non-empty set of \( \mathbb{R} \) which is bounded below. Define the set \( -A \) by 
\[ -A \equiv \{-x \in \mathbb{R} : x \in A\}. \]
Prove that
\[ \inf(A) = -\sup(-A). \]
\end{question}
\begin{proof}
Observe $\forall x\in -A, \sup (-A)\geq  x\implies \forall a \in A, -\sup (-A)\leq  a$. So $-\sup (-A)$ is an lower bound of $A$. \As{$-\sup (-A)$ is not the greatest lower bound of $A$} (greatest lower bound exists because bounded below and completeness). Let $b>-\sup (-A)$ be another lower bound of $A$. We have $-b<\sup (-A)$, so we know $-b$ is not an upper bound of $-A$, then we know  $\exists x \in -A, -b<x$. Then we know $\exists a \in A, -b<-a$, which implies $\exists a \in A, b>a$, but $b$ is an lower bound of $A\tCaC$
\end{proof}
\begin{question}{}{}
\begin{enumerate}
    \item Let \( A, B \) be non-empty subsets of \( \mathbb{R} \). Define \( A+B \) as 
    \[ A+B = \{ x+y : x \in A, y \in B \}. \]
    Justify if the following statements are true or false by providing a proof for the true statements and giving a counter-example for the false ones.
    \begin{enumerate}
        \item \( \sup(A+B) = \sup A + \sup B \).
        \item \( \inf(A+B) = \inf A + \inf B \).
        \item \( \sup(A \cap B) \leq \min\{\sup A, \sup B\} \).
        \item \( \sup(A \cap B) = \min\{\sup A, \sup B\} \).
        \item \( \sup(A \cup B) \geq \max\{\sup A, \sup B\} \).
        \item \( \sup(A \cup B) = \max\{\sup A, \sup B\} \).
    \end{enumerate}
\end{enumerate}
\end{question}
\begin{proof}
We prove $\sup (A+B)=\sup A+\sup B$. For all $a+b \in A+B$, we by definition have $a\leq \sup A, b\leq \sup B$, so we have $a+b\leq \sup A+\sup B$. This prove $\sup A+\sup B$ is an upper bound of $A+B$. \As{there exists an upper bound $x$ of $A+B$ smaller than $\sup A+\sup B$}. We have $x-\sup B<\sup A$, so we know $x-\sup B$ is not an upper bound of $A$. Then we know  $\exists a'\in A, x-\sup B<a'$. This implies $x-a'< \sup B$, so we know $x-a'$ is not an upper bound of  $B$. Then we know there exists  $b' \in B$ such that $x-a'< b'$. This implies $x<a'+b' \in A+B\tCaC$ to $x$ is an upper bound of  $A+B$
\end{proof}
\begin{proof}
We prove $\inf (A+B)= \inf A +\inf B$. For all $a+b \in A+B$, we by definition have $\inf A\leq a,\inf B\leq b$, so we have $\inf A+\inf B\leq a+b$. This prove $\inf A+ \inf B$ is an lower bound of $A+B$.  \As{there exists an lower bound $x$ of $A+B$ greater than $\inf A+\inf B$}. We have $x- \inf A > \inf B$, so we know $x - \inf A$ is not an lower bound of $B$. Then we know  $\exists b' \in B, x- \inf A > b'$. This implies $x-b'>\inf A$, so we know $x- b'$ is not an lower bound of $A$. Then we know  $\exists a' \in A, x-b'>a'$. So we know $x<a'+b' \in A+B\tCaC$ to $x$ is an lower bound of $A+B$ 
\end{proof}
\begin{proof}
We prove $\sup (A\cap B)\leq \min \set{\sup A,\sup  B}$.\\

WOLG, let $\sup A\leq \sup B$. By definition $x \in A\cap B \implies  x \in A \implies x\leq \sup A$, so we know $\sup A$ is an upper bound of $A\cap B$. This implies $\sup A\cap B\leq \sup (A\cap B)$ 
\end{proof}
\begin{proof}
We show $\sup (A\cap B)= \min \set{\sup A,\sup B}$ is not always correct. Let $A=[0,2]$ and $B=[0,1]\cup [3,4]$. We have $\sup (A\cap B= [0,1])=1\neq \min{\set{\sup A=2,\sup B=4}}$
\end{proof}
\begin{proof}
$\sup A\cup B$ is an upper bound of both $A$ and  $B$, so  $\sup A\cup B>\sup A\text{ and }\sup A\cup B>\sup B$

\end{proof}
\begin{proof}
We prove $\sup (A\cup B)=\max \set{\sup A,\sup B}$. WOLG, let $\sup A\geq \sup B$. \As{$\sup B\leq \sup A<\sup (A\cup B)$}. Let $x$ be a number between  $\sup A\text{ and }\sup A\cup B(x\text{ exists since it can be $\frac{\sup A+\sup (A\cup B)}{2}$ })$. Because $x<\sup (A\cup B)$, we know $x$ is not an upper bound of  $A\cup B$. By definition, we know there exists $z\in A\cup B$ such that $x<z$. We know either $z \in A$ or $z \in B$, but we see $z \in A\implies z\leq \sup A<x$ and we see $z \in B\implies z\leq \sup B\leq \sup A<x\tCaC$
\end{proof}
\begin{question}{}{}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item Let \( S \subseteq \mathbb{R} \) be bounded below and non-empty. Show that
    \[ \inf S = \sup\{ x \in \mathbb{R} : x \text{ is a lower bound for } S \}. \]
\end{enumerate}
\end{question}
\begin{proof}
Denote $B=\set{x\in R: x\text{ is a lower bound for $S$ }}$. \As{$\inf S> \sup B$}. Let $\inf S>x>\sup B$. Notice $\inf S>x$ implies there is a lower bound  $b$ of  $S$ greater than  $x$. Observe $b>x\text{ and }b \in B\implies x$ is not an upper bound of $B\tCaC x>\sup B$.\\

\As{$\inf S<\sup B$}. Let $\inf S<x<\sup B$. Notice $\inf S<x$ implies there exists $s' \in S$ such that $s'<x$, and notice $x> \sup B$ implies there exists $b'\in B$ such that $b'>x$. We see  $b'>x>s'$ while  $b'$ is an lower bound of  $S\tCaC$
\end{proof}
\begin{question}{}{}
8. Let \( f \) be a continuous function on \( \mathbb{R} \) and \( D \) is a dense subset in \( \mathbb{R} \). Prove that:
\begin{enumerate}
    \item \( \sup_{x \in D} f(x) = \sup_{x \in \mathbb{R}} f(x) \).
    \item There exists a sequence \( \{x_n\}_{n=1}^{\infty} \) in \( D \) such that 
    \[
    \lim_{n \to \infty} f(x_n) = \sup_{x \in \mathbb{R}} f(x).
    \]
\end{enumerate}
\end{question}
\begin{proof}
Because $D\subseteq \R$, we must have $\sup \set{f(x): x\in D}\leq \sup \set{f(x): x\in \R}$, since any upper bound of the latter will be one of the former.\\

\As{$\sup \set{f(x):x \in D}<\sup \set{f(x):x\in \R}$}. Because $\sup \set{f(x):x\in D}$ is not an upper bound of $\set{f(x):x\in \R}$, we know there exists $x'\in \R$ such that 
\begin{equation*}
f(x')>\sup \set{f(x):x\in D}
\end{equation*}
We first \vi{construct a sequence $\set{x_i}_{i=1}^\infty$ in $D$ such that} 
\begin{equation*}
  \vi{\lim_{i\to \infty}x_i=x'\text{ which reads } \forall \epsilon, \exists N, n>N \longrightarrow \abso{x_n - x'}<\epsilon}
\end{equation*}
By Axiom of Choice and the fact that $D$ is dense in  $\R$, we can pick $x_i \in (x'- \frac{1}{i},x' +\frac{1}{i})$, so we have
\begin{equation*}
\forall i, \abso{x_i-x'}<\frac{1}{i}
\end{equation*}
For all $\epsilon $, we can pick a natural $N>\epsilon $, so we have 
\begin{equation*}
n>N\implies \abso{x_n-x'}<\frac{1}{n}<\frac{1}{N}<\frac{1}{\epsilon }\vdone
\end{equation*}
We now prove 
\begin{equation*}
\blue{\lim_{i\to\infty}f(x_i)=f(x')\text{ which reads }\forall \epsilon , \exists N, n>N\longrightarrow  \abso{f(x_n)-f(x')}<\epsilon }
\end{equation*}
Because $f$ is continuous, we have 
\begin{equation*}
  \forall \epsilon ,\exists \delta, \forall u\in \R, \abso{u-x'}<\delta\implies \abso{f(u)-f(x')}<\epsilon  
\end{equation*}
Then for all $\epsilon $, we can first let $\delta$ satisfy $\abso{u-x}<\delta \implies \abso{f(u)-f(x')}<\epsilon $. Then by the \vi{violet fact} we can pick $N$ such that 
\begin{equation*}
n>N\implies \abso{x_n - x'}< \delta \implies \abso{f(x_n)-f(x')}<\epsilon \bdone 
\end{equation*}
Let $H=\sup \set{f(x): x\in D}$. Now we prove  
\begin{equation*}
  \vi{\lim_{i \to \infty}f(x_i)\leq H}
\end{equation*}
\As{$f(x')=\lim_{i \to \infty} f(x_i) >H $}. We know there exists $N$ such that 
\begin{equation*}
n>N\longrightarrow \abso{f(x_n)-f(x')}<\abso{H - f(x') }=f(x')-H
\end{equation*}
The last equality hold true due to the premise equation (5.2). Notice 
\begin{equation*}
\abso{f(x_n)-f(x')}<f(x')-H\implies H-f(x')< f(x_n)-f(x')\implies f(x_n)>H
\end{equation*}
so in fact we know there exists $N$ such that 
\begin{equation*}
n>N \longrightarrow f(x_n)>H= \sup \set{f(x): x \in D}\tCaC\vdone
\end{equation*}
Now, using all our proven facts, we have
\begin{equation*}
\sup \set{f(x): x\in D}<f(x')=\lim_{i\to \infty }f(x_i)\leq H=\sup \set{f(x): x \in D}\tCaC
\end{equation*}
where the first inequality follows from premise equation (5.2) 
\end{proof}
\begin{proof}
Let $H=\sup \set{f(x): x \in D}$. We first prove
\begin{equation*}
\vi{\forall i \in \N, \set{f(x): x \in D\text{ and }H-f(x)<\frac{1}{i}}\neq \varnothing}
\end{equation*}
\As{there exists some $n\inn$ such that the set is empty}. We then have
\begin{equation*}
\forall x\in D, H\geq f(x)+\frac{1}{n}
\end{equation*}
So we have
\begin{equation*}
\forall x\in D, H-\frac{1}{2n}\geq f(x)+\frac{1}{2n}>f(x)
\end{equation*}
Then we see $H-\frac{1}{2n}$ is an upper bound of $\set{f(x): x \in D}$ smaller than $H\tCaC\vdone$\\

By Axiom of Choice, we can construct a sequence $\set{x_i}_{i=1}^\infty$ by picking $x_i: f(x_i)\in \set{f(x):x \in D\text{ and }H-f(x)<\frac{1}{i}}$. Then we have
\begin{equation*}
\forall \epsilon, n> [\frac{1}{\epsilon }]+1\implies n>\frac{1}{\epsilon }\implies \abso{f(x_n)-H}<\frac{1}{n}<\epsilon 
\end{equation*}
This written in limit sign is 
\begin{equation*}
\lim_{i\to \infty}f(x_i)=H=\sup \set{f(x): x\in D}=\sup \set{f(x):x \inr}
\end{equation*}
\end{proof}
\section{HW2}
\begin{question}{}{}

Let \(A \subseteq \mathbb{R}^n\) be an open set and \(B \subset \mathbb{R}^n\) be any set. Then the set 
\[ A + B \equiv \{a + b : a \in A \text{ and } b \in B\} \]
is open.
\end{question}
\begin{proof}
Notice 
\begin{equation*}
A+B= \bigcup \set{\set{a+b:a\in A}:b\in B}
\end{equation*}
We only have to prove  \vi{for all $b\in B$, the set $A+b:=\set{a+b:a\in A}$ is open}.\\

Fix $b$. Arbitrarily pick $a+b\in A+b$. Because $A$ is open, we know there exists $r\inr^+$ such that
\begin{equation*}
B_r(a)\subseteq A
\end{equation*}
Let
\begin{equation*}
B_r(a)+b:=\set{x+b:x\in B_r(a)}
\end{equation*}
We now prove 
\begin{equation*}
\teal{B_r(a)+b=B_r(a+b)}
\end{equation*}
Arbitrarily pick $x+b\in B_r(a)+b$. We have
\begin{equation*}
\abso{(x+b)-(a+b)}=\abso{x-a}<r
\end{equation*}
We have proved $B_r(a)+b\subseteq B_r(a+b)$. Arbitrarily pick $y\in B_r(a+b)$. Let $z=y-b$. We have
\begin{equation*}
y=z+b\text{ and }\abso{z-a}=\abso{y-(a+b)}<r
\end{equation*}
The latter tell us $z \in B_r(a)$, so we have
\begin{equation*}
y=z+b \in B_r(a)+b
\end{equation*}
Because $y$ is arbitrarily picked from $B_r(a+b)$, we have proved $B_r(a+b)\subseteq B_r(a)+b\tdone$\\

Notice that  $r$ is selected to satisfy
\begin{equation*}
B_r(a)\subseteq A
\end{equation*}
and it is clear that
\begin{equation*}
B_r(a)+b\subseteq A+b
\end{equation*}
So we have
\begin{equation*}
B_r(a+b)=B_r(a)+b\subseteq A+b
\end{equation*}
Notice $a+b$ is arbitrarily picked from $A+b$. Our proof is done  $\vdone$
\end{proof}

\begin{question}{}{}
Show that every open set in \( \mathbb{R} \) is the union of at most countable collection of disjoint open intervals.
\end{question}
\begin{proof}
Because $\Q$ is countable and dense in  $\R$, so in \myref{Theorem}{3.9.1}, we have proved the set
\begin{equation*}
\mathcal{O}:=\set{B_r(p):p\inq\text{ and }r\inq^+}
\end{equation*}
is a countable base. Notice that every open ball $B_r(p)$ can be expressed as $(p-r,p+r)$, so we know $\mathcal{O}$ is a countable collection of open interval.\\

Let $A$ be an open set. We wish to prove \vi{$A$ can be expressed as union of a countable collection of disjoint open interval}.\\


Because $\mathcal{O}$ is a base, for each $a\in A$, we can find $B_{r_a}(p_a)$, such that
\begin{equation*}
a\in B_{r_a}(p_a)\subseteq A
\end{equation*}
Collect all such open ball for each points in $A$ and call this collection  $\mathcal{O}'$.\\

Define a relation $\sim$ on $\mathcal{O}'$ by 
\begin{gather*}
  (a,b)\sim (c,d) \text{ if }\exists S\in \power{\mathcal{O}'}\text{ such that }\\
  \bigcup S\text{ is an open interval and }(a,b)\subseteq\bigcup S\text{ and }(c,d)\subseteq \bigcup S
\end{gather*}
We wish to prove \teal{$\sim$ is an equivalence relation}.\\

To show $(a_1,b_1)\sim (a_1,b_1)$, use $S=\set{(a_1,b_1)}$. To show $(a_1,b_1)\sim (a_2,b_2)\implies (a_2,b_2)\sim (a_1,b_1)$, use the same $S$. It is left to prove
\begin{equation*}
  (a_1,b_1)\sim (a_2,b_2)\text{ and }(a_2,b_2)\sim (a_3,b_3)\implies (a_1,b_1)\sim (a_3,b_3)
\end{equation*}

Let $S_1\in \power{\mathcal{O}'}$ be from $(a_1,b_1)\sim (a_2,b_2)$, and let $S_2\in \power{\mathcal{O}'}$ be from $(a_2,b_2)\sim (a_3,b_3)$.\\

Define 
\begin{equation*}
S_3:=S_1\cup S_2 
\end{equation*}
Because $(a_2,b_2)\subseteq \bigcup S_1\cap \bigcup S_2$, we know $\bigcup S_1\cap \bigcup S_2$ are not disjoint. Then because union of two intersecting open interval is again an open interval, we know $\bigcup S_3$ is an open interval.\\

Deduce
\begin{equation*}
  (a_1,b_1)\subseteq \bigcup S_1\subseteq \bigcup S_3
\end{equation*}
and deduce
\begin{equation*}
  (a_3,b_3)\subseteq \bigcup S_2\subseteq \bigcup S_3\tdone
\end{equation*}
Let $E$ be the collection of union of each equivalent class of $\sim$ on $\mathcal{O}'$. Because $\mathcal{O}'$ is countable, we know $E$ is countable. Every element of $\mathcal{O}'$ is a subset of $A$ by definition, so we know  $\bigcup E=\bigcup \mathcal{O}'\subseteq A$. Every element of $A$ is in some  $O\in \mathcal{O}'$ by definition, so we have $A\subseteq \bigcup \mathcal{O}'=\bigcup E$. It is only left to prove \olive{each member in  $E$ is an open interval} and \brown{each two members of $E$ are disjoint}.\\

We first prove \brown{disjoint}.\\

Let $B,C\in E$ and let $B',C'$ be equivalent class that satisfy
\begin{equation*}
B=\bigcup B'\text{ and }C=\bigcup C'
\end{equation*}
\As{$B,C$ is not disjoint, say, $x\in B\cap C$}, we have
\begin{equation*}
\exists (a,b)\in B', x\in (a,b)\subseteq B
\end{equation*}
and have
\begin{equation*}
\exists (c,d)\in C',x \in (c,d)\subseteq C
\end{equation*}
We then can see $(a,b)$ and $(c,d)$ intersect, then we can use $S=\set{(a,b),(c,d)}$ to show $(a,b)\sim (c,d)\tCaC\bodone$.\\

We now prove  \olive{open interval}.\\
 
Let $B\in E$, and let $B'$ be equivalent class that satisfy  $B=\bigcup B'$. We wish to prove $B=(\inf B,\sup B)$ where negative and positive infinity is taken into account.\\

Because $B'$ contain only open sets, we know  $B$ is open. Then we know $\sup  B$ and $\inf B$ if exists, is not in $B$. To see such, just observe every open ball centering $\sup B$ has a number greater than all numbers in $B$, and similarly for $\inf B$ vice versa.\\

We have proved $B\subseteq (\inf B,\sup B)$.\\

We only have to prove every point in $(\inf B,\sup  B)$ is in $B$, where infimum and supremum can be negative or positive infinity.\\

\As{$\exists x\in (\inf B,\sup B), x\not\in B$}. We have
\begin{equation*}
\exists (y_1,z_1)\in B', (y_1,z_1)\subseteq (\inf B,x)\text{ and }\exists (y_2,z_2)\in B', (y_2,z_2)\subseteq (x,\sup B)
\end{equation*}
Because $(y_1,z_1)\sim (y_2,z_2)$ there exists $S\in \mathcal{O}'$ such that $(y_1,z_2)\subseteq \bigcup S\text{ and }\bigcup S$ is an open interval.\\

If $S$ contain any element  $(v,w)$ not in $B'$, we can use  $S$ to show  $(v,w)\sim (y_2,z_2)$, causing a contradiction. We have proved $S$ is a subset of  $B'$.\\

Now, Because $x\in \bigcup S$, we know there must exists some interval in $S\subseteq B'$ containing  $x$  \CaC to $x\not\in B\odone\vdone$
\end{proof}
\begin{question}{}{}
Let \( A \subseteq B \subseteq \mathbb{R} \). Suppose that \( A \) is a dense subset of \( B \).

1. Prove that \( B \subseteq \overline{A} \).

2. If \( B \) is closed, determine whether \( B = \overline{A} \).
\end{question}
\begin{proof}
$A$ is a dense subset of $B$ means that every point $b\in B\setminus A$ is a limit point of $A$ in the scope of $B$. Notice that $b$ is a limit point of  $A$ in the scope of $B$ also means $b$ is a limit point in the scope of $\R$. Then we have proved $B\setminus A\subseteq A'$. It follows
\begin{equation*}
B=A \cup (B\setminus A)\subseteq A\cup A'=\overline{A}
\end{equation*}
If $B$ is closed, we have
 \begin{equation*}
\overline{A}\subseteq \overline{B}=B
\end{equation*}
Then because $B\subseteq \overline{A}$, we have $B=\overline{A}$



\end{proof}
\textbf{Definition 0.1.} A metric space \( X \) is \textit{sequentially compact} if every sequence of points in \( X \) has a convergent sub-sequence converging to a point in \( X \).
\begin{question}{}{}

Let \( A \) and \( B \) be subsets of a metric space \( (M, d) \) and denote \( \text{cl}(A) = \overline{A} \). Show that

1. \( \text{cl}(\text{cl}(A)) = \text{cl}(A) \).

2. \( \text{cl}(A \cup B) = \text{cl}(A) \cup \text{cl}(B) \).

3. \( \text{cl}(A \cap B) \subseteq \text{cl}(A) \cap \text{cl}(B) \). Find an example such that \( \text{cl}(A \cap B) \subsetneq \text{cl}(A) \cap \text{cl}(B) \).
\end{question}
\begin{proof}
Notice that closure is the smallest closed set containing the set.\\

Because $\overline{A}$ is a closed set, we know the smallest closed set containing $\overline{A}$ is $\overline{A}$ itself.\\

Deduce
\begin{equation*}
A\subseteq A\cup B\implies \overline{A}\subseteq \overline{A\cup B}
\end{equation*}
and deduce
\begin{equation*}
B\subseteq A\cup B\implies \overline{B}\subseteq \overline{A\cup B}
\end{equation*}
so deduce
\begin{equation*}
\overline{A}\cup \overline{B}\subseteq \overline{A\cup B}
\end{equation*}
Notice that $\overline{A}\cup \overline{B}$ is a closed set containing both $A$ and  $B$, thus containing $A\cup B$, so because $\overline{A\cup B}$ by definition is the smallest closed set containing $A\cup B$, we have
\begin{equation*}
\overline{A\cup B}\subseteq \overline{A}\cup \overline{B}
\end{equation*}
Notice that $\overline{A}$ and $\overline{B}$ are both closed set containing $A\cap B$. Then because $\overline{A\cap B}$ is the smallest closed set containing  $A\cap B$. We have
\begin{equation*}
\overline{A\cap B}\subseteq \overline{A}\text{ and }\overline{A\cap B}\subseteq \overline{B}
\end{equation*}
Then have
\begin{equation*}
\overline{A\cap B}\subseteq \overline{A}\cap \overline{B}
\end{equation*}
Let $A=(0,1)$ and $B=(1,2)$. We have
\begin{equation*}
\overline{A\cap B}=\overline{\varnothing}=\varnothing\text{ and }\overline{A}\cap \overline{B}=[0,1]\cap [1,2]=\set{1}
\end{equation*}
\end{proof}
\begin{question}{}{}

Let \( K \) be a sequentially compact set in a metric space \( (M, d) \) and let \( F \subseteq K \) be closed. Prove that \( F \) is sequentially compact.
\end{question}
\begin{proof}
Let $\set{x_i}_{i=1}^\infty$ be a sequence in $F$
\begin{equation*}
  \set{x_i}_{i=1}^\infty \subseteq F\subseteq K
\end{equation*}
Because $K$ is sequentially compact, we know there exists a sub-sequence $\set{x_{n_i}}_{i=1}^\infty$ converge to some point $a$
 \begin{equation*}
\lim_{i\to\infty}\set{x_{n_i}}=a
\end{equation*}
We only have to prove  $a$ is in  $F$.\\

\As{$a\not\in F$}. Because $F$ is closed, we know  $F^c$ is open. Then  $a\in F^c$ tell us there exists an open ball $B_r(a)$ contain no point in $F$, thus disjoint to  $\set{x_{n_i}}\tCaC$

\end{proof}
\textbf{Definition 0.2.} The discrete metric \( d \) on a set \( X \) is defined by
\[
d(x,y) = 
\begin{cases} 
1 & \text{if } x \neq y, \\
0 & \text{if } x = y,
\end{cases}
\]
for any \( x, y \in X \). In this case \( (X,d) \) is called a discrete metric space.
\begin{question}{}{}

Let \( (M, d) \) be a metric space with discrete metric \( d \). Prove that every compact set in \( M \) is finite.
\end{question}
\begin{proof}
Notice we have
\begin{equation*}
\forall p\in M, B_1(p)=\set{p}
\end{equation*}
So we know no set in $M$ has a limit point. Let $K\subseteq M$ be compact. By \myref{Theorem}{3.7.7}, we know $K$ is limit point compact. \As{$K$ is infinite}. Then $K$ has a limit point  $\tCaC$.
\end{proof}
\begin{question}{}{}

Let \( \{ x_n \}_{n=1}^{\infty} \subseteq \mathbb{R} \) be a convergent sequence with the limit \( x \). Prove that the set \( \{ x_n : n \in \mathbb{N} \} \cup \{ x \} \) is compact.
\end{question}
\begin{proof}
Let 
\begin{equation*}
E\subseteq \set{x_n:n\inn}\cup \set{x}
\end{equation*}
If $E$ is infinite, we can regard  $E\setminus \set{x}$ as a sub-sequence of $\set{x_n}$. Then because $\set{x_n}_{n=1}^\infty$ converge to $x$, we know  $E\setminus \set{x}$ converge to $x$. Then we know $x$ is a limit point of $E\setminus \set{x}$ thus a limit point of $E$. We have proved every infinite subset of $\set{x_n:n\inn}\cup \set{x}$ has a limit point $x$ in $\set{x_n:n\inn}\cup \set{x}$. By \myref{Theorem}{3.7.7}, $\set{x_n:n\inn}\cup \set{x}$ is compact.
\end{proof}
\begin{question}{}{}

\begin{enumerate}
    \item Let \( A \) and \( B \) be two subsets of a metric space \( (M,d) \). The distance between \( A \) and \( B \) is defined by
    \[
    d(A,B) \equiv \inf\{d(x,y) : x \in A, y \in B\}.
    \]
    \begin{enumerate}
        \item Give an example of two disjoint, nonempty, closed sets \( A \) and \( B \) in \( \mathbb{R}^n \) for which \( d(A, B) = 0 \).
        \item Let \( A, B \) be nonempty sets in \( \mathbb{R}^n \) with \( A \) closed and \( B \) compact. Show that there are points \( a \in A \) and \( b \in B \) such that \( d(a, b) = |a-b| \). Deduce that \( d(A, B) \) is positive if such \( A, B \) are disjoint.
    \end{enumerate}
\end{enumerate}
\end{question}
\begin{proof}
Let $n=1$. Let 
\begin{equation*}
A=\set{a_k=k-1+\sum_{i=1}^{k-1}2^{-i-1}:k\inn}
\end{equation*}
Let
\begin{equation*}
B=\set{b_k=k-\sum_{i=1}^{k-1}2^{-i-1}:u\inn}
\end{equation*}
Notice the pattern
\begin{align*}
  a_1&=0<\frac{1}{2}< 1=b_1\\
  a_2&=1+\frac{1}{4}< 1+\frac{1}{2}< 2-\frac{1}{4}=b_2\\
  a_3&=2+\frac{1}{4}+\frac{1}{8}< 2+\frac{1}{2}< 3-\frac{1}{4}-\frac{1}{8}=b_3\\
  \vdots
\end{align*}
and compute
\begin{equation*}
b_k-a_k=2^{1-k}
\end{equation*}
First observe for each non-negative integer $m$m the interval  $[m,m+1]$ contain exactly one point from $A$ and contain exactly one point from  $B$. To see  $A$ and  $B$ are disjoint, observe that because  $A,B$ contain only non-negative numbers, so every point is in $[m,m+1]$ for some non-negative integer $m$, and we know each interval only contain $a_{m+1}$ from $A$ and $b_{m+1}$ from $B$ where  $b_{m+1}-a_{m+1}=2^{-m}$.\\

We now show $A,B$ are both closed. For negative  $-x\inr$, observe $B_{x}(-x)$ intersect with neither $A$ nor  $B$. We know $B_1(0)$ does not intersect with $B$ and  $0\in A$. For positive $x$ not in $A$, let $m=\lfloor x \rfloor$. We know point closest to $x$ is either $a_{m},a_{m+1}\text{ or }a_{m+2}$. Then we can let $r=\min \set{\abso{x-a_m},\abso{x-a_{m+1}},\abso{x-a_{m+2}}}$, and see $B_r(x)$ does not intersect with $A$.For positive $x$ not in $B$, let $m=\lfloor x \rfloor$. We know point closest to $x$ is either $b_{m},b_{m+1}\text{ or }b_{m+2}$. Then we can let $r=\min \set{\abso{x-b_m},\abso{x-b_{m+1}},\abso{x-b_{m+2}}}$, and see $B_r(x)$ does not intersect with $B$.\\

Lastly, to prove $d(A,B)=0$ is to prove for each positive real $r$, there exists a pair of $a,b$ such that  $\abso{a-b}<r$. Observe
 \begin{equation*}
b_k-a_k=2^{1-k}<r\iff 1-k<\log_2 r\iff k>1-\log_2 r
\end{equation*}
And we are done, by picking great enough $k$.\\



Now we do (b).\\

Because 
\begin{equation*}
\set{d(a,b):a\in A,b\in B}=\bigcup \set{\set{d(a,b):a\in A}:b \in B}
\end{equation*}
so we have 
\begin{align*}
d(A,B)&=\inf \set{d(a,b):a\inA,b\inB}\\
&=\inf \bigcup \set{\set{d(a,b):a\in A}:b\in B}\\
&=\inf \set{\inf \set{d(a,b):a\in A}:b\in B}\\
&=\inf \set{d(A,\set{b}):b \in B}
\end{align*}
where the third equality hold true by the fourth question of HW1.\\


We first prove \vi{if $d(A,B)=0$, then $A$ and  $B$ intersect.}\\

If $B$ is finite, then  $0=d(A,B)=\min \set{d(A,\set{b}:b\in B)}$, so we know there exists $b\in B$ such that $0=d(A,\set{b})=\inf \set{d(a,b):a\in A}$. Then we can deduce every open ball $B_r(b)$ contain a point $a\in A$, since there exists $a$ such that  $d(a,b)<r$. We have proved $b$ is a limit point of $A$, then because  $A$ is closed, we know  $b\in A$.\\

If $B$ is infinite, then  $0=\inf \set{d(A,\set{b}):b\in B}$. For each $n$, we then know there exists  $b_n$ such that  $d(A,\set{b_n})<\frac{1}{n}$.\\


Construct an infinite sequence $\set{b_n}_{n\inn}$ by picking $b_n$ such that $d(A,\set{b_n})<\frac{1}{n}$. We can see our method for picking $b_n$ will give us an infinite sequence as every $b$  satisfy  $d(A,\set{b})>0$. By \myref{Theorem}{3.7.7}, we know $B$ is limit point compact, so we know $\set{b_n}_{n\inn}$ has a limit point in $B$.\\

Denote the limit point for $\set{b_n}_{n\inn}$ as $p$. We now show $p\in A$. \As{$p\in A^c$}. Because $A$ is closed, we know there exists an open ball  $B_r(p)$ disjoint with $A$. Then we know $d(A,\set{p})\geq r$. \\

Notice that if we fix $b'\in B$. We have triangle inequality
\begin{equation*}
d(A,\set{p})\leq d(A,\set{b'})+d(b',p)*
\end{equation*}
Because 
\begin{equation*}
d(A,\set{b'})+d(b',p)=\inf \set{d(a,b'):a \in A}+d(b',p)=\inf \set{d(a,b')+d(b',p):a \in A}
\end{equation*}
where for each $a$, we have
 \begin{equation*}
d(A,\set{p})\leq d(a,p)\leq d(a,b')+d(b',p)
\end{equation*}



Then from
\begin{equation*}
  \forall b\in B, r\leq d(A,\set{p})\leq d(A,\set{b})+d(\set{b},\set{p})
\end{equation*}
we have
\begin{equation*}
  \forall b\in B [d(A,\set{b})< \frac{r}{2}\implies d(\set{b},\set{p})>\frac{r}{2}]
\end{equation*}
Then we see if $n>\frac{2}{r}$, then $b_n$ is not contained by the open ball  $B_{\frac{r}{2}}(p)$. In other words, the open ball $B_{\frac{r}{2}}(p)$ that center a limit point $p$ contain at most $\lfloor \frac{2}{r}\rfloor$ amount of point in $\set{b_n}_{n\inn}$ \CaC$\vdone$\\

Lastly, we prove there exists $a\in A,b\in B$ such that $d(A,B)=d(a,b)$. If  $A$ and  $B$ are not disjoint, the fact $\exists a\in A,\exists b\in B, d(A,B)=\abso{a-b}$ is clear by picking $a=b$. We only have to consider when $A$ and $B$ are disjoint.\\

Let $d(A,B)=u$. We know $u>0$, because of the result we have proved.\\

We have 
\begin{equation*}
  \inf \set{d(A,\set{b}):b\in B}=d(A,B)=u
\end{equation*}
At this stage, we wish to prove \blue{$\exists b\in B$ such that $d(A,\set{b})=u$}. \As{there does not exist such $b$}\\

We know, for each $n\inn$, there exists a point $b_n$ such that
 \begin{equation*}
d(A,\set{b_n})<u+\frac{1}{n}
\end{equation*}
Because $\forall b\in B, d(A,\set{b})>u$, we know if we collect one $b_n$ that satisfy $d(A,\set{b_n})<u+\frac{1}{n}$ for each $n\inn$, the sequence $\set{b_n}_{n\inn}$ is infinite.\\

Then by \myref{Theorem}{3.7.7}, we know there exists a limit point $p\in B$ for $\set{b_n}_n\inn$.\\

We know $d(A,\set{p})>u$ by our assumption.\\

Let $m=d(A,\set{p})>u$. Then from
\begin{equation*}
\forall b\in B,m=d(A,\set{p})<d(A,\set{b})+d(b,p)\text{ This was proved at * }
\end{equation*}
we have
\begin{equation*}
\forall b\inB, d(A,\set{b})<u+\frac{1}{n}\implies d(b,p)>m-u-\frac{1}{n}
\end{equation*}
Let $k$ be a natural such that  $u+\frac{1}{k}<m$. For each natural $t$ greater than  $k$, we have 
\begin{equation*}
d(b_t,p)>m-u-\frac{1}{t}>m-u-\frac{1}{k}
\end{equation*}
In other words, the open ball $B_{m-u-\frac{1}{k}}(p)$ centering a limit point contain at most $k$ amount of point in  $\set{b_n}_{n\inn}\tCaC\bdone$\\

Let $b\in B$ satisfy $d(A,\set{b})=u$. At the final stage, we wish to prove \vi{$\exists a\in A, d(a,b)=u$}.\\

\As{$\forall a\in A, d(a,b)>u$}. We know, for each $n\inn$, there exists a point $a_n$ such that 
\begin{equation*}
d(a_n,b)<u+\frac{1}{n}
\end{equation*}
Because $\forall a\in A, d(a,b)>u$, we know if we we collect one $a_n$ that satisfy  $d(a_n,b)<u+\frac{1}{n}$ for each $n\inn$, the sequence $\set{a_n}_{n\inn}$ is infinite.\\

Notice that the sequence $\set{a_n}_{n\inn}$ is also bounded as one can check $B_{2(u+1)}(a_1)$ contain all $\set{a_n}_{n\inn}$, using $b$ as a "pivot".\\

Then because we are in $\R^n$ and $\set{a_n}_{n\inn}$ is closed and bounded, we know $\set{a_n}_{n\inn}$ is compact, then by \myref{Theorem}{3.7.7}, we know $\set{a_n}_{n\inn}$ has a limit point in itself.\\

Let $a_k$ be a limit point of $\set{a_n}_{n\inn}$.\\

Let $m=d(a_k,b)>u$. Observe that 
\begin{equation*}
\forall a_n,m=d(a_k,b)<d(b,a_n)+d(a_n,a_k)
\end{equation*}
give us
\begin{equation*}
\forall a_n, d(a_n,b)<u+\frac{1}{n}\implies d(a_n,a_k)>m-u-\frac{1}{n}
\end{equation*}
Let $s\inn$ be great enough so that  $m-u-\frac{1}{s}>0$. Then we see for each natural $w$ greater than  $s$, we have
 \begin{equation*}
d(a_w,b)<u+\frac{1}{w}<u+\frac{1}{s}
\end{equation*}
so we have
\begin{equation*}
d(a_w,a_k)>m-u-\frac{1}{s}
\end{equation*}
Then we see the open ball $B_{m-u-\frac{1}{s}}(a_k)$ contain at most $s$ amount of point in  $\set{a_n}_{n\inn}\tCaC\vdone$
















\end{proof}
\begin{question}{}{}
\begin{enumerate}
    \setcounter{enumi}{8}
    \item Let \( (M, d) \) be a metric space.
    \begin{enumerate}
        \item Show that the union of a finite number of compact subsets of \( M \) is compact.
        \item Show that the intersection of an arbitrary collection of compact subsets of \( M \) is compact.
    \end{enumerate}
\end{enumerate}
\end{question}
\begin{proof}
  Let $\mathcal{K}=\set{K_1,\dots , K_n}$ be a finite collection of compact subset of $M$. We wish to show $\bigcup  \mathcal{K}$ is compact. Let $\mathcal{G}$ be an open cover for $\mathcal{K}$. For each $m \inn:1\leq m\leq n$, because $K_m\subseteq \bigcup \mathcal{K}$, we know $\mathcal{G}$ is also an open cover for $K_m$. Then, we can pick a finite sub-cover  $\mathcal{G}_m\subseteq \mathcal{G}$ for $K_m$. Collect all such finite sub-cover  $\mathcal{G}_1,\dots, \mathcal{G}_m$. The union $\bigcup \set{\mathcal{G}_1,\dots ,\mathcal{G}_m}$ is a finite open cover for $\bigcup \mathcal{K} $.\\

Notice that a compact set must be closed, and arbitrary collection of closed sets are closed. Then, we know the intersection of an arbitrary collection of compact set must be a closed set and a subset of every compact set in the collection. Then because closed subset of compact set is compact, we know the intersection is compact.
\end{proof}
\begin{question}{}{}
\begin{enumerate}
    \setcounter{enumi}{9}
    \item A metric space \( (M,d) \) is said to be \textit{separable} if there is a countable subset \( A \) which is dense in \( M \). Show that every compact set is separable.
\end{enumerate}
\end{question}
\begin{proof}
See the proof in \myref{Theorem}{3.7.7}, there is a summary where you can find the numbering of rigorous proof for each step. Some proof for corollary is omitted because they are immediate consequences of theorem before.\\

Because I use a program written by artificial intelligence to update my reference in latex code whenever I edit my latex code, some reference may be directed to a wrong theorem, but all the tools are there in Chapter 2. 
\end{proof}
\section{HW3}
\begin{question}{}{}
1. Let $S_1$ and $S_2$ be two nonempty subsets in a metric space with $S_1 \cap \overline{S_2} = \overline{S_1}\cap S_2 = \emptyset$. If $A \subseteq S_1 \cup S_2$ is a connected set, then either $A \subseteq S_1$ or $A \subseteq S_2$.
\end{question}
\begin{proof}
\As{we have $A\not\subseteq S_1\text{ and }A\not\subseteq S_2$}. Then we know $A\cap S_1\text{ and }A\cap S_2$ are both non-empty. We know
\begin{equation*}
A=(A\cap S_1)\cup (A\cap S_2)
\end{equation*}
We wish to show
 \begin{equation*}
A\cap S_1\text{ and }A\cap S_2\text{ are separated. }
\end{equation*}
Notice 
\begin{equation*}
\overline{A\cap S_1}\subseteq \overline{S_1}
\end{equation*}
Then because $\overline{S_1}$ and $S_2$ are disjoint, we know $\overline{A\cap S_1}$ and $S_2$ are disjoint. Then because $A\cap S_2\subseteq S_2$, we know $\overline{A\cap S_1}$ and $A\cap S_2$ are disjoint. Similarly, notice
\begin{equation*}
\overline{A\cap S_2}\subseteq \overline{S_2}
\end{equation*}
Then because $\overline{S_2}$ and $S_1$ are disjoint, we know $\overline{A\cap S_2}$ and $S_1$ are disjoint. Then because $A\cap S_1\subseteq S_1$, we know $\overline{A\cap S_2}$ and $A\cap S_1$ are disjoint.    \\

We have proved $A\cap S_1$ and $A\cap S_2$ are separated, which $\tCaC$ to $A$ is connected.
\end{proof}
\begin{question}{}{}
2. If $A_1$ and $A_2$ are two nonempty and connected sets with $A_1\cap A_2\neq \varnothing$. Prove or disprove that
\begin{enumerate}[label=(\alph*)]
  \item $A_1\cap A_2$ is connected
  \item $A_1\cup A_2$ is connected
\end{enumerate}
\end{question}
\begin{proof}
Let $A_1=\set{(x,y)\inr^2:x^2+y^2=1}$ and let  $A_2=\set{(0,y)\inr^2:y\inr}$.\\

To show $A_1,A_2$ are both connected, we can show they are both path connected.\\

Let $(\cos \alpha ,\sin \alpha  )\in A_1$ and $(\cos \beta ,\sin \beta )\in A_1$. Define $f:[0,1]\rightarrow A_1$ by
\begin{equation*}
 f(x)=(\cos (\alpha +x(\beta -\alpha )), \sin (\alpha +x(\beta -\alpha )))
\end{equation*}
Clearly $f$ is continuous, and  $f(0)=(\cos \alpha ,\sin \alpha ),f(1)=(\cos \beta ,\sin \beta )$.\\

Let $(0,z)\in A_2$ and $(0,y)\in A_2$. Define $g:[0,1]\rightarrow A_2$ by
\begin{equation*}
g(x)=(0,z+x(y-z))
\end{equation*}
Clearly $g$ is continuous and  $g(0)=(0,z),g(1)=(0,y)$.\\


We see $A_1\cap A_2=\set{(0,1),(0,-1)}$. We see $\set{(0,1)}$ and $\set{(0,-1)}$ are separated, because they are close.\\

For (b), see next three Theorems.
\end{proof}
\begin{theorem}
\label{8.3.1}
\textbf{(Connected)} If $A$ is disconnected, then $A$ can be partitioned into two non-empty disjoint relatively open subsets. 
\end{theorem}
\begin{proof}
Because $A$ is disconnected, we know  $A=E\cup F$ for some pair $E,F$ of separated sets. We wish to prove $E,F$ are both relatively open to $A$.\\

Because $\overline{E}\cap F=\varnothing$, we know the closure of $E$ in subspace topology of $A$ is disjoint to $F$. Then with respect to $A$, we can see $\overline{E}\subseteq F^c=E$, so we know $E$ is relatively closed to  $A$.\\


Similarly, because $\overline{F}\cap E=\varnothing$, we know the closure of $F$ in subspace topology of $A$ is disjoint to $E$. Then with respect to $A$, we can see $\overline{F}\subseteq E^c=F$, so we know $F$ is relatively closed to  $A$.\\

Then because $E$ and  $F$ are both relatively closed to  $A$ and  $A=E\cup F$ where $E,F$ are disjoint, we know  $E$ and  $F$ are both relatively open to  $A$.
\end{proof}
\begin{theorem}
\label{8.3.2}
\textbf{(Subspace Topology)} Let $Y$ be a subspace of $(X,d)$, let $E\subseteq X$, and let $p\in  Y$. We have
\begin{gather*}
p\text{ is an interior point of $E$ in $X$ }\implies p\text{ is an interior point of $E\cap Y$ in $Y$ } \\
\text{ $Y\cap E^\circ $ in $X$ is a subset of the interior of $E\cap Y$ in $Y$ }\\
E\text{ is open in $X$ }\implies E\cap Y\text{ is open in $Y$ }
\end{gather*}
where the converse may not hold true.
\end{theorem}
\begin{proof}
We first prove the first statement. Let $\set{x_n}$ be a sequence in $Y$ that converge to $p$. Because  $p$ is an interior point of  $E$ in  $X$, and  $\set{x_n}$ is in $X$, as  $Y\subseteq X$, we know there exists $N$ such that 
 \begin{equation*}
n>N\implies x_n\in E
\end{equation*}
Notice $x_n\in Y$, and we are done.\\

For a nontrivial example of the converse of the first statement may not hold true, let $E=(0,2)$, let $Y=\set{1}\cup (2,3)$. We see $1$ is an interior point of  $E$ in  $\R$, but  $1$ isn't an interior point of  $\set{1}=E\cap Y$ in $Y$.

The second and the third statement follows from the first statement. 
\end{proof}
\begin{theorem}
\label{8.3.3}
\textbf{(Union of Connected Sets that have Nonempty Intersection is Connected)} Let $\mathcal{F}$ be a class of connected sets. We have 
\begin{equation*}
\bigcap \mathcal{F}\neq \varnothing \implies \bigcup \mathcal{F}\text{ is connected }
\end{equation*}
\end{theorem}
\begin{proof}
\As{$\bigcup \mathcal{F}$ is not connected}. Let 
\begin{equation*}
\bigcup  \mathcal{F}= A \disjointunion B\text{ and }A\neq \varnothing\neq B
\end{equation*}
And let $A,B$ be relatively open to  $\bigcup \mathcal{F}$. We know $\bigcap \mathcal{F}$ must intersect with either $A$, or  $B$, or both.\\

WOLG, let
\begin{equation*}
A\cap \bigcap \mathcal{F}\neq \varnothing
\end{equation*}
Because $B$ is non-empty and $B\subseteq \bigcup \mathcal{F}$, we know $B$ must intersect with some $F_n\in \mathcal{F}$. Notice that because $A\cap \bigcap \mathcal{F}\neq \varnothing$, we have $A\cap F_n\neq \varnothing$. Then by \myref{Theorem}{8.3.2}, we see $A\cap F_n$ and $B\cap F_n$ are both relatively open to $F_n$, while $F_n= (A\cap F_n)\disjointunion (B\cap F_n)\tCaC$ 
\end{proof}
\begin{question}{}{}
3. Let $\{A_k\}_{k=1}^{\infty}$ be a family of connected subsets of $M$, and suppose that $A$ is a connected subset of $M$ such that $A_k \cap A \neq \emptyset$ for all $k \in \mathbb{N}$. Show that the union $\left(\bigcup_{k \in \mathbb{N}} A_k\right) \cup A$ is also connected.
\end{question}
\begin{proof}
\As{$\bigcup_{k\inn}A_k\cup A$ is not connected}. Let $\bigcup_{k\inn}A_k\cup A$ be partitioned into two non-empty  $E,F$ relatively open to  $\bigcup_{k\inn}A_k\cup A$.\\

If $E,F$ both intersect with  $A$, observe that $A$ can be partitioned into two non-empty $E\cap A$ and $F\cap A$, which are relatively open to $A$, causing a contradiction to  $A$ is connected.\\

Then, we only have to consider when only one of  $E,F$ intersect with  $A$. WOLG, let  $E$ intersect with $A$.\\

We know $F$ must intersect with some  $A_n$. From last question, we know $A_n\cup A$ is connected. Notice that $A_n\cup A$ can be partitioned into two non-empty $E\cap (A\cup A_n)$ and $F\cap (A\cup A_n)$, and they are relatively open to $A\cup A_n\tCaC$ to $A_n\cup A$ is connected.
\end{proof}
\begin{question}{}{}
4. Let $\{a_k\}_{k=1}^{\infty}$ be a sequence, and define $s_n = \frac{1}{n} \sum_{k=1}^{n} a_k$. Prove or disprove that
\begin{enumerate}[label=(\alph*)]
  \item If ${a_k}$ converges, then $s_n$ converges.\\
  \item If $s_n$ converges, then $a_k$ converges.\\
  \item Let $t = \frac{(2n-1)a_1 + (2n-3)a_2 + \ldots + 3a_{n-1} + a_n}{n^2}$. Assume ${a_k}$ converges to $a$. Does $t_n$ also converge to $a$?
\end{enumerate}
\end{question}
\begin{proof}
First notice
\begin{align*}
  \abso{s_n-L}=\abso{\frac{\sum_{k=1}^n a_k}{n}-L}&=\abso{\frac{(\sum_{k=1}^na_k)-nL}{n}}\\
  &=\abso{\frac{\sum _{k=1}^n a_k-L}{n}}\\
  &=\frac{1}{n}\abso{\sum_{k=1}^n a_k-L}\\
  &\leq \frac{1}{n} \sum_{k=1}^n \abso{a_k-L}
\end{align*}
We prove \vi{$\lim_{k\to \infty}a_k=L\implies \lim_{k\to\infty }s_k=L$}.\\

Arbitrarily pick $R\inr^+$. We wish to find $N$ such that 
 \begin{equation*}
k>N\implies \abso{s_k-L}<R
\end{equation*}
Because $\lim_{n\to\infty}a_n=L$, we know there exists $N_0$ such that 
 \begin{equation*}
n>N_0\iff \abso{a_n-L}<\frac{R}{2}
\end{equation*}
Let
\begin{equation*}
H=\sum_{k=1}^{N_0} \abso{a_k-L}\text{ and }m=\frac{2}{R}(H-N_0R)
\end{equation*}
We wish to prove
\begin{equation*}
n>N_0+m \implies \abso{s_n-L}<R
\end{equation*}
Let $n=N_0+u$ where $u>m$. Observe
\begin{align*}
  \abso{s_n-L}\leq \frac{1}{n}\sum_{k=1}^n \abso{a_k-L}&=\frac{\sum_{k=1}^n \abso{a_k-L}}{N_0+u}\\
  &= \frac{\sum_{k=1}^{N_0} \abso{a_k-L}+\sum_{k=N_0+1}^{N_0+u} \abso{a_k-L}}{N_0+u}\\
  &\leq \frac{H+u \frac{R}{2}}{N_0+u}
\end{align*}
and Observe 
\begin{align*}
  u>m=\frac{2}{R}(H-N_0R)&\implies \frac{Ru}{2}>H-N_0R\\
                         &\implies N_0R+ Ru>H+ \frac{Ru}{2}\\
&\implies R>\frac{H+\frac{Ru}{2}}{N_0+u}
\end{align*}
In other words, we have
\begin{equation*}
n>N_0+m \implies u>m\implies \abso{s_n-L}\leq \frac{H+\frac{Ru}{2}}{N_0+u}<R\vdone
\end{equation*}
For (b), we raise an counter-example. Let
\begin{equation*}
a_n=\begin{cases}
  1& \text{ if $n$ is odd }\\
  0& \text{ if $n$ is even }
\end{cases}
\end{equation*}
To see $\set{a_n}$ does not converge to any number, consider the sub-sequences
\begin{equation*}
\set{a_{n_k}}\text{ where }n_k=2k-1\text{ and }\set{a_{n_u}}\text{ where }n_u=2u
\end{equation*}
We have
\begin{equation*}
\forall k,a_{n_k}=1\text{ and }\forall u,a_{n_u}=0
\end{equation*}
Then we see 
\begin{equation*}
\lim_{k\to\infty}a_{n_k}=1\text{ and }\lim_{u\to\infty}a_{n_u}=0
\end{equation*}
If $\set{a_n}$ converge, then these two sub-sequence should converge to the same number.\\

We wish to show 
\begin{equation*}
\lim_{n\to\infty}s_n=\frac{1}{2}
\end{equation*}
With simple logical computation, we have
\begin{equation*}
s_n=\begin{cases}
  \frac{n-1}{2n}& \text{ if $n$ is odd }\\
  \frac{1}{2}& \text{ if $n$ is even }
\end{cases}
\end{equation*}
Notice 
\begin{equation*}
\frac{n-1}{2n}=\frac{1}{2}-\frac{1}{2n}
\end{equation*}
Fix $\epsilon \inr^+$. We see that 
\begin{equation*}
n>\frac{1}{2\epsilon }\implies \abso{s_n-\frac{1}{2}}=\begin{cases}
\frac{1}{2n}& \text{ if $n$ is odd }\\
 0& \text{ if $n$ is even } 
\end{cases}<\epsilon 
\end{equation*}
Lastly, we prove 
\begin{equation*}
\blue{\lim_{n\to\infty}a_n=a\implies \lim_{n\to\infty}t_n=a}
\end{equation*}
First notice
\begin{align*}
  \abso{t_n-a}&=\abso{\frac{(2n-1)a_1+(2n-3)a_2+\cdots + a_n}{n^2}-a}\\
&=\abso{\frac{1}{n^2}[(2n-1)(a_1-a)+(2n-3)(a_2-a)+\cdots +(a_n-a)]}\\
&=\frac{1}{n^2}\abso{\sum_{k=1}^n (2n-2k+1)(a_k -a)}\\
&\leq \frac{1}{n^2}\sum_{k=1}^n \abso{(2n-2k+1)(a_k-a)}\\
&= \frac{1}{n^2}\sum_{k=1}^n (2n-2k+1)\abso{a_k-a}\\
&\leq \frac{1}{n^2}\sum_{k=1}^n (2n-1)\abso{a_k-a}=\frac{2n-1}{n^2}\sum_{k=1}^n \abso{a_k-a}
\end{align*}
Arbitrarily pick $\epsilon \inr^+$. Because $\lim_{n\to\infty}a_n=a$. We know there exists $N_0$ such that
 \begin{equation*}
n>N_0\implies \abso{a_n-a}<\frac{\epsilon }{4}\implies (\frac{2n-1}{n})\abso{a_n-a}=(2-\frac{1}{n})\abso{a_n-a}<2\abso{a_n-a}<\frac{\epsilon}{2} 
\end{equation*}
Let
\begin{equation*}
H=\sum_{k=1}^{N_0} \abso{a_k-a}
\end{equation*}
We have
\begin{align*}
  \abso{t_n-a}\leq \frac{2n-1}{n^2}\sum_{k=1}^n \abso{a_k-a}&=\frac{2n-1}{n^2}\sum_{k=1}^{N_0}\abso{a_k-a}+\frac{2n-1}{n^2}\sum_{k=N_0+1}^n \abso{a_k-a}\\
&=\frac{2n-1}{n^2}H+\frac{2n-1}{n^2}\sum_{k=N_0+1}^n \abso{a_k-a}\\
&= \frac{2n-1}{n^2}H+\frac{1}{n}\sum_{k=N_0+1}^n \frac{2n-1}{n}\abso{a_k-a}\\
&\leq  \frac{2n-1}{n^2}H+ \frac{1}{n}(n-N_0) \frac{\epsilon }{2} 
\end{align*}
Then if we let 
\begin{equation*}
X_n:=\frac{2n-1}{n^2}H+\frac{(n-N_0)\epsilon }{2n}
\end{equation*}
For all $n>N_0$, we have
 \begin{equation*}
\abso{t_n-a}<X_n
\end{equation*}
Notice 
\begin{equation*}
X_n=\frac{2H}{n}-\frac{H}{n^2}+\frac{\epsilon}{2}-\frac{N_0\epsilon }{2n}
\end{equation*}
So we have
\begin{equation*}
\lim_{n\to\infty}X_n=\frac{\epsilon}{2}
\end{equation*}
Then we know there exists some $N_1$ such that 
 \begin{equation*}
   n>N_1\implies \abso{X_n-\frac{\epsilon}{2}}<\frac{\epsilon}{2}\implies \abso{t_n-a}\leq X_n<\epsilon \bdone
\end{equation*}


\end{proof}
\begin{question}{}{}
5. If $a_k > 0$ for all $k \in \mathbb{N}$, prove that
\[
  \liminf_{k\to\infty} \frac{a_{k+1}}{a_k} \leq \liminf_{k\to\infty} \sqrt[k]{a_k} \leq \limsup_{k\to\infty} \sqrt[k]{a_k} \leq \limsup_{k\to\infty} \frac{a_{k+1}}{a_k}
\]
Moreover, find a sequence $\{a_k\}_{k=1}^{\infty}$ such that 
\[
  \limsup_{k\to\infty} \sqrt[k]{a_k} < \limsup_{k\to\infty} \frac{a_{k+1}}{a_k}
\]
\end{question}

\begin{proof}
The fact 
\begin{equation*}
\liminf_{n\to\infty} \sqrt[n]{a_n}\leq \limsup_{n\to\infty} \sqrt[n]{a_n}  
\end{equation*}
follows from definition.\\

We first prove
\begin{equation*}
  \vi{\liminf_{n\to\infty} \frac{a_{n+1}}{a_n}\leq \liminf_{n\to\infty} \sqrt[n]{a_n}}
\end{equation*}
Let 
\begin{equation*}
\alpha =\liminf_{n\to\infty} \frac{a_{n+1}}{a_n}
\end{equation*}
Notice that $\set{a_k}$ being positive give us $\alpha \geq 0$ and $0\leq \liminf_{n\to\infty} \sqrt[n]{a_n} $. If $\alpha =0$, the proof is done trivially. We only have to consider when $\alpha $ is positive.\\

Arbitrarily pick positive $\beta $ smaller than $\alpha $:
\begin{equation*}
\beta <\alpha=\liminf_{n\to\infty} \frac{a_{n+1}}{a_n}
\end{equation*}
Then we know there exists $N$ such that
 \begin{equation*}
\forall n\geq N, \frac{a_{n+1}}{a_n}>\beta 
\end{equation*}
This implies 
\begin{equation*}
\forall k, a_{N+k}>\beta^k a_N
\end{equation*}
Then for all $n>N$, we have
 \begin{equation*}
   \sqrt[n]{a_n}>\sqrt[n]{ \beta^{n-N}a_{N}}=\beta \sqrt[n]{\beta^{-N}a_N} 
\end{equation*}
Because 
\begin{equation*}
\lim_{n\to\infty}\beta \sqrt[n]{\beta^{-N}a_N}=\beta 
\end{equation*}
We see
\begin{equation*}
\liminf_{n\to\infty} \sqrt[n]{a_n} \geq \beta 
\end{equation*} 
Notice that $\beta $ is arbitrarily pick from $\set{x\inr: 0\leq x<\alpha }$, so we have in fact proved
\begin{equation*}
0\leq x<\alpha \implies x\leq \liminf_{n\to\infty} \sqrt[n]{a_n} 
\end{equation*}
If $\liminf_{n\to\infty} \sqrt[n]{a_n} <\alpha $, there should exists $x<\alpha $ such that $\liminf_{n\to\infty} \sqrt[n]{a_n} <x$, which we have prove is impossible. $\vdone$\\

We now prove 
\begin{equation*}
\blue{\limsup_{n\to\infty} \sqrt[n]{a_n}\leq \limsup_{n\to\infty} \frac{a_{n+1}}{a_n}}
\end{equation*}
Let $\gamma =\limsup_{n\to\infty} \frac{a_{n+1}}{a_n}$.  If $\gamma =\infty$, the proof is done trivially. We only have to consider when $\gamma<\infty$.\\

Notice that $\set{a_k}$ being positive give us $\gamma \geq 0$. Arbitrarily pick positive $\delta$ greater than $\gamma $:
\begin{equation*}
\delta>\gamma =\limsup_{n\to\infty} \frac{a_{n+1}}{a_n}
\end{equation*}
Then we know there exists $N$ such that
 \begin{equation*}
\forall n\geq N, \frac{a_{n+1}}{a_n}<\delta 
\end{equation*}
This implies
\begin{equation*}
\forall k,a_{N+k}<\delta^k a_N
\end{equation*}
Then for all $n>N$, we have
 \begin{equation*}
\sqrt[n]{a_n}<\sqrt[n]{\delta^{n-N}a_N}=\delta \sqrt[n]{\delta^{-N}a_N} 
\end{equation*}
Because
\begin{equation*}
\lim_{n\to\infty}\delta \sqrt[n]{\delta ^{-N}a_N} =\delta 
\end{equation*}
We see
\begin{equation*}
\limsup_{n\to\infty} \sqrt[n]{a_n}\leq \delta 
\end{equation*}
Notice that $\delta $ is arbitrarily picked from $\set{x\inr:x>\gamma }$, so we have in fact proved
\begin{equation*}
x>\gamma \implies x\geq \limsup_{n\to\infty} \sqrt[n]{a_n} 
\end{equation*}
If $\limsup_{n\to\infty} \sqrt[n]{a_n} >\gamma $, there should exists some $x>\gamma $ such that $\limsup_{n\to\infty} \sqrt[n]{a_n}>x$, which we have proved is impossible. $\bdone$\\

Let
\begin{equation*}
a_k=\begin{cases}
  2& \text{ if  }k\text{ is odd }\\
  1& \text{ if $k$ is even }
\end{cases}
\end{equation*}
We see 
\begin{equation*}
\limsup_{n\to\infty} \sqrt[n]{a_n}=1 <2\limsup_{n\to\infty} \frac{a_{n+1}}{a_n}
\end{equation*}
\end{proof}
\begin{question}{}{}
6. If $s_1 = \sqrt{2}$, and
\begin{equation*}
s_{n+1} =  \sqrt{2+\sqrt{s_n}}, \quad n = 1, 2, 3, \ldot
\end{equation*}
prove that $s_n$ converge and bounded above by $2$
\end{question}
\begin{proof}
We first prove \vi{$\set{s_n}$ increase monotonically} by induction.\\

Base case:
\begin{equation*}
s_1^2=2<2+\sqrt{2+\sqrt{\sqrt{2} } }=s_2^2 \implies s_1<s_2
\end{equation*}
Induction case: Let $s_k<s_{k+1}$. We wish to prove $s_{k+1}<s_{k+2}$. Because $s_{k+1}=\sqrt{2+\sqrt{s_k} } $, we have
\begin{equation*}
s_k\leq s_{k+1}=\sqrt{2+\sqrt{s_k} } 
\end{equation*}
Then we can deduce
\begin{gather*}
s_k\leq \sqrt{2+\sqrt{s_k} } \\
\implies \sqrt{s_k}\leq \sqrt{\sqrt{2+\sqrt{s_k} } }\\
\implies 2+\sqrt{s_k}\leq 2+\sqrt{\sqrt{2+\sqrt{s_k} } }\\
\implies \sqrt{2+\sqrt{s_k} }\leq \sqrt{2+\sqrt{\sqrt{2+\sqrt{s_k} } } }\\
\implies s_{k+1}=\sqrt{2+\sqrt{s_k} }\leq \sqrt{2+\sqrt{s_{k+1}} }=s_{k+2}\vdone
\end{gather*}
We now prove \blue{$\set{s_n}$ is bounded above by 2} by induction.\\

Base case: $s_1=\sqrt{2}<2$.\\

Induction case: Let $s_k\leq 2$. We wish to prove  $s_{k+1}\leq 2$. Observe
\begin{align*}
  s_k\leq 2&\implies s_k\leq 4\\
  &\implies \sqrt{s_k}\leq \sqrt{4} =2\\
  &\implies 2+\sqrt{s_k} \leq 4\\
  &\implies s_{k+1}=\sqrt{2+\sqrt{s_k} }\leq \sqrt{4}=2 \bdone
\end{align*}
The fact that $\set{s_n}$ monotonically increase and bounded above tell us $\set{s_n}$ converge.
\end{proof}
\begin{question}{}{}
7. Suppose $a_n > 0$ and $s_n = \sum _{k=1}^{n} a_k$. If $s_n$ diverges, prove or disprove that $t_n=\sum_{k=1}^{n} \frac{a_k}{1+a_k}$ diverges. What can be said about
\begin{equation*}
S_n=\sum_{k=1}^n \frac{a_k}{1+ka_k}
\end{equation*}
\begin{equation*}
T_n=\sum_{k=1}^n \frac{a_k}{1+k^2a_k}
\end{equation*}
\begin{equation*}
\text{If } s_n=\sum_{k=1}^n a_k\text{ converge, does }J_n=\sum_{k=1}^n ka_k\text{ converge }
\end{equation*}
\end{question}
\begin{proof}
  We prove 
 \vi{
  \begin{equation*}
  t_n\text{ converge }\implies s_n\text{ converge }
\end{equation*}}
Notice that
\begin{equation*}
a_n=\frac{a_n+a_n^2}{1+a_n}=\frac{a_n}{1+a_n}+\frac{a_n^2}{1+a_n}
\end{equation*}
So we have
\begin{equation*}
s_n=t_n+\sum_{k=1}^n \frac{a_n^2}{1+a_n}
\end{equation*}
Because $t_n$ converge, above tell us we only have to prove  $\sum_{k=1}^n \frac{a_n^2}{1+a_n}$ converge.\\

Because $t_n$ converge, we know 
 \begin{equation*}
\lim_{n\to\infty}\frac{a_n}{1+a_n}=0
\end{equation*}
\As{$\lim_{n\to\infty}a_n\neq 0$}. Then there exists $\epsilon $ such that
\begin{equation*}
\forall N\inn, \exists n>N, a_n>\epsilon 
\end{equation*}
Notice
\begin{equation*}
a_n>\epsilon \implies \frac{a_n}{1+a_n}=1-\frac{1}{1+a_n}>1-\frac{1}{1+\epsilon }\implies \abso{\frac{a_n}{1+a_n}-0}>1-\frac{1}{1+\epsilon }
\end{equation*}
In other words, there exists a sub-sequence $\set{a_{f(n)}}$ such that
\begin{equation*}
\lim_{n\to\infty}\frac{a_{f(n)}}{1+a_{f(n)}}\neq 0\tCaC
\end{equation*}
We have proved $\lim_{n\to\infty}a_n=0$. Then we know there exists $N_1$ such that
 \begin{equation*}
\forall n>N_1, a_n<1
\end{equation*}
In other words,
\begin{equation*}
\forall n>N_1, \frac{a_n^2}{1+a_n}<\frac{a_n}{1+a_n}
\end{equation*}
By comparison test, our proof is done $\vdone$.\\

We show
\begin{equation*}
\blue{\text{ It is possible $s_n$ diverge and $S_n$ converge.}}
\end{equation*}
Let 
\begin{equation*}
a_k=\begin{cases}
  1& \text{ if $\exists u\inn, k=u^2$ }\\
  \frac{1}{k^2}& \text{ otherwise }
\end{cases}
\end{equation*}
Clearly,  $\lim_{n\to\infty}s_n=\infty$. Yet, we have
\begin{align*}
  \lim_{n\to\infty}S_n=\sum_{k=1}^\infty \frac{a_k}{1+ka_k}&=\sum_{u=1}\frac{a_{u^2}}{1+u^2a_{u^2}}+\sum_{k\inn\setminus \set{r^2:r\inn}}\frac{a_k}{1+ka_k}\\
&=\sum_{u=1}\frac{1}{1+u^2}+\sum_{k\inn\setminus \set{r^2:r\inn}}\frac{\frac{1}{k^2}}{1+\frac{1}{k}}\\
&=\sum_{u=1}\frac{1}{1+u^2}+\sum_{k\inn\setminus \set{r^2:r\inn}} \frac{1}{k(k+1)}
\end{align*}
Notice that 
\begin{equation*}
\frac{1}{1+u^2}<\frac{1}{u^2}\text{ and }\frac{1}{k(k+1)}<\frac{1}{k^2}
\end{equation*}
Then by comparison test, we know 
\begin{equation*}
\text{ both }\sum_{u=1}\frac{1}{1+u^2}\text{ and }\sum_{k\inn\setminus \set{r^2:r\inn}} \frac{1}{k(k+1)}\text{ converge }
\end{equation*}
Then we know
\begin{equation*}
S_n\text{ also converge }\bdone
\end{equation*}
Notice that for all $n$
\begin{equation*}
k\geq 1\implies 1+k^2a_k>1+ka_k\implies \frac{a_k}{1+k^2a_k}<\frac{a_k}{1+ka_k}\implies T_n<S_n
\end{equation*}
Then because the term is non-negative, by comparison test, we know the example above also satisfy $T_n$ converge while $s_n$ diverge.

Notice that if we let $a_k=\frac{1}{k^2}$, then $s_n$ converge and $J_n=\sum_{k=1}^n \frac{1}{k}$ diverge. 

\end{proof}
\begin{question}{}{}
8. Assume \( A \subset \mathbb{R} \) is compact and let \( a \in A \). Suppose \( \{ a_n \} \) is a sequence in \( A \) such that every convergent sub-sequence of \( \{ a_n \} \) converges to \( a \). 
    \begin{enumerate}
        \item Does the sequence \( \{ a_n \} \) also converge to \( a \)?
        \item Without the assumption that \( A \) is compact, does the sequence \( \{ a_n \} \) converge to \( a \)?
    \end{enumerate}
\end{question}
\begin{proof}
\As{$a_n$ does not converge to $a$}. We know there exists $\epsilon $ such that there exists a sub-sequence $\set{a_{n_k}}_{k\inn}$
\begin{equation*}
\forall k\inn, a_{n_k}\not\in B_{\epsilon }(a)
\end{equation*}
Because  $A$ is (sequentially ) compact,  we know there must exist a sub-sequence $\set{a_{n_{k_u}}}_u\inn$ such that
\begin{equation*}
\set{a_{n_{k_u}}}\text{ converge }
\end{equation*}
Notice that
\begin{equation*}
\set{a_{n_{k_u}}}\text{ is a sub-sequence of $\set{a_n}$ }
\end{equation*}
So we know 
 \begin{equation*}
\lim_{u\to\infty}a_{n_{k_u}}=a\tCaC\text{ to }\forall n\inn, a_{n_{k}}=\not\in B_\epsilon (a)
\end{equation*}
Let $A=\Q$, and let
 \begin{equation*}
a_n=\begin{cases}
  0& \text{ if $n$ is odd }\\
  n& \text{ if $n$ is even }
\end{cases}
\end{equation*}
and we see every convergent sub-sequence converge to $0$ but  $a_n$ itself does not converge to  $0$
\end{proof}
\begin{question}{}{}
\begin{enumerate}
    \setcounter{enumi}{8}
    \item Suppose that \( a_k \neq 0 \) for large \( k \) and that
    \[
    p = \lim_{{k \to \infty}} \frac{\ln\left(\frac{1}{|a_k|}\right)}{\ln(k)}
    \]
    exists as an extended real number.
    \begin{enumerate}
        \item If \( p > 1 \), then \(\sum_{k=1}^{\infty} a_k\) converges absolutely.
        \item If \( p < 1 \), then $\sum _{k=1}^\infty a_k$ diverge
    \end{enumerate}
\end{enumerate}
\end{question}
\begin{proof}
Let $p>\alpha >1$.  Then we know there exists $N$ such that
 \begin{equation*}
\forall n>N, \frac{\ln (\frac{1}{\abso{a_n}})}{\ln n}>\alpha >1
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \ln(\frac{1}{\abso{a_n}})>\alpha \ln n 
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \frac{\ln (\frac{1}{\abso{a_n}})}{\alpha }>\ln n
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \ln (\abso{a_n}^{-\frac{1}{\alpha }})>\ln n
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \abso{a_n}^{\frac{-1}{\alpha }}>n
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \abso{a_n}<n^{-\alpha }<n^{-1}
\end{equation*}
By comparison test, we are done.\\

Let $p<\beta <1$. Then we know there exists $N$ such that
\begin{equation*}
\forall n>N, \frac{\ln (\frac{1}{\abso{a_n}})}{\ln n}<\beta <1
\end{equation*}
This give us
\begin{equation*}
  \forall n>N, \frac{\ln(\abso{a_n}^{-1})}{\beta }<\ln n
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \ln(\abso{a_n}^{\frac{-1}{\beta }})<\ln n 
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \abso{a_n}^{\frac{-1}{\beta }}<n
\end{equation*}
This give us
\begin{equation*}
\forall n>N, \abso{a_n}>n^{-\beta }>n^{-1}
\end{equation*}
By comparison test, we are done.
\end{proof}
\begin{question}{}{}
\begin{enumerate}
    \setcounter{enumi}{9}
    \item Suppose that \( f : \mathbb{R} \to (0,\infty) \) is differentiable, that \( f(x) \to 0 \) as \( x \to \infty \), and that
    \[
    \alpha = \lim_{{x \to \infty}} \frac{xf'(x)}{f(x)}
    \]
    exists. If \( \alpha < -1 \), prove that
    \[
    \sum_{k=1}^{\infty} f(k)
    \]
    converges.
\end{enumerate}
\end{question}
\begin{proof}
Let $\beta $ satisfy
\begin{equation*}
\alpha<\beta <-1 
\end{equation*}
Because
\begin{equation*}
\lim_{x\to\infty}\frac{xf'(x)}{f(x)}=\alpha 
\end{equation*}
We know there exists $R$ such that
\begin{equation*}
\forall x>R, \frac{xf'(x)}{f(x)}\leq \beta 
\end{equation*}
Then we have
\begin{equation*}
\forall x>R, \frac{d}{dx}\ln f(x)=\frac{f'(x)}{f(x)}\leq \frac{\beta }{x}
\end{equation*}
This tell us 
\begin{equation*}
  \forall y>R, \int^y_R \frac{d}{dx}\ln f(x)dx\leq \int^y_R \frac{\beta}{x}dx
\end{equation*}
Which means
\begin{equation*}
\forall y>R,\ln ( \frac{f(y)}{f(R)})=\ln f(y)-\ln f(R)\leq \beta (\ln(y)-\ln(R))= \ln(\frac{y}{R})^\beta 
\end{equation*}
This shows
\begin{equation*}
\forall y>R, \frac{f(y)}{f(R)}\leq (\frac{y}{R})^{\beta }
\end{equation*}
It means
\begin{equation*}
\forall y>R, f(y)\leq \frac{f(R)}{R^{\beta }}y^{\beta }
\end{equation*}
Notice $\beta <-1$, we know the series 
\begin{equation*}
\sum_{n>R} \frac{f(R)}{R^{\beta }}n^{\beta }=\frac{f(R)}{R^{\beta }}\sum_{n>R}n^{\beta }\text{ converge }
\end{equation*}
Then because $f(y)$ is always positive (given by the question), we know $\sum_{n=1}^\infty f(n)$ converge by comparison test.
\end{proof}
\begin{lemma}
\label{8.3.4}
\textbf{(Bernoulli Inequality)} Let  $r\geq 1$ and $x\geq -1$. We have
\begin{equation*}
  (1+x)^r\geq 1+rx
\end{equation*}
\end{lemma}
\begin{proof}
Let $r\geq 1$, and let
\begin{equation*}
f(x)=(1+x)^r-1-rx
\end{equation*}
We wish to prove 
\begin{equation*}
\forall x\geq -1, f(x)\geq 0
\end{equation*}
We now split the proof into two parts
\begin{gather*}
  \vi{\forall x\geq 0, f(x)\geq 0}\\
  \blue{\forall x\in [-1,0), f(x)\geq 0}
\end{gather*}
First notice by computation
\begin{equation*}
f'(x)=r(1+x)^{r-1}-r=r((1+x)^{r-1}-1)
\end{equation*}
And notice by some algebra
\begin{equation*}
f'(x)\geq 0\iff  (1+x)^{r-1}\geq 1\iff  x\geq 0
\end{equation*}
We first prove 
\begin{equation*}
\vi{\forall x\geq 0, f(x)\geq 0}
\end{equation*}
By computation,
\begin{equation*}
f(0)=0
\end{equation*}
This give us 
\begin{equation*}
\forall x\geq 0, f(x)=f(x)-0=f(x)-f(0)=\int_{0}^x f'(t)dt\geq 0\vdone
\end{equation*}
We now prove
\begin{equation*}
\blue{\forall x\in [-1,0), f(x)\geq 0}
\end{equation*}
Notice that above have shown
\begin{equation*}
f'(x)\geq 0\iff  x\geq 0
\end{equation*}
So we know
\begin{equation*}
\forall x\in [-1,0), f'(x)<0
\end{equation*}
Observe that $f(x)$ is an polynomial, so we know  $f(x)$ is continuous. Then if for some $y\in [-1,0)$, we have $f(y)<0=f(0)$, there must exists $u\in (y,0)\subseteq [-1,0)$ such that $f'(u)>0$, which is impossible.
\end{proof}
\begin{question}{}{}
\begin{enumerate}
    \setcounter{enumi}{10}
    \item Suppose that \( \{ a_n \} \) is a sequence of nonzero real numbers and that
    \[
    p = \lim_{{k \to \infty}} k(1-\abso{\frac{a_{k+1}}{a_k}})
    \]
    exists as an extended real number. Prove that
    \[
    \sum_{k=1}^{\infty} |a_k|
    \]
    converges absolutely when \( p > 1 \).
\end{enumerate}
\end{question}
\begin{proof}
Pick $\alpha $ that satisfy
\begin{equation*}
\lim_{k\to\infty}k(1-\abso{\frac{a_{k+1}}{a_k}})>\alpha >1
\end{equation*}
We know there exist $N$ such that
 \begin{equation*}
\forall n>N, n(1-\abso{\frac{a_{n+1}}{a_n}})>\alpha 
\end{equation*}
With a little algebra,
\begin{equation*}
\forall n>N, 1-\frac{\alpha }{n}>\abso{\frac{a_{n+1}}{a_n}}
\end{equation*}
Plugin \myref{Lemma}{8.3.4} (Bernoulli Inequality) with $r=\alpha >1$ and $x=\frac{-1}{n}\geq -1$. We have
\begin{equation*}
\forall n>N,\abso{\frac{a_{n+1}}{a_n}}<1+\frac{-\alpha }{n}\leq (1-\frac{1}{n})^\alpha= (\frac{n-1}{n})^{\alpha }
\end{equation*}
Then we have
\begin{equation*}
  (*)\forall n>N, \abso{a_{n+1}}n^\alpha < \abso{a_n} (n-1)^{\alpha }
\end{equation*}
For each $n>N$, define
 \begin{equation*}
b_n=\abso{a_{n+1}}n^{\alpha }
\end{equation*}
Then we have
\begin{equation*}
\abso{a_n}(n-1)^{\alpha }=b_{n-1}
\end{equation*}
So by $(*)$ we have
\begin{equation*}
\forall n>N+1, b_n<b_{n-1}
\end{equation*}
This tell us $\set{b_n}_{n>N+1}$ is a decreasing sequence. Then we know $\set{b_n:n>N+1}$ is bounded above. More precisely, let
\begin{equation*}
M=b_{N+1}
\end{equation*}
We have
\begin{equation*}
\forall n>N+1, b_n< M
\end{equation*}
Recall the definition of $b_n$, we have
 \begin{equation*}
\forall n>N+1, \abso{a_{n+1}}n^{\alpha }<M
\end{equation*}
In other words
\begin{equation*}
\forall n>N+2, \abso{a_n}<Mn^{-\alpha }
\end{equation*}
Notice that because $\alpha>1 $. We know 
\begin{equation*}
\sum_{n\inn} Mn^{-\alpha }\text{ converge }
\end{equation*}
Then by comparison test, we know
\begin{equation*}
\sum_{n\inn} \abso{a_n}\text{ converge }
\end{equation*}

\end{proof}

\section{HW4}
\begin{theorem}
\label{8.4.1}
Let $B_n=\sum_{k=1}^n b_n$. Given
\begin{equation*}
  a_n\searrow 0\text{ or }a_n\nearrow 0\text{ as }n\to\infty
\end{equation*}
and 
\begin{equation*}
\exists M, \forall n, \abso{B_n}<M
\end{equation*}
We have
\begin{equation*}
\sum_{n=1}^\infty a_nb_n\text{ converge }
\end{equation*}
\end{theorem}
\begin{proof}
Let $N$ satisfy
 \begin{equation*}
a_{n+N}\nearrow 0\text{ or }a_{n+N}\searrow 0
\end{equation*}
We see 
\begin{equation*}
\sum_{k=N+1}^n b_n=B_n-B_N\text{ is also bounded }
\end{equation*}
If $a_{n+N}\searrow 0$, then by Dirichlet's test we see
\begin{equation*}
\sum_{k=1}^\infty a_kb_k=\sum_{k=1}^{N}a_kb_k+\sum_{n=N+1}^\infty a_kb_k\text{ converge }
\end{equation*}
If $a_{n+N}\nearrow 0$, we see $-a_{n+N}\searrow 0$. Then by Dirichlet's test we see
\begin{equation*}
\sum_{k=1}^\infty a_kb_k=\sum_{k=1}^N a_kb_k - \sum_{k=N+1}^\infty (-a_k)b_k\text{ converges }
\end{equation*}
\end{proof}


\begin{question}{}{}
Suppose that $\sum_{k=1}^{\infty} a_k$ 
converges and that 
$b_k \searrow b \text{ as } k \to \infty$. Prove that 
\begin{equation*}
  \sum_{k=1}^{\infty} a_k b_k \text{ converges.}
\end{equation*}
\end{question}
\begin{proof}
Define
\begin{equation*}
b'_n:=b_n-b
\end{equation*}
Deduce
\begin{align*}
  \sum_{k=1}^\infty a_kb_k&=\sum_{k=1}^\infty a_k(b+b'_k)\\
  &=b\sum_{k=1}^\infty a_k+\sum_{k=1}^\infty a_kb'_k
\end{align*}
Notice
\begin{equation*}
b_k\searrow b\text{ as }k\to\infty\implies b_k'\searrow 0\text{ as }k\to\infty
\end{equation*}
Then by Dirichlet's test, we know
\begin{equation*}
\sum_{k=1}^\infty a_kb'_k\text{ converge }
\end{equation*}
Because by premise $\sum_{k=1}^\infty a_k$ converge, we know $\sum_{k=1}^\infty a_kb_k=b\sum_{k=1}^\infty a_k+\sum_{k=1}^\infty a_kb'_k\text{ converge. }$


\end{proof}

\begin{question}{}{}
Show that under the hypotheses of Dirichlets test,
\[
\sum_{k=1}^{\infty} a_k b_k = \sum_{k=1}^{\infty} s_k (b_k - b_{k+1}),
\]
where 
\[
s_k = \sum_{j=1}^{k} a_j.
\]
\end{question}
\begin{proof}
Abel's formula give us
\begin{align*}
\sum_{k=1}^na_kb_k&=\sum_{k=1}^n (s_{k}-s_{k-1})b_k\\
&=\sum_{k=1}^n s_kb_k-\sum_{k=1}^{n-1} s_kb_{k+1}\\
&=\sum_{k=1}^{n-1}s_k(b_k-b_{k+1})+s_nb_n
\end{align*}
Define 
\begin{equation*}
X_n:=\sum_{k=1}^n a_kb_k\text{ and }Y_n:=\sum_{k=1}^{n}s_k(b_k-b_{k+1})
\end{equation*}
Abel's formula then can be rewritten into
\begin{equation*}
\forall n,Y_n=X_{n+1}-s_nb_n
\end{equation*}
Define
\begin{equation*}
\alpha :=\lim_{n\to\infty}X_n 
\end{equation*}
The question ask us to prove
\begin{equation*}
\lim_{n\to\infty}Y_n=\alpha 
\end{equation*}
The hypothesis of Dirichlet's test are 
\begin{equation*}
  s_n=\sum_{k=1}^n a_k\text{ are bounded and }b_n\searrow 0 
\end{equation*}
Let $M\inr^+$ satisfy
\begin{equation*}
\forall n, \abso{s_n}<M
\end{equation*}
Arbitrarily pick $\epsilon\inr^+$. We know there exists $N_1$ such that
 \begin{equation*}
\forall n>N_1, 0\leq b_n<\frac{\epsilon }{2M}
\end{equation*}
Because  $\alpha =\lim_{n\to\infty}X_n$, we also know there exists $N_2$ such that
 \begin{equation*}
\forall n>N_2, \abso{X_n-\alpha }<\frac{\epsilon }{2}
\end{equation*}
We then see 
\begin{align*}
  \forall n>\max \set{N_1,N_2}, \abso{Y_n-\alpha }&=\abso{X_{n+1}-\alpha -s_nb_n}\\
  &\leq \abso{X_{n+1}-\alpha }+\abso{s_nb_n}\\
  &<\frac{\epsilon}{2}+M\times \frac{\epsilon }{2M}=\epsilon 
\end{align*}
Because $\epsilon $ is arbitrarily picked from $\R^+$. We have proved  $\lim_{n\to\infty}Y_n=\alpha $
\end{proof}
\begin{lemma}
\label{8.4.2}
Given $n$
\begin{equation*}
  \sum_{k=1}^\infty x_k\text{ converge }\implies \sum_{k=n}^\infty x_k=\sum_{k=1}^\infty x_k-\sum_{k=1}^{n-1}x_k
\end{equation*}
\end{lemma}
\begin{proof}
For all $u\inn$, define 
\begin{equation*}
s_u:=\sum_{k=n}^{n+u}x_k\text{ and }v_u:=\sum_{k=1}^{u}x_k
\end{equation*}
From definition, we have
\begin{equation*}
  \forall u\inn,s_u=v_{n+u}-\sum_{k=1}^{n-1}x_k\text{ and }\lim_{u\to\infty}v_u\text{ converge }
\end{equation*}
Notice 
\begin{equation*}
\lim_{u\to\infty}v_{n+u}=\lim_{u\to\infty}v_u
\end{equation*}
Then we have
\begin{equation*}
\lim_{u\to\infty}s_u=\lim_{u\to\infty} (v_{n+u}-\sum_{k=1}^{n-1}x_k)=(\lim_{u\to\infty}v_{n+u})-\sum_{k=1}^{n-1}x_k=\lim_{u\to\infty}v_u-\sum_{k=1}^{n-1}x_k
\end{equation*}
Which written in series is
\begin{equation*}
\sum_{k=n}^{\infty} x_k=\sum_{k=1}^\infty x_k-\sum_{k=1}^{n-1}x_k
\end{equation*}
\end{proof}
\begin{lemma}
\label{8.4.3}
\begin{equation*}
\sum_{k=1}^\infty x_k\text{ converge }\implies \lim_{n\to\infty}\sum_{k=n}^\infty x_k=0
\end{equation*}
\end{lemma}
\begin{proof} 
By \myref{Lemma}{8.4.2}, we have
\begin{align*}
  \lim_{n\to\infty}\sum_{k=n}^\infty x_k&=\lim_{n\to\infty}\sum_{k=1}^\infty a_k-\sum_{k=1}^{n-1}a_k\\
  &=\sum_{k=1}^\infty a_k-\lim_{n\to\infty}\sum_{k=1}^{n-1}a_k\\
  &=\sum_{k=1}^\infty a_k-\sum_{k=1}^\infty a_k=0
\end{align*}
\end{proof}
\begin{question}{}{}
Suppose that $\sum_{k=1}^{\infty} a_k $ converges, that $b_k \nearrow \infty \text{ and that }\sum_{k=1}^{\infty} a_k b_k $
converge. Prove
\begin{equation*}
\lim_{m\to\infty}b_m\sum_{k=m}^\infty a_k=0
\end{equation*}
\end{question}
\begin{proof}
Define
\begin{equation*}
c_k=a_kb_k\text{ and }C_k=\sum_{j=k}^\infty c_j
\end{equation*}
By \myref{Lemma}{8.4.3}, we have
\begin{equation*}
\lim_{k\to\infty}C_k=0
\end{equation*}
Observe
\begin{align*}
  \sum_{k=n}^\infty a_k&=\sum_{k= n}^\infty \frac{c_k}{b_k}\\
  &=\sum_{k=n}^\infty \frac{C_k-C_{k+1}}{b_k}\\
  &=\lim_{N\to\infty}\sum^N_{k=n} \frac{C_k-C_{k+1}}{b_k}\\
  &=\lim_{N\to\infty}\sum_{k=n}^N \frac{C_k}{b_k}-\sum_{k=n+1}^{N+1}\frac{C_k}{b_{k-1}}\\
  &=\lim_{N\to\infty} \sum_{k=n+1}^{N} C_k(\frac{1}{b_k}-\frac{1}{b_{k-1}})-\frac{C_{N+1}}{b_N}+\frac{C_n}{b_n}
\end{align*}
Because $C_{N+1}\to 0$ and $b_N\to \infty$ as $N\to\infty$, we have
\begin{align*}
\sum_{k=n}^\infty a_k&=\lim_{N\to\infty}\sum_{k=n+1}^N C_k(\frac{1}{b_k}-\frac{1}{b_{k-1}})+\frac{C_n}{b_n}\\
&=\sum_{k=n+1}^\infty C_k(\frac{1}{b_k}-\frac{1}{b_{k-1}})+\frac{C_n}{b_n}
\end{align*}
This give us
\begin{align*}
\abso{b_n\sum_{k=n}^\infty a_n}&=\abso{b_n \sum_{k=n+1}^\infty C_k(\frac{1}{b_k}-\frac{1}{b_{k-1}})+b_n\times \frac{C_n}{b_n}}\\
&\leq \abso{b_n\sum_{k=n+1}^\infty C_k (\frac{1}{b_k}-\frac{1}{b_{k-1}})}+\abso{C_n}
\end{align*}
Arbitrarily pick $\epsilon \inr^+$. Because $C_n\to 0$ and $b_n\nearrow \infty$ as $n\to\infty$, we know there exists $N$ such that  $\abso{C_n}<\frac{\epsilon}{2}$  and $b_n>0$ when $n>N$.\\

Let $n>N$. We now have
 \begin{align*}
   \abso{b_n\sum_{k=n}^\infty a_n}&\leq b_n\abso{\sum_{k=n+1}^\infty C_k(\frac{1}{b_k}-\frac{1}{b_{k-1}})}+\frac{\epsilon}{2}\\
&\leq b_n\sum_{k=n+1}^\infty \abso{C_k(\frac{1}{b_k}-\frac{1}{b_{k-1}})}+\frac{\epsilon}{2}\\
&\leq b_n\sum_{k=n+1}^\infty \abso{C_k} \times\abso{\frac{1}{b_k}-\frac{1}{b_{k-1}}}+\frac{\epsilon}{2}\\
&= b_n\sum_{k=n+1}^\infty \abso{C_k}\times (\frac{1}{b_{k-1}}-\frac{1}{b_k})+\frac{\epsilon}{2}
\end{align*}
Notice that because $C_n\to 0$, we know  $\set{C_n:n\inn}$ is bounded. Then we know for each $n$, the supremum  $\sup_{k\geq n} \abso{C_k}$ exists. Define
\begin{equation*}
X_n=\sup_{k\geq n+1}\abso{C_k}
\end{equation*}
For $n>N$, we now have
 \begin{align*}
   \abso{b_n\sum_{k=n}^\infty a_n}&\leq b_n\sum_{k=n+1}^\infty \abso{C_k}\times (\frac{1}{b_{k-1}}-\frac{1}{b_k})+\frac{\epsilon}{2}\\
                                  &\leq b_n\sum_{k=n+1}^\infty X_n \times (\frac{1}{b_{k-1}}-\frac{1}{b_k})+\frac{\epsilon}{2}\\
   &=(X_nb_n\sum_{k=n+1}^\infty \frac{1}{b_{k-1}}-\frac{1}{b_k}) +\frac{\epsilon}{2}\\
   &=\lim_{u\to\infty} X_nb_n (\frac{1}{b_n}-\frac{1}{b_u})+\frac{\epsilon}{2}\\
   &=X_n \frac{b_n}{b_n}+\frac{\epsilon}{2}\\
   &=X_n+\frac{\epsilon}{2}
\end{align*}
Notice that 
\begin{equation*}
\lim_{n\to\infty}X_n=\lim_{n\to\infty}X_{n-1}=\lim_{n\to\infty}\sup_{k\geq n}\abso{C_k}=\limsup_{n\to\infty} \abso{C_n}
\end{equation*}
Then because $C_n\to 0$, we know 
 \begin{equation*}
\lim_{n\to\infty}X_n=\limsup_{n\to\infty} \abso{C_n}=\lim_{n\to\infty}\abso{C_n}=0
\end{equation*}
This tell us there exists $N_1$ such that  $X_n<\frac{\epsilon}{2}$. Then for $n>\max \set{N,N_1}$, we have
\begin{equation*}
\abso{b_n\sum_{k=n}^\infty a_n}\leq X_n+\frac{\epsilon}{2}=\epsilon 
\end{equation*}
This finish the proof. 
\end{proof}
\begin{question}{}{}
Suppose that \( a_k > 0 \) and 
\[
\sum_{k=1}^{\infty} a_k
\]
converges. Prove that there exist \( b_k \) such that 
\[
\lim_{k \to \infty} \frac{b_k}{a_k} = \infty
\]
and 
\[
\sum_{k=1}^{\infty} b_k
\]
converges.
\end{question}
\begin{proof}
Define
\begin{equation*}
r_n:=\sum_{k=n}^\infty a_k
\end{equation*}
Because $\set{a_k}$ are positive, by \myref{Lemma}{8.4.2}, we know $r_n$ monotonically decrease.\\

Moreover, by \myref{Lemma}{8.4.3}, we know $r_n\searrow 0$.\\

Observe
\begin{equation*}
\sqrt{r_n} -\sqrt{r_{n+1}}=\frac{r_n-r_{n+1}}{\sqrt{r_n} +\sqrt{r_{n+1}} }=\frac{a_n}{\sqrt{r_n} +\sqrt{r_{n+1}} }
\end{equation*}
Define $\set{b_n}$ by
\begin{equation*}
b_n:=\frac{a_n}{\sqrt{r_n} +\sqrt{r_{n+1}} }=\sqrt{r_n} -\sqrt{r_{n+1}} 
\end{equation*}
Observe
\begin{equation*}
\sum_{k=1}^\infty b_k=\sum_{k=1}^\infty \sqrt{r_k} -\sqrt{r_{k+1}}=\sqrt{r_1}  
\end{equation*}
Notice 
\begin{equation*}
\frac{b_n}{a_n}=\frac{1}{\sqrt{r_n} +\sqrt{r_{n+1}} }
\end{equation*}
And deduce
\begin{equation*}
r_n\searrow 0\implies \sqrt{r_n} \searrow 0\implies \frac{b_n}{a_n}=\frac{1}{\sqrt{r_n} +\sqrt{r_{n+1}} }\nearrow \infty
\end{equation*}
\end{proof}
\begin{question}{}{}
Suppose that \( a_k > 0 \) and 
\[
\sum_{k=1}^{\infty} a_k
\]
diverges. Prove that there exist \( b_k \) such that 
\[
\lim_{k \to \infty} \frac{b_k}{a_k} = 0
\]
and 
\[
\sum_{k=1}^{\infty} b_k
\]
diverges.
\end{question}
\begin{proof}
Define
\begin{equation*}
A_n:= \sum_{k=1}^n a_k
\end{equation*}
And define
\begin{equation*}
b_n:=\frac{a_n}{A_n}
\end{equation*}
Notice that $\set{a_n}$ are positive and $\sum_{k=1}^\infty a_k$ diverges implies 
\begin{equation*}
\lim_{n\to\infty}A_n=\sum_{k=1}^\infty a_k=\infty
\end{equation*}
So we have
\begin{equation*}
\lim_{n\to\infty}\frac{b_n}{a_n}=\lim_{n\to\infty}\frac{1}{A_n}=0
\end{equation*}
We now show the following, which will be essential for proving $\sum_{k=1}^\infty b_k$ diverges.\\

Define
\begin{equation*}
X^q_p:=\sum_{n=p}^q b_n
\end{equation*}
Let $q>p$. Because  $\set{a_n}$ are positive, we have 
\begin{equation*}
A_p<A_q
\end{equation*}
This give us
\begin{equation*}
X_p^q=\sum_{n=p}^q \frac{a_n}{A_n}\geq \sum_{n=p}^q \frac{a_n}{A_q}=\frac{A_q-A_{p-1}}{A_q}=1-\frac{A_{p-1}}{A_q}
\end{equation*}
Then because $\lim_{n\to\infty}A_n=\infty$, we have
\begin{equation*}
\forall p,\liminf_{q\to\infty}X_p^q\geq \liminf_{q\to\infty} 1-\frac{A_{p-1}}{A_q} =\lim_{q\to\infty}1-\frac{A_{p-1}}{A_q}=1
\end{equation*}
Define $p_1=1$. We now know there exists $p_2>p_1$ such that
\begin{equation*}
\sum_{n=p_1}^{p_2-1} b_n= X_{p_1}^{p_2-1}>\frac{1}{2}
\end{equation*}
Again, because $\forall p,\liminf_{q\to\infty} X_p^q=1$, we know there exists $p_3>p_2$ such that
 \begin{equation*}
\sum_{n=p_2}^{p_3-1} b_n=X_{p_2}^{p_3-1} >\frac{1}{2}
\end{equation*}
Proceeding the argument above, we know there exists an increasing index sequence $\set{p_k}_{k\inn}$ such that 
\begin{equation*}
\forall k,\sum_{n=p_k}^{p_{k+1}-1} b_k>\frac{1}{2}
\end{equation*}
Then we have
\begin{equation*}
\sum_{k=1}^\infty b_k=  \sum_{k=1}^\infty \sum_{n=p_k}^{p_{k+1}-1} b_k\geq \sum_{k=1}^\infty \frac{1}{2}=\infty
\end{equation*}

\end{proof}

\begin{question}{}{}
Prove that 
\[
\sum_{k=1}^{\infty} a_k \cos(kx)
\]
converges for every \( x \in (0, 2\pi) \) and every \( a_k \searrow 0 \). What happens when \( x = 0 \)?
\end{question}
\begin{proof}
Let $x\in (0,2\pi)$, and let 
\begin{equation*}
B_n=\sum_{k=1}^n \cos(kx)
\end{equation*}
We wish to prove
\begin{equation*}
\vi{\set{B_n}\text{ is bounded }}
\end{equation*}
Deduce
\begin{align*}
  \forall n,\abso{B_n}=\abso{\sum_{k=1}^n \cos (kx)}&= \abso{\sum_{k=1}^n \text{ Re }e^{i(kx)}}\\
  &=\abso{\sum_{k=1}^n \text{ Re }(e^{ix})^k}\\
  &=\abso{\text{ Re }\sum_{k=1}^n (e^{ix})^k}\\
  &=\abso{\text{ Re }\frac{e^{ix}(1-e^{nix})}{1-e^{ix}}}\\
  &\leq \abso{\frac{e^{ix}(1-e^{nix})}{1-e^{ix}}}\\
  &=\frac{\abso{e^{ix}-e^{(n+1)ix}}}{\abso{1-e^{ix}}}\\
  &\leq \frac{\abso{e^{ix}}+\abso{e^{(n+1)ix}}}{\abso{1-e^{ix}}}\\
  &\leq \frac{2}{\abso{1-e^{ix}}}
\end{align*}
Notice that because $x\in (0,2\pi)$, we have
\begin{equation*}
e^{ix}\neq 1
\end{equation*}
So we know $\frac{2}{\abso{1-e^{ix}}}\inr^+$. Let $M=\frac{2}{\abso{1-e^{ix}}}$. By the long inequality above, we have
\begin{equation*}
\forall n, \abso{B_n}\leq M\vdone
\end{equation*}

Because $\set{B_n}$ is bounded, by Dirichlet's Test, we are done.\\

If $x=0$, it is possible that  $a_k=\frac{1}{k}$, and we will have
\begin{equation*}
\sum_{k=1}^\infty a_k\cos(kx)=\sum_{k=1}^\infty a_k\text{ diverge when $a_k\searrow 0$ }
\end{equation*}
\end{proof}

\begin{question}{}{}
Prove that 
\[
\sum_{k=1}^{\infty} a_k \sin((2k + 1)x)
\]
converges for every \( x \in \mathbb{R} \) and every \( a_k \searrow 0 \).
\end{question}
\begin{proof}
Notice that  
\begin{equation*}
x\equiv 0\text{ mod $\pi$}\implies \forall k,\sin ((2k+1)x)=0
\end{equation*}
This make our proof trivial. We only have to consider when 
\begin{equation*}
x\not\equiv 0\text{ mod }\pi
\end{equation*}
Observe
\begin{align*}
 \forall n, \abso{\sum_{k=1}^n \sin((2k+1)x)}&= \abso{\sum_{k=1}^n \text{ Im }e^{i(2k+1)x}}\\
  &=\abso{\text{ Im }\sum_{k=1}^n e^{i(2k+1)x}}\\
  &\leq \abso{\sum_{k=1}^n e^{i(2k+1)x}}\\
  &=\abso{\frac{e^{i3x}(1-e^{i2nx})}{1-e^{i2x}}}\\
  &\leq \frac{\abso{e^{i3x}}+\abso{e^{i(3x+2nx)}}}{\abso{1-e^{i2x}}}\\
  &\leq \frac{2}{\abso{1-e^{i 2x}}}
\end{align*}
Because $x\not\equiv 0\text{ mod $\pi$}$, we know $\frac{2}{\abso{1-e^{i2x}}}\inr^+$. We have proved the partial sums $\sum_{k=1}^n \sin((2k+1)x)$ is bounded. The proof for this problems follows from Dirichlet's Test.

\end{proof}

\begin{question}{}{}
Suppose that 
\[
\sum_{k=1}^{\infty} a_k^2
\]
and 
\[
\sum_{k=1}^{\infty} b_k^2
\]
converges. Prove that the following series
\begin{enumerate}
    \item \[
    \sum_{k=1}^{\infty} |a_kb_k|
    \]
    \item \[
    \sum_{k=1}^{\infty} (a_k + b_k)^2
    \]
    \item \[
    \sum_{k=1}^{\infty} \frac{|a_k|}{k}
    \]
\end{enumerate}
converge.
\end{question}
\begin{proof}
Deduce 
\begin{align*}
  0\leq (a_k-b_k)^{2}&\implies 0\leq a_k^2-2a_kb_k+b_k^2\\
  &\implies a_kb_k\leq \frac{a_k^2+b_k^2}{2}
\end{align*}
Deduce
\begin{align*}
  0\leq (a_k+b_k)^2&\implies 0\leq a_k^2+2a_kb_k+b_k^2\\
&\implies a_kb_k\geq  -\frac{a_k^2+b_k^2}{2}
\end{align*}
Above give us
\begin{equation*}
\abso{a_kb_k}\leq \frac{a_k^2+b_k^2}{2}
\end{equation*}
Clearly
\begin{equation*}
\sum_{k=1}^\infty \frac{a_k^2+b_k^2}{2}\text{ converges }
\end{equation*}
Then by comparison test, we know $\sum_{k=1}^\infty \abso{a_kb_k}$ converges.\\

Notice that
\begin{equation*}
  0\leq (a_k+b_k)^2=a_k^2 +2a_kb_k+b_k^2 \leq a_k^2+2\abso{a_kb_k}+b_k^2 
\end{equation*}
Then, because 
\begin{equation*}
\sum_{k=1}^\infty a_k^2 +2\abso{a_kb_k}+b_k^2\text{ converges }
\end{equation*}
By comparison test, we now know 
\begin{equation*}
\sum_{k=1}^\infty (a_k+b_k)^2 \text{ converges }
\end{equation*}
Notice
\begin{equation*}
\sum_{k=1}^\infty a_k^2\text{ converge }\implies \lim_{k\to\infty}a_k^2=0
\end{equation*}
Then we know there exists $N$ such that
 \begin{equation*}
\forall n>N, a_n^2<1
\end{equation*}
Because $a_n^2<1\implies \abso{a_n}<1$. This give us
\begin{equation*}
\forall n, \abso{a_n}\leq \max \set{\abso{a_1},\abso{a_2},\dots ,\abso{a_N},1}
\end{equation*}
We have proved $\abso{a_n}$ is bounded. Notice that 
\begin{equation*}
\frac{1}{k}\searrow 0
\end{equation*}
Then by Dirichlet's Test, we are done.

\end{proof}

\begin{question}{}{}
Does the series
\[
\sum_{k=1}^{\infty} \frac{(-1)^k \ln(k + 1)}{k}
\]
converge? Does it converge absolutely? Justify your answer.
\end{question}
\begin{proof}
First notice
\begin{equation*}
\frac{d}{dt}\frac{\ln (t+1)}{t} =\frac{\frac{t}{t+1}- \ln (t+1)}{t^2}
\end{equation*}
Because 
\begin{equation*}
\frac{t}{t+1}\nearrow 1 \text{ and }\ln (t+1)\nearrow \infty
\end{equation*}
We know for large $k$, 
\begin{equation*}
\frac{d}{dt}\frac{\ln (t+1)}{t} \bigg|_{t=k} =\frac{1}{k^2}(\frac{k}{k+1}-\ln (k+1))<0
\end{equation*}
By fundamental theorem of calculus, 
\begin{equation*}
\frac{\ln (k+2)}{k+1}-\frac{\ln (k+1)}{k}=\int_{k}^{k+1} \frac{d}{dt} \frac{\ln (t+1)}{t}dt<0\text{ for large $k$ }
\end{equation*}
We now know 
\begin{equation*}
\frac{\ln (k+1)}{k}\text{ monotonically decrease for large $k$}
\end{equation*}
Notice that 
\begin{equation*}
\lim_{k\to\infty} \frac{\ln (k+1)}{k}\stackrel{\mathrm{H}}{=}\lim_{k\to\infty} \frac{\frac{1}{k+1}}{1}=0
\end{equation*}
We have proved $\frac{\ln (k+1)}{k}\searrow 0$ as $k\to\infty$. Then by Alternating Series Test, we know 
\begin{equation*}
\sum_{k=1}^\infty (-1)^k \frac{\ln (k+1)}{k}\text{ converges }
\end{equation*}
Notice 
\begin{equation*}
\int \frac{\ln k}{k}dk=\frac{(\ln k)^2}{2}
\end{equation*}
Then we have
\begin{equation*}
\int_{1}^\infty \frac{\ln k}{k}dk= \frac{(\ln k)^2}{2}\bigg|_{k=1}^\infty =\infty 
\end{equation*}
Notice that 
\begin{equation*}
\frac{\ln k}{k}\leq \frac{\ln (k+1)}{k}\text{ for large $k$ }
\end{equation*}
Then we know
\begin{equation*}
\int_1^\infty \frac{\ln (k+1)}{k}dk =\infty
\end{equation*}
By integral test, we now know 
\begin{equation*}
\sum_{k=1}^\infty \abso{(-1)^k \frac{\ln (k+1)}{k}}=\ln 2+\sum_{k=2}^\infty \frac{\ln (k+1)}{k}\text{ diverges }
\end{equation*}


\end{proof}

\begin{question}{}{}
Find all values of \( p \in \mathbb{R} \) that make the following series converge absolutely:
\[
\sum_{k=1}^{\infty} (-1)^k \frac{1}{\ln^p (k)}
\]
\end{question}
\begin{proof}
Suppose $p\leq 0$. Let $q=-p$. We have
 \begin{equation*}
\abso{(-1)^k \frac{1}{\ln^p(k)}}=\ln^q (k)\text{ where $q\geq 0$ }
\end{equation*}
Then we have
\begin{equation*}
\lim_{k\to\infty} \abso{(-1)^k \frac{1}{\ln^p (k)}}= \lim_{k\to\infty}\ln^q (k)\neq 0
\end{equation*}
This tell us if $p\leq 0$, the series do not converge.\\

Suppose $p>0$. We see 
 \begin{equation*}
\ln(x^{\frac{1}{p}})\leq  x^{\frac{1}{p}}\text{ for large $x$ }
\end{equation*}
Then for large $x$ we have
 \begin{equation*}
\frac{1}{p}\ln x\leq x^{\frac{1}{p}}
\end{equation*}
This give us
\begin{equation*}
\ln x\leq  px^{\frac{1}{p}}
\end{equation*}
This give us
\begin{equation*}
  (\ln x)^p\leq p^p x
\end{equation*}
This give us 
\begin{equation*}
\frac{1}{ (\ln x)^p}\geq \frac{1}{p^px}
\end{equation*}
Then because 
\begin{equation*}
\frac{1}{p^p}\sum_{x=1}^\infty \frac{1}{x}\text{ diverge }
\end{equation*}
By Comparison Test, we see 
\begin{equation*}
\sum_{x=1}^\infty \abso{(-1)^x \frac{1}{\ln^p (x)}}=\sum_{x=1}^\infty \frac{1}{(\ln x)^p}\text{ diverge }
\end{equation*}
In conclusion, for all $p$, the series diverge.
\end{proof}

\begin{question}{}{}
Let \( a_k \) and \( b_k \) be real sequences. Decide which of the following statements are true and which are false. Prove the true ones and give counterexamples to the false ones.

\begin{enumerate}
    \item If \( a_k \searrow 0 \) as \( k \to \infty \), and  $\sum_{k=1}^{\infty} b_k$ converges conditionally, then 
    \[
    \sum_{k=1}^{\infty} a_kb_k
    \]
    converges.
    
    \item If \( a_k \to 0 \) as \( k \to \infty \), then 
    \[
    \sum_{k=1}^{\infty} (-1)^k a_k
    \]
    converges.
    
    \item If \( a_k \to 0 \) as \( k \to \infty \), and \( a_k \geq 0 \) for all \( k \in \mathbb{N} \), then 
    \[
    \sum_{k=1}^{\infty} (-1)^k a_k
    \]
    converges.
    
    \item If \( a_k \to 0 \) as \( k \to \infty \), and 
    \[
    \sum_{k=1}^{\infty} (-1)^k a_k
    \]
    converges, then \( a_k \searrow 0 \) as \( k \to \infty \).
\end{enumerate}
\end{question}
\begin{proof}
From now, we let 
\begin{equation*}
A_n=\sum_{k=1}^n a_n\text{ and }B_n=\sum_{k=1}^n b_n
\end{equation*}
We claim
\begin{equation*}
  \vi{a_k\searrow 0\text{ as $k\to \infty$ and }\sum_{k=1}^\infty b_k\text{ converges }\implies \sum_{k=1}^\infty a_kb_k\text{ converges }}
\end{equation*}
Notice 
\begin{equation*}
\sum_{k=1}^\infty b_k\text{ converges }\implies \exists L\inr, \lim_{n\to\infty}B_n=L
\end{equation*}
Let $\epsilon =1$, we know there exists $N$ such that  
\begin{equation*}
\forall n>N, B_n<L+1
\end{equation*}
Then we see 
\begin{equation*}
\forall k, B_k<\max \set{B_1+1,B_2+1,\dots ,B_{N}+1,L+1}
\end{equation*}
Then by Dirichlet's Test, we are done. $\vdone$\\

Define
\begin{equation*}
a_k=(-1)^k \frac{1}{k}
\end{equation*}
We have
\begin{equation*}
\lim_{k\to\infty}a_k=0\text{ and }\sum_{k=1}^\infty (-1)^k a_k=\sum_{k=1}^\infty (-1)^{2k}\frac{1}{k}=\sum_{k=1}^\infty \frac{1}{k}\text{ diverges }
\end{equation*}
Define
\begin{equation*}
a_k=\begin{cases}
\frac{1}{\sqrt{\lceil \frac{k}{2} \rceil +1}-1}& \text{ if $k$ is odd }\\
\frac{1}{\sqrt{\lceil \frac{k}{2} \rceil +1}+1 }& \text{ if $k$ is even }
\end{cases}
\end{equation*}
The sequence we have is
\begin{equation*}
a_1=\frac{1}{\sqrt{2} -1},a_2=\frac{1}{\sqrt{2} +1},a_3=\frac{1}{\sqrt{3} -1},a_4=\frac{1}{\sqrt{3} +1},a_5=\frac{1}{\sqrt{4} -1},\dots
\end{equation*}
Because 
\begin{equation*}
\lim_{k\to\infty} \lceil \frac{k}{2}\rceil =\infty
\end{equation*}
We have
\begin{equation*}
\lim_{k\to\infty} a_k=0
\end{equation*}
Clearly, $\forall k,a_k\geq 0$. Define
\begin{equation*}
B_n:=\sum_{k=1}^n (-1)^k a_k
\end{equation*}
Notice
\begin{equation*}
B_{2n}=\sum_{k=1}^n \frac{-1}{\sqrt{k+1}-1}+\frac{1}{\sqrt{k+1} +1}=\sum_{k=1}^n \frac{-2}{k}
\end{equation*}
Then we have
\begin{equation*}
\lim_{n\to\infty}B_{2n}=\sum_{k=1}^n \frac{-2}{k}\text{ diverge }
\end{equation*}
This tell us
\begin{equation*}
\sum_{n=1}^\infty (-1)^n a_n=\lim_{n\to\infty}B_{n}\text{ diverge }
\end{equation*}
Otherwise the sub-sequence $\lim_{n\to\infty}B_{2n}$ would have converge.\\

Define
\begin{equation*}
a_n=\begin{cases}
 0& \text{ if $n$ is odd }\\
 \frac{1}{(\frac{n}{2})^2}& \text{ if $n$ is even }
\end{cases}
\end{equation*}
The form of this sequence is
\begin{equation*}
a_1=0,a_2=\frac{1}{1^2},a_3=0,a_4=\frac{1}{2^2},a_5=0,a_6=\frac{1}{3^2},a_7=0,\dots 
\end{equation*}
Clearly, $a_n\to 0$. We see
\begin{equation*}
\sum_{n=1}^\infty a_n=\sum_{n=0}^\infty a_{2n+1}+\sum_{n=1}^\infty a_{2n}=\sum_{n=0}^\infty 0+\sum_{n=1}^\infty \frac{1}{n^2}=\sum_{n=1}^\infty \frac{1}{n^2}\text{ converge }
\end{equation*}
Yet, for all even  $n$, we have
 \begin{equation*}
a_n=\frac{1}{(\frac{n}{2})^2}>0=a_{n+1}
\end{equation*}
So we don't have $a_n\searrow 0$ as  $n\to\infty$. This serve as a counter example for the last question.
\end{proof}
\section{HW5}

\begin{question}{}{}
1. Let \((X, d)\), \((Y, d)\) be metric spaces; let \(f : X \rightarrow Y\). Show that \(f\) is continuous on \(X\) if and
only if for every open set \(V\subseteq Y\), \(f^{-1}(V)\) is open.
\end{question}
\begin{proof}
See \myref{Theorem}{2.4.4}
\end{proof}

\begin{question}{}{}

2. Let \(X\) and \(Y\) be metric spaces; let \(f : X \rightarrow Y\). Show that the following statements are
equivalent:

\begin{enumerate}
    \item \(f\) is continuous.
    \item For every subset \(A\) of \(X\), one has \(f(A) \subseteq \overline{f(A)}\).
    \item For every closed set \(B\) of \(Y\), the set \(f^{-1}(B)\) is closed in \(X\).
\end{enumerate}
\end{question}
\begin{proof}
See \myref{Theorem}{2.4.4}
\end{proof}

\begin{question}{}{}
3. Suppose that \(E\) is a nonempty subset of \(\mathbb{R}\), let \(a \in E\), and that \(f : E \rightarrow \mathbb{R}\). If \(f\) has
the property that if \(\{x_n\} \subseteq E\) and \(x_n\) converges to \(a\), then \(f(x_n) \rightarrow f(a)\) as \(n \rightarrow \infty\). Is \(f\)
continuous at \(a\)? Justify your result.
\end{question}
\begin{proof}
We prove that $f$ is continuous at $a$. \As{ $f$ is not continuous at  $a$}. Then we know $a$ is an limit point of $E$ and 
 \begin{equation*}
\lim_{x\to a}f(x)\neq f(a)
\end{equation*}
Then there exists a sequence $\set{x_n}\subseteq E$ that converge to $a$ and satisfy
 \begin{equation*}
\forall n,a\neq x_n\text{ and }\lim_{n\to\infty}f(x_n)\neq f(a)\tCaC\text{ to premise }
\end{equation*}
\end{proof}

\begin{question}{}{}
4. Let \(f : M \rightarrow N\) be a map from metric space \(M\) to another metric space \(N\) with the property
that if a sequence \(\{p_n\}\) in \(M\) converges, then the sequence \(\{f(p_n)\}\) in \(N\) also converges. Is
\(f\) continuous? Justify your result.
\end{question}
\begin{proof}
  We prove $f$ is continuous. \As{$f$ is not continuous at  $p$}. Then we know there exists a sequence $\set{p_n}\subseteq M$ such that 
  \begin{equation*}
  \lim_{n\to\infty}p_n=p \text{ and } \forall n\inn, p_n\neq p
  \end{equation*}
From premise, we know 
\begin{equation*}
\lim_{n\to\infty} f(p_n)\text{ exists }
\end{equation*}
Because $f$ is not continuous at  $p$, we know 
 \begin{equation*}
\lim_{n\to\infty}f(p_n)\neq f(p)
\end{equation*}
Consider the sequence 
\begin{equation*}
q_{n}=\begin{cases}
p_n& \text{ if $n$ is odd }\\
p & \text{ if $n$ is even }
\end{cases}
\end{equation*}
$\set{q_n}$ clearly is a sequence that converge to $p$. By premise, we should have 
\begin{equation*}
\lim_{n\to\infty}f(q_n)\text{ exists }
\end{equation*}
Yet, we see 
\begin{equation*}
\lim_{n\to\infty}f(q_{2n})=f(p)\neq \lim_{n\to\infty}f(p_n)=\lim_{n\to\infty}f(q_{2n+1})\tCaC
\end{equation*}
\end{proof}
\begin{question}{}{}
5. Suppose \(f\) is a real function defined on \(\mathbb{R}\) which satisfies
\[
\lim_{h \rightarrow 0} [f(x + h) - f(x - h)] = 0
\]
for every \(x \in \mathbb{R}\). Does this imply that \(f\) is continuous?

\end{question}
\begin{proof}
No. Define 
\begin{equation*}
x\mapsto \begin{cases}
  0& \text{ if $x\neq 0$ }\\
  1& \text{ if $x=0$ }
\end{cases}
\end{equation*}
Clearly $f$ is not continuous at $0$. We now prove  $f$ satisfy the premise.\\

Let $y\neq 0$. We see  
\begin{equation*}
\abso{h}\leq \abso{y}\implies f(y+h)-f(y-h)=0-0=0\in B_\epsilon (0)
\end{equation*}
This shows 
\begin{equation*}
\lim_{h\to 0}[f(y+h)-f(y-h)]=0
\end{equation*}
Let $y=0$. We see  
\begin{equation*}
0<h\implies f(y+h)-f(y-h)=1-1=0\in B_\epsilon (0)
\end{equation*}
\end{proof}
\begin{question}{}{}
6. Let \( f: \mathbb{R} \rightarrow \mathbb{R} \) and \( g : I (\subseteq \mathbb{R}) \rightarrow \mathbb{R} \) be a map. Please state the definition of following nouns.
(There is no standard answer, you can write down all equivalent statements as possible as you can)
\begin{enumerate}
    \item \( f \) is continuous on \( \mathbb{R} \).
    \item \( f \) is uniformly continuous on \( \mathbb{R} \).
    \item \( f \) is Lipschitz continuous on \( \mathbb{R} \).
    \item \( g \) is absolutely continuous on \( I \).
\end{enumerate}
\end{question}
\begin{proof}
For continuous, see \myref{Definition}{3.11.7}, \myref{Definition}{3.11.6} and  \myref{Definition}{3.11.1}.\\

For uniform continuous, see \myref{Definition}{5.5.1}.\\

For Absolute continuous, see \myref{Definition}{5.5.1}.\\

For Lipshchitz continuous, see \myref{Definition}{5.5.4}
\end{proof}
\begin{question}{}{}
7. Let \( f : \mathbb{R} \rightarrow \mathbb{R} \) and \( g : I (\subseteq \mathbb{R}) \rightarrow \mathbb{R} \) be a map. Prove or disprove following statement.
\begin{itemize}
    \item[(i)] if \( f \) is continuous on \( \mathbb{R} \), then \( f \) is uniformly continuous on \( \mathbb{R} \).
    \item[(ii)] if \( f \) is uniformly continuous on \( \mathbb{R} \), then \( f \) is continuous on \( \mathbb{R} \).
    \item[(iii)] if \( g \) is continuous on \( I \) and \( I \) is compact, then \( g \) is uniformly continuous on \( I \).
    \item[(iv)] if \( f \) is continuous on \( \mathbb{R} \), then \( f \) is Lipschitz continuous on \( \mathbb{R} \).
    \item[(v)] if \( f \) is Lipschitz continuous on \( \mathbb{R} \), then \( f \) is continuous on \( \mathbb{R} \).
    \item[(vi)] if \( g \) is Lipschitz continuous on \( I \), then \( g \) is uniformly continuous on \( I \).
    \item[(vii)] if \( g \) is uniformly continuous on \( I \), then \( g \) is Lipschitz continuous on \( I \).
    \item[(viii)] if \( g \) is absolutely continuous on \( I \), then \( g \) is uniformly continuous on \( I \).
    \item[(ix)] if \( g \) is uniformly continuous on \( I \), then \( g \) is absolutely continuous on \( I \).
    \item[(x)] if \( g \) is absolutely continuous on \( I \), then \( g \) is Lipschitz continuous on \( I \).
    \item[(xi)] if \( g \) is Lipschitz continuous on \( I \), then \( g \) is absolutely continuous on \( I \).
\end{itemize}
\end{question}
\begin{proof}
\begin{enumerate}[label=(\roman*)]
  \item No, use $x^2$ on non-compact set. See \myref{Theorem}{5.5.4}.
  \item Yes, see \myref{Theorem}{5.5.3}.
  \item Yes. This is Heine-Cantor Theorem. See \myref{Theorem}{5.5.1}.
  \item No, use $\sqrt{x} $. See \myref{Theorem}{5.5.8}, and notice that absolutely continuous is stronger than continuous. This fact is proved by \myref{Theorem}{5.5.2} and \myref{Theorem}{5.5.3}. 
  \item Yes. See \myref{Theorem}{5.5.5}, and \myref{Theorem}{5.5.3}
  \item Yes. See \myref{Theorem}{5.5.5}.
  \item No. Use $\sqrt{x}$. See \myref{Theorem}{5.5.8} and notice that absolutely continuous is stronger than uniformly continuous. See \myref{Theorem}{5.5.2}.
  \item Yes. See \myref{Theorem}{5.5.2}.
  \item No. See \myref{Theorem}{5.5.3}.
  \item No. Use $\sqrt{x} $. See \myref{Theorem}{5.5.8}
  \item Yes. See \myref{Theorem}{5.5.6}
\end{enumerate}
\end{proof}
\begin{question}{}{}
8. Suppose that \( f : [0, \infty) \rightarrow \mathbb{R} \) is continuous and there is an \( L \in \mathbb{R} \) such that \( f(x) \rightarrow L \) as \( x \rightarrow \infty \). Prove that \( f \) is uniformly continuous on \( [0, \infty) \).
\end{question}
\begin{proof}
Fix $\epsilon $. 
Because $f(x)\to L$ as $x\to \infty$, we know there exists $R\inr^+$ such that 
\begin{equation*}
\forall x>R, \abso{f(x)-L}<\frac{\epsilon}{2}
\end{equation*}
Also, because $[0,R+1]$ is compact and $f$ is continuous, we know $f$ is uniformly continuous on  $[0,R+1]$, then we know there exists $\delta'$ such that 
\begin{equation*}
\forall y',z'\in [0,R+1], \abso{y'-z'}<\delta'\implies \abso{f(y')-f(z')}<\epsilon 
\end{equation*}
We now prove 
\begin{equation*}
\delta:=\min \set{\delta',1}\text{ works }
\end{equation*}
WOLG, let $z+\delta>y>z\geq 0$. If $z<R$, then  $y<z+\delta<R+\delta\leq R+1$. This tell us $z,y\in [0,R+1]$. Then because $y-z<\delta<\delta'$, we have $\abso{f(y')-f(z')}<\epsilon $. If $z\geq R$. We have $y>z>R$. Then we have 
 \begin{equation*}
\abso{f(y)-f(z)}\leq \abso{f(y)-L}+\abso{f(z)-L}<\epsilon 
\end{equation*}
\end{proof}
\begin{question}{}{}

9. Show that if \( f \) is monotone on an interval \( I \), then \( f \) has at most countably many points of discontinuity on \( I \).
\end{question}
\begin{proof}
See \myref{Theorem}{3.11.17}.
\end{proof}
\section{HW6}
\begin{question}{}{}
1. Let \( f(x, y) = \frac{x^2y}{x^4 + y^2} \) for \( (x, y) \neq (0, 0) \), prove that \( \lim_{(x, y) \to (0,0)} f(x, y) \) does NOT exist.
(Hint: Choose \( a_n = (0, \frac{1}{n}) \) and \( b_n = (\frac{1}{n}, \frac{1}{n^2}) \), evaluate \( \lim_{n \to \infty} f(a_n) \) and \( \lim_{n \to \infty} f(b_n) \), and conclude by sequential characterization of limit.)
\end{question}
\begin{proof}
Let 
\begin{enumerate}[label=(\alph*)]
  \item $a_n=(0,\frac{1}{n})$ 
  \item $b_n=(\frac{1}{n},\frac{1}{n^2})$
\end{enumerate}
It is clear that 
\begin{enumerate}[label=(\alph*)]
  \item $a_n\to 0$ 
  \item  $b_n\to 0$
  \item $f(a_n)=0$ 
  \item $f(b_n)=\frac{1}{2}$
\end{enumerate}
This conclude $\lim_{(x,t)\to (0,0)}f(x,y)$ D.N.E.
\end{proof}
\begin{question}{}{}
2. Let \( f(x, y) = \frac{x^3 - x^2y + y^3}{x^2 + y^2} \) for \( (x, y) \neq (0, 0) \). Use \( \varepsilon - \delta \) definition of continuity to prove that \( \lim_{(x, y) \to (0,0)} f(x, y) = 0 \).
\end{question}
\begin{proof}
\begin{align*}
  \abso{f(x,y)}&\leq \abso{\frac{x^3}{x^2+y^2}} +\abso{\frac{x^2y}{x^2+y^2}}+\abso{\frac{y^3}{x^2+y^2}}\\
  &\leq \abso{\frac{x^3}{x^2}}+ \abso{\frac{x^2y}{x^2}}+\abso{\frac{y^3}{y^2}}(\because x^2+y^2\geq x^2\text{ and }x^2+y^2\geq y^2\\
  &=\abso{x}+2\abso{y}
\end{align*}
Let $\delta=\frac{\epsilon ^2}{9}$. We now see 
\begin{align*}
x^2+y^2<\delta \implies x^2<\delta \implies \abso{x}<\sqrt{\delta}=\frac{\epsilon}{3}
\end{align*}
And also, $\abso{y}<\frac{\epsilon}{3}$. Then we have  
\begin{align*}
\abso{f(x,y)}\leq \abso{x}+2\abso{y}<\epsilon 
\end{align*}
\end{proof}
\begin{question}{}{}
3. (a) (Converse statement of extreme value theorem)
If \( f : [a, b] \to \mathbb{R} \) admits its maximum and minimum, does this imply \( f \) continuous? Justify your result.

(b) (Converse statement of intermediate value theorem)
Let \( f : [a, b] \to \mathbb{R} \) with \( f(a) < f(b) \), if for all \( L \) in \( (f(a), f(b)) \), there always exists some \( c \) in \( (a, b) \) such that \( f(c) = L \), does this imply \( f \) continuous? Justify your result.
\end{question}
\begin{proof}
  For $(a)$, No. We see 
  \begin{align*}
  f(x)=\begin{cases}
    2& \text{ if $x=b$ }\\
    1& \text{ if $x\in (a,b)$ }\\
    0& \text{ if $x=a$ }
  \end{cases}
  \end{align*}
  has maximum $f(b)=2$ and minimum $f(a)=0$, yet it is clearly not continuous at $a$ and  $b$.\\

For $(b)$, No. We see 
\begin{align*}
f(x)=\begin{cases}
  2& \text{ if  }x=3\\
  1& \text{ if  }x\in (2,3)\\
  2(x-1)& \text{ if $x\in [1,2]$ }\\
  1& \text{ if  }x\in (0,1)\\
  0& \text{ if  }x=0
\end{cases}
\end{align*}
We have 
\begin{align*}
f(a)=0<f(b)=2
\end{align*}
and for all $L\in (0,2)=(f(a),f(b))$ 
\begin{align*}
x=\frac{L}{2}+1\implies x\in (1,2)\implies f(x)=L
\end{align*}
That is $f(\frac{L}{2}+1)=L$. Notice that $f$ has jump discontinuous at  $\set{0,1,2,3}$ and we are done.
\end{proof}
\begin{question}{}{}
4. Let \( f: [a, b] \rightarrow \mathbb{R} \) satisfy  
\begin{enumerate}[label=(\alph*)]
  \item $\forall (c_1,c_2)\subseteq [a,b], \forall L \in [f(c_1),f(c_2)],\exists  c\in (c_1,c_2),f(c)=L$
  \item $\forall r\inq, f^{-1}[r]$ is closed
\end{enumerate}
Prove $f$ is continuous.\\

(Hint: If not, there exists a sequence \( \{x_n\}_{n\in\mathbb{N}} \rightarrow x_0 \) but \( f(x_n) \) doesn't converge to \( f(x_0) \), which means for large \( n \), there's still a gap between \( f(x_n) \) and \( f(x_0) \). Can you relate this with \( \mathbb{Q} \)?)
\end{question}
\begin{proof}
  \As{$f$ has a discontinuity at $x$}. Then there exists a sequence $x_n\to x$ such that $f(x_n)\not \to f(x)$. Because $f(x_n)\not \to f(x)$, we know there exists a sub-sequence $x_{n_k}\to x$ such that $f(x_{n_k})\to y \neq f(x)$. Notice that it is possible $y=\pm \infty$, which won't damage the validity of the proof. WOLG, suppose $y>f(x)$. Let $N$ satisfy 
   \begin{align*}
  \forall k>N, f(x_{n_k})>\frac{y+f(x)}{2}
  \end{align*}
Because $\Q$ is dense in $\R$, we can let $L$ satisfy 
\begin{align*}
L\in (f(x),\frac{f(x)+y}{2})\cap \Q
\end{align*}
For each $k>N$, because $f(x_{n_k})>\frac{y+f(x)}{2}$, we see 
\begin{align*}
L\in (f(x),f(x_{n_k}))
\end{align*}
By premise $(a)$, we know there exists $y_{n_k}$ between $x$ and  $x_{n_k}$ such that $f(y_{n_k})=L$. Notice that because $y_{n_k}$ is bounded by $x_{n_k}$ and $x$, where  $x_{n_k}\to x$, we know $y_{n_k}\to x$. Notice that $y_{n_k}\in f^{-1}[L]$ and that $x\not \in f^{-1}[L]\tCaC$ to that $f^{-1}[L]$ is closed. 
\end{proof}
\begin{question}{}{}
5. Let \( (X, d) \) be a metric space and \( f : X \to \mathbb{R} \) be a continuous function. Show that \( Z(f) = \{x \in X \mid f(x) = 0\} \) is closed. Please state at least two different methods.
\end{question}
\begin{proof}
\textbf{(Method 1)}
\As{$Z(f)$ is not closed}. Then there exists a convergent sequence $x_n\rightarrow x$ in $Z(f)$ such that $x\not \in Z(f)$. Because $x\not\in Z(f)$, we see $f(x)\neq 0$. Notice that  $x_n$ is a sequence in  $Z(f)$, so we have 
\begin{align*}
\forall n,f(x_n)=0
\end{align*}
Then 
\begin{align*}
x_n\rightarrow x\text{ and }f(x_n)\rightarrow 0\neq f(x)\tCaC \text{ to sequential definition of continuous }
\end{align*}
\textbf{(Method 2)} Because $\set{0}$ is closed, we see the inverse image  $Z(f)=\set{x\in X:f(x)=0}=f^{-1}[\set{0}]$ must be closed.
\end{proof}

\begin{question}{}{}

6. Let \( (X, d_X) \) and \( (Y, d_Y) \) be metric spaces and \( f : X \to Y \). Show that if \( f \) is continuous and \( f = 0 \) on some dense subset \( A \) of \( X \), then \( f \equiv 0 \) on \( X \).
\end{question}
\begin{proof}
Let $x\in X\setminus A$. We wish to prove 
\begin{align*}
\vi{f(x)=0}
\end{align*}
Because $A$ is dense in  $X$, we know there exists a sequence  $a_n\to x$ in $A$ that converge to $x$.  Because $f$ is continuous, we now have 
 \begin{align*}
f(x)=\lim_{n\to\infty}f(a_n)=0(\because \forall n,f(a_n)=0 )\vdone
\end{align*}
\end{proof}
\begin{question}{}{}

7. (a) Define \( f(x) = \begin{cases} 
\frac{1}{n} & \text{if } x = \frac{m}{n} \in \mathbb{Q} \\
0 & \text{if } x \in \mathbb{R} \setminus \mathbb{Q} 
\end{cases} \)
Determine whether \( f \) is continuous at each irrational point. Justify your result.

(b) Define \( f(x) = \begin{cases} 
\frac{1}{n} & \text{if } x = \frac{m}{2^n} \in \mathbb{Q} \cap [0, 1] \text{ for some } m, n \in \mathbb{N} \\
0 & \text{if } x \in [0, 1] \setminus \mathbb{Q} 
\end{cases} \)
Determine whether \( f \) is continuous at each irrational point. Justify your result.

\end{question}
\begin{proof}
  We prove \vi{$f$ in  $(a)$ is continuous at all irrational point}. Let $x_n \to x \in \R\setminus \Q$. We wish to prove $f(x_n) \to f(x)$. Because $f$ is  $0$ at all irrational, WOLG, we can suppose $x_n$ are all dyadic rationals. Fix $\epsilon $, we wish to find $N$ such that 
   \begin{align*}
  \forall n>N, f(x_n)<\epsilon 
  \end{align*}
Let $m$ be the smallest natural such that  $\epsilon >\frac{1}{m}$. For each $k\leq m$, let $\frac{q_k}{k}$ satisfy 
\begin{align*}
x\in (\frac{q_k}{k},\frac{q_k+1}{k})
\end{align*}
Define 
\begin{align*}
  \alpha = \min \big(\set{\abso{x-\frac{q_k}{k}}\inr:\forall k\leq m} \cup \set{\abso{x-\frac{q_k+1}{k}}\inr:\forall k\leq m}\big) 
\end{align*}
Let $N$ satisfy 
 \begin{align*}
\forall n>N, x_n\in (x-\alpha ,x+\alpha )
\end{align*}
We prove such $N$ works. Let $n>N$. We wish to prove 
\begin{align*}
f(x_n)<\epsilon 
\end{align*}
Because $x_n$ is rational, we can express $x_n$ in the form 
\begin{align*}
x_n=\frac{q}{p}\text{ where gcd$(q,p)=1$ and $p\inn $} 
\end{align*}
\As{$p\leq m$}. Notice that $x\in (\frac{q_p}{p},\frac{q_p+1}{p})$ and $\alpha \leq \min \set{d(x,\frac{q_p}{p}),d(x,\frac{q_p+1}{p})}$, this tell us 
\begin{align*}
x+\alpha \leq \frac{q_p+1}{p}\text{ and }x-\alpha \geq \frac{q_p}{p}
\end{align*}
Then we see 
\begin{align*}
x_n\in (x-\alpha ,x+\alpha )\subseteq (\frac{q_p}{p},\frac{q_p+1}{p})
\end{align*}
Which is impossible. \CaC\\

We now have $p>m$. Then we see 
 \begin{align*}
f(x_n)=\frac{1}{p}<\frac{1}{m}<\epsilon \vdone
\end{align*}
We now prove \blue{$f$ in  $(b)$ is continuous at all irrationals}. Let $x_n\to x\inr\setminus \Q$. We wish to prove $f(x_n)\to 0$. Because $f$ is  $0$ at all points that isn't dyadic irrationals, WOLG, we can suppose  $x_n$ are all dyadic rationals. Fix  $\epsilon $, we wish to find $N$ such that 
 \begin{align*}
\forall n>N, f(x_n)<\epsilon 
\end{align*}
Let $m$ be the smallest naturals such that 
 \begin{align*}
\frac{1}{m}<\epsilon 
\end{align*}
For each $k\leq m$, let $\frac{q_k}{2^k}$ satisfy 
\begin{align*}
x\in (\frac{q_k}{2^k},\frac{q_k+1}{2^k})
\end{align*}
Define 
\begin{align*}
\alpha = \min \bigg( \set{d(x,\frac{q_k}{2^k})\inr:k\leq m } \cup \set{d(x,\frac{q_k+1}{2^k})\inr:k\leq m}\bigg)
\end{align*}
Because $x_n\to x$, we know there exists $N$ such that 
 \begin{align*}
n>N\implies d(x_n,x)<\alpha 
\end{align*}
We prove such $N$ works. Let $n>N$. We wish to prove 
 \begin{align*}
f(x_n)<\epsilon 
\end{align*}
Because $x_n$ is dyadic rational, we can express $x_n$ in the form of 
 \begin{align*}
x_n=\frac{q}{2^p}\text{ where $q$ is odd }
\end{align*}
\As{$p\leq m$}. Notice that $x\in (\frac{q_p}{2^p},\frac{q_p+1}{2^p})$ and $\alpha \leq \min \set{d(x,\frac{q_p}{2^p}),d(x,\frac{q_p+1}{2^p})}$. This tell us 
\begin{align*}
x_n\in (x-\alpha ,x+\alpha )\subseteq (\frac{q_p}{2^p},\frac{q_p+1}{2^p})
\end{align*}
which is impossible. \CaC\\

We now have $p>m$. Then we see 
 \begin{align*}
f(x_n)=\frac{1}{p}<\frac{1}{m}<\epsilon \bdone
\end{align*}





\end{proof}
\begin{question}{}{}

8. Let \( f : \mathbb{R} \to \mathbb{R} \) be a continuous function. If for all open subset \( U \) in \( \mathbb{R} \), \( f(U) \) is open in \( \mathbb{R} \), prove that \( f \) is monotone.
(Hint: Observe \( f(x) = x^2 \), which is not monotone on \( \mathbb{R} \), does \( f((-1, 1)) \) open in \( \mathbb{R} \)? You may use extreme value theorem to help you complete the whole proof.)
\end{question}
\begin{proof}
\As{$f$ is not monotone}. WOLG, suppose $f$ has a local maximum at $x$
\begin{align*}
\forall y\in (x-\epsilon ,x+\epsilon ),f(y)\leq f(x)
\end{align*}
Because $f$ is continuous, we know 
 \begin{align*}
f[(a,b)]\text{ is an interval }
\end{align*}
By the premise that $f$ is an open mapping, we then can express 
 \begin{align*}
f[(a,b)]=(c,d)\tCaC x \text{ is a local maximum }
\end{align*}
since $(c,d)$ has no maximum.
\end{proof}
\begin{question}{}{}
9. We say \( f : (a, b) \to \mathbb{R} \) is "convex" if \( \forall x, y \in (a, b), \forall \lambda \in [0, 1] \),
\( f(\lambda x + (1 - \lambda)y) \leq \lambda f(x) + (1 - \lambda)f(y) \)

(a) For \( a < s < t < u < b \), show that
\[
\frac{f(t) - f(s)}{t - s} \leq \frac{f(u) - f(s)}{u - s} \leq \frac{f(u) - f(t)}{u - t}
\]

(b) Prove that every convex function is continuous.
(Hint: Use the result in (a): fix \( u \), as \( t \to s^+ \), what happens on \( f(t) - f(s) \)?)
\end{question}
\begin{proof}
See the proof of \myref{Lemma}{5.2.8} and \myref{Theorem}{5.2.9}
\end{proof}
\begin{question}{}{}

10. If \( f : (a, b) \to \mathbb{R} \) is continuous and \( \forall x, y \in (a, b) \),
\[
f \left( \frac{x + y}{2} \right) \leq \frac{f(x) + f(y)}{2}
\]
Prove that \( f \) is convex.
(Hint: for \( \lambda \in [0, 1] \), write \( \lambda = \sum_{n=0}^{\infty} \frac{a_n}{2^n} \), where \( a_n = 0 \) or \( 1 \) for all \( n \).)
\end{question}
\begin{proof}
See \myref{Theorem}{5.2.9} and \myref{Theorem}{5.2.7}. 
\end{proof}
\section{HW7}
\begin{question}{}{}
\text{Let } f \text{ be a real-valued function defined on an open interval } E \text{ such that}
\[
|f(x) - f(y)| \leq |x - y|^\alpha, \forall x, y \in E
\]
\text{Then show that (a) } f \text{ is continuous on } E, \text{ if } \alpha > 0 \text{ (b) } f \text{ is constant on } E, \text{ if } \alpha > 1.

\end{question}
\begin{proof}
Suppose $\alpha >0$. Let $\delta=\epsilon ^{\frac{1}{\alpha }}$. We see 
\begin{align*}
\abso{x-y}\leq \delta=\epsilon^{\frac{1}{\alpha }}\implies \abso{f(x)-f(y)}\leq \abso{x-y}^{\alpha }\leq (\epsilon ^{\frac{1}{\alpha }})^{\alpha }=\epsilon 
\end{align*}
We have proved $f$ is uniformly continuous on $E$, which implies $f$ is continuous on $E$.\\

Suppose $\alpha >1$. Arbitrarily pick $x\in E$. We only wish to prove 
\begin{align*}
  \vi{f'(x)=0}
\end{align*}
Observe 
\begin{align*}
\forall y\in E, \abso{\frac{f(x)-f(y)}{x-y}}\leq \abso{x-y}^{\alpha -1} 
\end{align*}
Then because $\alpha -1>0$, we have
\begin{align*}
\abso{x-y}^{\alpha -1}\to 0\text{ as $y\to x$ }
\end{align*}
This give us 
\begin{align*}
f'(x)=\lim_{y \to x}\frac{f(x)-f(y)}{x-y}=0 \vdone
\end{align*}
Now, by Mean Value Theorem, we see $f$ must stay constant on  $E$. 

\end{proof}
\begin{question}{}{}

Let \( f \) be a differentiable real function defined in \( (a,b) \). Prove that \( f \) is convex if and only if \( f' \) is monotonically increasing. Assume next that \( f''(x) \) exists for every \( x \in (a,b) \), and prove that \( f \) is convex if and only if \( f''(x) \geq 0 \) for \( x \in (a,b) \).
\end{question}
\begin{proof}
We first prove 
\begin{align*}
\vi{f \text{ is convex }\implies f'\text{ increase }}
\end{align*}
Let $(c,d)\subseteq (a,b)$. We wish to prove 
\begin{align*}
\vi{f'(c)\leq f'(d)}
\end{align*}
Let $x\in (c,d)$, by \myref{Lemma}{5.2.8}, we see 
\begin{align*}
\frac{f(x)-f(c)}{x-c}\leq \frac{f(d)-f(c)}{d-c}
\end{align*}
Taking limit $x \to c$, we see 
\begin{align*}
f'(c)=\lim_{x\to c}\frac{f(x)-f(c)}{x-c}\leq \frac{f(d)-f(c)}{d-c}
\end{align*}
Again by \myref{Lemma}{5.2.8}, we see 
\begin{align*}
\frac{f(d)-f(c)}{d-c}\leq \frac{f(d)-f(x)}{d-x}
\end{align*}
Again taking limit $x \to d$, we see 
\begin{align*}
f'(d)=\lim_{x\to d}\frac{f(d)-f(x)}{d-x}\geq \frac{f(d)-f(c)}{d-c}
\end{align*}
We now have 
\begin{align*}
f'(c)\leq \frac{f(d)-f(c)}{d-c}\leq f'(d)\vdone
\end{align*}
We now prove 
\begin{align*}
\blue{f' \text{ increase }\implies f\text{ is convex }}
\end{align*}
Fix $(x_1,x_2)\subseteq (a,b),\ld \in [0,1],x_\ld =\ld x_1+(1-\ld )x_2$. We only wish to prove 
\begin{align*}
f(x_{\ld })\leq \ld f(x_1)+(1-\ld )f(x_2)
\end{align*}
By Mean Value Theorem, we know there exists $c_1\in (x_1,x_\ld ),c_2\in (x_\ld ,x_2)$ such that 
\begin{align*}
f'(c_1)&=\frac{f(x_\ld )-f(x_1)}{x_\ld -x_1}\\
f'(c_2)&=\frac{f(x_2)-f(x_\ld )}{x_2-x_\ld }
\end{align*}
Because $c_1<x_\ld <c_2$ and $f'$ is increasing, we have 
 \begin{align*}
\frac{f(x_\ld )-f(x_1)}{x_\ld -x_1}=f'(c_1)\leq f'(c_2)=\frac{f(x_2)-f(x_\ld )}{x_2 -x_\ld }
\end{align*}
Acknowledging $x_1<x_\ld <x_2$, we now have 
\begin{align*}
  (x_2-x_\ld )[f(x_\ld )-f(x_1)]\leq (x_\ld -x_1)[f(x_2)-f(x_\ld )]
\end{align*}
Substituting $x_\ld $ with $\ld x_1+(1-\ld )x_2$, we now can deduce
\begin{align*}
  &\ld  (x_2-x_1)[f(x_\ld )-f(x_1)]\leq (1-\ld )(x_2-x_1)[f(x_2)-f(x_\ld )]\\
  \implies & \ld [f(x_\ld )-f(x_1)]\leq (1-\ld )[f(x_2)-f(x_\ld )]\\
  \implies & f(x_\ld )\leq \ld f(x_1)+(1-\ld )f(x_2)\bdone
\end{align*}
Following the result above and given the hypothesis, we now prove 
\begin{align*}
f'\text{ increase }\implies \forall x\in (a,b),f''(x)\geq 0
\end{align*}
Because $f'$ increase, we can deduce
\begin{align*}
f''(x)=\lim_{t \to x^{-}}\frac{f(x)-f(t)}{x-t}\geq 0\bdone
\end{align*}
We now prove 
\begin{align*}
\forall x\in (a,b),f''(x)\geq 0\implies f'\text{ increase }
\end{align*}
\As{$\exists (x_1,x_2)\subseteq (a,b),f'(x_1)>f'(x_2)$}. By Mean Value Theorem, we know there exists $c\in (x_1,x_2)$ such that 
\begin{align*}
f''(c)=\frac{f'(x_2)-f'(x_1)}{x_2-x_1}<0\tCaC
\end{align*}



\end{proof}
\begin{question}{}{}

Let \( f \) be a bounded real-valued function defined on \( (a, \infty) \) for some \( a \in \mathbb{R} \) such that \( f' \) and \( f'' \) exist on \( (a, \infty) \). Further, if \( M_0, M_1, M_2 \) are the least upper bounds of \( \abso{f}, \abso{f'}, \abso{f''} \), on \( (a, \infty) \), respectively. Prove that \( M_1^2 \leq 4M_0M_2 \).
\end{question}
\begin{proof}
Arbitrarily pick $x\in (a,\infty)$ and let $h\inr^+$. By \myref{Theorem}{6.5.1} (Taylors' Theorem) and \myref{Corollary}{6.5.3} (Lagarange Form of Remainder), expanding $f$ at $x$ and approximate $f(x+2h)$, we have 
\begin{enumerate}[label=(\alph*)]
  \item $f(x+2h)=P_1(x+2h)+R_1(x+2h)$ 
  \item $P_1(x+2h)=f(x)+2hf'(x)$ 
  \item $\exists \xi \in (x,x+2h), R_1(x+2h)=2h^2f''(\xi)$
\end{enumerate}
Express $(a)$ in the form 
\begin{align*}
f(x+2h)=f(x)+2hf'(x)+2h^2f''(\xi)
\end{align*}
This give us 
\begin{align*}
f'(x)=\frac{f(x+2h)-f(x)}{2h}+hf''(\xi)
\end{align*}
Then we can deduce 
\begin{align*}
\abso{f'(x)}&\leq \frac{\abso{f(x+2h)}+\abso{f(x)}}{2h}+ h \abso{f''(\xi)}\\
&=\frac{M_0}{h}+hM_2
\end{align*}
Then we have 
\begin{align*}
\abso{f'(x)}^2\leq \frac{M_0^2}{h^2}+h^2M_2^2+2M_0M_2
\end{align*}
We can deduce 
\begin{align*}
\abso{f'(x)}^2-4M_0M_2\leq (\frac{M_0}{h}-hM_2)^2
\end{align*}
Notice that if $M_2=0$, because $f$ is bounded, we must have  $M_1=0$ and  $f$ is constant, then the proof become trivial. We only have to consider when $M_2>0$. Let $h=\sqrt{\frac{M_0}{M_2}} $. We see 
\begin{align*}
\abso{f'(x)}^2-4M_0M_2\leq (\frac{M_0}{h}-hM_2)^2=0
\end{align*}
This give us 
\begin{align*}
\abso{f'(x)}^2\leq 4M_0M_2
\end{align*}
Because $x$ is arbitrarily picked from  $(a,\infty)$, we now have 
\begin{align*}
\forall x\in (a,\infty), \abso{f'(x)}\leq 2\sqrt{M_0M_2} 
\end{align*}
Then by the definition of least upper bound $M_1$
 \begin{align*}
M_1\leq 2\sqrt{M_0M_2} 
\end{align*}
The result follows from squaring both side.

\end{proof}
\begin{question}{}{}
Let \( f \) be a real-valued function defined on \([-1,1]\) such that \( f^{(n)} \) exists on \([-1,1]\) for \( n = 1,2,3 \), and
\begin{align*}
f(-1) &= 0, \\
f(0) &= 0, \\
f(1) &= 1, \\
f'(0) &= 0.
\end{align*}
Prove that \( f^{(3)}(x) \geq 3 \) for some \( x \in (-1,1) \).
\end{question}
\begin{proof}
Taylor expanding $f$ at  $0$ to polynomial of degree $2$ and express the remainder in Lagarange form, we have 
\begin{enumerate}[label=(\alph*)]
  \item $\exists b\in (0,1), f(1)=f(0)+f'(0)+\frac{f''(0)}{2}+\frac{f'''(b)}{6}$
  \item $\exists a\in (-1,0),f(-1)=f(0)-f'(0)+\frac{f''(0)}{2}-\frac{f'''(a)}{6}$
\end{enumerate}
Plugging the value given and minus the two equations, we have 
\begin{align*}
1=\frac{f'''(b)+f'''(a)}{6}
\end{align*}
This give us 
\begin{align*}
f'''(a)+f'''(b)=6
\end{align*}
Then if $f'''(a)\geq 3$, we get the solution $a$. If  $f'''(a)<3$, we have $f'''(b)>3$, which implies $b$ is a solution.
\end{proof}
\begin{question}{}{}
\begin{enumerate}
\item Let \( f(x) = \exp\left(-\frac{1}{|x|}\right), x \neq 0 \) and \( f(0) = 0 \). Then calculate \( f^{(n)}(0) \) by definition of derivative \( \forall n \in \mathbb{N} \).
\item Show that \( f(x) \) can NOT have its Taylor expansion at \( x = 0 \) (or, say the expand radius of \( f(x) \) at \( x = 0 \) is zero).
\end{enumerate}
\end{question}
\begin{proof}
We show that 
\begin{align*}
\vi{\forall n\inn \cup \set{0},f^{(n)}(x)=\begin{cases}
 g_n(x)e^{\frac{-1}{x}}& \text{ if $x>0$ }\\
 0& \text{ if $x=0$ }\\
 p_n(x)e^{\frac{1}{x}}& \text{ if $x<0$ }
\end{cases}\text{ where $\forall n\inn\cup \set{0},g_n,p_n$ are polynomials }}
\end{align*}
by induction.\\

Base Case: 
\begin{align*}
g_0(x)=1=p_0(x)
\end{align*}
Induction Case: Suppose
\begin{align*}
f^{(k)}(x)=\begin{cases}
  g_k(x)e^{\frac{-1}{x}}& \text{ if $x>0$ }\\
  0& \text{ if $x=0$ }\\
  p_k(x)e^{\frac{1}{x}}& \text{ if $x<0$ }
\end{cases}\text{ where $g_k,p_k$ are polynomial }
\end{align*}
Compute
\begin{align*}
f^{(k+1)}(x)=\begin{cases}
  g_k'(x)e^{-\frac{1}{x}}+(x^{-2})g'_k(x)e^{\frac{-1}{x}}=[g'_k(x)+(x^{-2})g'_k(x)]e^{\frac{-1}{x}}& \text{ if $x>0$ }\\
  p_k'(x)e^{\frac{1}{x}}+(-x^{-2})p_k'(x)e^{\frac{1}{x}}=[p_k'(x)e^{\frac{1}{x}}+(-x^{-2})p_k'(x)]e^{-\frac{1}{x}}& \text{ if $x<0$ }
\end{cases}
\end{align*}
Notice that polynomials forms a ring and is closed under differentiation. We have proved $g_{k+1},p_{k+1}$ are polynomials. Because $g_k,p_k$ are polynomial, we can compute 
\begin{align*}
\lim_{h \to 0^+}\frac{g_k(h)e^{\frac{-1}{h}}}{h}=0=\lim_{h\to 0^-}\frac{p_k(h)e^{\frac{1}{h}}}{h}
\end{align*}
Then we see 
\begin{align*}
f^{(k+1)}(0)=\lim_{h\to 0}\frac{f^{(k)}(h)-f^{(k)}(0)}{h}=\lim_{h\to 0}\frac{f^{(k)}(h)}{h}=0\vdone
\end{align*}
Following the result above, we see the Maclaurin Series of $f(x)$ is 
 \begin{align*}
\sum_{k=0}^{\infty} \frac{f^{(k)}(0) x^k}{k!}=0\neq f(y)\text{ if $y\neq x$ }
\end{align*}


\end{proof}
\begin{question}{}{}

Let \( f \) have a continuous derivative in the interval \( [a,b] \), and let \( f''(x) \geq 0 \) for every value of \( x \). Then if \( \xi \) is any point in the interval, the curve nowhere falls below its tangent at the point \( x = \xi, y = f(\xi) \). (Hint: It suffices to show \( f(x) \geq f(\xi) + f'(\xi)(x - \xi) \) on \( [a,b] \), i.e. consider the Taylor expansion of \( f(x) \) at \( x = \xi \)).
\end{question}
\begin{proof}
Taylor expand $f$ at $\xi$ and express the remainder in Lagrange Mean Value Form (\myref{Corollary}{6.5.3}). We have 
\begin{align*}
\exists \xi_0\in (x,\xi)\text{ or }(\xi,x), f(x)=f(\xi)+f'(\xi)(x-\xi)+f''(\xi_0)(x-\xi)^2
\end{align*}
Then because $f''(\xi_0)\geq 0$ and $(x-\xi)^2\geq 0$, we now see 
\begin{align*}
f(x)\geq f(\xi)+f'(\xi)(x-\xi)
\end{align*}
\end{proof}
\begin{lemma}
\label{8.7.1}
Given 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous on $[a,b]$ 
  \item $\exists c\in [a,b], f(c)>0$ 
\end{enumerate}
Then there exists $\delta $ such that 
\begin{align*}
\forall y \in [c-\delta ,c+\delta ], f(y)>\frac{f(c)}{2}
\end{align*}
\end{lemma}
\begin{proof}
Because $f$ is continuous at $c$,  we know 
\begin{align*}
\lim_{x \to c} f(x)=f(c)>\frac{f(c)}{2}
\end{align*}
The Lemma now follows from definition of limit and taking $\delta=\frac{\delta (\frac{f(c)}{4})}{2}$
\end{proof}
\begin{question}{}{}
Let \( f \) be a continuous function on \( [a, b] \), then prove that \( f(x) = 0 \) for all \( x \in [a, b] \) if and only if 
\begin{enumerate}
\item[(a)] \(\int_{a}^{b} |f(x)| \, dx = 0\),
\item[(b)] \(\int_{a}^{c} f(x) \, dx = 0, \quad \forall c \in [a, b]\).
\end{enumerate}
\end{question}
\begin{proof}
We first prove 
\begin{align*}
\vi{\forall x\in [a,b], f(x)=0\implies \int_a^b \abso{f(x)}dx=0}
\end{align*}
Let $P$ be an arbitrarily partition of $[a,b]$. Express 
\begin{align*}
P=\set{x_0,x_1,\dots ,x_n}
\end{align*}
Because $\forall i,\set{f(x):x\in [x_{i-1},x_i]}=\set{0}$, we know 
\begin{align*}
\forall i,m_i=0=M_i
\end{align*}
Then we have 
\begin{align*}
L(P,f)&=\sum_{i=1}^n m_i\Delta x_i=0\\
U(P,f)&=\sum_{i=1}^n M_i\Delta x_i=0\vdone
\end{align*}
We now prove 
\begin{align*}
\blue{\int_a^b \abso{f(x)}dx=0\implies \forall x\in [a,b],f(x)=0}
\end{align*}
\As{$\exists c \in [a,b], f(c)=p \neq 0$}. WOLG, suppose $p >0$. Let $P$ be an arbitrary parition of  $[a,b]$. Express 
\begin{align*}
P=\set{x_0,x_1,\dots, x_n}
\end{align*}
Let $\delta$ be from \myref{Lemma}{8.7.1}. Let $j$ be the greatest and $k$ be the smallest such that 
 \begin{align*}
x_j\leq c-\delta \text{ and }x_k\geq c+\delta
\end{align*}
We then see 
\begin{align*}
U(P,f)&=\sum_{i\leq j\text{ or }k<i\leq n} M_i \Delta x_i+ \sum_{i=j+1}^k M_i\Delta x_i\\
&\geq \sum_{i=j+1}^k M_i \Delta x_i (\because \abso{f}\geq 0)\\
&\geq \sum_{i=j+1}^k \frac{f(c)}{2}\Delta x_i \\
&= \frac{f(c)}{2} (x_k-x_j)\geq f(c)\delta
\end{align*}
Because $P$ is an arbitrary partition,  we now see  
\begin{align*}
0<f(c)\delta \leq \overline{\int_a^b}\abso{f}dx \tCaC \int_a^b \abso{f}dx=0 \bdone
\end{align*}
The proof for 
\begin{align*}
\forall x\in [a,b],f(x)=0\implies \forall c\in [a,b], \int_a^cfdx=0
\end{align*}
is trivial. We now prove 
\begin{align*}
\vi{\forall c\in [a,b],\int_a^c f(x)dx=0 \implies \forall x\in [a,b], f(x)=0}
\end{align*}
\As{$\exists c\in [a,b],f(c)=p\neq 0$}. WOLG, suppose  $p>0$. Let $\delta$ be from  \myref{Lemma}{8.7.1}. We see $\frac{f(c)}{2}$ is an lower bound of $f$ on  $[c-\delta,c+\delta]$. Then because $f$ is continuous, we can write 
\begin{align*}
\int_{c-\delta}^{c+\delta} fdx\geq f(c)\delta >0
\end{align*}
Then because $\int_0^{c+\delta}fdx=0$, we have 
\begin{align*}
\int_0^{c-\delta}fdx= \int_0^{c+\delta}fdx-\int_{c-\delta}^{c+\delta}fdx<0\tCaC \vdone
\end{align*}
\end{proof}
\begin{question}{}{}

Prove that if \(\lim_{b \to \infty} \int_{1}^{b} |f(x)| \, dx\) exists and finite, then \(\lim_{n \to \infty} \int_{1}^{\infty} f(x^n) \, dx = 0\).
\end{question}
\begin{proof}
Let $u=x^n,du=nx^{n-1}dx$, we have 
 \begin{align*}
\int_1^\infty f(x^n)dx&=\int_1^\infty  f(u)\frac{1}{nu^{\frac{n-1}{n}}}du 
\end{align*}
Notice that we have 
\begin{align*}
\int_1^\infty - \abso{f(u)} \frac{1}{n u^{\frac{n-1}{n}}}du \leq \int_1^\infty f(u) \frac{1}{nu^{\frac{n-1}{n}}}du \leq  \int_1^\infty \abso{f(u)} \frac{1}{n u^{\frac{n-1}{n}}}du
\end{align*}
Then we can reduce the problem into proving 
\begin{align*}
  \vi{\lim_{n \to \infty} \int_1^\infty \abso{f(u)} \frac{1}{n u^{\frac{n-1}{n}}}du=0}
\end{align*}
Notice that 
\begin{align*}
u\geq 1 \implies  \forall n\inn,  \frac{1}{u^{\frac{n-1}{n}}}\leq 1
\end{align*}
Then 
\begin{align*}
\int_1^\infty \abso{f(u)} \frac{1}{n u^{\frac{n-1}{n}}}du\leq  \int_1^\infty \abso{f(u)} \frac{1}{n}du=\frac{1}{n} \int_1^\infty \abso{f(u)}du 
\end{align*}
Because $\int_1^\infty \abso{f(x)}dx$ is finite, we know 
\begin{align*}
\frac{1}{n}\int_1^\infty \abso{f(u)}du\to 0 \text{ as $n \to \infty$ }
\end{align*}
Then because $\abso{f(u)}\frac{1}{nu^{\frac{n-1}{n}}}\geq 0$,  our proof is done. $\vdone$
\end{proof}
\begin{question}{}{}

Prove that if \( f \) is continuous on \( [a, b] \), and \( \phi \) is monotone increasing on \( [a, b] \), then there exists \( \xi \in [a, b] \) such that
\[ \int_{a}^{b} f \, d\phi = f(\xi)(\phi(b) - \phi(a)) \]
\end{question}
\begin{proof}
WOLG, suppose $\phi$ increase on $[a,b]$. If $\phi(a)=\phi (b)$, then we see $\phi$ stay constant, so for every partition $P$, we have 
\begin{align*}
\forall i, \Delta \phi(x_i)=0
\end{align*}
Then we have $\int_a^bfd\phi=0$. It is clear that $\phi(a)=\phi(b)\implies f(\xi)(\phi(b)-\phi(a))=0$. We have proved the $\phi(a)=\phi(b)$ case. We now can suppose $\phi(a)<\phi(b)$. Notice that we wish to find $\xi \in [a,b]$ such that 
\begin{align*}
f(\xi)=\frac{\int_a^b fd\phi}{\phi(b)-\phi(a)}
\end{align*}
Because $f$ is continuous on $[a,b]$ thus satisfying IVT, we can reduced the problem into proving 
\begin{align*}
\inf_{x\in [a,b]}f\leq \frac{\int_a^b fd\phi}{\phi (b)-\phi (a)} \leq \sup_{x\in [a,b]} f
\end{align*}
WOLG, we only prove 
\begin{align*}
  \vi{\frac{\int_a^b fd\phi}{\phi(b)-\phi(a)}\geq \inf_{x\in [a,b]}f}
\end{align*}
This is equivalent to 
\begin{align*}
\int_a^b fd\phi \geq  (\phi(b)-\phi (a)) \inf_{x\in [a,b]}f
\end{align*}
Let $P=\set{x_0,\dots ,x_n}$ be an arbitrary partition of  $[a,b]$. We see 
\begin{align*}
U(P,f)&=\sum_{i=1}^n  m_i\Delta \phi(x_i)\\
&\geq  \sum_{i=1}^n \inf_{x\in [a,b]}f \Delta \phi (x_i)\\
&=(\phi (b)-\phi (a))\inf_{x\in [a,b]} f 
\end{align*}
Because $P$ is arbitrary, we now have 
 \begin{align*}
\overline{\int_a^b}fd\phi \geq (\phi(b)-\phi(a))\inf_{x\in [a,b]}f\vdone
\end{align*}



\end{proof}
\begin{lemma}
\label{8.7.2}
Suppose $f,g$ are both bounded on  $[x_{i-1},x_i]$. Let $M_f,M_g,M_{fg}$ be the supremum of the values of there corresponding function on $[x_{i-1},x_i]$. Let $m_f,m_g,m_{fg}$ similarly be the same vice versa. We have 
\begin{align*}
M_{fg}-m_{fg}\leq (M_f-m_f)(M_g-m_g)
\end{align*}
\end{lemma}
\begin{proof}
Let $[x,y]\subseteq [x_{i-1},x_i]$. Observe 
\begin{align*}
\abso{fg(x)-fg(y)}&=\abso{f(x)(g(x)-g(y))}+\abso{(f(x)-f(y))g(y)}
\end{align*}
Then the result follows from \myref{Theorem}{7.1.7}. \end{proof}
\begin{question}{}{}

Let \( \{g_n\} \) be a sequence of non-negative and Stieltjes integrable with respect to an increasing function \( \phi \) on \( [a, b] \) such that
\[ \lim_{n \to \infty} \int_{a}^{b} g_n \, d\phi = 0 \]
Prove that if \( f \) is also integrable, then
\[ \lim_{n \to \infty} \int_{a}^{b} f g_n \, d\phi = 0 \]
\end{question}
\begin{proof}
Because $g_n$ are non-negative, we know 
 \begin{align*}
\int_a^b -\abso{f}g_nd\phi\leq \int_a^b fg_nd\phi \leq \int_a^b \abso{f}g_nd\phi
\end{align*}
Because of Squeeze Theorem, we can now suppose $f$ is non-negative.\\

For each interval $[x,y]\subseteq [a,b]$, observe 
\begin{align*}
\sup_{t\in [x,y]}fg_n(t)\leq \sup_{t \in [x,y]} f(t)\sup_{t \in [x,y]} g_n(t)
\end{align*}
Let $M_i^f,M_i^{g_n},M_i^{fg_n}$ has the meaning of those in Rudin, and let $M=\sup_{t \in [a,b]} f(t)$. We have 
\begin{align*}
  U(P,fg_n,\phi)=\sum_{i=1}^m M_{i}^{fg_n} \Delta \phi (x_i)&\leq \sum_{i=1}^m (M_i^f M_i^{g_n})\Delta  \phi (x_i)\\
&\leq M \sum_{i=1}^m M_i^{g_n}\Delta \phi (x_i)\\
&=M U(P,g_n,\phi)
\end{align*}
This give us 
\begin{align*}
\overline{\int_a^b}fg_n d\phi \leq M \overline{\int_a^b}g_n d\phi
\end{align*}
Because  
\begin{align*}
\lim_{n\to \infty}\overline{\int_a^b}g_n d\phi = 0
\end{align*}
We now have 
\begin{align*}
\limsup_{n\to\infty} \overline{\int_a^b}fg_nd\phi =0
\end{align*}
Because $\forall n,fg_n$ is non-negative and $\phi$ is increasing, we see  
\begin{align*}
\forall n,\int_a^b fg_n d\phi \geq 0
\end{align*}
This now give us 
\begin{align*}
\lim_{n \to \infty}\int_a^b fg_n d\phi =0
\end{align*}
 

\end{proof}

\begin{question}{}{}

Let \( \phi \) be a strictly monotone increasing function on \( [a, b] \).
\begin{enumerate}
\item[(a)] Show that if \( f \) is Riemann-Stieltjes integrable with respect to \( \phi \), then the following quantities all exist and finite for \( p > 0 \):
\[ \|f\|_\infty = \sup\{|f(x)| : x \in [a, b]\} \]
\[ \|f\|_p = \left( \int_{a}^{b} |f|^p \, d\phi \right)^{1/p} \]

\item[(b)] Prove that if \( f \) is continuous on \( [a, b] \), then
\[ \lim_{p \to \infty} \|f\|_p = \|f\|_\infty \]
\end{enumerate}
\end{question}
\begin{proof}
Because $f$ is integrable, we know  $f$ must be bounded. This tell us $\norm{f}_{\infty}$ do exists as a real number. \\

Notice that $g,h:\R\rightarrow \R$ defined by 
\begin{align*}
g(x)=\abso{x}\text{ and }h(x)=x^p
\end{align*}
are both continuous, then by \myref{Theorem}{2.4.5}, we know 
\begin{align*}
h\circ g\text{ is continuous }
\end{align*}
Then by \myref{Theorem}{7.1.17}, we know 
\begin{align*}
h \circ  g \circ  f \in \mathscr{R}(\phi)\text{ on $[a,b]$ }
\end{align*}
This tell us 
\begin{align*}
\int_a^b \abso{f}^p d\phi \text{ exists }
\end{align*}
Notice that the integrand $\abso{f}$ is non-negative and $\phi$ increase, so we know the integral $\int_a^b \abso{f}^p d\phi$ is non-negative. Then we know 
\begin{align*}
\norm{f}_p=(\int_a^b \abso{f}^pd\phi)^{\frac{1}{p}}\text{ exists }
\end{align*}
We now prove 
\begin{align*}
\vi{\lim_{p\to \infty}\norm{f}_p=\norm{f}_\infty}
\end{align*}
Let $M:=\norm{f}_\infty=\sup_{x\in [a,b]}\abso{f(x)}$. We have 
\begin{align*}
  \int_a^b \abso{f}^p d\phi \leq \int_a^b M^p d \phi =M^p (\phi(b)-\phi(a))
\end{align*}
This give us 
\begin{align*}
\norm{f}_p=(\int_a^b \abso{f}^p d\phi)^{\frac{1}{p}}\leq M (\phi(b)-\phi(a))^{\frac{1}{p}}
\end{align*}
Because $(\phi(b)-\phi(a))^{\frac{1}{p}}\to 1$ as $p\to \infty$, we now have 
\begin{align*}
\limsup_{p\to\infty} \norm{f}_p \leq M
\end{align*}
Notice that $\abso{f}$ is  continuous and $[a,b]$ is compact. Then we know there must exists $y\in [a,b]$ such that 
\begin{align*}
\abso{f(y)}=M
\end{align*}
Again because $\abso{f}$ is continuous on $[a,b]$, for each $n\inn$, we know there exists $\delta_n$ such that 
\begin{align*}
  \forall x\in [y-\delta_n,y+\delta_n], \abso{f(x)}>M-\frac{1}{n}
\end{align*}
Define 
\begin{align*}
[c,d]:=[y-\delta_n,y+\delta_n]\cap [a,b]
\end{align*}
We now see 
\begin{align*}
\int_c^d (M-\frac{1}{n})^n d\phi\leq \int_c^d \abso{f}^n d\phi\leq \int_a^b \abso{f}^n d\phi 
\end{align*}
Then we have 
\begin{align*}
  (M-\frac{1}{n})(\phi(d)-\phi(c))=(\int_c^d (M-\frac{1}{n})^nd\phi )^{\frac{1}{n}}\leq (\int_a^b \abso{f}^n d\phi)^{\frac{1}{n}}=\norm{f}_n
\end{align*}
Notice that 
\begin{align*}
  (M-\frac{1}{n})(\phi(d)-\phi(c))\to M \text{ as }n \to \infty
\end{align*}
Then we have 
\begin{align*}
\liminf_{n\to\infty} \norm{f}_n\geq M
\end{align*}
Combining $\limsup_{p\to\infty} \norm{f}_p\leq M $, we now have 
\begin{align*}
\lim_{n \to \infty} \norm{f}_n=M=\norm{f}_\infty \vdone
\end{align*}
\end{proof}

\begin{theorem}
\label{8.7.3}
\textbf{(Important Lemma of Continuity)} 
\begin{align*}
f\text{ is continuous at }x\implies \sup_{[x-\frac{1}{n},x+\frac{1}{n}]}f \to f(x)
\end{align*}
\begin{align*}
\exists n, \inf_{[x-\frac{1}{n},x+\frac{1}{n}]}f\geq f(x)-\alpha 
\end{align*}

\end{theorem}




\end{document}
