\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}

\title{Calculus Done Taiwan}
\author{Eric Liu}
\date{}
\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}

\tableofcontents
\pagebreak

\chapter{Easy Calculus}
\section{Tests}
\label{Basic Technique on Sequence and Series}
\begin{abstract}
This section prove some basic result on sequence and series, which will be heavily used in \customref{Analytic Functions}{next section on analytic functions} and \customref{Beauty}{Chapter: Beauty}. Although written in an almost glossary form, we present the Theorems in a structural order based on the necessity of notion of absolute convergence and limit superior. Note that in this section, $z,v,w$ always represent complex numbers, and $a,b,c$ always represent real numbers.  
\end{abstract}
\begin{theorem}
\label{WM-t}
\textbf{(Weierstrass M-test)} Given sequences $f_n:X\rightarrow \C$, and suppose 
\begin{align*}
\forall n\inn,\forall x\in X, \abso{f_n(x)}\leq M_n
\end{align*}
Then 
\begin{align*}
\sum_{n=1}^\infty M_n\text{ converge }\implies \sum_{n=1}^\infty f_n\text{ uniformly converge }
\end{align*} 
\end{theorem}
\begin{proof}
The proof follows from noting 
\begin{align*}
  \forall x\in X, \abso{\sum_{k=m}^n f_k(x)}\leq \sum_{k=m}^n \abso{f_k(x)}\leq \sum_{k=m}^n M_k
\end{align*}
\end{proof}
\begin{mdframed}
  Note that in our proof of  \customref{WM-t}{Weierstrass M-test}, we reduce the proof for uniform convergence into uniform Cauchy, which is a technique we shall also use later in \customref{Abel's Test for Uniform Convergence}{Abel's test for uniform convergence}. We now prove \customref{Summation by Part}{summation by part}, which is a result hold in all fields, and is the essence of the proof of \customref{Dirichlet's Test}{Dirichlet's test} and \customref{Abel's Test for Uniform Convergence}{Abel's test for uniform convergence}. 
\end{mdframed}
\begin{theorem}
\label{Summation by Part}
\textbf{(Summation by Part)} 
\begin{align*}
  f_ng_n-f_mg_m&=\sum_{k=m}^{n-1}f_k \Delta g_k + g_k \Delta f_k+ \Delta f_k \Delta g_k \\
  &=\sum_{k=m}^{n-1}f_k \Delta g_k + g_{k+1}\Delta f_k
\end{align*}
\end{theorem}
\begin{proof}
The proof follows induction which is based on 
\begin{align*}
f_mg_m+ f_m\Delta g_m+ g_m \Delta f_m +\Delta f_m \Delta g_m=f_{m+1}g_{m+1}
\end{align*} 
\end{proof}
\begin{theorem}
\label{Dirichlet's Test}
\textbf{(Dirichlet's Test)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $a_n\to 0$ monotonically. 
  \item $\sum_{n=1}^N z_n$ is bounded.
\end{enumerate}
We have 
\begin{align*}
\sum a_nz_n\text{ converge }
\end{align*}
\end{theorem}
\begin{proof}
Define $Z_n\triangleq \sum_{n=1}^N z_n$ and let $M$ bound  $\abso{Z_n}$. Using \customref{Summation by Part}{summation by part} by letting $f_k=a_k$ and $g_k=Z_{k-1}$, we have
\begin{align*}
  \abso{\sum_{k=m}^{n}a_kz_k}&= \abso{a_{n+1}Z_n- a_mZ_{m-1}- \sum_{k=m}^n Z_k(a_{k+1}-a_k)}\\
                             &\leq \abso{a_{n+1}Z_n}+ \abso{a_mZ_{m-1}}+ \abso{\sum_{k=m}^n Z_k (a_{k+1}-a_k)}\\
  (\because a_n\text{ is monotone })\hspace{1cm}&\leq M\Big(\abso{a_{n+1}}+\abso{a_{m}}+\abso{a_{n+1}-a_m} \Big)\hspace{5cm}
\end{align*}
\end{proof}
\begin{theorem}
\label{Abel's Test for Uniform Convergence}
\textbf{(Abel's Test for Uniform Convergence)} Suppose $g_n:X\rightarrow \R$ is a uniformly bounded pointwise monotone sequence. Then given a sequence $f_n:X\rightarrow \R$, 
\begin{align*}
\sum f_n\text{ uniformly converge }\implies \sum f_ng_n\text{ uniformly converge }
\end{align*}
\end{theorem}
\begin{proof}
Define $R_n\triangleq \sum_{k=n}^{\infty}f_k$. Let $M$ uniformly bound $g_n$. Because $R_n\to 0$ uniformly, we can let $N$ satisfy 
 \begin{align*}
\forall n\geq N, \forall x\in X,\abso{R_n(x)}<\frac{\epsilon}{6M} 
\end{align*}
Then for all $n,m\geq N$, using \customref{Summation by Part}{summation by part}, we have
\begin{align*}
  \abso{\sum_{k=m}^{n}f_kg_k}&=\abso{\sum_{k=m}^n g_k\Delta R_k }\\
  &\leq \abso{R_{n+1}g_{n+1}}+ \abso{R_{m+1}g_{m+1}}+ \sum_{k=m}^n\abso{ R_{k+1}\Delta g_k} \\
  (\because g_n\text{ is pointwise monotone })\hspace{1cm}&\leq \abso{R_{n+1}g_{n+1}}+ \abso{R_{m+1}g_{m+1}}+ \frac{\epsilon }{6M} \abso{g_{n+1}-g_m}\leq \epsilon 
\end{align*}
\end{proof}
\begin{mdframed}
Although the proofs of \customref{Dirichlet's Test}{Dirichlet's test} and \customref{Abel's test for uniform convergence}{Abel's test for uniform convergence} are quite similar, one should note that the "ways" \customref{Summation by Part}{summation by part} is applied are slightly different, as one use $R_n\triangleq \sum_{k=n}^{\infty}f_k$ instead of $\sum_{k=1}^n f_k$, like $Z_n\triangleq \sum_{j=1}^n z_j$. As corollaries of  \customref{Dirichlet's Test}{Dirichlet's test}, one have the famous \textbf{alternating series test} and \customref{Abel's Test for Complex Series}{Abel's test for complex series}. 
\end{mdframed}
\begin{theorem}
\label{Abel's Test for Complex Series}
\textbf{(Abel's Test for Complex Series)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $\sum z_n$ converge. 
  \item $b_n$ is a bounded monotone sequence.
\end{enumerate}
We have 
\begin{align*}
\sum z_nb_n\text{ converge }
\end{align*}
\end{theorem}
\begin{proof}
Denote $B\triangleq \lim_{n\to \infty}b_n$. By \customref{Dirichlet's Test}{Dirichlet's Test}, we know $\sum z_n(b_n-B)$ converge. The proof now follows form noting 
\begin{align*}
\sum z_nb_n= \sum z_n(b_n-B)+ B \sum  z_n
\end{align*}
\end{proof}
\begin{mdframed}
We now introduce the idea of absolute convergence, which we shall use throughout the remaining of the section. By a \textbf{permutation} $\sigma:E\rightarrow E$ on some set $E$, we merely mean  $\sigma$ is a bijective function. We say $\sum z_n$ \textbf{absolutely converge} if $\sum \abso{z_n}$ converge, and say $\sum z_n$ \textbf{unconditionally converge} if for all permutation $\sigma:\N\rightarrow \N$, the series $\sum z_{\sigma (n)}$ converge and converge to the same value.
\end{mdframed}
\begin{theorem}
\label{ACSUC}
\textbf{(Absolutely Convergent Series Unconditionally Converge)} 
\begin{align*}
\sum z_n\text{ absolutely converge }\implies \sum z_n\text{ unconditionally converge }
\end{align*}
\end{theorem}
\begin{proof}
The fact $\sum z_n$ converge  follows from noting 
\begin{align*}
  \abso{\sum_{k=n}^mz_k}\leq \sum_{k=n}^m \abso{z_k}\leq \sum_{k=n}^{\infty} \abso{z_k}
\end{align*}
Now, fix $\epsilon $ and permutation $\sigma$. Let $N_1$ and $N_2$ satisfy 
\begin{align*}   
\sum_{n=N_1}^{\infty}\abso{z_n}<\frac{\epsilon}{2}\text{ and }\abso{\sum_{n=N}^{\infty}z_n}<\frac{\epsilon}{2}\text{ for all $N>N_2$ }
\end{align*}
Let $M\triangleq \max \set{N_1,N_2}$. Observe that for all  $N> \max_{1\leq r\leq M} \sigma^{-1}(r)$, we have 
\begin{align*}
\abso{\sum z_n - \sum_{n=1}^N z_{\sigma (n)}}\leq \abso{\sum_{n=M+1}^{\infty} z_n }+ \sum_{n=M+1}^{\infty} \abso{z_n}<\epsilon 
\end{align*}
\end{proof}
\begin{theorem}
\label{RRT}
\textbf{(Riemann Rearrangement Theorem)} If $\sum a_n$ converge but not absolutely, then for each $L\in\overline{\R}$, there exists a permutation $\sigma$ such that 
\begin{align*}
\sum a_{\sigma (n)}=L
\end{align*}
\end{theorem}
\begin{proof}
Define $a_n^+\text{ and }a_n^-$ by 
\begin{align*}
a_n^+\triangleq \max \set{a_n,0}\text{ and }a_n^-\triangleq \min \set{a_n,0}
\end{align*}
Because 
\begin{align*}
\sum (a_n^++a_n^-)\text{ converge but }\sum (a_n^+-a_n^-)=\infty
\end{align*}
We know 
\begin{align*}
\sum a_n^+=\sum (-a_n^-)=\infty
\end{align*}
WOLG, (why?), fix $L\in\R$ and suppose $a_n\neq 0$ for all $n$. Let $A=B=L$, and let two increasing sequence $\sigma^+,\sigma^-:\N\rightarrow \N$ satisfy 
\begin{align*}
  \sigma^+(k+1)&=\min \set{n\inn:a_n>0\text{ and }n>\sigma^+(k)}
\end{align*}
and similar for $\sigma^-$. Now, recursively define $p_k,q_k$ by 
\begin{align}
  \text{ $p_1$ is the smallest number such that  }&\sum_{n=1}^{p_1} a_{\sigma ^+(n)}\geq A\label{ba1}\\
  q_1\text{ is the smallest number such that }&\sum_{n=1}^{p_1}a_{\sigma^+(n)}+\sum_{n=1}^{q_1}a_{\sigma^-(n)}\leq B \label{ba2}\\
  p_{k+1}\text{ is the smallest number such that }&\sum_{n=1}^{p_{k+1}}a_{\sigma^+(n)}+\sum_{n=1}^{q_k}a_{\sigma^-(n)}\geq A\label{ba3}\\
  q_{k+1}\text{ is the smallest number such that }&\sum_{n=1}^{p_{k+1}}a_{\sigma^+(n)}+\sum_{n=1}^{q_{k+1}}a_{\sigma^-(n)}\leq B\label{ba4}
\end{align}
We then define $\sigma$ by 
\begin{align*}
  \sigma^+(1),\dots ,\sigma^+(p_1),\sigma^-(1),\dots ,\sigma^-(q_1),\sigma^+(p_1+1),\dots ,\sigma^+(p_2),\sigma^-(q_1+1),\dots ,\sigma^-(q_2),\dots 
\end{align*}
It then follows from 
\begin{align*}
  \abso{\sum_{n=1}^{p}a_{\sigma^+}(n)+\sum_{n=1}^{q_k}a_{\sigma^-}(n)-L}\leq \min \set{a_{\sigma^+(p_{k+1})},\abso{a_{\sigma^-(q_k)}}}\text{ for all  }p_k\leq p\leq p_{k+1}
\end{align*}
and $a_n\to 0$ that $\sum a_{\sigma (n)}=L$.
\end{proof}

\begin{mdframed}
Note that the method we deploy in the proof of \customref{RRT}{Riemann rearrangement Theorem} can be used to control the sequence to have arbitrary large set of subsequential limits by modifying the number of $A,B$ in  \customref{ba1}{Equation (4.1), (4.2), (4.3) and (4.4)}.\\

Using \customref{RRT}{Riemann rearrangement Theorem} and equation
\begin{align*}
\max_{1\leq r\leq d}\abso{x_n}\leq \abso{\textbf{x}}\leq \sum_{r=1}^d \abso{x_r}
\end{align*}
we can now generalize and strengthen \myref{Theorem}{ACSUC} to 
\begin{align*}
  \sum \textbf{x}_n \text{ absolutely converge }&\iff  \sum_n x_{n,r}\text{ absolutely converge for all }r\\
  &\iff \sum_n x_{n,r}\text{ unconditionally converge for all }r\\
  &\iff \sum \textbf{x}_n\text{ unconditionally converge }
\end{align*}
With this in mind, we can now well state the \customref{FTfDS}{Fubini's Theorem for Double Series}.
\end{mdframed}
\begin{theorem}
\label{FTfDS}
\textbf{(Fubini's Theorem for Double Series)} If 
\begin{align*}
\sum_n \sum_k \abso{z_{n,k}}\text{ converge } 
\end{align*}
Then 
\begin{align*}
\sum_{n,k}\abso{z_{n,k}}\text{ converge and }\sum_{n,k}z_{n,k}=\sum_n \sum_k z_{n,k} =\sum_k \sum_n z_{n,k}
\end{align*}
\end{theorem}
\begin{proof}
The fact $\sum z_{n,k}$ absolutely converge follow from 
\begin{align*}
\sum_{n=1}^N \sum_{k=1}^N \abso{z_{n,k}}\leq \sum_n \sum_k \abso{z_{n,k}}\text{ for all }N
\end{align*}
WOLG, it remains to prove 
\begin{align*}
\vi{\sum_{n,k}z_{n,k}=\sum_n \sum_k z_{n,k}}
\end{align*}
Because $\sum_n \sum_k \abso{z_{n,k}}$ converge, we can reduce the problem into proving the same statement for nonnegative series $a_{n,k}$. (why?)
\begin{align*}
  \vi{\sum_n \sum_k \abso{a_{n,k}}\text{ converge }\implies \sum_{n,k}a_{n,k}=\sum_n \sum_k a_{n,k}}
\end{align*}
Because 
\begin{align*}
\sum_{n=1}^N \sum_{k=1}^N a_{n,k} \leq  \sum_{n=1}^N \sum_{k} a_{n,k}\leq \sum_n \sum_k a_{n,k}\text{ for all }N
\end{align*}
we see 
\begin{align*}
\sum_{n,k}a_{n,k}\leq \sum_n \sum_k a_{n,k}
\end{align*}
It remains to prove 
\begin{align*}
  \vi{\sum_{n,k}a_{n,k}\geq \sum_{n} \sum_k a_{n,k}}
\end{align*}
Fix  $N\text{ and }\epsilon $. We reduce the problem into proving 
\begin{align*}
  \vi{\sum_{n,k}a_{n,k} \geq \sum_{n=1}^N \sum_{k} a_{n,k} - \epsilon}
\end{align*}
Let $K$ satisfy 
 \begin{align*}
\text{ For all $1\leq n\leq N$, }\sum_{k=K+1}^\infty a_{n,k} < \frac{\epsilon}{N} 
\end{align*}
It then follows 
\begin{align*}
\sum_{n,k} a_{n,k}\geq \sum_{n=1}^N \sum_{k=1}^K a_{n,k}\geq \sum_{n=1}^N \sum_k a_{n,k}-\epsilon  \vdone
\end{align*}

\end{proof}
\begin{Example}{\textbf{(Counter-Example for Fubini's Theorem for Double Series)}}{}
\begin{align*}
a_{n,k}\triangleq \begin{cases}
  1& \text{ if $n=k$ }\\
  -1& \text{ if $n=k+1$ }\\
  0& \text{ if otherwise }
\end{cases}
\end{align*}
\begin{align*}
\sum \abso{a_{n,k}}=\infty \text{ and }\sum_n \sum_k a_{n,k}=1\text{ and }\sum_k \sum_n a_{n,k}=0
\end{align*}
\end{Example}
\begin{theorem}
\label{Merten Cau}
\textbf{(Merten's Theorem for Cauchy Product)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $\sum_{n=0}^\infty z_n$ converge absolutely 
  \item $\sum_{n=0}^\infty z_n=Z$
  \item $\sum_{n=0}^\infty v_n=V$ 
  \item $w_n=\sum_{k=0}^n z_kv_{n-k}$
\end{enumerate}
Then we have 
\begin{align*}
\sum_{n=0}^{\infty}w_n=ZV
\end{align*}
\end{theorem}
\begin{proof}
We prove 
\begin{align*}
  \vi{\abso{V\sum_{n=0}^N z_n- \sum_{n=0}^Nw_n}\to 0\text{ as }N\to \infty}
\end{align*}
Compute 
\begin{align*}
V\sum_{n=0}^N z_n - \sum_{n=0}^N w_n&=\sum_{n=0}^N z_n (V-\sum_{k=0}^{N-n}v_k)\\
&=\sum_{n=0}^N z_n \sum_{k=N-n+1}^{\infty}v_k
\end{align*}
Because $\sum_{k=n}^{\infty}v_k\to 0$ as $n\to \infty$, we know there exists $M$ such that 
\begin{align*}
\label{me1}
\abso{\sum_{k=n}^{\infty}v_k}<M\text{ for all }n
\end{align*}
Let $N_0$ satisfy 
 \begin{align*}
\sum_{n=N_0+1}^{\infty} \abso{z_n}< \frac{\epsilon }{2M}
\end{align*}
Let $N_1>N_0$ satisfy 
 \begin{align*}
  \label{me2}
   \abso{\sum_{k=N-N_0+1}^{\infty}v_k}<\frac{\epsilon}{2(N_0+1)\sum_n \abso{z_n}} \text{ for all }N>N_1
\end{align*}
Now observe that for all $N>N_1$
\begin{align*}
  \abso{\sum_{n=0}^N z_n \Big(\sum_{k=N-n+1}^\infty v_k \Big)}\leq \sum_{n=0}^{N_0} \abso{z_n} \abso{\sum_{k=N-n+1}^{\infty}v_k}+ \sum_{n=N_0+1}^{N} \abso{z_n} \abso{\sum_{k=N-n+1}^{\infty}v_k}<\epsilon \vdone 
\end{align*}
\end{proof}
\begin{mdframed}
We first define the \textbf{limit superior} by 
\begin{align*}
\limsup_{n\to\infty} a_n\triangleq \lim_{n\to \infty} (\sup_{k\geq n}a_k)
\end{align*}
Note that $\limsup_{n\to\infty} a_n$ must exists because $(\sup_{k\geq n}a_k)_n$ is a decreasing sequence. 
\end{mdframed}
\begin{theorem}
\label{Equivalent Definition for Limit Superior}
\textbf{(Equivalent Definition for Limit Superior)}
If we let $E$ be the set of subsequential limits of $a_n$
 \begin{align*}
E\triangleq \set{L\in\overline{\R}:L=\lim_{k\to \infty}a_{n_k}\text{ for some }n_k}
\end{align*}
The set $E$ is non-empty and 
\begin{align*}
\max E=\limsup_{n\to\infty} a_n
\end{align*}
\end{theorem}
\begin{proof}
Let $n_1\triangleq 1$. Recursively, because
\begin{align*}
\sup_{j\geq n_k}a_k\geq \limsup_{n\to\infty} a_n>\limsup_{n\to\infty} a_n - \frac{1}{k}\text{ for each }k
\end{align*}
We can let $n_{k+1}$ be the smallest number such that 
\begin{align*}
a_{n_{k+1}}>\limsup_{n\to\infty} a_n - \frac{1}{k}
\end{align*}
It is straightforward to check $a_{n_k}\to \limsup_{n\to\infty} a_n$ as $k\to \infty$. Note that no subsequence can converge to $\limsup_{n\to\infty} a_n+\epsilon $ because there exists $N$ such that  $\sup_{k\geq N}a_k<\limsup_{n\to\infty} a_n+\epsilon $. 
\end{proof}
\begin{mdframed}
  We can now state the \textbf{limit comparison test} as follows. Given a positive sequence $b_n$, 
\begin{align*}
  \limsup_{n\to\infty} \frac{\abso{z_n}}{b_n}\inr \text{ and }\sum b_n\text{ converge }&\implies \sum z_n\text{ absolutely converge } \\
 \liminf_{n\to\infty} \frac{b_n}{\abso{z_n}}>0\text{ and }\sum z_n \text{ diverge }&\implies \sum b_n\text{ diverge }
\end{align*}
\end{mdframed}
\begin{theorem}
\label{geometric series}
\textbf{(Geometric Series)} 
\begin{align*}
\abso{z}< 1 \implies \sum_{n=0}^{\infty} z^n= \frac{1}{1-z}
\end{align*}
\end{theorem}
\begin{proof}
The proof follows from noting  
\begin{align*}
  (1-z)\sum_{n=0}^N z^n=1-z^{N+1}\to 1\text{ as }N\to \infty
\end{align*}
\end{proof}

\begin{theorem}
\label{Ratio and Root Test}
\textbf{(Ratio and Root Test)} 
\begin{align*}
  \limsup_{n\to\infty} \sqrt[n]{\abso{z_n}}<1 \text{ or }\limsup_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}<1&\implies \sum z_n\text{ absolutely converge }\\
  \liminf_{n\to\infty} \sqrt[n]{\abso{z_n}}>1\text{ or }\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}>1 &\implies \sum  z_n\text{ diverge }
\end{align*}
\end{theorem}
\begin{proof}
The convergent part follows from comparison to an appropriate geometric series and the diverge part follows from noting $\abso{z_n}$ does not converge to $0$.
\end{proof}
\begin{theorem}
\label{Root Test is Stronger Than Ratio Test}
\textbf{(Root Test is Stronger Than Ratio Test)}
\begin{align*}
\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}\leq \liminf_{n\to\infty} \sqrt[n]{\abso{z_n}} \leq \limsup_{n\to\infty} \sqrt[n]{\abso{z_n}} \leq \limsup_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $ and WLOG suppose $\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}>0$. We prove 
\begin{align*}
  \vi{\liminf_{n\to\infty} \sqrt[n]{\abso{z_n}}\geq \liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}-\epsilon  }
\end{align*}
Let $\alpha \inr $ satisfy 
\begin{align*}
\liminf_{n\to\infty}  \abso{\frac{z_{n+1}}{z_n}}-\epsilon <\alpha < \liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}
\end{align*}
Let $N$ satisfy  
\begin{align*}
\text{ For all }n\geq N,\abso{\frac{z_{n+1}}{z_n}}>\alpha  
\end{align*}
We then see 
\begin{align*}
  \sqrt[N+n]{\abso{z_{N+n}}}\geq \sqrt[N+n]{\abso{z_N}\alpha^n}=\alpha \Big(\frac{\abso{z_N}^{\frac{1}{N+n}}}{\alpha ^{\frac{N}{N+n}}} \Big)\to \alpha \text{ as }n\to \infty \vdone
\end{align*}
The proof for the other side is similar.
\end{proof}
\begin{theorem}
\label{Root Test Trick}
\textbf{(Root Test Trick)} For all $k\inn$ 
\begin{align*}
\limsup_{n\to\infty} \abso{z_{n+k}}^{\frac{1}{n}}=\limsup_{n\to\infty} \abso{z_n}^{\frac{1}{n}}
\end{align*}
\end{theorem}
\begin{proof}
This is a direct corollary of \customref{Equivalent Definition for Limit Superior}{equivalent definition for limit superior}.  
\end{proof}
\begin{mdframed}
Lastly, we prove \customref{Cauchy's Condensation Test}{Cauchy's condensation Test}, whose existence is almost solely for investigating \customref{p-Series}{p-Series}.
\end{mdframed}
\begin{theorem}
\label{Cauchy's Condensation Test}
\textbf{(Cauchy's Condensation Test)} Suppose $a_n\searrow 0$. We have 
\begin{align*}
\sum_{n=0}^{\infty} 2^na_{2^n}\text{ converge }\iff \sum_{n=1}^{\infty}a_n\text{ converge }
\end{align*}
\end{theorem}
\begin{proof}
Observe that for all $N\inn$ 
\begin{align*}
\sum_{n=0}^N 2^n a_{2^n}\geq \sum_{n=0}^N \sum_{k=1}^{2^n} a_{2^n+k-1} =\sum_{n=1}^{2^{N+1}-1}a_n
\end{align*}
and
\begin{align*}
 2\sum_{n=1}^{2^N-1} a_n= 2\sum_{n=1}^N \sum_{k=0}^{2^{n-1}-1}a_{2^{n-1}+k}\geq 2\sum_{n=1}^N 2^{n-1} a_{2^n}=\sum_{n=1}^N 2^na_{2^n}
\end{align*}
\end{proof}
\begin{theorem}
\label{p-Series}
\textbf{(p-Series)}
\begin{align*}
\sum_{n=1}^{\infty} \frac{1}{n^p}\text{ converge }\iff p>1
\end{align*}
\end{theorem}
\begin{proof}
Observe that
\begin{align*}
\sum_{n=0}^{\infty} 2^n \frac{1}{(2^n)^p}=\sum_{n=0}^{\infty}(2^{1-p})^n
\end{align*}
The result then follows from  \customref{Cauchy's Condensation Test}{Cauchy's Condensation Test} and \customref{geometric series}{geometric series}. 
\end{proof}
\section{Analytic Functions}
\label{Analytic Functions}
\begin{abstract}
This section introduces the concept of analytic functions and proves some of their basic properties, including the \customref{Identity Theorem}{Identity Theorem}. We will rely on the tools developed in \customref{Basic Technique on Sequence and Series}{the previous section on sequences and series}. Note that throughout this section,  
$z$ will always denote a complex number.
\end{abstract}
\begin{mdframed}
In this section, by a  \textbf{power series}, we mean a pair $(z_0,c_n)$ where $z_0 \inc$ is called the \textbf{center} of power series, and $c_n\inc$ are the coefficients sequence. By \textbf{radius of convergence}, we mean a unique  $R \in \R^+_0 \cup  \infty$ such that 
 \begin{align*}
\sum_{n=0}^\infty c_n(z-z_0)^n
\begin{cases}
  \text{ converge absolutely }& \text{ if  }\abso{z-z_0}<R\\
  \text{ diverge }& \text{ if  }\abso{z-z_0}>R\\
\end{cases}
\end{align*}
Such $R$ always exist (and is unique, the uniqueness can be checked without computing the actual value of $R$) and is exactly 
\begin{align}
\label{Cauchy-Hadamard}
R=\frac{1}{\limsup_{n\to\infty} \sqrt[n]{c_n} }
\end{align}
This result is called \textbf{Cauchy-Hadamard Theorem} and is proved by applying \customref{Ratio and Root Test}{Root Test} to $\sum c_n(z-z_0)^n$. Note that Cauchy-Hadamard Theorem does not tell us whether a power series converges at points of boundary of disk of convergence. It require extra works to determine if the power series converge at boundary. 
\end{mdframed}
\begin{theorem}
\label{Abel's Test for Power Series}
\textbf{(Abel's Test for Power Series)} Suppose $a_n\to0$ monotonically and $\sum a_nz^n$ has radius of convergence $R$ . 
\begin{align*}
  \text{ The power series $\sum a_nz^n$ at least converge on $\overline{D_R(0)}\setminus \set{R}$ }
\end{align*}
\end{theorem}
\begin{proof}
Note that 
\begin{align*}
\sum \frac{a_n}{R^n}z^n\text{ has radius of convergence }R
\end{align*}
Fix $z\in \overline{D_R(0)}\setminus \set{R}$. Note that 
\begin{align*}
\abso{\sum_{n=0}\frac{z^n}{R^n}}= \abso{\frac{1-(\frac{z}{R})^{N+1}}{1-\frac{z}{R}}} \leq \frac{2}{\abso{1-\frac{z}{R}}}\text{ for all }N
\end{align*}
It then follows from  \customref{Dirichlet's Test}{Dirichlet's Test} that $\sum a_n (\frac{z}{R})^n$ converge.
\end{proof}
\begin{Example}{\textbf{(Discussion of Convergence on Boundary)}}{}
\begin{align*}
f_q(z)=\sum_{n=0}^\infty n^q z^n \text{ provided $q\inr$ }
\end{align*}
It is clear that  $f_q$ has convergence radius  $1$ for all  $q\inr$. For boundary, we have
\begin{align*}
\begin{cases}
  q<-1 \implies f_q\text{ converge on $S^1$ }\\
  -1\leq q<0 \implies f_q\text{ converge on }S^1\setminus \set{1}\\
  0 \leq q \implies f_q \text{ diverge on }S^1
\end{cases}
\end{align*}
Note that
\begin{enumerate}[label=(\alph*)]
  \item At $z=1$, the discussion is just \customref{p-Series}{p-series}.  
  \item $n^q\searrow 0$ if and only if  $q<0$; and if  $n^q\searrow 0$, then the series converge by  \customref{Abel's Test for Power Series}{Abel's test for power series}. 
  \item If $q\geq 0$, $n^qz^n$ does not converge to  $0$ on  $S^1\setminus \set{1}$
\end{enumerate}
\end{Example}

\begin{mdframed}
Notice that the fact $\sum c_n(z-z_0)^n$ absolutely converge in $D_R(z_0)$ implies the convergence is uniform on all $\overline{D_{R-\epsilon }(z_0)}$ by \customref{WM-t}{M-Test}. However, on $D_R(z_0)$, the convergence is not always uniform. 
\end{mdframed}
\begin{Example}{\textbf{(Failure of Uniform Convergence on $D_R(z_0)$)}}{}
\begin{align*}
f(z)=\sum_{n=0}^\infty z^n
\end{align*}
Note $R=1$. Use \customref{geometric series}{Geometric series formula} to show $f(z)=\frac{1}{1-z}$ on $D_1(0)$. It is then clear that $f$ is unbounded on  $D_1(0)$ while all partial sums $\sum_{k=0}^n z^k$ is bounded on $D_1(0)$. 
\end{Example}
\begin{mdframed}
We now introduce some terminologies. We say a complex function $f$ is \textbf{analytic at} $z_0\inc$ if $f$ there exists a power series $(z_0,c_n)$ whose convergence radius is greater than $0$ and $f$ agrees with $\sum_{n=0}^\infty c_n(z-z_0)^n$ on $D_R(z_0)$ for some $R$ (of course, such $R$ must not be strictly greater than the radius of convergence of $(a,c_n)$). It shall be quite clear that if $f,g$ are both analytic at  $z\inc$ with radius $R_f\leq R_g$, then by \customref{Merten Cau}{Merten's Theorem for Cauchy product}, $f+g\text{ and }fg$ are analytic at  $z$ with radius at least $R_f$. We now investigate deeper into analytic functions.   
\end{mdframed}
\begin{theorem}
\label{AfaS}
\textbf{(Term by Term Differentiation)} Given a power series $(z_0,c_n)$ of convergence radius $R>0$, if we define $f:D_R(z_0)\rightarrow \C$ by
\begin{align*}
f(z)\triangleq \sum _{n=0}^{\infty}c_n(z-z_0)^n 
\end{align*}
Then $f$ is holomorphic on $D_{R}(z_0)$ and its derivative at $z_0$ is also a power series with radius of convergence $R$  
 \begin{align*}
f'(z)= \sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n
\end{align*}
\end{theorem}
\begin{proof}
Because $(n+1)^{\frac{1}{n}}\to 1$, we can use \myref{Theorem}{Root Test Trick} to deduce 
\begin{align*}
  \limsup_{n\to\infty} \big( (n+1)\abso{c_{n+1}} \big)^{\frac{1}{n}}=\limsup_{n\to\infty}  \abso{c_{n+1}}^{\frac{1}{n}}=\limsup_{n\to\infty} \abso{c_n}^{\frac{1}{n}} 
\end{align*}
which implies that the power series $\sum_{n=0}^{\infty} (n+1)c_{n+1}(z-z_0)^n$ is of radius of convergence $R$. We now prove 
\begin{align*}
\vi{f'(z)=\sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n\text{ on }D_R(z_0)}
\end{align*}
Define $f_m:D_R(z_0)\rightarrow \C$ by
 \begin{align*}
f_m(z)\triangleq \sum_{n=0}^{m} c_n(z-z_0)^n
\end{align*}
Observe 
\begin{enumerate}[label=(\alph*)]
  \item $f_m\to f$ pointwise on $D_R(a)$ 
  \item $f'_m(z)=\sum_{n=0}^{m-1}(n+1)c_{n+1}(z-z_0)^n$ for all $m$
\end{enumerate}
Fix $z\in D_R(z_0)$. Proposition (b) allow us to reduce the problem into proving
\begin{align}
\label{an1}
  \vi{f'(z)=\lim_{m\to \infty}f'_m(z)\text{ on $D_R(a)$ }}
\end{align}
Let $z\in D_r(z_0)$ where $r<R$. With proposition (a) in mind, to show \myref{Equation}{an1}, by \myref{Theorem}{UCaH}, we only have to prove $f'_m$ uniformly converge on $D_r(z_0)$, which follows from \customref{WM-t}{M-Test} and the fact that $\sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n$ absolutely converge on $D_R(z_0)$. $\vdone$ 
\end{proof}
\begin{mdframed}
Suppose 
\begin{align*}
f(z)\triangleq \sum _{n=0}^{\infty}c_n(z-z_0)^n 
\end{align*}
Now by repeatedly applying \myref{Theorem}{AfaS}, we see 
\begin{align}
\label{fkz}
f^{(k)}(z)&=\sum_{n=0}^{\infty} (n+k) \cdots (n+1)c_{n+k}(z-z_0)^n\text{ for all }k\inz_0^+
\end{align}
This then give us 
\begin{align}
\label{ckf}
c_k =\frac{f^{(k)}(z_0)}{k!}\text{ for all }k \inz_0^+ 
\end{align}
and 
\begin{align}
\label{Taylor expansion}
f(z)=f(z_0)+  f'(z_0)(z-z_0)+ \frac{f''(z_0)}{2!}(z-z_0)^2+\cdots \text{ on }D_R(z_0) 
\end{align}
\myref{Equation}{Taylor expansion} is often called the \textbf{Taylor expansion of $f$ at $z_0$}. Notably, \myref{Equation}{ckf} tell us that if $f$ is constant  $0$, then $c_n=0$ for all  $n$. 
\end{mdframed}
\begin{Example}{\textbf{(Smooth but not Analytic Function)}}{}
\begin{align*}
f(x)=\begin{cases}
  e^{\frac{-1}{x^2}}& \text{ if $x\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}
\end{align*}
Use induction to show that 
\begin{align*}
f^{(k)}(x)=P_k(\frac{1}{x})e^{-(\frac{1}{x})^2}\hspace{0.5cm}\exists P_k\inr[x^{-1}],\forall k\inz^+_0,\forall x\inr^*
\end{align*}
and again use induction to show that  
\begin{align*}
f^{(k)}(0)=0\hspace{0.5cm}\forall k\inz_0^+
\end{align*}
The trick to show $f^{(k)}(0)=0$ is let $u=\frac{1}{x}$.\\

Now, with \myref{Theorem}{AfaS}, we see that $f$ is not analytic at $0$. 
\end{Example}
\begin{Example}{\textbf{(Bump Function)}}{}
\begin{align*}
f(x)=\begin{cases}
  e^{\frac{-1}{1-x^2}}& \text{ if $\abso{x}<1$ }\\
  0& \text{ otherwise }
\end{cases}
\end{align*}
Use the same trick (but more advanced) to show $f$ is smooth, and note that $f$ is not analytic at $\pm 1$. 
\end{Example}
\begin{mdframed}
Now, it comes an interesting question. Given a complex-valued function $f$ analytic at $z_0$ with radius  $R$, and suppose $z_1\in D_R(z_0)$.
\begin{enumerate}[label=(\alph*)]
  \item Is $f$ also analytic at $z_1$?
   \item What do we know about the radius of convergence of $f$ at $z_1$?
   \item Suppose $f$ is indeed analytic at $z_1$. It is trivial to see that the power series $(z_0,c_{0;n})$ and $(z_1,c_{1;n})$ must agree in the intersection of their convergence disks, and because $f$ is given, we by \myref{Theorem}{AfaS} and \myref{Equation}{ckf}, have already known the value of $c_{1;n}$. Can we verify that the power series  $(z_0,c_{0;n})$ and $(z_1,c_{1;n})$ do indeed agree with each other on the common convergence interval?
\end{enumerate}
\customref{TT}{Taylor's Theorem for power series} give satisfying answers to these problems. 
\end{mdframed}
\begin{theorem}
\label{TT}
  \textbf{(Taylor's Theorem for Power Series)} Given a function $f$ analytic at $z_0$ with radius $R$, and suppose $z_1\in D_R(z_0)$. Then 
\begin{align*}
f(z)=\sum_{k=0}^\infty \frac{f^{(k)}(z_1)}{k!}(z-z_1)^k\text{ on $D_{R- \abso{z_1-z_0}}(z_1)$ }
\end{align*}
\end{theorem}
\begin{proof}
WOLG, let $z_0=0$. Suppose $z$ satisfy $\abso{z-z_1}<R-\abso{z_1}$. By \myref{Equation}{ckf}, we can compute 
 \begin{align*}
f(z)&=\sum_{k=0}^{\infty}\frac{f^{(k)}(0)}{k!}z^k\\
&=\sum_{k=0}^{\infty}\frac{f^{(k)}(0)}{k!}(z-z_1+z_1)^k\\
&=\sum_{k=0}^{\infty}\frac{f^{(k)}(0)}{k!}\sum_{n=0}^k \binom{k}{n} (z-z_1)^n z_1^{k-n}\\
&=\sum_{k=0}^\infty \sum_{n=0}^k \frac{f^{(k)}(0)}{k!}\binom{k}{n}(z-z_1)^n z_1^{k-n}
\end{align*}
Note that 
\begin{align*}
  \sum_{k=0}^\infty \abso{\sum_{n=0}^\infty \frac{f^{(k)}(0)}{k!}\binom{k}{n}(z-z_1)^n z_1^{k-n}}&\leq \sum_{k=0}^\infty \sum_{n=0}^\infty \abso{\frac{f^{(k)}(0)}{k!}}\binom{k}{n}\abso{z-z_1}^n \cdot \abso{z_1}^{k-n}\\
  &=\sum_{k=0}^{\infty} \abso{\frac{f^{(k)}(0)}{k!}}\sum_{n=0}^{\infty}\binom{k}{n}\abso{z-z_1}^{n}\cdot \abso{z_1}^{k-n}\\
  &=\sum_{k=0}^\infty \abso{\frac{f^{(k)}(0)}{k!}} \big(\abso{z-{z_1}}+\abso{z_1} \big)^k
\end{align*}
is a convergent series, by \customref{Cauchy-Hadamard}{Cauchy-Hadamard Theorem} and $\abso{z-z_1}+\abso{z_1}<R$; thus, we can use \customref{FTfDS}{Fubini's Theorem for double series} to deduce 
\begin{align*}
\sum_{k=0}^\infty \sum_{n=0}^{k} \frac{f^{(k)}(0)}{k!}\binom{k}{n}(z-z_1)^{n}z_1^{k-n}&=\sum_{k=0}^\infty \sum_{n=0}^{\infty} \frac{f^{(k)}(0)}{k!} \binom{k}{n}(z-z_1)^n z_1^{k-n}\\
&=\sum_{n=0}^\infty \sum_{k=0}^\infty \frac{f^{(k)}(0)}{k!} \binom{k}{n}(z-z_1)^n z_1^{k-n}\\
&=\sum_{n=0}^\infty \sum_{k=n}^\infty \frac{f^{(k)}(0)}{k!} \binom{k}{n}(z-z_1)^n z_1^{k-n}\\
&=\sum_{n=0}^\infty \Big[ \sum_{k=n}^\infty \frac{f^{(k)}(0)}{k!} \binom{k}{n}z_1^{k-n}\Big] (z-z_1)^n\\
\end{align*}
We have reduced the problem into proving 
\begin{align*}
\vi{\sum_{k=n}^\infty \frac{f^{(k)}(0)}{k!} \binom{k}{n}z_1^{k-n}=\frac{f^{(n)}(z_1)}{n!}}
\end{align*}
Because $z_1$ is in $D_R(0)$, by \myref{Equation}{fkz} and \myref{Equation}{ckf}, we can compute 
\begin{align*}
f^{(n)}(z_1)&=\sum_{k=0}^{\infty}(k+n)\cdots (k+1)\cdot  \frac{f^{(n+k)}(0)}{(n+k)!}z_1^{k}\\
&=\sum_{k=n}^\infty (k)\cdots (k-n+1) \cdot \frac{f^{(k)}(0)}{k!} \cdot z_1^{k-n}\\
&=\sum _{k=n}^{\infty}\frac{f^{(k)}(0)}{(k-n)!}z_1^{k-n}
\end{align*}
We now have 
\begin{align*}
\frac{f^{(n)}(z_1)}{n!}= \sum_{k=n}^{\infty} \frac{f^{(k)}(0)}{n!(k-n)!}z_1^{k-n}= \sum_{k=n}^{\infty} \frac{f^{(k)}(0)}{k!}\binom{k}{n}z_1^{k-n} \vdone
\end{align*}
\end{proof}
\begin{mdframed}
Lastly, to close this section, we prove the \customref{Identity Theorem}{Identity Theorem}, which is extremely useful in complex analysis. 
\end{mdframed}
\begin{theorem}
\label{Identity Theorem}
\textbf{(Identity Theorem)} Given two analytic complex-valued function $f,g:D\rightarrow \C$ defined on some open connected $D\subseteq \C$, if $f,g$ agree on some subset $S \subseteq D$ such that $S$ has a limit point in $D$, then  $f,g$ agree on the whole region $D$.
\end{theorem}
\begin{proof}
Define 
\begin{align*}
T \triangleq \set{z\in D:f^{(k)}(z)=g^{(k)}(z)\text{ for all }k\geq 0}
\end{align*}
Since $D$ is connected, we can reduce the problem into proving  \vi{$T$ is non-empty, open and closed in $D$}. Let $c$ be a limit point of $S$ in $D$. We first show 
\begin{align*}
\olive{c \in T}
\end{align*}
\As{$c\not\in T$}. Let $m$ be the smallest integer such that  $f^{(m)}(c)\neq g^{(m)}(c)$. We can write the Taylor expansion of $f-g$ at  $c$ by 
 \begin{align*}
   (f-g)(z)&= (z-c)^{m} \Big[ \frac{(f-g)^{(m)}(c)}{m!} + \frac{(f-g)^{(m+1)}(c)}{(m+1)!}(z-c)+\cdots  \Big] \\
   &\triangleq (z-c)^m h(z)
\end{align*}
Clearly, $h(c)\neq 0$. Now, because $h$ is continuous at $c$  ($h$ is a well-defined power series at  $c$ with radius greater than $0$), we see $h$ is non-zero on some  $B_{\epsilon }(c)$, which is impossible, since $(f-g)\equiv 0$ on $S\setminus \set{c}$ implies  $h=0$ on $S\setminus \set{c}$.  \CaC $\odone$\\

Fix $z \in T$. Because $f,g$ are analytic at $z$ and  $f^{(k)}(z)=g^{(k)}(z)$ for all $k$, we see $f-g$ is constant $0$ on some open disk $B_\epsilon (z)$. We have proved that $T$ is open. To see $T$ is closed in  $D$, one simply observe that 
\begin{align*}
T= \bigcap_{k\geq 0} \set{z \in D:(f-g)^{(k)}(z)=0}
\end{align*}
and $(f-g)^{(k)}$ is continuous on $D$. $\vdone$
\end{proof}
\section{L'Hospital Rule}
\begin{abstract}
  This section state and prove the \customref{L'Hospital Rule}{L'Hospital Rule}, and provide examples to show the necessity of each hypotheses of L'Hospital Rule. Note that although \customref{L'Hospital Rule}{L'Hospital Rule} is not really directly used in most results in Theory of Calculus, it is used in the proof of \customref{Taylor's Theorem}{Taylor's Theorem}. 
\end{abstract}
\begin{theorem}
\label{L'Hospital Rule}
\textbf{(L'Hospital Rule)} Let $I \subseteq \R$ be an open interval containing $c$ and let $f,g:I\rightarrow \R$ be two function continuous on $I$ and differentiable on $I$ everywhere except possibly at  $c$, where 
\begin{align*}
g'(x)\neq  0\text{ for all }x\in I \setminus \set{c}
\end{align*}
If $\frac{f}{g}$ is indeterminate form, i.e. 
\begin{align*}
\lim_{x\to c}f(x)=\lim_{x\to c}g(x)=L\text{ where $L \in \set{0,\infty,-\infty}$ }
\end{align*}
and 
\begin{align*}
\lim_{x\to c} \frac{f'(x)}{g'(x)}\inr
\end{align*}
Then we have 
\begin{align}
\label{result L}
\lim_{x\to c}\frac{f(x)}{g(x)}= \lim_{x\to c}\frac{f'(x)}{g'(x)}
\end{align}
\end{theorem}
\begin{proof}
Suppose $I=(a,b)$. Note that since $g'(x)\neq 0$ on $(c,b)$, by  \customref{MVT}{MVT}, we know there exists at most one $x \in (c,b)$ such that $g(x)=0$. With similar argument for $(a,c)$, we see that 
\begin{align*}
g(x)\neq 0\text{ on }(c-\epsilon ,c+\epsilon )\setminus \set{c}\text{ for some }\epsilon 
\end{align*}
We now see that the expression $\lim_{x\to c}\frac{f(x)}{g(x)}$ is at least well-defined, and WOLG, we can suppose $g(x)\neq 0$ on $I \setminus \set{c}$. Define $m,M:I\setminus \set{c}\rightarrow \R$ by 
\begin{align*}
m(x)\triangleq \inf \frac{f'(t)}{g'(t)}\text{ and }M(x)\triangleq \sup \frac{f'(t)}{g'(t)}\text{ where }t\text{ ranges over values between $x$ and  $c$ }
\end{align*}
Because the value $\frac{f'(t)}{g'(t)}$ converge at $c$, we can deduce
 \begin{align}
  \label{sq1}
\lim_{x\to c}m(x)=\lim_{x\to c}M(x)=\lim_{x\to c} \frac{f'(x)}{g'(x)}
\end{align}
We now prove  
\begin{align*}
  \vi{\text{ the case when }\lim_{x\to c}f(x)=\lim_{x\to c}g(x)=0}
\end{align*}
Fix $x \in I \setminus \set{c}$, and WOLG suppose $x<c$. By \customref{CMVT}{Cauchy's MVT}, we know that 
\begin{align*}
m(x)\leq \frac{f(x)-f(y)}{g(x)-g(y)}\leq M(x)\text{ for all $y \in (x,c)$ }
\end{align*}
Note that $g(x)\neq g(y)$ by \customref{MVT}{MVT}, since $g'\neq 0$ on $I \setminus \set{c}$. It now follows from 
\begin{align*}
\lim_{y\to c^-} \frac{f(x)-f(y)}{g(x)-g(y)}= \frac{f(x)}{g(x)}
\end{align*}
that 
\begin{align*}
m(x)\leq \frac{f(x)}{g(x)}\leq M(x)
\end{align*}
The proof of \myref{Equation}{result L} then follows from \myref{Equation}{sq1}. $\vdone$  \\


We now prove 
\begin{align*}
\blue{\text{ the case when }\lim_{x\to c}f(x)=\lim_{x\to c}g(x)=\infty}
\end{align*}
Again, fix $x \in I \setminus \set{c}$, and WOLG suppose $x<c$. By  \customref{CMVT}{Cauchy's MVT}, we know that 
\begin{align*}
m(x)\leq \frac{f(y)-f(x)}{g(y)-g(x)}\leq M(x)\text{ for all }y \in (x,c)
\end{align*}
Note that $g(x)\neq g(y)$ by \customref{MVT}{MVT}, since $g'\neq 0$ on $I \setminus \set{c}$. It now follows 
\begin{align*}
\frac{f(y)-f(x)}{g(y)-g(x)}=\frac{\frac{f(y)}{g(y)} - \frac{f(x)}{g(y)}}{1- \frac{g(x)}{g(y)}}\to \lim_{x\to c} \frac{f(x)}{g(x)}\text{ as }y\to c^-
\end{align*}
The proof of \myref{Equation}{result L} then follows form \myref{Equation}{sq1}. $\bdone$
\end{proof}
\chapter{Difficult Calculus}
\section{Compact}
Let $X$ be a topological space. We say $X$ is \textbf{compact} if every of its open cover has a finite subcover. We say $X$ is \textbf{sequentially compact} if every sequence in $X$ has a \textbf{convergent} subsequence. We say $X$ is \textbf{limit point compact} if every infinite subset has a limit point.\\


Let $M$ be a metric space.  We say $M$ is \textbf{totally bounded} if for all $\epsilon $, space $M$ can be covered by finite number of open balls of radius $\epsilon $. We say $M$ is  \textbf{complete}, if every Cauchy sequence in $M$ converge.  
\begin{equiv_def}
\label{Com}
\textbf{(Compactness of metric space)} Let $M$ be a metric space. The followings are equivalent: 
\begin{enumerate}[label=(\roman*)]
  \item $M$ is compact. 
  \item $M$ is limit point compact. 
  \item $M$ is sequentially compact. 
  \item $M$ is totally bounded and complete. 
\end{enumerate}
\end{equiv_def}
\begin{proof}
(i)$\implies $(ii): The argument by nature relies on contradiction. Assume for a contradiction that some sequence  $(x_n)$ of distinct elements in $M$ whose image has no limit point\footnote{Note that if finite metric spaces are trivially limit point compact.}. For each $x \in M$, since $x$ is not a limit point of  $\set{x_n}$, there exists some open ball centered at $x$ that contains at most one point of $\set{x_n}$, namely $x$ itself if and only if it appears in the sequence. The collection of these open balls is then a open cover of $M$ that has no finite subcover of  $M$ because  $\set{x_n}$ is infinite, a contradiction. \\

 (ii)$\implies $(iii) is clear. \\

 (iii)$\implies $(iv): It is easy to show Cauchy sequences converge in sequentially compact metric space. We now show $M$ is totally bounded. Assume $M$ is not for a contradiction. This implies the existence of some $\epsilon $ and some sequence of distinct elements in $M$ whose every points are  $\epsilon $-away. Such sequence has no convergent subsequence, a contradiction. \\

 (iv)$\implies $(i): Again the argument by nature relies on contradiction. Assume for one that $(U_\alpha )$ is an open cover of $M$ that has no finite subcover. Due to totally boundedness of $M$, we know $M$ can be covered by some finite set of radius $1$ open balls. Since $(U_\alpha )$ has no finite subcover, one of these radius $1$ open balls can't be covered by finite numbers of $(U_\alpha) $. Denote the center of that radius  $1$ open ball by  $x_1$. Clearly,  $B_1(x_1)$ is again totally bounded, so it can be covered by some finite set of radius $\frac{1}{2}$ open balls centered inside. Again, because $B_1(x_1)$ can not be covered by finite numbers of $(U_\alpha) $, one of these radius $\frac{1}{2}$ open balls can't be covered by finite number of $(U_\alpha) $. Denote the center of that radius $\frac{1}{2}$ open ball by $x_2$. Repeating the same procedure, we have a Cauchy sequence  $\set{x_n}\subseteq M$. Let $x$ be the limit of this Cauchy sequence. Fix some $\alpha $ such that $x \in B_\epsilon (x)\subseteq U_\alpha $. Clearly, we may find $N$ large enough, for example $N>2\quotient \epsilon $, so that $B_{\frac{1}{N}}(x_N)\subseteq B_\epsilon (x)\subseteq U_\alpha $, a contradiction to the construction of $B_{\frac{1}{N}}(x_N)$.  
\end{proof}
\section{Holder Inequality}
\section{Gamma function for $\operatorname{Re}(z)>0$}
\section{Stirling approximation}
\begin{theorem}
\textbf{(Stirling approximation)} 
\begin{align*}
n!\sim \sqrt{2\pi n} \left(\frac{n}{e} \right)^n  ,\quad \text{ as }n\rightarrow \infty
\end{align*}
\end{theorem}
\chapter{Vector Calculus}
\section{Operator norm}
We define the \textbf{operator norm} of linear $T: \R^n\rightarrow \R^m$ by 
\begin{align*}
\norm{T}_{\operatorname{op}}\triangleq \sup_{\textbf{x}\neq 0} \frac{\abso{T\textbf{x}}}{\abso{\textbf{x}}}=  \max_{\textbf{x}=1} \abso{T\textbf{x}}
\end{align*}
One may check that this forms a norm on the real vector space of linear mappings from $\R^n$ to  $\R^m$. \textbf{Singular Value Decomposition Theorem} said that for each linear $T :\R^n\rightarrow \R^m$, there exists some orthonormal basis $\set{\textbf{v}_1,\dots ,\textbf{v}_n}$ such that $\set{T(\textbf{v}_1),\dots ,T(\textbf{v}_m)}- \set{\textbf{0}}$ is orthogonal. We call $\abso{T(\textbf{v}_1)},\dots ,\abso{T(\textbf{v}_n)}$ the \textbf{singular values} of $T$. 
\begin{theorem}
\textbf{(Operator norm is the largest singular value)} The operator norm of linear $T:\R^n\rightarrow \R^m$ is exactly the largest singular value of $T$. 
\end{theorem}
\begin{proof}

\end{proof}
\begin{theorem}
\textbf{(Equivalence of norms on $\R^n$)}
\end{theorem}
Now use the fact that determinant is continuous to conclude that collection of invertible function in $\operatorname{End}_{\text{linear}}(\R^n)$ is open. 
\begin{theorem}
\textbf{(Inversion is Continuous)} Let $\Omega$ be the space of invertible linear functions from $\R^n$ to itself. The mapping $\Omega\rightarrow \Omega,A \mapsto  A^{-1}$ is continuous. 
\end{theorem}
\begin{proof}
Try to sue the fact that all norms are equivalent and the fact that $\R^{n^2}$ is a topological group to do such. (Not faster). So stick to the one in General Analysis. 
\end{proof}
\section{Inverse function theorem}
\begin{theorem}
\textbf{(Inverse function theorem)} Let $M$ be some neighborhood of  $\textbf{a}\in \R^n$. If differentiable $\textbf{f}:M\rightarrow \R^n$ is $C^1$ at $\textbf{a}$ and has full rank Jacobian at $\textbf{a}$, then  $\textbf{f}$ maps some $B_\delta (\textbf{a})\subseteq U$ injectively to some open subset of $\R^n$, with inverse  differentiable at $\textbf{f}(\textbf{a})$. 
\end{theorem} 
\section{Lagrange multiplier theorem}
\section{Integration on manifolds}
\chapter{NTU Math M.A. Program Entrance Exam}
\section{Year 113}
\begin{question}{}{}
If every closed and bounded set of a metric space $M$ is compact, does it follows that $(M,d)$ is complete?  
\end{question}
\begin{proof}
  Yes. Let $\set{x_n}$ be a Cauchy sequence in $M$. To prove $\set{x_n}$ converge in $M$, one let  $E$ be the closure of  $\set{x_n}$, and prove that $E$ is indeed bounded. This by premise implies $E$ is compact, which implies there exists some convergent subsequence  $x_{n_k}\rightarrow x \in E$, since \customref{Com}{compactness is equivalent to sequential compactness for metric space}. The proof then follows from proving $x_n \rightarrow x$. 
\end{proof}
\begin{question}{}{}
Determine the values of $h$ for which the following series converges uniformly on  $I_h=\set{x\inr:\abso{x}\leq h}$.
\begin{align}
\label{tgq}
\sum_{n=1}^{\infty} \frac{(n!)^2x^n}{(2n)!}
\end{align}
\end{question}
\begin{proof}
Defining $c_n\triangleq (n!)^2 \quotient (2n)!$, we may write  \myref{Series}{tgq} as $\sum c_n x^n$. Because 
\begin{align*}
\frac{c_{n+1}}{c_n}= \frac{(n+1)^2}{(2n+2)(2n+1)}\text{ converges to }\frac{1}{4}
\end{align*}
We may apply \myref{Theorem}{Root Test is Stronger Than Ratio Test} to deduce $\sqrt[n]{c_n}\rightarrow \frac{1}{4}$. This together with \customref{WM-t}{Weierstrass $M$-test} and \customref{Cauchy-Hadamard}{Cauchy-Hadamard Theorem} implies \myref{Series}{tgq} converges uniformly on $I_h$ at least for any $h<4$. 

\end{proof}
\begin{question}{}{}
Consider 
\begin{align*}
F(x)\triangleq \int_0^{\infty} \frac{e^{-xt}-e^{-t}}{t}dt
\end{align*}
on $I\triangleq \set{x\inr:\frac{1}{2}\leq x\leq 2}$. 
\begin{enumerate}[label=(\roman*)]
  \item Show that $F$ is defined on $I$ and  $F$ is continuous on $I$.  
  \item Show that 
    \begin{align*}
    F'(x)=\int_0^{\infty} -e^{-xt} dt
    \end{align*} 
    \item Evaluate $F(x)$
\end{enumerate}
\end{question}
\begin{proof}
Fix $x \in [\frac{1}{2},2]$, and  define $f(t)\triangleq (e^{-xt}-e^{-t})\quotient t$. We first show: 
\begin{enumerate}[label=(\roman*)]
  \item $f$ is integrable as $t \rightarrow 0$.
  \item $f$ is integrable as $t \rightarrow  \infty$.   
\end{enumerate}
The former is easy, as applying   \customref{L'Hospital Rule}{L'Hospital rule}, we see $f$ moreover converges at $0^+$. For the latter, we have to observe that because $x\geq \frac{1}{2}$, we have:   
\begin{align}
\label{b1}
\abso{e^{-xt}-e^{-t}}\leq \abso{e^{-xt}}+ \abso{e^{-t}}\leq e^{\frac{-t}{2}}+ e^{-t}\leq 2e^{\frac{-t}{2}}
\end{align}
for any large $t$. The latter now follows from \textbf{Comparison test} and the fact $e^{-ct}$ integrable as $t \rightarrow \infty$ for positive $c$. We now prove (ii) using \textbf{measure-theoretical Feymann's trick}, and the continuity of $F$ will follow. To perform Feymann's trick, we have to establish the existence of some $L^1$ function $g:(0,\infty)\rightarrow \R$ that dominate $f:(0,\infty)\rightarrow \R$ for all $x\in [\frac{1}{2},2]$. As \myref{bound}{b1} have shown, 
\begin{align*}
g(t)\triangleq \begin{cases}
  M& \text{ if $t$ is small }\\
  \frac{2e^{\frac{-t}{2}}}{t}& \text{ if $t$ is large } 
\end{cases}
\end{align*}
suffices. Lastly, we compute $F(x)$ on $[1\quotient 2,2]$ using  \textbf{FTC}: 
\begin{align*}
 F(1)=0\quad \text{ and }\quad F'(x)= \int_0^{\infty} -e^{-xt}dt= \frac{1}{-x} \implies F(x)= \ln \left(\frac{1}{x} \right)
\end{align*}
\end{proof} 
\begin{question}{}{}
  Consider smooth $f:\R^n\rightarrow \R$ whose \textbf{Hessian Determinant} is $2$  everywhere. We denote its \textbf{gradient} $\nabla f:\R^n\rightarrow \R^n$ by $F$.  
\begin{enumerate}[label=(\roman*)]
  \item Show that there exists neighborhood $U \subseteq \R^n$ of origin such that the restriction of $F$ forms a smooth diffeomorphism from $U$ to the image of  $U$. Note that the image is guaranteed to be open by \textbf{invariance of domain} if we really prove that the action is injective on $U$.  
\item Denote the inverse map in part (i) by $\boldsymbol{\xi}(\textbf{y})\triangleq (\xi_1 (\textbf{y}),\dots ,\xi_n(\textbf{y}))$. For any $\textbf{y}$ in the image of $U$, define
\begin{align*}
  f^*(\textbf{y})\triangleq -f (\boldsymbol{\xi}(\textbf{y}))+ \sum_{i=1}^n y_i \xi_i (\textbf{y})
\end{align*}
Compute the Hessian Determinant of $f^*$. 
\end{enumerate}
\end{question}
\begin{proof}
  (i) is a direct consequence of the \textbf{Inverse function theorem} and the observation that $dF=\operatorname{Hess}f$. Fix $k$, and compute:   
\begin{align*}
\frac{\partial f^*}{\partial y_k}\Bigg|_\textbf{y}&= - \left(\sum_{j=1}^n \frac{\partial f}{\partial x_j}\Bigg|_{\boldsymbol{\xi}(\textbf{y})} \frac{\partial \xi_j}{\partial y_k}\Bigg|_{\textbf{y}} \right)+ \left(\sum_{i \neq k} y_i \frac{\partial \xi_i}{\partial y_k} \Bigg|_\textbf{y} \right) + \xi_k(\textbf{y}) +y_k \frac{\partial \xi_k}{\partial y_k}\Bigg|_{\textbf{y}}\\
&= - \left(\sum_{j=1}^n y_j \frac{\partial \xi_j}{\partial y_k}\Bigg|_{\textbf{y}} \right)+ \left(\sum_{i \neq k} y_i \frac{\partial \xi_i}{\partial y_k} \Bigg|_\textbf{y} \right) + \xi_k(\textbf{y}) +y_k \frac{\partial \xi_k}{\partial y_k}\Bigg|_{\textbf{y}} \\
&= \xi_k (\textbf{y})
\end{align*}
Note that the second equality hold true by definition of $\boldsymbol{\xi}$. We now see  that the matrix 
\begin{align*}
\operatorname{Hess}f^*= (d\boldsymbol{\xi})^t=d\boldsymbol{\xi}=(dF)^{-1}=(\operatorname{Hess}f)^{-1}
\end{align*}
always have determinant $\frac{1}{2}$. 
\end{proof}
\begin{question}{}{}
Let $C^1$ function $f:[0,\infty)\rightarrow \R$ satisfies: 
\begin{enumerate}[label=(\roman*)]
  \item $f(x)\geq 0$ and $f'(x)\leq 1$ for all $x \geq 0$. 
  \item $\int_0^{\infty}f(x)dx$ converges. 
\end{enumerate}
Does $f(x)$ converge as $x \rightarrow \infty$?   
\end{question}
\begin{proof}
  Yes. We will moreover show that $f(x)$ converge to $0$ as  $x\rightarrow \infty$. Assume for a contradiction that there exists some $\epsilon $ and $x_n\nearrow \infty$ such that $f(x_n)\geq  \epsilon $ for all $n$ and WLOG, $x_{n+1}-x_n \geq \epsilon $.  Fix $n$. Because $f$ is $C^1$ with derivative always smaller than $1$, by \textbf{MVT}, every $x \in [x_n-\epsilon ,x_n)$ must satisfies $[f(x_n)-f(x)]\quotient (x_n-x)\leq 1$. In other words, every $x \in [x_n-\epsilon ,x_n)$ satisfies 
\begin{align*}
\epsilon - (x_n-x)\leq f(x_n)-(x_n-x)\leq f(x)  
\end{align*}
We may now estimate 
\begin{align*}
  \int_{x_n-\epsilon }^{x_n} f(x)dx  \geq \int_{x_n-\epsilon }^{x_n} \epsilon - (x_n-x)dx =  \frac{\epsilon ^2}{2}
\end{align*}
This cause a contradiction: 
\begin{align*}
\int_0^{\infty} f(x)dx \geq \sum_n \int_{x_n-\epsilon }^{x_n}f(x)dx =\infty
\end{align*}
\end{proof}
\section{Year 112}
\begin{question}{}{}
Let 
\begin{align*}
M\triangleq \Bigg\{f:[0,\infty]\rightarrow [0,\infty)\Bigg| \int_0^{\infty}f^2(x)dx \leq 1\Bigg\}
\end{align*}
Evaluate 
\begin{align}
\label{fM}
\sup_{f\in M}\int_0^{\infty}f(x)e^{-x}dx
\end{align}
\end{question}
\begin{proof}
Note that $M$ is the space of nonnegative functions whose  $L^2$ norm is smaller than  $1$, and note that \myref{value}{fM} equals to $\sup \norm{e^{-x}f}_1$. By \textbf{Holder inequality}, definition of $M$, and direct computation, we see that for all $f \in M$,    
\begin{align*}
\norm{e^{-x}f}_1 \leq \norm{f}_2 \cdot \norm{e^{-x}}_2 \leq \norm{e^{-x}}_2= \frac{1}{\sqrt{2}}
\end{align*}
Because $\norm{e^{-x}}_2= 1 \quotient \sqrt{2}$, we know $\sqrt{2}e^{-x} \in M$. It is easy to compute that when $f=\sqrt{2}e^{-x}$, we have $\norm{e^{-x}f}_1=1 \quotient \sqrt{2}$.   Therefore \myref{value}{fM} equals to $1\quotient \sqrt{2}$. \\

Remark: The solution is (only) obvious if one have learned some real analysis.  
\end{proof}
\begin{question}{}{}
Let $a \in A \subseteq \R^n$, set $A$ be compact, and all convergent subsequences of the sequence $(a_n)\subseteq A$ converge to $a$. 
 \begin{enumerate}[label=(\roman*)]
   \item Does $(a_n)$ converge to  $a$? 
   \item If we remove the hypothesis of $A$ being compact, does  $(a_n)$ converge to $a$?  
\end{enumerate}
\end{question}
\begin{proof}
For (i), yes. Assume not for a contradiction. There exists some $\epsilon $ and subsequence $(a_{n_k})$ such that every $a_{n_k}$ is $\epsilon $-away from $a$. By \customref{Com}{definition of compact metric space}, there exists some convergent subsequence of $(a_{n_k})$, while being a subsequence of $(a_n)$, converges to a point other than $a$, a contradiction. For (ii), no. Let $A\triangleq [0,1)$ and $a_n$ be  $\frac{1}{n}$ if $n$ is even and  $1-\frac{1}{n}$ if $n$ is odd. 
\end{proof}
\begin{question}{}{}
Let  $A$ be some compact metric space, and let continuous $f:A\rightarrow A$ never maps two points strictly closer to each other. Show that $f$ is onto.    
\end{question}
\begin{proof}
Assume $a_0 \in A - f(A)$ for a contradiction, and define    $a_n\triangleq f^n(a_0)$. Because $f$ is continuous and $A$ is compact, the image $f(A)$ is compact. This implies the existence of some positive real $r$ smaller than the distance between $a_0$ and  $p$ for any  $p \in f(A)$. Because $a_n \in f(A)$ for all positive $n$, we know $d(a_n,a_0)\geq r$ for every positive $n$. Note that by induction, 
\begin{align*}
d(a_n,a_{n+1})=d(f(a_{n-1}),f(a_n))\geq d(a_{n-1},a_n)\geq r, \quad \text{ for all }n
\end{align*}
which by induction implies 
\begin{align}
\label{dan}
d(a_n,a_m)=d(f(a_{n-1}),f(a_{m-1}))\geq d(a_{n-1},a_{m-1})\geq r,\quad \text{ for all }0\leq n <m. 
\end{align}
The fact that $\set{a_n}$ is a sequence in $f(A)$ that satisfies \myref{inequality}{dan} contradicts to the \customref{Com}{sequential compactness} of $f(A)$.  
\end{proof}
\begin{question}{}{}
Define a sequence of function $\set{f_n(x)}$ on $[0,1]$ as: 
\begin{align*}
f_n(x)\triangleq \begin{cases}
  1& \text{ if $x=0$ }\\
  1& \text{ if $x\in (\frac{2k}{2^n}, \frac{2k+1}{2^n}],k \in \set{0,1,\dots ,2^{n-1}-1}$ }\\
  -1& \text{ if $x\in (\frac{2k+1}{2^n},\frac{2k+2}{2^n}],k \in \set{0,1,\dots ,2^{n-1}-1}$ }
\end{cases}
\end{align*}
Let  $g$ be a continuous function. Prove or disprove that $\lim_{n\to \infty} \int_{0}^1 f_ngdx$ always converge to $0$.  \\


\textbf{Remark.} The original question, as stated, lacks precision. It doesn't specify the domain of the function \( g \). If \( g \) is only defined on \( (0,1) \), then the product \( f_n g \) may fail to be integrable on $(0,1)$, making the expression \( \int_0^1 f_n g \, dx \) ill-defined—for instance, if \( g(x) = \frac{1}{x} \). At a minimum, the question should require \( g \in L^1(0,1) \), so that each \( f_n g \) is integrable on \( (0,1) \). With this added hypothesis, one can show that the sequence of integrals \( \int_0^1 f_n g \, dx \) always converges to zero, regardless of whether \( g \) has singularities at the endpoints. \\

I will just suppose $g$ is continuous on  $[0,1]$. 
\end{question}
\begin{proof}
Write
\begin{align*}
\int_0^1 f_ngdx= \sum_{k=0}^{2^{n-1}-1} \int_{\frac{2k}{2^n}}^{\frac{2k+1}{2^n}} g(x)- g(x+ \frac{1}{2^n}) dx 
\end{align*}
Because  \textbf{continuous function on a compact domain is uniformly continuous}, for all $\epsilon $, there exists some $N$ such that
\begin{align*}
\abso{g(x)-g(x+ \frac{1}{2^n})} \leq \epsilon ,\quad \text{ for all }n\geq N\text{ and }x \in [0,1-\frac{1}{2^n}]
\end{align*}
We may now estimate: 
\begin{align*}
\abso{\int_0^1f_ngdx}\leq \sum_{k=0}^{2^{n-1}-1} \int_{\frac{2k}{2^n}}^{\frac{2k+1}{2^n}} \abso{g(x)-g(x+\frac{1}{2^n})} dx \leq \sum_{k=0}^{2^{n-1}-1} \frac{\epsilon }{2^n}= \frac{\epsilon }{2}
\end{align*}
for all $n\geq N$. 
\end{proof}
\begin{question}{}{}
Let $P_2$ be the set of real polynomial with degree no greater than  $2$, and let $S\triangleq \set{p \in P_2: p(1)=1}$. Define $G:P_2\rightarrow \R$ by 
\begin{align*}
G(p)\triangleq \int_0^1 p^2(x)dx
\end{align*}
Does $G|_S$ have any extreme value? If it does, find them.\\ 

\textbf{Remark}: The original phrasing of the question is misleading. 
\end{question}
\begin{proof}
Because $F:\R^3 \rightarrow P_2,(a,b,c)\mapsto  ax^2+bx+c$ is surjective, the set on which $G|_S$ attains extreme values is exactly the image of the set on which $G\circ F|_{F^{-1}(S)}$ attains extreme values under $F$. We have transformed our \textbf{optimization problem} to the form in which the  \textbf{objective function} is:
\begin{align*}
G\circ F(a,b,c)= \frac{a^2}{5}+ \frac{ab}{2}+ \frac{2ac+b^2}{3}+ bc + c^2
\end{align*}
and there is only one \textbf{constraint function}, being:  
\begin{align*}
g(a,b,c)\triangleq a+b+c-1
\end{align*}
This is best solved using \textbf{Lagrange multiplier theorem for single constraint}. Note that there are lots of versions of Lagrange multiplier theorem for single constraint, and one of them requires: 
\begin{enumerate}[label=(\roman*)]
  \item Both objective and constraint functions are $C^1$.
  \item The dimension of codomain of the single constraint function is no greater than the dimension of its domain. 
\end{enumerate}
Moreover, Lagrange multiplier theorem can only detect local extremum on which the Jacobian of the constraint function is full rank. As harsh as these conditions seems, note that our case clearly satisfies both hypotheses and note that the Jacobian of our constraint function $g$ is full rank globally, so we don't need to worry about any of these conditions. \\

Compute: 
\begin{align*}
\nabla (G\circ F)(a,b,c)= \left(\frac{2a}{5}+\frac{b}{2}+\frac{2c}{3}, \frac{a}{2}+ \frac{2b}{3}+ c, \frac{2a}{3}+ b+2c  \right)
\end{align*}
Compute: 
\begin{align*}
\nabla g(a,b,c)= (1,1,1)
\end{align*}
We are now required to solve a system of linear equations with four unknown:  
\begin{align*}
\begin{cases}
  \frac{2}{5}a+ \frac{1}{2}b+ \frac{2}{3}c&= \ld \\
  \frac{1}{2}a + \frac{2}{3}b+c&=\ld    \\
  \frac{2}{3}a+ b+ 2c &= \ld \\
  a+b+c &= 1 
\end{cases}
\end{align*}
One may verify that there is only one solution, i.e., 
\begin{align*}
  (a,b,c)= \left(\frac{10}{3}, -\frac{8}{3}, \frac{1}{3} \right)\text{ and }\ld =\frac{2}{9}
\end{align*}
It is easy to see that this is a local minimum. Because the Jacobian of our constraint function $g$ is full rank globally, we know this local minimum is the only local extremum and also a global minimum. 
\end{proof}
\begin{question}{}{}
Let $C\inr^+$, function $f:[a,b]\rightarrow \R$ be proper Riemann integrable, and $g:\R\rightarrow \R$ be $C$-Lipschitz continuous. Prove that $g\circ f:[a,b]\rightarrow \R$ is proper Riemann integrable.\\ 

\textbf{Remark}: The original question write "Riemann integrable" instead of "proper Riemann integrable." The statement doesn't  hold true if we include improper Riemann integrable $f$. For example, let  $[a,b]\triangleq [0,1],f(x)\triangleq (\sin (1\quotient x))\quotient x$, and $g(x)\triangleq \abso{x}$. 
\end{question}
\begin{proof}
There are essentially two way to "answer" this question. One is to construct an honest bound. Another is to cheat by quoting \textbf{Lebesgue's criteria for proper Riemann integrability}. The honest bound proof for this question and the proof for Lebesgue's criteria for proper Riemann integrability are morally the same. To quote Lebesgue's criteria for proper Riemann integrability, one only have to prove that $g\circ f$ is continuous at $x\in [a,b]$ if $f$ is continuous at  $x$.  
\end{proof}
\section{Year 110}
\begin{question}{}{}
  Show that $C^1$ \textbf{proper} map  $f:\R^n\rightarrow \R^n$ (proper as preimage of compact subspace is compact) whose Jacobian is globally full rank is surjective. 
\end{question}
\begin{proof}
  Because $\R^n$ is connected,  to prove $f$ is surjective we only have to prove image of $f$ is clopen in  $\R^n$.  The image of $f$ is open in $\R^n$ by  \textbf{Inverse function theorem}. The image of $f$ is closed in  $\R^n$ is morally a topological result. In particular, we prove \myref{Theorem}{Pcmf}, which finish the proof.
\end{proof}
\begin{theorem}
\label{Pcmf}
\textbf{(Proper continuous map from locally compact Hausdorff space to Hausdorff space is closed.)} Given two locally compact Hausdorff topological spaces $X$ and $Y$, if preimage of compact set under continuous $f:X\rightarrow Y$ is always compact, then  $f$ as a map from  $X$ to  $Y$ is a closed map.  
\end{theorem}
\begin{proof}
Fix closed $A\subseteq X$ and $y \in \overline{f(A)}$. We are required to find $x \in A$ mapped to $y$. Because $y \in \overline{f(A)}$ and $Y$ is locally compact, WLOG, there exists some sequence $\set{x_n}\subseteq A$ and compact set $K\subseteq Y$  such that 
\begin{enumerate}[label=(\roman*)]
  \item $y$ and $\set{f(x_n)}$ are all contained by $K$. 
  \item $f(x_n)$ converges to $y$. 
\end{enumerate}
By premise, $f^{-1}(K)$ is compact. Because $A\cap f^{-1}(K)$ is closed in the  compact space $f^{-1}(K)$, we know $A\cap f^{-1}(K)$ is compact\footnote{Closed subset of compact space is compact. This is a standard topological proposition}. It now follows from $X$ being Hausdorff that  $A \cap f^{-1}(K)$ is closed\footnote{Compact subspace of Hausdorff space is closed. This is a standard topological proposition.}. Because $\set{x_n}\subseteq A \cap f^{-1}(K)$ and $A\cap f^{-1}(K)$ is compact Hausdorff, we know there exists some  subsequence $\set{x_{n_k}}$ that converges to some $x'\in A \cap f^{-1}(K)$.\footnote{This is a standard topological proposition, generalized of \textbf{Bolzano-Weierstrass Theorem}.} Because $f$ is continuous, we know  $f(x_{n_k})$ converges to $f(x')$. It now follows from $Y$ being Hausdorff that  $y=\lim f(x_{n_k})=f(x')$. We have shown this $x'$ suffices. 
\end{proof}
\begin{question}{}{}
Let $\textbf{F}:\R^3 \rightarrow \R^3$ be the identity map, and let $M\triangleq \set{(x,y,z)\inr^3: x^2+y^2 \leq z \leq \sqrt{2-x^2+y^2}}$ be positively oriented, so the normal vector field point "outward."\\

Compute:
\begin{align*}
\int_{\partial M} \textbf{F} \cdot \textbf{n} dA
\end{align*}
\end{question}
\begin{question}{}{}
Let $M$ be a metric space with countable elements. Prove or disprove that $M$ is connected. 
\end{question}
\begin{proof}
If $M$ has only one element, then  $M$ is trivially connected. We now prove that if  countable $M$ has more than one element then  $M$ is disconnected. Let $x\in M$. Because $\set{d(y,x)\inr^+: y \in M}$ is countable and $(0,r)\subseteq \R^+$ isn't for any $r\inr^+$,  there exists some  $r$ such that $d(y,x)\neq r$ for all $r$. The proof then follows from showing $B_r(x)$ is closed.  
\end{proof}
\begin{question}{}{}
Define $f:\R\rightarrow \R$ by $f(x)\triangleq \frac{1}{4}+x-x^2$. 
\begin{enumerate}[label=(\roman*)]
  \item Show that $\set{f^n(0)}$ converges, and find its limit. 
  \item Find all $x\inr$ that make $\set{f^n(x)}$ converge to $\lim f^n(0)$. 
\end{enumerate}
\end{question}
\begin{proof}
We will show $f^n(0)\rightarrow 1\quotient 2$ by proving 
\begin{align*}
f^n(0)= \frac{2^{2^n-1}-1}{2^{2^n}},\quad \text{ for all }n\inn
\end{align*}
via induction. The base case is clear. The proof for inductive case is also clear from computing:  
\begin{align*}
  \frac{1}{4}+  \frac{2^{2^n-1}-1}{2^{2^n}}- \left( \frac{2^{2^n-1}-1}{2^{2^n}} \right)^2 &= \frac{1}{4}+ \frac{2^{2^n-1}-1}{2^{2^n}}- \frac{(2^{2^n-1}-1)^2}{2^{2^{n+1}}}  \\
&= \frac{2^{2^{n+1}-2}+ 2^{2^n}(2^{2^n-1}-1)-(2^{2^n-1}-1)^2}{2^{2^{n+1}}} \\
&=\frac{2^{2^{n+1}-2}+ 2^{2^{n+1}-1}-2^{2^n}-2^{2^{n+1}-2}+ 2^{2^n}-1}{2^{2^{n+1}}}= \frac{2^{2^{n+1}-1}-1}{2^{2^{n+1}}}
\end{align*}

\end{proof}
\begin{question}{}{}
Let $X$ consist of all real-valued function $f$ on  $[0,1]$ such that $f(0)=0$ and 
 \begin{align*}
\norm{f}\triangleq \sup_{x\neq y} \frac{\abso{f(x)-f(y)}}{\abso{x-y}^{\frac{1}{3}}} \quad \text{ is finite. }
\end{align*}
Show that $X$ forms a vector space,  $\norm{\cdot}$ forms a norm on $X$, and  $X$ is complete with respect to this norm. 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $f:(a,b)\rightarrow \R$ be differentiable. Show that $f':(a,b)\rightarrow \R$ has no jump discontinuity. 
\end{question}
\begin{proof}

\end{proof}
\section{Year 109}
\begin{question}{}{}
Does there exists real sequence $\set{a_k}$ that makes $\sum_{k=2}^{\infty}a_k \sin (kx)$ converges uniformly to $\sin (x)$ on $[0,\pi ]$? 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $f:\R\rightarrow \R$ be uniformly continuous. Show that $f$ must be bounded by a linear function, i.e., there exists constant $A,B \inr$ such that 
\begin{align*}
\abso{f(x)}\leq A+ B\abso{x},\quad \text{ for all }x\inr
\end{align*}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $M,N$ be two metric spaces, and let $f:M\rightarrow N$ be a function. 
 \begin{enumerate}[label=(\roman*)]
  \item Given the hypothesis that for each convergent sequence $\set{x_n}\subseteq M$, the sequence $\set{f(x_n)}\subseteq N$ also converges, is $f$ continuous? 
  \item Suppose the function $g:M\rightarrow \R$ makes $E\triangleq \set{(x,g(x))\in M\times \R:x\in M}$ compact in the product topology. Is $g$ continuous? 
\end{enumerate}
\end{question}
\begin{proof}
The answers for both questions are positive. Assume for a contradiction that $f$ is discontinuous at some $x \in M$. This implies the existence of some $\epsilon $ such that for all $\delta$, we have $f(B_\delta(x))\not \subseteq B_\epsilon (f(x))$. For all $n\inn$, fix $x_n \in B_{\frac{1}{n}}(x)$ that satisfies $f(x_n)\not\in B_\epsilon (f(x))$. We now see that the image of the convergent sequence $(x_1,x,x_2,x,x_3,x,\dots) $  doesn't converge, a contradiction.\\

Before we prove the continuity of $g$, we need to first make three necessary topological remarks: 
 \begin{enumerate}[label=(\alph*)]
  \item $g(M)$ is compact because $E$ is compact and the projection $M\times \R\rightarrow \R$ is continuous by definition of product topology.   
  \item $M\times \R$ is Hausdorff because it is the product of two Hausdorff spaces. Therefore $E$ as a compact subspace of $M\times \R $ is closed in $M\times \R$.  
  \item Product topology have the property that if $y_n \rightarrow y$ in topological space $Y$ and  $z_n \rightarrow z$ in topological space $Z$, then $(y_n,z_n)\rightarrow (y,z)$ in $Y\times Z$. 
\end{enumerate}
We may now prove the continuity of $g$ using the result from question (i). Suppose $x_n\rightarrow x$ in $M$. We are required to prove $g(x_n)\rightarrow g(x)$. Assume not for a contradiction: There exists some $\epsilon $ and subsequence such that $g(x_{n_k})$ are all $\epsilon$-away from $g(x)$.  Because $g(M)$ is \customref{Com}{sequentially compact}, there exists some $y \in M$ and some convergent subsequence $g(x_{n_{k_l}})\rightarrow g(y)$. Because $x_n\rightarrow x$, we now have $(x_{n_{k_l}},g(x_{n_{k_l}}))\rightarrow (x,g(y))$. Moreover, this by closedness of $E$ implies  $(x,g(y))\in E$, which implies $g(y)=g(x)$, a contradiction to the assumption that $g(x_{n_k})$ are all $\epsilon $-away from $g(x)$.  




\end{proof}
\begin{question}{}{}
Let $L:\R^3 \rightarrow \R^3$ be an invertible linear map, and let function $g:\R^3 \rightarrow \R^3$ be $C^1$ while satisfying: 
 \begin{align*}
\text{ For some }\epsilon ,\abso{g(x)}\leq C \abso{x}^{1+\epsilon }\text{ for all }x\inr^3
\end{align*}
Show that $L+g:\R^3 \rightarrow \R^3$ is invertible near the origin.  
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Find the volume of the ellipsoid: 
\begin{align*}
  (x+2y)^2+ (x-2y+z)^2 +3z^2 \leq 1
\end{align*}
Let $C\subseteq \R^2$ be a positively oriented simple closed curve. Find the curve $C$ that maximize the integral: 
\begin{align*}
\int_C y^3 dx+ (3x-x^3)dy
\end{align*}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $f:[0,1]\rightarrow \R$ be continuous. Find the limit 
\begin{align*}
\lim_{n\to \infty}(n+1)\int_0^1 x^nf(x)dx
\end{align*}
\end{question}
\begin{proof}

\end{proof}
\section{Year 108}
\begin{question}{}{}
Does the series
\begin{align*}
\sum_{n=1}^{\infty} \frac{(-1)^n \ln (n+1)}{n}
\end{align*}
converge? Does it absolutely converge?  
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Define $\Omega \triangleq  \set{(x,y)\inr^2 :0\leq x,y\leq 1}- \set{(1,1)}$, and define $f:\Omega \rightarrow \R$ by 
\begin{align*}
f(x,y)\triangleq \frac{1}{(1-xy)^2}
\end{align*}
\begin{enumerate}[label=(\roman*)]
  \item For any $\kappa \in (0,1)$, let $U_\kappa\triangleq \set{(x,y)\in^2: 0\leq x,y\leq \kappa}$. Is $f$ uniformly continuous on  $U_\kappa$ ?   
  \item Is $f$ uniformly continuous on $\Omega$? 
\end{enumerate}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $I\triangleq [0,1]$, and define $f_n:I\rightarrow \R$ for all $n\inn$ by $f_n(x)\triangleq nx^n(1-x)$. 
\begin{enumerate}[label=(\roman*)]
  \item Determine $\lim f_n(x)$ for all $x\in I$. 
  \item Is the convergence uniform on $I$? 
\end{enumerate}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Define 
\begin{align*}
F(x)\triangleq \int_0^{\infty} \frac{1-\cos (xt)}{t^2e^t}dt
\end{align*}
\begin{enumerate}[label=(\roman*)]
  \item Can we switch the order of differentiation and integration to obtain formulas for $F'(x)$ and $F''(X)$? 
  \item Find explicit formulas (Not an improper integral) for $F'(x)$ and $F''(x)$.  
\end{enumerate}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Consider the function $F:\R^4\rightarrow \R^2$ defined by 
\begin{align*}
F(x,y,u,v)\triangleq \left(\int_{x-y^2}^{x^2+y}(e^{t^2}+u)dt,x^3+v \right)
\end{align*}
\begin{enumerate}[label=(\roman*)]
  \item Prove that near $(1,1,0,0)$, the two equations $F(x,y,u,v)=(\int_0^2 e^{t^2}dt,1)$ can be solved for $u,v$ as  $C^1$ functions of  $x,y$.  
  \item Find the first order partial derivatives of the functions $u(x,y)$ and $v(x,y)$ in part (i). 
\end{enumerate}
\end{question}
\begin{proof}

\end{proof}
\section{Year 107}
\begin{question}{}{}
Evaluate the integral
\begin{align*}
\int^{\frac{5\pi }{4} }_{\frac{\pi }{4}} \frac{1}{1+3 \sin^2(x)}dx
\end{align*}
Hint: Let $u=\tan x$. 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
State and prove Leibniz criterion for convergence of alternating series. 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $p \inr^2$ and let smooth $F:\R^2\rightarrow \R$ satisfies: 
\begin{align*}
  &\frac{\partial F}{\partial x}\Bigg|_p=\frac{\partial F}{\partial y}\Bigg|_p=0\\
  &\frac{\partial^2 F}{\partial x^2}\Bigg|_p>0   \\
  &\left( \frac{\partial^2 F}{\partial x^2} \cdot \frac{\partial ^2 F}{\partial y^2} - \frac{\partial^2 F}{\partial x\partial y}\right) \Bigg|_p >0
\end{align*}
Show that $p$ is a local minimum of  $F$. 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $X$ be a metric space. We say a  sequence  $(f_n:X\rightarrow \R)$ of real-valued functions \textbf{converges compactly} if for every compact $K \subseteq X$, the sequence $(f_n|_K:K\rightarrow \R)$ converges uniformly. \\

Show that if a pointwise equicontinuous sequence $(f_n:X\rightarrow \R)$ of real-valued functions pointwise converge to some continuous function $f:X\rightarrow \R$, then the convergence is compact.  
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Compute the outward flux of the vector field $(x+ye^z,e^x \sin (yz),ye^{zx})$ through the boundary of the region $\set{(x,y,z)\inr^3: (\sqrt{x^2+y^2}-3 )^2+z^2<1}$. 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Show that the function 
\begin{align*}
f(x)\triangleq \sum_{n=1}^{\infty}  \frac{\cos (ne)}{n^x}
\end{align*}
is well-defined and continuous on $(0,\infty)$. 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $f,g:\R^2 \rightarrow \R$ be smooth, and define $S\triangleq \set{(x,y)\inr^2: f(x,y)=0}$. Suppose $p \in S$ satisfies 
\begin{align*}
\nabla f|_p= (-1,2)\\
\nabla g |_p = (3,-6) \\
\begin{pmatrix} 
  \frac{\partial ^2 f}{\partial x^2} & \frac{\partial ^2 f}{\partial x \partial y} \\
  \frac{\partial ^2 f}{\partial y \partial x} & \frac{\partial ^2 f}{\partial y^2}
\end{pmatrix} \Bigg|_p = \begin{pmatrix} 
 1 & 3 \\
 3 & 0
\end{pmatrix}  \\
\begin{pmatrix} 
  \frac{\partial ^2 g}{\partial x^2} & \frac{\partial ^2 g}{\partial x \partial y} \\
  \frac{\partial ^2 g}{\partial y \partial x} & \frac{\partial ^2 g}{\partial y^2}
\end{pmatrix} \Bigg|_p = \begin{pmatrix} 
 3 & -1 \\
 -1 & 2
\end{pmatrix}  \\
\end{align*}
Show that $p$ is a local minimum for  $g|_S$. 
\end{question}
\begin{proof}

\end{proof}
\section{Year 106}
\begin{question}{}{}
Let $M$ be the unit ball  $B_1(0)$ in $\R^3$. Compute 
 \begin{align*}
\int_M \cos (x+y+z)dxdydz
\end{align*}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $f:[-\pi ,\pi ]\rightarrow \R$ be proper Riemann integrable, and define its Fourier coefficients by 
\begin{align*}
  a_n&\triangleq \frac{1}{\pi }\int_{-\pi }^{\pi } f(x)\cos (nx)dx,\quad \text{ for all }n \in \Z_0^+ \\
  b_n &\triangleq \frac{1}{\pi }\int_{-\pi }^{\pi }f(x)\sin (nx)dx,\quad \text{ for all }n\inn
\end{align*}
\begin{enumerate}[label=(\roman*)]
  \item Show that $f^2:[-\pi ,\pi ]$ is proper Riemann integrable. 
  \item Show that the series $\sum a_n^2 +b_n^2$ converges. 
\end{enumerate}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
\begin{enumerate}[label=(\roman*)]
  \item Let $\set{a_n}$ and $\set{b_n}$ be two sequences of real numbers, and define $B_n\triangleq b_1 + \cdots +b_n$. Show that if $a_n\searrow 0$ and  $\set{B_n}$ is bounded, then the series $\sum a_nb_n$ converges. 
  \item Show that the function series: 
    \begin{align*}
    \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n} \sin (nx)
    \end{align*}
    converges uniformly on $[-\pi +\epsilon ,\pi -\epsilon ]$. 
\end{enumerate}
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $(F_n:[-2,2]\rightarrow \R)$ be a uniformly bounded sequence of convex functions. Show that it has a subsequence that converges uniformly on $[-1,1]$. 
\end{question}
\begin{proof}

\end{proof}
\begin{question}{}{}
Let $U\subseteq \R^n$ be open, let $C^1$ function $f:U\rightarrow \R^n$ has globally full rank Jacobian, let $V \subseteq \R^n$ be open, and let continuous $g:V\rightarrow U$ satisfies $f\circ g(x)=x$ for all $x\in V$. Show that $g$ is also $C^1$. 
\end{question}
\begin{proof}

\end{proof}
\end{document}
