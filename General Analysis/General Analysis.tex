\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}

\title{\Huge{NCKU 112.2}\\
General Analysis}
\author{\huge{Eric Liu}}
\date{}
\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}

\tableofcontents
\pagebreak

\chapter{General Topology}
\section{Equivalent Characterizations of Basic Notions}
\begin{abstract}
This section give a compact and comprehensive development of some of the most basic notions in the study of topology. In this section, $(X,\mathscr{T })$ is a topological space.
\end{abstract}
\begin{mdframed}
Given a collection $\mathcal{B}\subseteq \mathscr{T }$ of open sets, we say $\mathcal{B}$ is a  
\begin{enumerate}[label=(\alph*)]
  \item  \textbf{basis} if for each $O\in \mathscr{T }$ there exists a subcollection $\mathcal{B}_0$ such that $O\subseteq \mathcal{B}_0$.
  \item \textbf{subbase} if  $\mathscr{T }$ is the collection of unions of finite intersections of $\mathcal{B}$. In a more formal language, $\mathcal{B}$ has to satisfy $\mathscr{T }=\set{\bigcup A:A\subseteq \mathcal{A}}$ where $\mathcal{A}=\set{\bigcap \mathcal{B}_0: \text{ card }\mathcal{B}\inn\text{ and }\mathcal{B}_0 \subseteq \mathcal{B}}$ 
\end{enumerate}
\end{mdframed}

\begin{theorem}
  \textbf{(Equivalent Definition of Basis)} The following statements are equivalent.
\begin{enumerate}[label=(\alph*)]
  \item $\mathcal{B}$ is a basis.
  \item For all $O\in \mathscr{T }$ and $x\in O$, there exists $B \in \mathcal{B}$ such that $x \in B \subseteq O$.
\end{enumerate}
\end{theorem}
\begin{proof}
Check straightforward.
\end{proof}
\begin{theorem}
\textbf{(Equivalent Definition of Subbase)} The following statements are equivalent. 
\begin{enumerate}[label=(\alph*)]
  \item $\mathcal{B}$ is a subbase of $\mathscr{T }$. 
  \item $\mathcal{B}$ cover $X$ and $\mathscr{T }$ is the smallest topology containing $\mathcal{B}$. 
\end{enumerate}
\end{theorem}
\begin{proof}
Check straightforward. 
\end{proof}
\begin{mdframed}
Immediately, with the equivalent definitions, one should check
\begin{enumerate}[label=(\alph*)]
    \item Given any cover $\mathcal{B}$ of $X$, there always exists a unique topology $\mathscr{T}$ containing $\mathcal{B}$ as a subbase. We say $\mathscr{T}$ is the \textbf{topology generated by} $\mathcal{B}$.
    \item The set $\mathcal{A} \triangleq \set{ \bigcap S : S \subseteq \mathcal{B}, \text{ card }S \in \mathbb{N} }$ of finite intersections of cover $\mathcal{B}$ is a basis of the topology generated by $\mathcal{B}$.
    \item Not every cover $\mathcal{B}$ of $X$ has some topology $\mathscr{T}$ containing $\mathcal{B}$ as a basis. Consider $\mathcal{B} \triangleq \set{ (-\infty, a) : a \in \mathbb{R} } \cup \set{ (b, \infty) : b \in \mathbb{R} }$. Even the smallest topology containing $\mathcal{B}$, i.e. the standard topology, does not have $\mathcal{B}$ as a basis.
    \item However, cover $\mathcal{B}$ is the basis of the topology $\mathscr{T}$ generated by itself if for all $B_1, B_2 \in \mathcal{B}$ and $x \in B_1 \cap B_2$, there exists $B_3$ such that $x \in B_3 \subseteq B_1 \cap B_2$.
    \item If $\mathcal{B}$ is a basis of $\mathscr{T}$, then $\mathcal{B}$ is also a subbase of $\mathscr{T}$.
    \item Basis is not necessarily closed under finite intersection. Consider the basis $\set{(a, a + \frac{1}{n}) : a \in \mathbb{R}, n \in \mathbb{N}}$ for $\mathbb{R}$'s standard topology.
\end{enumerate}

Note that in (a), to check the generated $\mathscr{T}$ is indeed a topology, one may need to utilize the identity
\begin{align*}
\Big( \bigcup_{i \in I} A_i \Big) \cap \Big( \bigcup_{j \in J} B_j \Big) = \bigcup_{i \in I, j \in J} A_i \cap B_j.
\end{align*}
Now, given an arbitrary subset $E\subseteq X$, we 
\begin{enumerate}[label=(\alph*)]
  \item say $x\in X$ is a \textbf{limit point of $E$} if every open $O$ containing $x$ contain a point $y\in E$ such that $y \neq x$.
  \item say $x\in E$ is an \textbf{interior point of $E$} if there exists $O\in \mathscr{T }$ such that $x \in O \subseteq E$.
  \item define the \textbf{interior $E^\circ $ of $E$} to be the union of all open sets contained by  $E$.
   \item say $E\subseteq X$ is a \textbf{closed set} if $E^c \in \mathscr{T }$.
   \item define the \textbf{closure $\overline{E}$ of $E$} by $\overline{E}\triangleq E\cup E'$ where $E'$ is the set of limit points of $E$. 
    \item say $E$ is \textbf{dense} in $X$ if  $\overline{E}=X$.
    \item define the \textbf{boundary $\partial E$ of $E$} by $\partial E\triangleq \overline{E}\setminus E^\circ $
\end{enumerate}
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definitions of Interior)} The following sets are equivalent
\begin{enumerate}[label=(\alph*)]
  \item $E^\circ $
  \item The largest open set contained by $E$.
  \item The set of interior points of $E$.
\end{enumerate}
\end{theorem}
\begin{proof}
Check straightforward.
\end{proof}
\begin{theorem}
\textbf{(Equivalent Definitions of Closed)} The following statements are equivalent.
\begin{enumerate}[label=(\alph*)]
  \item $E$ is closed.
  \item the set of limit points of $E$ is contained by $E$.
  \item $\overline{E}=E$.
\end{enumerate}
\end{theorem}
\begin{proof}
The proof of $(\text{a})\implies (\text{b})\implies (\text{c})$ are straight forward. The proof of $(\text{c})\implies (\text{a})$ follows from first noting no  $x\in E^c$ is a limit point of $E$. Then shows $E^c = \bigcup_{x \not \in E}O_x$ where $O_x$ is an open set containing $x$ and disjoint with $E$.
\end{proof}
\begin{theorem}
\textbf{(Equivalent Definitions of Closure)} The following sets are equivalent.
\begin{enumerate}[label=(\alph*)]
  \item $\overline{E}$
  \item $((E^c)^\circ )^c$ 
  \item The smallest closed set containing $E$.
  \item $\set{x\in X:\text{ every open $O$ containing $x$ intersect with $E$ }}$
\end{enumerate}
\end{theorem}
\begin{proof}
$(\text{a})=(\text{d})$ is obvious. To verify $(\text{a})=(\text{c})$, check $(\overline{E})' \subseteq E'$ and check $E'\subseteq F'\subseteq F$ for each closed $F$ containing $E$. Lastly, to verify $(\text{b})=(\text{c})$, check $(\overline{E})^c=(E^c)^\circ $ using the largest open set and the smallest closed set characterization of interior and closure.
\end{proof}
\begin{theorem}
\textbf{(Equivalent Definitions of Dense)} The following statements are equivalent. 
\begin{enumerate}[label=(\alph*)]
  \item $E$ is dense in $X$. 
  \item Every non-empty open set intersect with $E$. 
  \item $(E^c)^\circ =\varnothing$
\end{enumerate}
\end{theorem}
\begin{proof}
$(\text{a})=(\text{c})$ follows from $\overline{E}=((E^c)^\circ )^c$, and $(\text{a})=(\text{b})$ follows from $\overline{E}=\set{x\in X:\text{ every open }O\text{ containing }x\text{ intersect with }E}$.
\end{proof}
\begin{theorem}
\textbf{(Equivalent Definitions of Boundary)} The following sets are equivalent.
\begin{enumerate}[label=(\alph*)]
  \item $\partial E$
  \item $\overline{E}\cap \overline{E^c}$
  \item $\set{x \in X:\text{ every open $O$ containing $x$ intersect with both $E$ and $E^c$ }}$
\end{enumerate}
\end{theorem}
\begin{proof}
$(\text{a})=(\text{b})$ follows from $(E^\circ )^c = \overline{E^c}$ and $(\text{b})=(\text{c})$ follows from $\overline{E}=\set{x\in X:\text{ every open $O$ containing  $x$ intersect with $E$}}$. 
\end{proof}
\begin{mdframed}
We now develop the theory of continuity by first giving a pointwise definition. Given another topological space $(Y,\mathscr{S})$ and a function $f:X\rightarrow Y$, we say $f$ is \textbf{continuous at $x\in X$} if for all open $O$ containing $f(x)$, there exists open $E$ containing $x$ such that $f(E)\subseteq O$. We say $f$ is a \textbf{continuous (or $(\mathscr{T },\mathscr{S })$-continuous, if necessary) function} if $f$ is continuous at all $x\in X$. \\

It is easy to see the composition of two continuous function must be continuous. However, one should notice that the composition of a continuous function and a discontinuous function can be continuous. Just let one of them be a constant function.
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definitions of Continuous function)} The following are equivalent 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous.
  \item  $f^{-1}(O)\in \mathscr{T }$ for all $O\in \mathscr{S }$.
  \item $f^{-1}(F)$ is closed for all closed $F$ in $Y$. 
  \item For all $B\subseteq Y$, $f^{-1}(B^\circ )\subseteq (f^{-1}(B))^{\circ }$
  \item For all $A\subseteq X$, $f(\overline{A})\subseteq \overline{f(A)}$. 
  \item For all $B\subseteq Y,\overline{f^{-1}(B)}\subseteq f^{-1}(\overline{B})$
  \item For all subbase $\mathcal{B}$ of $Y$, $f^{-1}(B)\in \mathscr{T }$ for all $B \in \mathcal{B}$.
\end{enumerate}
\end{theorem}
\begin{proof}
It is straightforward to check  $(\text{a})\implies (\text{b})\implies (\text{c})$. To verify $(\text{c})\implies (\text{a})$, check $x\in (f^{-1}(O^c))^c$ and $f((f^{-1}(O^c))^c)\subseteq )$ for each $x \in X$ and $O \in \mathscr{S}$ containing $f(x)$. Respectively, to verify $(\text{b})\implies (\text{d}),(\text{c})\implies (\text{e})\text{ and }(\text{c})\implies (\text{f})$, check  $f^{-1}(B^\circ )\subseteq f^{-1}(B),A\subseteq f^{-1}(\overline{f(A)})\text{ and }f^{-1}(B)\subseteq f^{-1}(\overline{B})$. Note that $(\text{e})\implies (\text{f})$ follows from noting $A=f^{-1}(B)$. Check $(\text{d})\implies (\text{b})$ and $(\text{f})\implies (\text{c})$ straightforwardly, and we have proved the equivalency of statements from (a) to (f). Lastly, check $(\text{b})\iff (\text{g})$ straightforwardly.  
\end{proof}
\begin{mdframed}
One may wonder: Why isn't "For all $A \subseteq X$, $f(A)^\circ  \subseteq f(A^\circ )$" a characterization of $f$ being continuous? Consider a function that maps some topological space with a subset that has an empty interior into the topological space $Y$ having only a single point.
\end{mdframed}
\section{Equivalent Definition of Subspace and Product}
\begin{abstract}

\end{abstract}
\begin{theorem}
\textbf{(Equivalent Definition of Finer/Coarser Topologies)} Given another topology $\mathscr{T}'$ on $X$, the following are equivalent:
\begin{enumerate}[label=(\alph*)]
    \item $\mathscr{T} \subseteq \mathscr{T}'$.
    \item $\textbf{id} : (X, \mathscr{T}') \to (X, \mathscr{T})$ is continuous.
    \item Given any basis $\mathcal{B}$ of $\mathscr{T}$ and any basis $\mathcal{B}'$ of $\mathscr{T}'$, for all $x \in X$ and basic open $B \in \mathcal{B}$ containing $x$, there exists a basic open $B' \in \mathcal{B}'$ such that $x \in B' \subseteq B$.
    \item There exists subbases $\mathcal{B}, \mathcal{B}'$ of $\mathscr{T}, \mathscr{T}'$ such that $\mathcal{B} \subseteq \mathcal{B}'$.
\end{enumerate}
\end{theorem}
\begin{proof}
$(\text{d})\iff (\text{a})\iff (\text{b})$ are straightforward. From (a) to (c), one finds $B'$ by noting $B \in \mathscr{T}'$ and utilizing the definition of basis. From (c) to (a), one shows $O \in \mathscr{T}$ belongs to $\mathscr{T}'$ by taking $O = \bigcup_{x \in O} B'_x$.
\end{proof}
\begin{mdframed}
Now, given a collection $(X_\alpha )_{\alpha \in J}$ of topological spaces, we define the \textbf{product topology} on $X \triangleq \prod_{\alpha \in J} X_\alpha$ to be the smallest topology such that for all $\alpha \in J$, the projection $\pi_\alpha : X \to X_\alpha$ that maps $(y_\alpha)_{\alpha \in J}$ to $y_\alpha$ is continuous.
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definition of Product Topology)} Let $\mathcal{B}_\alpha $ each be the subbase of $X_\alpha $. The following topologies are equivalent.
\begin{enumerate}[label=(\alph*)]
    \item The product topology on $X$.
    \item The topology on $X$ generated by the basis $\set{\prod U_\alpha : U_\alpha \neq X_\alpha \text{ for finitely many } \alpha \text{ and } U_\alpha \in \mathcal{B}_\alpha}$.
    \item The smallest topology on $X$ satisfying the statement: For all topological spaces $(Z, \mathscr{T}_Z)$, a function $f : Z \to X$ is continuous if and only if for all $\alpha \in J$, the function $f \circ \pi_\alpha : Z \to X_\alpha$ is continuous.
\end{enumerate}
\end{theorem}

\begin{proof}
By definition, we know $X$ has the subbase $\bigcup_{\alpha \in J} \mathcal{G}_\alpha$ where $\mathcal{G}_\alpha \triangleq \set{\pi_\alpha^{-1}(U) : U \in \mathscr{T}_\alpha}$. This gives us (a) = (b). For (a) = (c), we have to prove that: Given a topology on $X$, the following two statements are equivalent:
\begin{enumerate}[label=(\roman*)] 
    \item All projections $\pi_\alpha$ are continuous.
    \item For all topological spaces $(Z, \mathscr{T}_Z)$ and functions $f : Z \to X$, the function $f$ is continuous if and only if for all $\alpha \in J$, the function $f \circ \pi_\alpha : Z \to X_\alpha$ is continuous.
\end{enumerate}
Taking $Z \triangleq X$ and $f \triangleq \textbf{id}$ proves (ii) $\implies$ (i). For (i) $\implies$ (ii), if $f$ is continuous, then $f \circ \pi_\alpha$ is clearly continuous for all $\alpha$. Conversely, if $f \circ \pi_\alpha$ is continuous for all $\alpha$, use the pointwise definition of continuity and the basis given in (b) to show that $f$ is continuous.
\end{proof}
\begin{mdframed}
Immediately, one should check:
\begin{enumerate}[label=(\alph*)]
    \item In $X$, a sequence $(p_n)$ converges to $p$ if and only if $\pi_\alpha(p_n)$ converges to $\pi_\alpha(p)$ for all $\alpha \in J$. This generalize what happen in $\R^n$. 
    \item If $f : X \times Y \to Z$ is continuous, then for all $x \in X$, the function $f(x,\cdot) : Y \to Z$ defined by $f(x,\cdot)(y) \triangleq f(x, y)$ is continuous. The converse is not true, in the sense that $f$ can be discontinuous even if $f(x,\cdot)$ and $f(\cdot, y)$ are continuous for all $x$ and $y$. Elementary
counterexample can be constructed in $\R^2$.
    \item The third characterization of product topology shows that the product is independent of expression. For example, given an enumeration $(X_\alpha)_{\alpha \leq \gamma}$ of topological spaces, $X_1 \times \prod_{1 < \alpha \leq \gamma} X_\alpha$ is homeomorphic to $\prod_{1 \leq \alpha \leq \gamma} X_\alpha$.
\end{enumerate}
We will later introduce an inferior alternative to assigning topology onto the Cartesian product of topological spaces. Now, given a topological space $(X, \mathscr{T})$ and a subset $E \subseteq X$, we define the \textbf{subspace topology} $\mathscr{T}_E$ on $E$ by $\mathscr{T}_E \triangleq \set{O \cap E : O \in \mathscr{T}}$. Immediately, one can check that $\mathscr{T}_E$ is indeed a topology, and:
\begin{enumerate}[label=(\alph*)]
    \item Given a subset $F \subseteq E$, viewing $F$ as a subspace of $E$ or $X$ makes no difference.
    \item The collection of closed sets in $(E, \mathscr{T}_E)$ is $\set{F \cap E : F \text{ is a closed set in } (X, \mathscr{T})}$.
    \item For all $F \subseteq X$, $\operatorname{cl}_E(F \cap E) \subseteq \operatorname{cl}_X(F) \cap E$. The equality holds when $F \subseteq E$.
  \item Given a function $f : X \to Y$ and a subset $F$ of $Y$ containing $f(X)$, $f : X \to Y$ is continuous at $p$ if and only if $f : X \to F$ is continuous at $p$.
    \item Given a function $f : X \to Y$ and $p \in E \subseteq X$, if $f$ is continuous at $p$, then $f|_E : E \to Y$ is continuous at $p$. The converse is true only when $E$ is open in $X$.
  \item Given a finite collection of closed subspace $E_j$ such that  $X=\bigcup E_j$, if $f|_{E_j}:E_j\rightarrow Y$ are all continuous, then $f:X\rightarrow Y$ is continuous.
\end{enumerate}
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definition of Subspace Topology)} Given a basis $\mathcal{B}$ and a subbase $\mathcal{B}'$ of $\mathscr{T}$, the following sets are equivalent:
\begin{enumerate}[label=(\alph*)]
    \item $\mathscr{T}_E$. 
    \item The topology on $E$ generated by the basis $\mathcal{B}_E \triangleq \set{B \cap E : B \in \mathcal{B}}$. 
    \item The topology on $E$ generated by the subbase $\mathcal{B}'_E \triangleq \set{B' \cap E : B' \in \mathcal{B}'}$. 
    \item The smallest topology on $E$ such that the inclusion map $\iota : E \to X$ is continuous.
\end{enumerate}
\end{theorem}

\begin{proof}
Check straightforward.
\end{proof}
\begin{mdframed}
At this point, one should check the compatibility between the definitions of subspace topology and product topology. Given a collection $(X_\alpha)_{\alpha \in J}$ of topological spaces and a subspace $(A_\alpha)_{\alpha \in J}$ of each $X_\alpha$, one can view $A \triangleq \prod A_\alpha$ either as a subspace of the product $X \triangleq \prod X_\alpha$ or the product of subspaces $(A_\alpha)_{\alpha \in J}$. The two topologies are identical, and the proof goes as follows:
\begin{enumerate}[label=(\alph*)]
    \item Show that the product topology has the subbase $\set{\pi_{\alpha,A}^{-1}(U_\alpha) : \alpha \in J, U_\alpha \in \mathscr{T}_{A_\alpha}}$, where $\pi_{\alpha,A} : A \to A_\alpha$ is the projection mapping.
    \item Show that the subspace topology has the subbase $\set{\pi_{\alpha,X}^{-1}(U_\alpha) \cap A : \alpha \in J, U_\alpha \in \mathscr{T}_{X_\alpha}}$, where $\pi_{\alpha,X} : X \to X_\alpha$ is the projection mapping.
    \item Show that $\set{\pi_{\alpha,A}^{-1}(U_\alpha) \subseteq A : \alpha \in J, U_\alpha \in \mathscr{T}_{A_\alpha}} = \set{\pi_{\alpha,X}^{-1}(U_\alpha) \cap A : \alpha \in J, U_\alpha \in \mathscr{T}_{X_\alpha}}$.
\end{enumerate}
\end{mdframed}
\section{Equivalent Definition of Compactness and Connectedness}
\begin{abstract}

\end{abstract}
\begin{mdframed}
We now give definitions to three notions so important that they drive us to study Topology in the first place. Given a topological space $(X, \mathscr{T})$, we say nonempty $E \subseteq X$ is

\begin{enumerate}[label=(\alph*)]
    \item \textbf{connected} if $E$ can not be written as $E = A \cup B$ so that $\overline{A} \cap B = A \cap \overline{B} = \varnothing$ and $A \neq \varnothing \neq B$.
    \item \textbf{path-connected} if for each $p, q \in E$, there exists a continuous function $f : [0, 1] \to E$ such that $f(0) = p, f(1) = q$.
    \item \textbf{compact} if every open cover has a finite subcover.
\end{enumerate}
These three properties are often called \textbf{topological properties}, since they are invariant under continuous function, the "morphism" between topological space. Put more precisely, If $E \subseteq X$ satisfy a topological property and $f : X \to Y$ is continuous, then $f(E)$ also satisfy the topological property.\\

Immediately, one should again check the "natural" behaviors of subspace topology: Whether a set $E$ is connected, path-connected, or compact is independent of the choices of ambient space. In other words, given $E \subseteq X$, $E$ is connected, path-connected or compact in $(X, \mathscr{T})$ if and only if $E$ is connected, path-connected or compact in $(E, \mathscr{T}_E)$.
\end{mdframed}
\begin{theorem}
\label{Equivalent Definitions of Connected} 
\textbf{(Equivalent Definitions of Connected)} The following statements are equivalent
\begin{enumerate}[label=(\alph*)]
    \item $E$ is connected in $(X, \mathscr{T})$.
    \item $E$ is connected in $(E, \mathscr{T}_E)$.
    \item The only clopen sets in $(E, \mathscr{T}_E)$ are $E$ and $\varnothing$.
    \item In $(E, \mathscr{T}_E)$, the only set that has empty boundary are $E$ and $\varnothing$.
    \item All continuous function from $(E, \mathscr{T}_E)$ to $\set{0, 1}$ with discrete topology is constant.
\end{enumerate}
\end{theorem}
\begin{proof}
For (a) $\iff$ (b), use the identity $\forall A \subseteq E$, $\operatorname{cl}_X(A) \cap E = \operatorname{cl}_E(A)$. Check straightforward for (b) $\iff$ (c) and (d) $\iff$ (c) $\iff$ (e).
\end{proof}
\begin{mdframed}
Three things to note here
\begin{enumerate}[label=(\alph*)]
    \item If $E \subseteq X$ is connected, $E$ can not be covered by any two disjoint open sets intersecting with $E$ in $(X, \mathscr{T}_X)$. The converse is not true. Consider finite subset of an infinite set with cofinite topology.
    \item Union of collection $(A_\alpha)_{\alpha \in J}$ of connected sets with non-empty intersection is connected. Prove this by a proof of contradiction. Path-connectedness has the same property, and the proof is much easier.
    \item If $E$ is connected and $E \subseteq F \subseteq \operatorname{cl}(E)$, then $F$ is connected. Use \customref{Equivalent Definitions of Connected}{the fifth equivalent definitions for connected} to prove this. Path-connectedness doesn't have the same property this time. Consider the "fattened" Topologist’s sine curve $\set{(x, y) \in \mathbb{R}^2 : \left| y - \sin \frac{1}{x} \right| < x}$. It’s closure can be easily proved to be $\set{(x, y) \in \mathbb{R}^2 : \left| y - \sin \frac{1}{x} \right| \leq x} \cup \set{(0, y) \in \mathbb{R}^2 : y \in [-1, 1]}$.
    \item Path-connectedness is strictly stronger than connectedness. This can be proved using a proof of contradiction and supremum. Two famous counterexamples of the converse are \customref{Topologist's Sine Curve}{Topologist's Sine Curve} and \customref{Long Line}{Long line}.
\end{enumerate}
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definitions of Compact)} The following statements are equivalent
\begin{enumerate}[label=(\alph*)]
    \item $E$ is compact in $(X, \mathscr{T})$. 
    \item $E$ is compact in $(E, \mathscr{T}_E)$. 
    \item Given subbase $\mathcal{B}$ of $(X, \mathscr{T})$, every cover of $E$ consisting of the elements of $\mathcal{B}$ has a finite subcover. 
    \item Every infinite subset $M$ of $E$ has a complete limit point in $E$, that is, a point $x \in E$ such that all open set $O$ containing $x$ satisfy $|O \cap M| = |M|$. 
    \item Every collection of closed sets of $(E, \mathscr{T}_E)$ that has finite intersection property has non-empty intersection. 
    \item For all topological space $Y$, the projection $\pi_Y : E \times Y \to Y$ is a closed mapping.
\end{enumerate}
\end{theorem}
\begin{proof}
For (b) $\iff$ (e), use proofs by contradiction. (b) $\iff$ (a) $\implies$ (c) are clear. We now prove
\begin{align*}
\vi{(\text{c})\implies (\text{b})}
\end{align*}
Fix $\mathcal{B}$. \As{ $E$ is not compact}. Then the collection $\S$ of all open covers that have no finite subcover is non-empty. Let $\mathcal{C}$ be a maximal element of $\S$. It is clear that $\mathcal{C} \cap \mathcal{B}$ is not a cover of $E$ by premise. Let $x \in E \setminus \bigcup (\mathcal{C} \cap \mathcal{B})$. Let $U$ be an element of $\mathcal{C} \setminus \mathcal{B}$ containing $x$. Because $\mathcal{B}$ is a subbase, there exists finite $B_1, \dots, B_n \in \mathcal{B}$ such that $x \in B_1 \cap \cdots \cap B_n \subseteq U$. Because $\mathcal{C}$ is a maximal element of $\S$,  for all $j$, the collection $\mathcal{C} \cup \{B_j\}$ does not belong to $\S$. This implies that for each $j \in \{1, \dots, n\}$, there exists a finite sub-collection $\mathcal{C}_j \subseteq \mathcal{C}$ such that $\mathcal{C}_j \cup \{B_j\}$ covers $E$. Let $\mathcal{C}_F \triangleq \bigcup_{j=1}^n \mathcal{C}_j$. Because $\mathcal{C}_j \cup \{B_j\}$ are covers of $E$, $\mathcal{C}_F \cup \{B_1, \dots, B_n\}$ is a cover of $E$. This implies $\mathcal{C}_F \cup \{U\} \subseteq \mathcal{C}$ is a finite subcover. \CaC $\vdone$\\

We now prove 
\begin{align*}
\blue{(\text{a})\implies (\text{d})}
\end{align*}
\As{there exists infinite $M \subseteq E$ that has no complete limit point}. Because of our assumption, for each $x \in E$, there exists an open set $O_x$ containing $x$ such that $|M \cap O_x| < |M|$. Because $(O_x)_{x \in E}$ is an open cover of $E$, there exists a finite sub-cover $(O_x)_{x \in I}$. Note that $M$ is infinite, so we can deduce
\begin{align*}
|M| = \left| \bigcup_{x \in I} M \cap O_x \right| \leq \sum_{x \in I} |M \cap O_x| < |M| \tCaC \bdone
\end{align*}
We now prove 
\begin{align*}
\vi{(\text{d})\implies (\text{a})}
\end{align*}
\As{$E$ is not compact}. Let $O$ be an open cover of $E$ that has no finite subcover with smallest cardinality $c$. Well-order $O$ by $O \triangleq \{O_\alpha\}_{\alpha < c}$. Use transfinite recursion to build $M \triangleq \{x_\alpha : \alpha < c\}$ where $x_\alpha \in E \setminus \bigcup_{\beta < \alpha} O_\beta$. Such $x_\alpha$ always exists; otherwise, there exists an open cover of $E$ that has no finite subcover with cardinality smaller than $c$. To cause a contradiction, it remains to show
\begin{align*}
  \vi{M\text{  has no complete limit point in }E}
\end{align*}
Because $O$ is an open cover of $E$, for all $x$, there exists some $O_\alpha$ containing $x$. Observe using the definition of $M$
\begin{align*}
|O_\alpha \cap M| \leq |\{x_\gamma : \gamma \leq \alpha\}| \leq |\alpha| < c = |M| \tCaC \vdone
\end{align*}
Before we prove (a) $\implies$ (f), we first prove the \olive{Generalized Tube Lemma}. That is,
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \olive{Given a product space $X \times Y$, compact $A \subseteq X$, compact $B \subseteq Y$, and $N \subseteq X \times Y$ open containing $A \times B$, there exists $U \subseteq X$ open, $V \subseteq Y$ open such that $A \times B \subseteq U \times V \subseteq N$.}
   \end{minipage}
\end{center}
First note that for all $(a, b) \in A \times B$, there exists $U_{(a,b)} \subseteq X$ open and $V_{(a,b)} \subseteq Y$ open such that $(a, b) \in U_{(a,b)} \times V_{(a,b)} \subseteq N$. Because $A$ is compact and for all $b$, the collection $(U_{(a,b)})_{a \in A}$ is an open cover of $A$, there exists a finite subset $A_b \subseteq A$ for all $b$ such that $A \subseteq \bigcup_{a \in A_b} U_{(a,b)}$. Now, let $U_b \triangleq \bigcup_{a \in A_b} U_{(a,b)}$ and $V_b \triangleq \bigcap_{a \in A_b} V_{(a,b)}$. It is clear that $U_b, V_b$ are open, and it is straightforward to check $A \times \{b\} \subseteq U_b \times V_b \subseteq N$. Again, because $B$ is compact and $(V_b)_{b \in B}$ is an open cover of $B$, there exists a finite subset $B_0 \subseteq B$ such that $B \subseteq \bigcup_{b \in B_0} V_b$. Let $V \triangleq \bigcup_{b \in B_0} V_b$ and $U \triangleq \bigcap_{b \in B_0} U_b$. It is straightforward to check $U, V$ suffice. $\odone$ \\


We now prove 
\begin{align*}
\blue{(\text{a})\implies (\text{f})}
\end{align*}
Given $A \subseteq X \times Y$ closed, we are required to prove $\pi_Y(A)$ is closed. WOLG, assume $\pi_Y(A) \neq Y$. Fix $y \in Y \setminus \pi_Y(A)$. Because $X$ and $\{y\}$ are compact and $X \times \{y\}$ is a subset of the open set $A^c$, by the \olive{Generalized Tube Lemma}, there exists open $V \subseteq Y$ such that $X \times \{y\} \subseteq X \times V \subseteq A^c$. It is straightforward to check $V \cap \pi_Y(A) = \varnothing$. $\bdone$ \\

Lastly, we prove 
\begin{align*}
\vi{(\text{f})\implies (\text{a})}
\end{align*}
\As{$X$ is not compact}. Let $(O_\alpha)_{\alpha \in J}$ be an open cover of $X$ with no finite subcover. Consider the following construction:
\begin{enumerate}[label=(\alph*)]
    \item $\mathcal{U} \triangleq \set{\bigcup_{\alpha \in I} O_\alpha : I \text{ is a finite subset of } J}$ is an open cover of $X$ with no finite subcover,
    \item $\mathcal{U}$ is closed under finite union,
    \item $\mathcal{F} \triangleq \set{U^c : U \in \mathcal{U}}$ is a collection of non-empty closed sets that has the finite intersection property.
    \item If we let $Y \triangleq X \cup \{p\}$ where $p \notin X$, then $\mathscr{T}_Y \triangleq \mathcal{P}(X) \cup \set{\{p\} \cup A : \exists F \in \mathcal{F}, F \subseteq A \subseteq X}$ is a topology on $Y$, where $\mathcal{P}(X)$ is the collection of all subsets of $X$.
    \item Let $C \triangleq \operatorname{cl}_{X \times Y} \set{(x, x) \in X \times Y : x \in X}$.
    \item Fix $x \in X$. Because $\mathcal{U}$ is an open cover of $X$, there exists $U \in \mathcal{U}$ containing $x$. Note that $\{p\} \cup U^c$ is open in $Y$. This implies $U \times (\{p\} \cup U^c)$ is an open subset of $X \times Y$ containing $(x, p)$. We have proved $C \subseteq X \times X$.
    \item It is clear that $X$ is not closed in $Y$. Now observe that $\pi_Y$ maps the closed set $C$ to the open set $X \subseteq Y$. \CaC $\vdone$
\end{enumerate}
\end{proof}
\begin{theorem}
\label{Compact Subspace of Hausdorff Space is Closed}
\textbf{(Compact Subspace of Hausdorff Space is Closed)} If $E$ is a compact subspace of Hausdorff space $X$, then $E$ is closed in  $X$. 
\end{theorem}
\begin{proof}
Fix $x \in E^c$. Because $X$ is Hausdorff, we can associate each $y \in E$ an open set $U_y$ containing $y$ and an open set  $U_{x,y}$ containing $x$ such that  $U_y,U_{x,y}$ are disjoint. Now, because $E$ is compact and $(U_y)$ is an open cover of $E$, we know there exists a finite sub-cover 
\begin{align*}
E\subseteq \bigcup_{n=1}^N U_{y_n} 
\end{align*}
It then follows that open $\bigcap_{n=1}^n U_{x,y_n}$ contain $x$ and disjoint with $E$. 
\end{proof}
\begin{corollary}
\label{HbC}
\textbf{(Homeomorphism between Compact Space and Hausdorff Space)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $X$ is compact.  
  \item $Y$ is Hausdorff.  
  \item $f:X\rightarrow Y$ is a continuous bijective function. 
\end{enumerate}
Then 
\begin{align*}
f\text{ is a homeomorphism between }X\text{ and }Y
\end{align*}
\end{corollary}
\begin{proof}
Because closed subset of compact set is compact and continuous function send compact set to compact set, we see for each closed $E\subseteq X$, $f(E)\subseteq Y$ is compact. The result then follows from \customref{Compact Subspace of Hausdorff Space is Closed}{$f(E)\subseteq Y$ being closed since $Y$ is Hausdorff}. 
\end{proof}

\section{Quotient Topology and Topological Group}
\begin{abstract}
  This section introduce quotient topology and topological space
\end{abstract}
\begin{mdframed}
Let $\sim $ be an equivalence relation on some topological space $X$, and let $\pi:X\rightarrow Y\triangleq X\setminus \sim$ be the projection map, we can define the \textbf{quotient topology} on $Y$ by  
\begin{align*}
U\subseteq Y\text{ is open }\overset{\triangle}{\iff }\pi^{-1}(U)\subseteq X\text{ is open }
\end{align*}
It is easily checked that 
\begin{enumerate}[label=(\alph*)]
\label{quotient property}
  \item Quotient topology is indeed a topology.  
  \item Quotient topology is the largest (finest) topology on $Y$ such that $\pi:X\rightarrow Y$ is continuous. 
  \item A function $f:Y\rightarrow Z$ is continuous if and only if $f\circ \pi$ is continuous. (Universal Property)
\end{enumerate}
Because the quotient map  $\pi$ is continuous, we know quotient preserve topological properties like connected and compact, yet notably, quotient does not preserve separation axioms. Consider the following examples.
\end{mdframed}
\begin{Example}{\textbf{(Quotient does NOT preserve Second Countable)}}{}
\begin{align*}
X\triangleq \R\text{ and }x\sim y \overset{\triangle}{\iff }x=y\text{ or }x,y\inz
\end{align*}
Let $Y\triangleq X\setminus \sim $. We show that $Y$ is not even first countable at $[0]$. Let $U_n\subseteq Y$ be an arbitrary sequence of open-neighborhood of $[0]$. It is easily checked that for each $n\inn$, there exists $\epsilon _{n,k}$ such that 
\begin{align*}
\pi \Big[ \bigcup_{k\inz} (k-\epsilon_{n,k},k+\epsilon_{n,k}) \Big] \subseteq U_n
\end{align*}
Define $\delta_k\triangleq \frac{\epsilon_{k,k}}{2}$ and 
\begin{align*}
V=\pi \Big[\bigcup_{k\inz} (k-\delta_k,k+\delta_k) \Big] 
\end{align*}
It is easily checked that $V$ is an open neighborhood of  $[0]$ contained in no $U_n$.
\end{Example}
\begin{Example}{\textbf{(Quotient does NOT preserve Hausdorff)}}{}
\begin{align*}
X=\R\text{ and }Y=\set{(-\infty,0),[0,\infty)}
\end{align*}
\end{Example}
\begin{mdframed}
However, with the criterion of $\pi$ being an open mapping, we can draw some useful conclusions. For example, if $\pi:X\rightarrow Y$ is an open mapping and $\mathcal{B}$ is a basis of $X$, then  $\pi (\mathcal{B})$ is a basis of $Y$. 
\end{mdframed}
\begin{theorem}
\label{Hausdorff and Quotient}
  \textbf{(Hausdorff and Quotient)} If $\pi:X\rightarrow Y$ is an open mapping, and we define
\begin{align*}
R_\pi\triangleq \set{(x,y)\in X^2 : \pi (x)=\pi (y)} 
\end{align*}
Then
\begin{align*}
R_\pi\text{ is closed }\iff Y\text{ is Hausdorff }
\end{align*}
\end{theorem}
\begin{proof}
  Suppose $R_\pi$ is closed. Fix some $x,y$ such that $\pi (x)\neq \pi (y)$. Because $R_\pi$ is closed, we know there exists open neighborhood $U_x,U_y$ such that $U_x \times U_y \subseteq (R_\pi)^c$. It is clear that $\pi (U_x),\pi (U_y)$ are respectively open neighborhood of $\pi (x)$ and $\pi (y)$. To see $\pi (U_x)$ and $\pi (U_y)$ are disjoint, \red{assume} that $\pi (a)\in \pi (U_x)\cap \pi (U_y)$. Let $a_x\in U_x$ and $a_y\in U_y$ satisfy $\pi (a_x)=\pi (a)=\pi (a_y)$, which is impossible because $(a_x,a_y)\in (R_\pi)^c$. \CaC\\


Suppose $Y$ is Hausdorff. Fix some  $x,y$ such that  $\pi (x)\neq \pi (y)$. Let $U_x,U_y$ be open neighborhoods of  $\pi (x),\pi (y)$ separating them. Observe that $(x,y)\in \pi^{-1}(U_x)\times \pi^{-1}(U_y)\subseteq (R_\pi)^c$
\end{proof}
\begin{mdframed}
Notably, quotient topology give us a famously weird homeomorphism.
\end{mdframed}
\begin{Example}{\textbf{(Weird Quotient)}}{}
\begin{align*}
\R \setminus \Z \simeq S^1\triangleq \set{e^{ix}\inc:x\inr}
\end{align*}
Clearly, we can well define a map $F:\R\setminus \Z\rightarrow S^1$ by 
\begin{align*}
F(\pi (x))\triangleq e^{i2\pi x}
\end{align*}
It is straightforward to check
$F$ is a continuous bijection and $\R\setminus \Z$ is compact. \customref{HbC}{It then follows $F$ is a homeomorphism}.
\end{Example}
\section{Topological Group}
\begin{abstract}
This section introduce the notion of topological group 
and prove that \customref{Quotient Group of Topological Group}{quotient group of a topological group when equipped with the quotient topology is again a topological group}.  
\end{abstract}
\begin{mdframed}
By a \textbf{topological group}, we mean a topological space $M$ equipped with a group structure such that addition $M^2 \rightarrow M$ and inversion $M\rightarrow M$ are both continuous. Equivalently, one can simply require 
\begin{align*}
  M^2\rightarrow M;(g,h)\mapsto gh^{-1}
\end{align*}
to be continuous. 
\end{mdframed}

\begin{theorem}
\label{Quotient Group of Topological Group}
\textbf{(Quotient Group of Topological Group)}
\end{theorem}
\chapter{Metric Space}
\section{Completion}
\section{Bounded and Totally Bounded}
\section{Compactness}
\section{Limit Interchange}
\begin{mdframed}
In this section, we 
\begin{enumerate}[label=(\alph*)]
  \item discuss the condition in which we can change the limit order of double sequence in general metric space. (\myref{Theorem}{COLO} and \myref{Theorem}{COLO2})
  \item prove that the space of functions is complete if and only if the codomain is complete. (\myref{Theorem}{Tsof})
  \item prove that the uniform limit of a sequence of convergent sequences in a complete metric space converge. (\myref{Theorem}{CSaC})
\end{enumerate}
\textbf{Remark on structure of the Theory}: The proof of  (\myref{Theorem}{CSaC}: convergent sequences in complete metric space is closed under uniform convergence) relies on (\myref{Theorem}{COLO}: exchange limit order), while that of (\myref{Theorem}{Tsof}: Space of functions $(X^Y, d_\infty)$ is complete iff $Y$ is complete) does not.\\

(\myref{Theorem}{COLO}: exchange limit order) will later be used to prove the Uniform Limit Theorem (\myref{Theorem}{ULT}) which is a "pointwise" Theorem, and justify abundant of limit exchange, e.g. (\myref{Theorem}{COiC}: exchange limit order for functions)\\

An important consequence of (\myref{Corollary}{SoB}: space of bounded functions into complete space is complete) is that $\big(L(\R^n,\R^m), \norm{\cdot}_{\text{op}} \big)$ is complete. This will be later shown with extra tools. 
\end{mdframed}
\begin{theorem}
\label{COLO}
\textbf{(Change Order of Limit Operations: Part 1)} Given a double sequence $a_{n,k}$ whose codomain is $(Y,d)$. Suppose
\begin{enumerate}[label=(\alph*)]
  \item $a_{n,k}\to a_{\bullet,}$ uniformly as $n\to \infty$ 
  \item $a_{n,k}\to A_n$ pointwise as  $k\to \infty$.
  \item $A_n \to A$ 
\end{enumerate}
We have  
\begin{align*}
\lim_{k\to \infty}a_{\bullet,k}=A
\end{align*}
In other words, we can switch the order of limit operations
\begin{align*}
\lim_{k\to \infty}\lim_{n\to \infty}a_{n,k}=\lim_{n\to \infty}\lim_{k\to \infty}a_{n,k}
\end{align*}
\end{theorem}
\begin{proof}
We wish to prove 
\begin{align*}
\vi{a_{\bullet,k}\to A\text{ as }k\to \infty}
\end{align*}
Fix $\epsilon $. Because $a_{n,k}\to a_{\bullet,k}$ uniformly and $A_n\to A$ as $n\to \infty$, we know there exists $m$ such that 
 \begin{align}
  \label{K1}
d(A_m,A)<\frac{\epsilon}{3}\text{ and }\forall k\inn,d(a_{m,k},a_{\bullet,k})<\frac{\epsilon}{3}
\end{align}
Then because $a_{m,k}\to A_m$ as $k\to \infty$, we know there exists $K$ such that
\begin{align}
\label{K2}
\forall k>K, d(a_{m,k},A_m)<\frac{\epsilon}{3}
\end{align}
We now claim 
\begin{align*}
  \vi{\forall k>K, d(a_{\bullet,k},A)<\epsilon}
\end{align*}
The claim is true since by \myref{Equation}{K1} and \myref{Equation}{K2}, we have 
\begin{align*}
  \forall k>K, d(a_{\bullet,k},A)\leq d(a_{\bullet,k},a_{m,k})+d(a_{m,k},A_m)+d(A_m,A)<\epsilon \vdone
\end{align*}
\end{proof}
\begin{theorem}
\label{COLO2}
\textbf{(Change Order of Limit Operations: Part 2)} Given a double sequence $a_{n,k}$ whose codomain is $(Y,d)$. Suppose 

\begin{enumerate}[label=(\alph*)]
  \item $a_{n,k}\to a_{\bullet,k}$ uniformly as $n\to \infty$ 
  \item $a_{n,k}\to A_n$ pointwise as $k\to \infty$ 
  \item $a_{\bullet,k}\to A$ as $k\to \infty$
\end{enumerate}
We have
\begin{align*}
A_n \to A
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish to find $N$ such that 
 \begin{align*}
   \vi{\forall n>N, d(A_n,A)<\epsilon}
\end{align*}
Because  $a_{n,k}\to a_{\bullet,k}$ uniformly as $n \to \infty$, we can let $N$ satisfy 
\begin{align}
\label{K3}
\forall n>N,\forall k\inn, d(a_{n,k},a_{\bullet,k})<\frac{\epsilon}{3}
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Arbitrarily pick $n>N$. Because $a_{\bullet,k} \to A$, and because $a_{n,k} \to A_n$, we know there exists $j$ such that 
 \begin{align}
  \label{K4}
d(a_{\bullet,j},A)<\frac{\epsilon}{3}\text{ and }d(a_{n,j},A_n)<\frac{\epsilon}{3}
\end{align}
From \myref{Equation}{K3} and \myref{Equation}{K4}, we now have
\begin{align*}
d(A_n,A)\leq d(A_n,a_{n,j})+d(a_{n,j},a_{\bullet,j})+d(a_{\bullet,j},A)<\epsilon \vdone
\end{align*}
\end{proof}
\begin{mdframed}
In summary of \myref{Theorem}{COLO} and \myref{Theorem}{COLO2}, given a double sequence $a_{n,k}$ converging both side 
\begin{enumerate}[label=(\alph*)]
  \item $a_{n,k}\to a_{\bullet,k}$ pointwise as $n \to \infty$
  \item $a_{n,k}\to a_{n,\bullet}$ pointwise as $k\to \infty$
\end{enumerate}
As long as 
\begin{enumerate}[label=(\alph*)]
  \item one side of convergence is uniform 
  \item between two sequence $\set{a_{\bullet,k}}_{k\inn}$ and $\set{a_{n,\bullet}}_{n\inn}$, one of them converge, say, to $A$
\end{enumerate}
Then the other sequence also converge, and the limit is also $A$.\\

It is at this point, we shall introduce two other terminologies. Suppose $f_n$ is a sequence of functions from an arbitrary set  $X$ to a metric space  $Y$. We say $f_n$ is  \textbf{pointwise Cauchy} if for all fixed $x \in X$, the sequence $f_n(x)$ is Cauchy. We say $f_n$ is  \textbf{uniformly Cauchy} if for all $\epsilon $, there exists $N\inn$ such that 
\begin{align*}
\forall n,m>N, \forall x\in X, d\big(f_n(x),f_m(x) \big)<\epsilon 
\end{align*}
In last Section (\myref{Section}{27CaB}), we define the \textbf{uniform metric} $d_{\infty}$ on $X^Y$ by 
 \begin{align*}
d_{\infty}(f,g)=\sup_{x\in X}d\big(f(x),g(x) \big)
\end{align*}
and say that $f_n\to f$ uniformly if and only if $f_n\to f$ in $(X^Y,d_{\infty})$. Similar to this clear fact, we have 
\begin{align*}
f_n\text{ is uniformly Cauchy }\iff \text{ $f_n$ is Cauchy in  $(X^Y, d_\infty)$ }
\end{align*}
It should be very easy to verify that if $f_n$ uniformly converge, then $f_n$ is uniformly Cauchy, and just like sequences in metric space, the converse hold true if and only if the space $\big(X^Y,d_\infty \big)$ is complete. In \myref{Theorem}{Tsof}, we give a necessary and sufficient condition for $\big(X^Y,d_{\infty}\big)$ to be complete. 
\end{mdframed}
\begin{theorem}
\label{Tsof}
\textbf{(Space of functions $\big(X^Y,d_{\infty}\big)$ is Complete iff $Y$ is Complete)} Given an arbitrary set $X$ and a metric space  $(Y,d)$, we have 
\begin{align*}
\text{ the extended metric space $\big(X^Y,d_{\infty} \big)$ is complete }\iff Y\text{ is complete }
\end{align*}
\end{theorem}
\begin{proof}
$(\longleftarrow)$\\

Suppose $f_n$ is uniformly Cauchy. We wish 
\begin{align*}
  \vi{\text{ to construct a $f:X\rightarrow Y$ such that }f_n\to f\text{ uniformly }}
\end{align*}
Because $f_n$ is uniformly Cauchy, we know that for all $x \in X$, the sequence $f_n(x)$ is Cauchy in $(Y,d)$. Then because $Y$ is complete, we can define $f:X\rightarrow Y$ by 
 \begin{align*}
f(x)=\lim_{n\to \infty}f_n(x)
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such $f$ works, i.e.  $f_n\to f$ uniformly }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
  \vi{\text{ to find $N\inn$ such that for all $n>N$ and  $x \in X$ we have $d\big(f_n(x),f(x)\big)<\epsilon $ }}
\end{align*}
Because $f_n$ is uniformly Cauchy, we know there exists $N$ such that  
\begin{align}
\label{K5}
\forall n,m>N, \forall x\in X, d\big(f_n(x),f_m(x) \big)<\frac{\epsilon }{2}
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
 \As{there exists $n>N\text{ and }x\in X$ such that  $d\big(f_n(x),f(x) \big)\geq \epsilon $}. Because $f_k(x)\to f(x)$ as $k\to \infty$, we know 
\begin{align}
  \label{K6}
\exists m \inn, d\big(f_m(x),f(x) \big)<\frac{\epsilon}{2}
\end{align}
Then from \myref{Equation}{K5} and \myref{Equation}{K6}, we can deduce 
\begin{align*}
\epsilon \leq d\big(f_n(x),f(x)\big)\leq  d\big(f(x),f_m(x)\big)  + d\big(f_n(x),f_m(x)\big)<\epsilon \tCaC \vdone
 \end{align*}
 $(\longrightarrow)$\\

Let $K$ be the set of constant functions  in $X^Y$. We first prove 
 \begin{align*}
\blue{K\text{ is closed }}
\end{align*}
Arbitrarily pick $f \in K^c$. We wish 
\begin{align*}
\blue{\text{ to find $\epsilon \inr^+$ such that $B_{\epsilon }(f) \in K^c$ }}
\end{align*}
Because $f$ is not a constant function, we know there exists $x_1,x_2 \in X$ such that 
\begin{align*}
d\big(f(x_1),f(x_2) \big)>0
\end{align*}
We claim that 
\begin{align*}
\blue{\epsilon=\frac{d\big(f(x_1),f(x_2) \big)}{3}\text{ works }}
\end{align*}
Arbitrarily pick $g \in B_\epsilon(f)$. We wish 
\begin{align*}
\blue{\text{ to show $g\in K^c$  }}
\end{align*}
Notice the triangle inequality 
\begin{align}
\label{K7}
3\epsilon =d\big(f(x_1),f(x_2) \big)\leq d\big(f(x_1),g(x_1) \big)+d\big(g(x_1),g(x_2) \big)+d\big(g(x_2),f(x_2) \big)
\end{align}
Also, because $g\in B_\epsilon (f)$, we have
\begin{align}
  \label{K8}
\forall x\in X, d\big(f(x),g(x) \big)<\epsilon 
\end{align}
Then by \myref{Equation}{K7} and \myref{Equation}{K8}, we see 
\begin{align*}
d\big(g(x_1),g(x_2) \big)> \epsilon 
\end{align*}
This then implies $g$ is not a constant function.  $\bdone$\\

Now, Because by premise $(X^Y,d_{\infty})$ is complete, and we have proved $K$ is closed in  $(X^Y,d_\infty)$, we know $K$ is complete. Then, we resolve the whole problem into proving 
\begin{align*}
\vi{Y \text{ is isometric to }K}
\end{align*}
Define $\sigma:Y \to K $ by 
\begin{align*}
y \mapsto \tilde{y}\text{ where }\forall x \in X, \tilde{y}(x)=y 
\end{align*}
It is easy to verify $\sigma$ is an isometry. $\vdone$ 
\end{proof}
\begin{corollary}
\label{SoB}
\textbf{(Space of Bounded functions $\big(B(X,Y),d_{\infty} \big)$ is Complete iff  $Y$ is Complete)} 
\begin{align*}
\big(B(X,Y),d_\infty \big)\text{ is complete }\iff  Y\text{ is complete }
\end{align*}
\end{corollary}
\begin{proof}
$(\longleftarrow)$\\

By \myref{Theorem}{Tsof}, the space $(X^Y, d_{\infty})$ is complete. Then because $B(X,Y)$ is closed in $(X^Y,d_\infty)$, we know $B(X,Y)$ is complete.\\

$(\longrightarrow)$\\

Notice that the set of constant function $K$ is a subset of the galaxy  $B(X,Y)$. The whole proof in \myref{Theorem}{Tsof} works in here too. 
\end{proof}
\begin{mdframed}
Remember in the beginning of this section we say we will prove convergent sequences in $Y$ is closed under uniform convergence if $Y$ is complete. The proof of this result relies on \myref{Theorem}{Tsof}.
\end{mdframed}
\begin{theorem}
\label{CSaC}
\textbf{(Convergent Sequences are Closed under Uniform Convergence if Codomain $\big(Y,d\big)$ is Complete)} Given a complete metric space $\big(Y,d\big)$, let $\mathcal{C}_\N^Y$ be the set of convergent sequences in $Y$.
\begin{align*}
Y\text{ is complete }\implies \text{ $\mathcal{C}_\N^Y$ is closed under uniform convergent }
\end{align*}
\end{theorem}
\begin{proof}
Let $a_{n,k}\to a_{\bullet,k}$ uniformly as $n\to \infty$ where for all $n,k \inn, a_{n,k}\in Y$ and let $A_n= \lim_{k\to \infty}a_{n,k}$ for all $n\inn$.  
\begin{align*}
  \vi{\text{ to prove $a_{\bullet,k}$ converge}}
\end{align*}
By \myref{Theorem}{COLO2}, we can reduce the problem to 
\begin{align*}
\vi{\text{ proving $A_n$ converge }}
\end{align*}
Then because $Y$ is complete, we can then reduce the problem into proving 
\begin{align*}
  \vi{A_n\text{ is Cauchy }}
\end{align*}
Fix $\epsilon $. We wish to find $N$ such that 
 \begin{align*}
   \vi{\forall n,m>N, d(A_n,A_m)<\epsilon }
\end{align*}
Because $a_{n,k}\to a_{\bullet,k}$ uniformly, we can find $N$ such that  
 \begin{align}
\label{K10}
\forall n,m> N, d_\infty(\set{a_{n,k}}_{k\inn},\set{a_{m,k}}_{k\inn})<\frac{\epsilon}{3} 
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Arbitrarily pick $n,m>N$. We wish to prove 
\begin{align*}
\vi{d(A_n,A_m)<\epsilon }
\end{align*}
Because $a_{n,k} \to A_n$ and $a_{m,k}\to A_m$ as $k \to \infty$, we can find $j$ such that  
\begin{align}
\label{K11}
d(a_{n,j},A_n)<\frac{\epsilon}{3}\text{ and }d(a_{m,j},A_m)<\frac{\epsilon}{3}
\end{align}
Then from  \myref{Equation}{K10} and \myref{Equation}{K11}, we can deduce
\begin{align*}
d(A_n,A_m)\leq d(A_n,a_{n,j})+d(a_{n,j},a_{m,j})+d(a_{m,j},A_m)<\epsilon \vdone
\end{align*}
\end{proof}


\section{Closed under Uniform Convergence}
\begin{mdframed}
Given $(E,d_E),(Y,d_Y)$ and a sequence of functions $f_n:E\rightarrow Y$, converging uniformly to some $f:E\rightarrow Y$ such that each $f_n$ has the property 
\begin{enumerate}[label=(\alph*)]
  \item Boundedness 
  \item Unboundedness
  \item Continuity
  \item Uniform continuity
  \item $K$-Lipschitz continuity
\end{enumerate}
on $E$, then $f$ also has the same property. These fact will later be proved in \myref{}{} and \myref{}{}.







are again all closed under uniform convergence, where the proof for continuity is closed under uniform convergence use \myref{Theorem}{COLO} as a lemma.\\

The reason we require the co-domain $Y$ of sequence to be complete is explained in the last paragraph of \myref{Section}{27CaB}. An example of such beautiful closure is lost if the codmain $(Y,d)$ is not complete is $Y=\R^*$ and  $a_{n,k}=\frac{1}{n}+\frac{1}{k}$. 
\end{mdframed}
\begin{theorem}
\label{COiC}
\textbf{(Change Order of Limit Operation in Complete Metric Space)} Given a sequence of function $f_n:E\rightarrow (Y,d)$ and a function $f:E\rightarrow (Y,d)$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f_n\to f$ uniformly on $E$ 
   \item $\lim_{t\to x}f_n(t)$ exists for all $n \inn$
    \item $(Y,d)$ is complete
\end{enumerate}
We have
\begin{align*}
\lim_{n\to \infty }\lim_{t\to x}f_n(t)=\lim_{t\to x}\lim_{n\to \infty}f_n(t)
\end{align*}
\end{theorem}
\begin{proof}
Fix a sequence $t_k$ in $E$ that converge to  $x$. We reduced the problem into proving
\begin{align*}
  \vi{\lim_{n\to \infty}\lim_{k \to \infty}f_n(t_k)=\lim_{k\to \infty}\lim_{n\to \infty}f_n(t_k)}
\end{align*}
Set 
\begin{align}
a_{n,k}\triangleq f_n(t_k)
\end{align}
We then reduced the problem into proving 
\begin{align*}
  \vi{\lim_{n\to \infty}\lim_{k\to \infty}a_{n,k}=\lim_{k\to \infty}\lim_{n\to \infty}a_{n,k}}
\end{align*}
Set 
\begin{align*}
A_n\triangleq \lim_{t \to x}f_n(t)\text{ and }a_{\bullet,k}\triangleq \lim_{n \to \infty}f_n(t_k)
\end{align*}
We now prove 
\begin{align*}
\blue{\text{ $A_n$ converge }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\blue{\text{ to find $N$ such that }d(A_n,A_m)\leq \epsilon \text{ for all $n,m>N$ }}
\end{align*}
Because $a_{n,k}$ uniformly converge (to $a_{\bullet , k}$) as $n \to \infty$ by our setting, we know there exists $N$ such that
\begin{align*}
\forall n,m>N,\forall k\inn, d(a_{n,k},a_{m,k})<\frac{\epsilon}{3}
\end{align*}
We claim 
\begin{align*}
\blue{\text{ such $N$ works }}
\end{align*}
Fix $n,m>N$. Because $a_{n,k}\to A_n$ and $a_{m,k}\to A_m$, we know there exists $j \inn$ such that 
\begin{align*}
d(a_{n,j},A_n)<\frac{\epsilon}{3}\text{ and }d(a_{m,j},A_m)<\frac{\epsilon}{3}
\end{align*}
We now have
\begin{align*}
  d(A_n,A_m)&\leq  d(A_n,a_{n,j})+d(a_{n,j},a_{m,j})+d(a_{m,j},A_m)\\
&< \frac{\epsilon}{3}+\frac{\epsilon}{3}+\frac{\epsilon}{3}=\epsilon \bdone
\end{align*}
Now, because $a_{n,k}\to a_{\bullet ,k}$ uniformly, by \myref{Theorem}{COLO2}, we have 
\begin{align*}
\lim_{n\to \infty}\lim_{k\to \infty}a_{n,k}=\lim_{n\to \infty}A_n=\lim_{k\to \infty}a_{\bullet ,k}=\lim_{k\to \infty}\lim_{n\to \infty}a_{n,k}\vdone
\end{align*}
\end{proof}
\label{UCaCH}
\begin{mdframed}
The end goal for this section is to prove that the following properties 
\begin{enumerate}[label=(\alph*)]
  \item continuity 
  \item uniform continuity 
  \item $K$-Lipschitz Continuity 
\end{enumerate}
\end{mdframed}
\begin{theorem}
\label{ULT}
\textbf{(Uniform Limit Theorem)} Given a sequence of function $f_n$ from a topological space $(X,\tau)$ to a metric space $(Y,d)$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f_n\to f$ uniformly as $n\to \infty$
  \item $f_n$ is continuous for all  $n\inn$ 
\end{enumerate}
Then $f$ is also continuous. 
\end{theorem}
\begin{proof}
Fix $x \in X$, and let $x_k\to x$. We wish to prove
\begin{align*}
  \vi{f(x_k)\to f(x)}
\end{align*}
Because $f_n\to f$ uniformly as $n\to \infty$, we know 
\begin{align}
\label{282e1}
\bset{f_n(x_k)}_{k\inn}\to \bset{f(x_k)}_{k\inn}\text{ uniformly as }n\to \infty
\end{align}
Also, because for each $n\inn$, the function $f_n$ is continuous at $x$, we know 
\begin{align}
\label{282e2}
\forall n\inn, f_n(x_k)\to f_n(x)\text{ as $k\to \infty$ }
\end{align}
Then because $f_n\to f$ pointwise, we know 
\begin{align}
\label{282e3}
f_n(x)\to f(x)
\end{align}
Now, because \myref{Equation}{282e1},  \myref{Equation}{282e2} and  \myref{Equation}{282e3}, by \myref{Theorem}{COLO}, we have
\begin{align*}
\lim_{k\to \infty}f(x_k)=\lim_{k\to \infty}\lim_{n\to \infty}f_n(x_k)=\lim_{n\to \infty}\lim_{k\to \infty}f_n(x_k)=\lim_{n\to \infty}f_n(x)=f(x)\vdone
\end{align*}
\end{proof}
\begin{mdframed}
Suppose $X$ is a compact Hausdroff space,  with \myref{Theorem}{}, we can now say that the set $\mathcal{C}(X)$ of complex-valued continuous functions on $X$ 
\end{mdframed}
\begin{theorem}
\textbf{(Uniformly Continuous functions are Closed under Uniform Convergence)} Given a sequence of functions $f_n$ from a metric space  $(X,d_X)$ to metric space $(Y,d_Y)$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f_n\to f$ uniformly 
  \item $f_n$ is uniformly continuous for all  $n\inn$
\end{enumerate}
Then $f$ is also uniformly continuous
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find $\delta$ such that $\forall x,y \in X, d_X(x,y)<\delta \implies d_Y\big(f(x),f(y) \big)<\epsilon $}}
\end{align*}
Because $f_n\to f$ uniformly, we know there exists  $m \inn$ such that 
\begin{align}
\label{L1}
\forall x \in X, d_Y\big(f_m(x),f(x) \big)<\frac{\epsilon}{3}
\end{align}
Because $f_m$ is uniformly continuous, we know 
\begin{align}
\label{L2}
\exists \delta, \forall x,y \in X, d_X(x,y)<\delta \implies d_Y\big(f_m(x),f_m(y) \big)<\frac{\epsilon}{3}
\end{align}
We claim 
\begin{align*}
\text{ \vi{such $\delta$ works} }
\end{align*}
Let $x,y \in X$ satisfy $d_X(x,y)<\delta$. We wish 
\begin{align*}
\vi{\text{ to prove }d_Y\big(f(x),f(y) \big)<\epsilon }
\end{align*}
From \myref{Equation}{L1} and \myref{Equation}{L2}, we have 
\begin{align*}
d_Y\big(f(x),f(y) \big)\leq d_Y\big(f(x),f_m(x) \big)+d_Y\big(f_m(x),f_m(y) \big)+d_Y\big(f_m(y),f(y) \big)=\epsilon \vdone
\end{align*}
\end{proof}
\begin{theorem}
\textbf{($K$-Lipschitz functions are Closed under Uniform Convergence)} Given a sequence of functions $f_n$ from metric space $(X,d_X)$ to metric space $(Y,d_Y)$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $f_n\to f$ uniformly as $n\to \infty$ 
  \item $f_n$ is $K$-Lipschtize continuous for all $n\inn$
\end{enumerate}
Then $f$ is also $K$-Lipschtize continuous. 
\end{theorem}
\begin{proof}
Arbitrarily pick $x,y \in X$, to show $f$ is $K$-Lipschtize continuous, we wish 
\begin{align*}
\vi{\text{ to show $d_Y\big(f(x),f(y) \big)\leq Kd_X(x,y)$ }}
\end{align*}
Fix $\epsilon $. We reduce the problem into proving 
\begin{align*}
  \vi{d_Y\big(f(x),f(y) \big)<Kd_X(x,y)+\epsilon }
\end{align*}
Because $f_n\to f$ uniformly as $n\to \infty$, we know there exists $m$ such that 
 \begin{align}
\label{L4}
\forall z \in X, d_Y\big(f(z),f_m(z) \big)<\frac{\epsilon}{2}
\end{align}
Because $f_m$ is $K$-Lispchitz continuous, we know 
\begin{align}
\label{L3}
d_Y\big(f_m(x),f_m(y) \big)\leq Kd_X(x,y)
\end{align}
Now, from \myref{Equation}{L3} and \myref{Equation}{L4}, we now see 
\begin{align*}
  d_Y\big(f(x),f(y) \big)&\leq d_Y\big(f(x),f_m(x) \big)+d_Y\big(f_m(x),f_m(y) \big)+d_Y\big(f_m(y),f(y) \big)< Kd_X(x,y)+\epsilon 
\end{align*}
\end{proof}
\begin{mdframed}
An example of sequences of Lipschitz continuous functions with unbounded Lipschitz constant can uniformly converge to a non-Lipschitz continuous function is given below 
\begin{Example}{\textbf{(Lipschitz functions with Unbounded Lipschitz constant Uniformly Converge to a non-Lipschitz function)}}{}
\begin{align*}
X=[0,1]\text{ and }f_n(x)=\sqrt{x+\frac{1}{n}} 
\end{align*}
\end{Example}
\end{mdframed}
\section{Modes of Convergence} 
\label{27CaB}
\begin{mdframed}
This section is the starting point for us to study spaces of function. At first, we will define two modes of convergence for sequence of function and point out some basic properties and the difference between two modes of convergence.\\



Given an arbitrary set $X$ and a metric space  $Y$, we say a sequence of functions $f_n$ from $X$ to  $Y$ \textbf{pointwise converge} to $f$ if for all $\epsilon \text{ and }x$ in $X$, there exists $N$ such that 
\begin{align*}
\forall n>N, f_n(x)\in B_{\epsilon }\big(f(x) \big)
\end{align*}
In other words, for each fixed $x$ in $X$, we have $f_n(x)\to f(x)$.\\

We say $f_n$ \textbf{uniformly converge} to $f$ if for all $\epsilon $ there exists $N$ such that 
\begin{align*}
\forall x\in X, \forall n>N, f_n(x)\in B_\epsilon \big(f(x)\big)
\end{align*}
The difference between pointwise convergence and uniform convergence is that if we require $f_n(x)$ to be $\epsilon $-close to $f(x)$ for all $n>N$, then 
\begin{enumerate}[label=(\bullet)]
  \item $N$ depend on both  $\epsilon $ and $x$ if $f_n\to f$ pointwise
  \item $N$ depend on only  $\epsilon $ if $f_n\to f$ uniformly 
\end{enumerate}
A few properties of sequence of functions similar to that of sequences in metric space is obvious. If $f_n\to f$ pointwise, then all sub-sequences  $f_{n_k}\to f$ pointwise. If $f_n \to f$ uniformly, then all sub-sequences $f_{n_k}\to f$ uniformly. Suppose $Z\subseteq X$. It is clear that if $f_n \to f$ uniformly (resp: pointwise) the restricts $f_n|_Z\to f|_Z$ uniformly (resp: pointwise). Also, if $f_n \to f$ uniformly, then $f_n\to f$ pointwise.\\

Suppose we have a family $\mathcal{F}$ of functions $f:X\rightarrow (Y,d)$. If we define 
\begin{align*}
d_{\infty}(f,g)=\sup_{x \in X}d\big(f(x),g(x) \big)
\end{align*}
instead of a metric, $d_{\infty}$ become an extended metric. If $f$ is bounded and  $g$ is unbouneded, we have  $d_{\infty}(f,g)=\infty$. If $f,g$ are both bounded, then  $d_{\infty}(f,g)\inr^+$. Because of such, for $d_{\infty}$ to be a metric, one but not the only condition is for  $\mathcal{F}$ to be space of bounded functions.\\

Now, regardless of $d_{\infty}$ is an extended metric or not, we have 
\begin{align*}
f_n\to f\text{ uniformly }\iff d_{\infty}(f_n,f)\to 0
\end{align*}
With this in mind, it shall be clear that the uniform limit of bounded (resp: unbounded) functions is always bounded (resp: unbounded).\\ 

Examples for bounded (resp: unbounded) function $f_n$ pointwise converge to unbounded (resp: bounded) function $f$ are as follows.

\begin{Example}{\textbf{(Bounded functions pointwise converge to unbounded function)}}{}
\label{bfpce}
\begin{align*}
  X=(0,1],f_n(x)=\min  \set{n,\frac{1}{x}}
\end{align*}
It is clear that $\forall n\inn,f_n(x) \in [0,n]$, and the limit $f:X\rightarrow \R$ is $f(x)=\frac{1}{x}$
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
\includegraphics[height=9cm,width=15cm]{pwise converge1.png}
   \end{minipage}
\end{center}
\end{Example}
\begin{Example}{\textbf{(Unbounded functions pointwise converge to bounded function)}}{}
\begin{align*}
X=\R,f_n(x)=\frac{1}{n}x
\end{align*}
The limit function is $f(x)=0$
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering  \includegraphics[height=7cm,width=15cm]{pwise converge2.png}
   \end{minipage}
\end{center}
\end{Example}
\end{mdframed} 
\begin{mdframed}
As pointed out earlier, if $f:X\rightarrow (Y,d)$ is bounded and $g:X\rightarrow (Y,d)$ is unbounded, then $d_{\infty}(f,g)=\infty$. This means that if $Y$ is unbounded, the uniform metric $d_{\infty}$ is extended on $X^Y$. For this, it is necessary to develop some basic fact concerning extended metric space.\\

Suppose $(X,d)$ is an extended metric space. If we define $\sim$ on $X$ by $x \sim y\iff  d(x,y)<\infty$, then $\sim $ is an equivalence relation. We say each equivalence class is a \textbf{galaxy} of $(X,d)$. Suppose $T$ is the collection of the galaxies of $(X,d)$. For each $\mathcal{T} \in T$, the space $(\mathcal{T},d)$ is just a metric space.\\

It is easy to see that the way we induce topology from metric space is still valid if the metric is extended. That is 
\begin{align*}
  \tau=\set{Z \in X: \forall z \in Z, \exists \epsilon ,B_\epsilon (z)\subseteq Z}
\end{align*}
is still a topology, even though $d$ is an extended metric on $X$.\\

We can verify that a set $Y$ in $X$ is open if and only if for all  $\mathcal{T} \in T$, the set $Y \cap \mathcal{T}$ is open, and the set $Y$ in $X$ is closed if and only if all convergent sequences $y_n$ in  $Y$ converge to points in $Y$. \\
\end{mdframed}
\begin{mdframed}
Now, suppose we are given an arbitrary set  $X$ and a complete metric space  $(\overline{Y},d)$, and on $X^{\overline{Y}}$, we define the uniform metric  $d_{\infty}$. We say a set $\mathcal{F}\subseteq X^{\overline{Y}}$ of functions is \textbf{closed under uniform convergence} if  for all uniform convergent sequence $f_n \subseteq \mathcal{F}$, the limit function $f$ is also in  $\mathcal{F}$. There are justified reasons for us to give the premise that $\overline{Y}$ is complete prior to the definition of the term \textbf{closed under uniform convergence}. One reason is that by \myref{Theorem}{Tsof}, if $Y$ is not complete, then the extended metric space  $(X^Y,d_{\infty})$ is also not complete, which implies the possibility a Cauchy sequence $f_n$ in  $X^Y$ converge to a function $f \in X^{\overline{Y}} \setminus X^Y$ where $\overline{Y}$ is the completion of  $Y$. For instance, if we let $Y=\R\setminus \set{1}$ where $X=\R$, and let $f_n(x)=\begin{cases}
  0& \text{ if  }x\neq 0\\
  1+\frac{1}{n}& \text{ if $x=0$ }
\end{cases}\in Y$, we see that the set $\mathcal{F}=\set{f_n:n\inn}$ is "closed under uniform convergence" in the context of $X^Y$, but when in fact $f_n$ uniformly converge to  $f(x)=\begin{cases}
  0& \text{ if  }x\neq 0\\
  1& \text{ if $x=0$ }
\end{cases}$ which is not in $\mathcal{F}$. This awkward usage of words can be solved if we define the term  \textbf{closed under uniform convergence} after the premise that $Y$ is complete.\\

Now, given a set of functions $\mathcal{F}\subseteq X^{\overline{Y}}$, one can verify that 
\begin{align*}
  \mathcal{F}\text{ is closed under uniform convergence }&\iff (\mathcal{F},d_{\infty})\text{ is complete }\\
&\iff \mathcal{F}\text{ is closed with respect to $(X^{\overline{Y}},d_{\infty})$ }
\end{align*}
Let $\mathcal{G}$ be a galaxy of $(X^{\overline{Y}},d_{\infty})$. With multiple ways, we can verify that $\mathcal{G}$ is closed with respect to $(X^{\overline{Y}},d_{\infty})$. Then, acknowledging the space of bounded functions $B(X,\overline{Y})$ is a galaxy of $X^{\overline{Y}}$, we see that $B(X,\overline{Y})$ is closed under uniform convergence. The statement that $B(X,\overline{Y})$ is closed under uniform convergence, although already "proved" before as we pointed out the limit of uniform convergent sequence of bounded functions must be bounded, is now in fact actually proved in the sense the term "closed under uniform convergence" is formally given a satisfying definition.
\end{mdframed}
\section{Arzelà–Ascoli Theorem}
\begin{mdframed}
In this section, we will give a complete proof of Arzelà–Ascoli Theorem for functions from arbitrary compact topological space to arbitrary metric space. Note that in Baby Rudin, Arzelà–Ascoli Theorem are given for functions from compact metric space to metric space. Because Arzelà–Ascoli Theorem are concerned with family of equicontinuos functions, it is crucial for us to give a definition to equicontinuity for functions from topological  space to metric space, for the sake of our generalization.\\

Let $X,Y$ be metric space. Let  $Z$ be topological space. Let  $\mathcal{F}_X$ be family of functions from $X$ to  $Y$, and let  $\mathcal{F}_Z$ be family of functions from $Z$ to  $Y$. We say  $\mathcal{F}_Z$ is \textbf{pointwise equicontinuous} if 
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
       For all $\epsilon $ and for all $x$, there exists a neighborhood $U_x$ such that  $d_Y(f(x),f(y))<\epsilon $ for all $y \in U_x$   
   \end{minipage}
\end{center}
We say $\mathcal{F}_X$ is \textbf{equicontinuous} if 
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
       For all $\epsilon $, there exists $\delta$ such that  $d_{Y}(f(x),f(y))<\epsilon $ for all $\delta$-close $x,y \in X$ and all  $f \in \mathcal{F}$. 
   \end{minipage}
\end{center}
It is easy to verify that if $\mathcal{F}_X$ is equicontinuous, then $\mathcal{F}_X$ is pointwise equicontinuous. The converse don't always hold true. Say, $\mathcal{F}=\set{n+x^2}_{n\inn}$, the set $\set{n+x^2}_{n\inn}$ is clearly pointwise equicontinuous on $\R$, and is not equicontinuous on $\R$, since no function $n+x^2$  is uniform continuous on $\R$. However, the same set $\mathcal{F}=\set{n+x^2}$ is equicontinous on compact domain $[a,b]$. This is a general result, as we shall prove below.
\end{mdframed}
\begin{theorem}
\textbf{(Pointwise Equicontinous is Uniform on Compact Domain)} Given two metric space $(X,d_X),(Y,d_Y)$, and a family $\mathcal{F}$ of functions from $X$ to  $Y$ such that 
 \begin{enumerate}[label=(\alph*)]
  \item $X$ is compact 
   \item $\mathcal{F}$ is pointwise equicontinuous
\end{enumerate}
Then 
\begin{align*}
\mathcal{F}\text{ is equicontinuous }
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish to 
\begin{align*}
\vi{\text{ find $\delta$ such that $d_X(x,y)<\delta \implies d_Y(f(x),f(y))\leq \epsilon $ for all $f \in \mathcal{F}$ }}
\end{align*}
Because $\mathcal{F}$ is pointwise equicontinuous, we know for each $x \in X$, there exists $\delta_x$ such that 
\begin{align}
\label{PEe1}
\forall y\in B_{\delta_x}(x), d_Y\big(f(x),f(y) \big)<\frac{\epsilon}{2} \text{ for all $f\in \mathcal{F}$ }
\end{align}
It is clear that $\set{B_{\frac{\delta_x}{2}}(x):x \in X}$ form an open cover of $X$. Then because  $X$ is compact, we know  
 \begin{align*}
\text{ there exists a finite open sub-cover: }\set{B_{\frac{\delta_x}{2}}(x):x \in X_{\text{finite}}}
\end{align*}
We claim 
\begin{align*}
\vi{\delta=\min_{x \in X_{\text{finite}}}\frac{\delta_x}{2}\text{ works }}
\end{align*}
Fix $y,z \in X: d_X(y,z)<\delta$. We have to prove 
\begin{align*}
\vi{d_Y\big(f(y),f(z) \big)< \epsilon }
\end{align*}
We know $y$ must lie in some  $B_\frac{\delta_x}{2}(x)$ for some $x \in X_\text{finite}$. Because $d_X(y,z)<\frac{\delta_x}{2}$, we see that $z$ must lie in  $B_{\delta_x}(x)$. We now know $y,z$ are both in  $B_{\delta_x}(x)$. Then from \myref{(}{PEe1}), we can now deduce 
\begin{align*}
d_Y\big(f(y),f(z) \big)\leq d_Y\big(f(y),f(x) \big)+d_Y\big(f(x),f(z) \big)< \epsilon \vdone
\end{align*}
\end{proof}
\begin{mdframed}
The proof above should be a great example why in the discussion of metric space, instead of using sequential definition of compactness, which leads to the beautiful Bolzano-Weierstrass Theorem, some people prefer the open-cover definitions.\\

Now, we give proof for the Arzelà–Ascoli Theorem. 
\end{mdframed}
\begin{theorem}
\textbf{(Arzelà–Ascoli Theorem)} Given a compact topological space $(X,\tau)$, a metric space $(Y,d_Y)$, and a family $\mathcal{F}\subseteq C\big(X,Y \big)$ of continuous function  
 \begin{align*}
&\mathcal{F}\text{ is pointwise equicontinuous and $\set{f(x):f \in \mathcal{F}}$ has compact closure in $Y$ for all $x \in X$}\\
&\implies \mathcal{F}\text{ has a compact closure in $C(X,Y)$}
\end{align*}
\end{theorem}
\begin{proof}
Fix a sequence $f_n$ in $\mathcal{F}$. We wish to show
\begin{align*}
\vi{f_n\text{ has a sub-sequence $f_{n_k}$ uniformly converge to some }f:X\rightarrow Y}
\end{align*}
First, we prove 
\begin{align*}
\blue{\text{ there exists a countable set $P$ such that $P$ works like a dense set }}
\end{align*}
Because $\mathcal{F}$ is pointwise equicontinuous, we know for all $x \in X$ 
\begin{align*}
\exists U_{x,n}, \forall y \in U_{x,n},\forall f \in \mathcal{F}, d_Y\big(f(x),f(y) \big)<\frac{1}{n} \text{ for each fixed $n\inn$ }
\end{align*}
Now, because $X$ is compact, for each $n\inn$, there exists a finite subset $P_n\subseteq X$ such that $\bset{U_{x,n}:x \in P_n}$ is a cover of $X$. Let $P=\bigcup_{n\inn}P_n$. $\bdone$\\

Now, we wish to 
\begin{align*}
\olive{\text{ construct a sub-sequence $f_{n_k}$ pointwise converge on $P$ }}
\end{align*}

Express $P=\set{p_k}_{k\inn}$. By premise (pointwise image has compact closure), we know there exists a compact set that contain $\set{f_n(p_1)}_{n\inn}$, so by Bolzano-Weierstrass Theorem, there exists a sub-sequence 
\begin{align*}
\bset{f_{g_1(k)}(p_1)}_{k\inn}\text{ converge to some point in $Y$ }
\end{align*}
Now, again by premise and Bolzano-Weierstrass Theorem, there exists a sub-sequence 
\begin{align*}
\bset{f_{g_2\circ g_1(k)}(p_2)}_{k\inn}\text{ converge to some point in $Y$ }
\end{align*}
Repeatedly doing such, we have 
\begin{align*}
\begin{matrix} 
  f_{g_1(1)}(p_1) & f_{g_2\circ g_1 (1)}(p_2) & f_{g_3\circ g_2 \circ  g_1 (1)}(p_3) & \cdots \\
  f_{g_1(2)}(p_1) & f_{g_2\circ g_1 (2)}(p_2) & f_{g_3\circ g_2 \circ  g_1 (2)}(p_3) & \cdots \\
  f_{g_1(3)}(p_1) & f_{g_2\circ g_1 (3)}(p_2) & f_{g_3\circ g_2 \circ  g_1 (3)}(p_3) & \cdots \\
  \vdots &\vdots &\vdots & \ddots\\
  \downarrow & \downarrow &\downarrow &\\
  y_1 & y_2 & y_3 & \cdots 
\end{matrix}
\end{align*}
Now, let 
 \begin{align*}
n_k=g_k\circ \cdots \circ g_1(k)
\end{align*}
Then 
\begin{align*}
n_k\text{ is eventually a sub-sequence of $g_m\circ \cdots \circ g_1(k)$ for all $m$ }
\end{align*}
This then implies 
\begin{align*}
f_{n_k}(p_m)\to y_m\text{ for all $p_m \in P \odone$ }
\end{align*}
Next, we show 
\begin{align*}
\blue{\text{ To prove $f_{n_k}$ uniformly converge on $X$, it suffice to prove $f_{n_k}$ is uniformly Cauchy on $X$. }}
\end{align*}
By premise (pointwise image has compact closure), if $f_{n_k}$ is uniformly Cauchy, then we know $f_{n_k}$ pointwise converge to some $f$.\\

Fix $\epsilon $. We reduced the problem into 
\begin{align*}
\blue{\text{ finding  $N$ such that for all $k>N$, we have  $d_Y\big(f_{n_k}(x), f(x)\big)\leq \epsilon $ for all $x\in X$ }}
\end{align*}
Because $f_{n_k}$ is uniformly Cauchy, we know there exists $N$ such that for all $m,k>M$ $d_Y\big(f_{n_k}(x),f_{n_m}(x) \big)\leq \frac{\epsilon}{2}$ for all $x \in X$. We claim 
\begin{align*}
 \blue{\text{ such $N$ works }}
\end{align*}
Let $k>N$.  \As{$d_Y\big(f_{n_k}(x),f(x) \big)>\epsilon $}. We see that 
 \begin{align*}
   d_Y\big(f(x),f_{n_m}(x) \big)\geq d_Y\big(f(x),f_{n_k}(x)\big)-d_Y\big(f_{n_k}(x),f_{n_m}(x) \big)>\frac{\epsilon}{2}\text{ for all $m>N\tCaC \bdone$} 
\end{align*}
Lastly, we wish to prove 
\begin{align*}
\vi{f_{n_k}\text{ is uniformly Cauchy }}
\end{align*}
Fix $\epsilon $. We wish  
\begin{align*}
\vi{\text{ to find }N\text{ such that }\forall j,k >N,\forall x\in X, d_Y\big(f_{n_j}(x),f_{n_k}(x)\big)\leq \epsilon  }
\end{align*}
Fix $m > \frac{3}{\epsilon }$. Express $P_m=\set{p^m_1,\dots ,p^m_u}$. Because $f_{n_k}(p^m_t)$ converge for each $t \in \set{1,\dots ,u}$, we know 
\begin{align*}
\forall t , \exists N_t, d_Y\big(f_{n_j}(p_t^m),f_{n_k}(p_t^m) \big)<\frac{\epsilon}{3}\text{ for all $j,k>N_t$ }
\end{align*}
We claim 
\begin{align*}
\vi{N=\max_t N_t\text{ works }}
\end{align*}
Fix $j,k>N$ and $x \in X$. We have to show 
\begin{align*}
  \vi{d_Y\big(f_{n_j}(x),f_{n_k}(x) \big)\leq \epsilon}
\end{align*}
Because $\set{U_{p_t^m,m}}$ form an open cover of $X$, we know there exists  $t$ such that $x \in U_{p_t^m,m}$. We can now deduce 
\begin{align*}
d_Y\big(f_{n_j}(x),f_{n_k}(x) \big)\leq d_Y\big(f_{n_j}(x),f_{n_j}(p_t^m) \big)+d_Y\big(f_{n_j}(p_t^m),f_{n_k}(p_t^m) \big)+d_Y\big(f_{n_k}(p_t^m),f_{n_k}(x) \big)<\epsilon 
\end{align*}
$\vdone$\\

\end{proof}
\section{Banach Fixed Point Theorem}
\begin{mdframed}
This section give a complete statement and proof of Banach Fixed Point Theorem. The setting is 
\begin{enumerate}[label=(\alph*)]
  \item a metric space $\Big(X,d_X \Big)$ 
  \item a subset $E\subseteq X$
  \item another metric space $\Big(Y,d_Y \Big)$ 
  \item a function $f:E\rightarrow Y$ 
  \item another function $g:E\rightarrow X$
\end{enumerate}
We say $f$ is a \textbf{contraction} on $E$ if there exists $r\in [0,1)$ such that 
\begin{align*}
\hspace{3cm}d_Y(f(x),f(y))\leq rd_X(x,y)\hspace{1.5cm}(x,y \in E)
\end{align*}
or equivalently 
\begin{align*}
\sup_{x\neq y \in E} \frac{d_Y(f(x),f(y))}{d_X(x,y)}<1
\end{align*}
Note that the restriction of a contraction is again a contraction. We say $g$ admits a \textbf{fixed point} $x$ if we have 
\begin{align*}
g(x)=x
\end{align*}
\end{mdframed}
\begin{theorem}
\label{BFPT}
\textbf{(Banach Fixed Point Theorem)} If $g$ is a contraction that maps $E$ into $X$, then 
\begin{align*}
g\text{ admits at most one fixed point }
\end{align*}
Moreover, if $E$ is complete and $g(E)\subseteq E$, then 
\begin{align*}
\text{ the fixed point exists }
\end{align*}
And if we use the notation $g^n$ to denote $g\circ g^{n-1}$, then for all $x\in E$,  
\begin{align*}
  \text{ the fixed point can be written in the form }\lim_{n\to \infty}g^n(x)
\end{align*}
\end{theorem}
\begin{proof}
We first 
\begin{align*}
\vi{\text{ prove the uniqueness of the fixed point }}
\end{align*}
Suppose $x,y$ are both fixed by $g$.  We have
\begin{align*}
d(g(x),g(y))=d(x,y)
\end{align*}
Because  $g$ is a contraction mapping,  this implies $d(x,y)=0$. $\vdone$ \\

Suppose $E$ is complete and $g(E)\subseteq E$. We now 
\begin{align*}
\blue{\text{ prove the existence of the fixed point }}
\end{align*}
Fix $x \in E$. Because we have already prove the uniqueness of the fixed point, we only have to prove 
\begin{align*}
\blue{\lim_{n\to \infty}g^n(x)\text{ exists and }\lim_{n\to \infty}g^n(x)\text{ is a fixed point of $g$ }}
\end{align*}
Because $E$ is complete, to prove $\lim_{n\to \infty}g^n(x)$ exists, we only have to prove
\begin{align*}
\olive{\set{g^n(x)}_{n\inn}\text{ is Cauchy }}
\end{align*}
Observe 
\begin{align*}
  d(g^n(x),g^{n+k}(x))&\leq \sum_{i=0}^{k-1}d(g^{n+i}(x),g^{n+i+1}(x))\\
  &\leq d(x,g(x))\sum_{i=0}^{k-1}r^{n+i}\\
  &\leq \frac{r^n}{1-r}d(x,g(x))\to 0\text{ as $n\to \infty$ }\odone
\end{align*}
Note that contraction is Lipschitz thus continuous, and note that $\lim_{n\to \infty}g^n(x) \in E$. This allow us to carry the below limit process 
\begin{align*}
  g\big(\lim_{n\to \infty}g^n(x)\big)=\lim_{n\to \infty}g(g^n(x))=\lim_{n\to \infty}g^{n+1}(x)=\lim_{n\to \infty}g^n(x)\bdone
\end{align*}
\end{proof}
\begin{mdframed}
Banach Fixed Point Theorem is one of the most important Theorem in Mathematics. It will be used to prove 
\begin{enumerate}[label=(\alph*)]
  \item \customref{IFT}{Inverse Function Theorem} 
  \item Picard-Lindelof Theorem  
  \item Nash-Embedding Theorem
\end{enumerate}
\end{mdframed}
\chapter{Algebraic Topology}
\section{Fundamental Group}
\chapter{Differential Calculus}
\section{Basic Technique on Sequence and Series}
\begin{abstract}
This 
\end{abstract}
\begin{theorem}
\label{WM-t}
\textbf{(Weierstrass M-test)} Given sequences $f_n:X\rightarrow \C$, and suppose 
\begin{align*}
\forall n\inn,\forall x\in X, \abso{f_n(x)}\leq M_n
\end{align*}
Then 
\begin{align*}
\sum_{n=1}^\infty M_n\text{ converge }\implies \sum_{n=1}^\infty f_n\text{ uniformly converge }
\end{align*} 
\end{theorem}
\begin{proof}
The proof follows from noting 
\begin{align*}
  \forall x\in X, \abso{\sum_{k=m}^n f_k(x)}\leq \sum_{k=m}^n \abso{f_k(x)}\leq \sum_{k=m}^n M_k
\end{align*}
\end{proof}
\begin{mdframed}
Note that in our proof of  \customref{WM-t}{Weierstrass M-test}, we reduce the proof for uniform convergence into uniform Cauchy, which is a technique we shall also use later in \customref{Abel's Test for Uniform Convergence}{Abel's Test for Uniform Convergence}.
\end{mdframed}
\begin{theorem}
\label{Summation by Part}
\textbf{(Summation by Part)} 
\begin{align*}
  f_ng_n-f_mg_m&=\sum_{k=m}^{n-1}f_k \Delta g_k + g_k \Delta f_k+ \Delta f_k \Delta g_k \\
  &=\sum_{k=m}^{n-1}f_k \Delta g_k + g_{k+1}\Delta f_k
\end{align*}
\end{theorem}
\begin{proof}
The proof follows induction which is based on 
\begin{align*}
f_mg_m+ f_m\Delta g_m+ g_m \Delta f_m +\Delta f_m \Delta g_m=f_{m+1}g_{m+1}
\end{align*} 
\end{proof}
\begin{mdframed}
Note that \customref{Summation by Part}{summation by part} is a result hold in all fields. We now introduce two tests that directly rely on \customref{Summation by Part}{summation by part}. 
\end{mdframed}

\begin{theorem}
\label{Dirichlet's Test}
\textbf{(Dirichlet's Test)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $a_n\to 0$ monotonically. 
  \item $\sum_{n=1}^N z_n$ is bounded.
\end{enumerate}
We have 
\begin{align*}
\sum a_nz_n\text{ converge }
\end{align*}
\end{theorem}
\begin{proof}
Define $Z_n\triangleq \sum_{n=1}^N z_n$ and let $M$ bound  $\abso{Z_n}$. Using \customref{Summation by Part}{summation by part} by letting $f_k=a_k$ and $g_k=Z_{k-1}$, we have
\begin{align*}
  \abso{\sum_{k=m}^{n}a_kz_k}&= \abso{a_{n+1}Z_n- a_mZ_{m-1}- \sum_{k=m}^n Z_k(a_{k+1}-a_k)}\\
                             &\leq \abso{a_{n+1}Z_n}+ \abso{a_mZ_{m-1}}+ \abso{\sum_{k=m}^n Z_k (a_{k+1}-a_k)}\\
  (\because a_n\text{ is monotone })\hspace{1cm}&\leq M\Big(\abso{a_{n+1}}+\abso{a_{m}}+\abso{a_{n+1}-a_m} \Big)\hspace{5cm}
\end{align*}
\end{proof}
\begin{theorem}
\label{Abel's Test for Uniform Convergence}
\textbf{(Abel's Test for Uniform Convergence)} Suppose $g_n:X\rightarrow \R$ is a uniformly bounded pointwise monotone sequence. Then given a sequence $f_n:X\rightarrow \R$, 
\begin{align*}
\sum f_n\text{ uniformly converge }\implies \sum f_ng_n\text{ uniformly converge }
\end{align*}
\end{theorem}
\begin{proof}
Define $R_n\triangleq \sum_{k=n}^{\infty}f_k$. Let $M$ uniformly bound $g_n$. Because $R_n\to 0$ uniformly, we can let $N$ satisfy 
 \begin{align*}
\forall n\geq N, \forall x\in X,\abso{R_n(x)}<\frac{\epsilon}{6M} 
\end{align*}
Then for all $n,m\geq N$, using \customref{Summation by Part}{summation by part}, we have
\begin{align*}
  \abso{\sum_{k=m}^{n}f_kg_k}&=\abso{\sum_{k=m}^n g_k\Delta R_k }\\
  &\leq \abso{R_{n+1}g_{n+1}}+ \abso{R_{m+1}g_{m+1}}+ \sum_{k=m}^n\abso{ R_{k+1}\Delta g_k} \\
  (\because g_n\text{ is pointwise monotone })\hspace{1cm}&\leq \abso{R_{n+1}g_{n+1}}+ \abso{R_{m+1}g_{m+1}}+ \frac{\epsilon }{6M} \abso{g_{n+1}-g_m}\leq \epsilon 
\end{align*}
\end{proof}
\begin{mdframed}
Although the proofs of \customref{Dirichlet's Test}{Dirichlet's Test} and \customref{Abel's Test for Uniform Convergence}{Abel's Test for Uniform Convergence} are quite similar, one should note that the "ways" \customref{Summation by Part}{summation by part} is applied are slightly different, as one use $R_n\triangleq \sum_{k=n}^{\infty}f_k$ instead of $\sum_{k=1}^n f_k$, like $Z_n\triangleq \sum_{j=1}^n z_j$. \\

Using \customref{Dirichlet's Test}{Dirichlet's Test}, one have the famous \textbf{alternating series test}. Another notable corollary of \customref{Dirichlet's Test}{Dirichlet's Test} is \customref{Abel's Test for Complex Series}{Abel's Test for Complex Series}. 
\end{mdframed}
\begin{theorem}
\label{Abel's Test for Complex Series}
\textbf{(Abel's Test for Complex Series)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $\sum z_n$ converge. 
  \item $b_n$ is a bounded monotone sequence.
\end{enumerate}
We have 
\begin{align*}
\sum z_nb_n\text{ converge }
\end{align*}
\end{theorem}
\begin{proof}
Denote $B\triangleq \lim_{n\to \infty}b_n$. By \customref{Dirichlet's Test}{Dirichlet's Test}, we know $\sum z_n(b_n-B)$ converge. The proof now follows form noting 
\begin{align*}
\sum z_nb_n= \sum z_n(b_n-B)+ B \sum  z_n
\end{align*}
\end{proof}
\begin{mdframed}
We now introduce the idea of absolute convergence, which we shall use throughout the remaining of the section.\\

By a \textbf{permutation} $\sigma:E\rightarrow E$ on some set $E$, we merely mean  $\sigma$ is a bijective function. We say $\sum z_n$ \textbf{absolutely converge} if $\sum \abso{z_n}$ converge, and say $\sum z_n$ \textbf{unconditionally converge} if for all permutation $\sigma:\N\rightarrow \N$, the series $\sum z_{\sigma (n)}$ converge and converge to the same value.
\end{mdframed}
\begin{theorem}
\label{ACSUC}
\textbf{(Absolutely Convergent Series Unconditionally Converge)} 
\begin{align*}
\sum z_n\text{ absolutely converge }\implies \sum z_n\text{ unconditionally converge }
\end{align*}
\end{theorem}
\begin{proof}
The fact $\sum z_n$ converge  follows from noting 
\begin{align*}
  \abso{\sum_{k=n}^mz_k}\leq \sum_{k=n}^m \abso{z_k}\leq \sum_{k=n}^{\infty} \abso{z_k}
\end{align*}
Now, fix $\epsilon $ and permutation $\sigma$. Let $N_1$ and $N_2$ satisfy 
\begin{align*}   
\sum_{n=N_1}^{\infty}\abso{z_n}<\frac{\epsilon}{2}\text{ and }\abso{\sum_{n=N}^{\infty}z_n}<\frac{\epsilon}{2}\text{ for all $N>N_2$ }
\end{align*}
Let $M\triangleq \max \set{N_1,N_2}$. Observe that for all  $N> \max_{1\leq r\leq M} \sigma^{-1}(r)$, we have 
\begin{align*}
\abso{\sum z_n - \sum_{n=1}^N z_{\sigma (n)}}\leq \abso{\sum_{n=M+1}^{\infty} z_n }+ \sum_{n=M+1}^{\infty} \abso{z_n}<\epsilon 
\end{align*}
\end{proof}
\begin{theorem}
\label{RRT}
\textbf{(Riemann Rearrangement Theorem)} If $\sum a_n$ converge but not absolutely, then for each $L\in\overline{\R}$, there exists a permutation $\sigma$ such that 
\begin{align*}
\sum a_{\sigma (n)}=L
\end{align*}
\end{theorem}
\begin{proof}
Define $a_n^+\text{ and }a_n^-$ by 
\begin{align*}
a_n^+\triangleq \max \set{a_n,0}\text{ and }a_n^-\triangleq \min \set{a_n,0}
\end{align*}
Because 
\begin{align*}
\sum (a_n^++a_n^-)\text{ converge but }\sum (a_n^+-a_n^-)=\infty
\end{align*}
We know 
\begin{align*}
\sum a_n^+=\sum (-a_n^-)=\infty
\end{align*}
WOLG, (why?), fix $L\in\R$ and suppose $a_n\neq 0$ for all $n$. Let $A=B=L$, and let two increasing sequence $\sigma^+,\sigma^-:\N\rightarrow \N$ satisfy 
\begin{align*}
  \sigma^+(k+1)&=\min \set{n\inn:a_n>0\text{ and }n>\sigma^+(k)}
\end{align*}
and similar for $\sigma^-$. Now, recursively define $p_k,q_k$ by 
\begin{align}
  \text{ $p_1$ is the smallest number such that  }&\sum_{n=1}^{p_1} a_{\sigma ^+(n)}\geq A\label{ba1}\\
  q_1\text{ is the smallest number such that }&\sum_{n=1}^{p_1}a_{\sigma^+(n)}+\sum_{n=1}^{q_1}a_{\sigma^-(n)}\leq B \label{ba2}\\
  p_{k+1}\text{ is the smallest number such that }&\sum_{n=1}^{p_{k+1}}a_{\sigma^+(n)}+\sum_{n=1}^{q_k}a_{\sigma^-(n)}\geq A\label{ba3}\\
  q_{k+1}\text{ is the smallest number such that }&\sum_{n=1}^{p_{k+1}}a_{\sigma^+(n)}+\sum_{n=1}^{q_{k+1}}a_{\sigma^-(n)}\leq B\label{ba4}
\end{align}
We then define $\sigma$ by 
\begin{align*}
  \sigma^+(1),\dots ,\sigma^+(p_1),\sigma^-(1),\dots ,\sigma^-(q_1),\sigma^+(p_1+1),\dots ,\sigma^+(p_2),\sigma^-(q_1+1),\dots ,\sigma^-(q_2),\dots 
\end{align*}
It then follows from 
\begin{align*}
  \abso{\sum_{n=1}^{p}a_{\sigma^+}(n)+\sum_{n=1}^{q_k}a_{\sigma^-}(n)-L}\leq \min \set{a_{\sigma^+(p_{k+1})},\abso{a_{\sigma^-(q_k)}}}\text{ for all  }p_k\leq p\leq p_{k+1}
\end{align*}
and $a_n\to 0$ that $\sum a_{\sigma (n)}=L$.
\end{proof}

\begin{mdframed}
Note that the method we deploy in the proof of \customref{RRT}{Riemann Rearrangement Theorem} can be used to control the sequence to have arbitrary large set of subsequential limits by modifying the number of $A,B$ in  \customref{ba1}{Equation (4.1), (4.2), (4.3) and (4.4)}.\\

Using \customref{RRT}{Riemann Rearrangement Theorem} and equation
\begin{align*}
\max_{1\leq r\leq d}\abso{x_n}\leq \abso{\textbf{x}}\leq \sum_{r=1}^d \abso{x_r}
\end{align*}
we can now generalize and strengthen \myref{Theorem}{ACSUC} to 
\begin{align*}
  \sum \textbf{x}_n \text{ absolutely converge }&\iff  \sum_n x_{n,r}\text{ absolutely converge for all }r\\
  &\iff \sum_n x_{n,r}\text{ unconditionally converge for all }r\\
  &\iff \sum \textbf{x}_n\text{ unconditionally converge }
\end{align*}
With this in mind, we can now well state the \customref{FTfDS}{Fubini's Theorem for Double Series}.
\end{mdframed}
\begin{theorem}
\label{FTfDS}
\textbf{(Fubini's Theorem for Double Series)} If 
\begin{align*}
\sum_n \sum_k \abso{z_{n,k}}\text{ converge } 
\end{align*}
Then 
\begin{align*}
\sum_{n,k}\abso{z_{n,k}}\text{ converge and }\sum_{n,k}z_{n,k}=\sum_n \sum_k z_{n,k} =\sum_k \sum_n z_{n,k}
\end{align*}
\end{theorem}
\begin{proof}
The fact $\sum z_{n,k}$ absolutely converge follow from 
\begin{align*}
\sum_{n=1}^N \sum_{k=1}^N \abso{z_{n,k}}\leq \sum_n \sum_k \abso{z_{n,k}}\text{ for all }N
\end{align*}
WOLG, it remains to prove 
\begin{align*}
\vi{\sum_{n,k}z_{n,k}=\sum_n \sum_k z_{n,k}}
\end{align*}
Because $\sum_n \sum_k \abso{z_{n,k}}$ converge, we can reduce the problem into proving the same statement for nonnegative series $a_{n,k}$. (why?)
\begin{align*}
  \vi{\sum_n \sum_k \abso{a_{n,k}}\text{ converge }\implies \sum_{n,k}a_{n,k}=\sum_n \sum_k a_{n,k}}
\end{align*}
Because 
\begin{align*}
\sum_{n=1}^N \sum_{k=1}^N a_{n,k} \leq  \sum_{n=1}^N \sum_{k} a_{n,k}\leq \sum_n \sum_k a_{n,k}\text{ for all }N
\end{align*}
we see 
\begin{align*}
\sum_{n,k}a_{n,k}\leq \sum_n \sum_k a_{n,k}
\end{align*}
It remains to prove 
\begin{align*}
  \vi{\sum_{n,k}a_{n,k}\geq \sum_{n} \sum_k a_{n,k}}
\end{align*}
Fix  $N\text{ and }\epsilon $. We reduce the problem into proving 
\begin{align*}
  \vi{\sum_{n,k}a_{n,k} \geq \sum_{n=1}^N \sum_{k} a_{n,k} - \epsilon}
\end{align*}
Let $K$ satisfy 
 \begin{align*}
\text{ For all $1\leq n\leq N$, }\sum_{k=K+1}^\infty a_{n,k} < \frac{\epsilon}{N} 
\end{align*}
It then follows 
\begin{align*}
\sum_{n,k} a_{n,k}\geq \sum_{n=1}^N \sum_{k=1}^K a_{n,k}\geq \sum_{n=1}^N \sum_k a_{n,k}-\epsilon  \vdone
\end{align*}

\end{proof}
\begin{Example}{\textbf{(Counter-Example for Fubini's Theorem for Double Series)}}{}
\begin{align*}
a_{n,k}\triangleq \begin{cases}
  1& \text{ if $n=k$ }\\
  -1& \text{ if $n=k+1$ }\\
  0& \text{ if otherwise }
\end{cases}
\end{align*}
\begin{align*}
\sum \abso{a_{n,k}}=\infty \text{ and }\sum_n \sum_k a_{n,k}=1\text{ and }\sum_k \sum_n a_{n,k}=0
\end{align*}
\end{Example}
\begin{theorem}
\label{Merten Cau}
\textbf{(Merten's Theorem for Cauchy Product)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $\sum_{n=0}^\infty a_n$ converge absolutely 
  \item $\sum_{n=0}^\infty a_n=A$
  \item $\sum_{n=0}^\infty b_n=B$ 
  \item $c_n=\sum_{k=0}^n a_kb_{n-k}$
\end{enumerate}
Then we have 
\begin{align*}
\sum_{n=0}^{\infty}c_n=AB
\end{align*}
\end{theorem}
\begin{proof}
We prove 
\begin{align*}
  \vi{\abso{B\sum_{n=0}^N a_n- \sum_{n=0}^Nc_n}\to 0\text{ as }N\to \infty}
\end{align*}
Compute 
\begin{align*}
B\sum_{n=0}^N a_n - \sum_{n=0}^N c_n&=\sum_{n=0}^N a_n (B-\sum_{k=0}^{N-n}b_k)\\
&=\sum_{n=0}^N a_n \sum_{k=N-n+1}^{\infty}b_k
\end{align*}
Because $\sum_{k=n}^{\infty}b_k\to 0$ as $n\to \infty$, we know there exists $M$ such that 
\begin{align*}
\label{me1}
\abso{\sum_{k=n}^{\infty}b_k}<M\text{ for all }n
\end{align*}
Let $N_0$ satisfy 
 \begin{align*}
\sum_{n=N_0+1}^{\infty} \abso{a_n}< \frac{\epsilon }{2M}
\end{align*}
Let $N_1>N_0$ satisfy 
 \begin{align*}
  \label{me2}
   \abso{\sum_{k=N-N_0+1}^{\infty}b_k}<\frac{\epsilon}{2(N_0+1)\sum_n \abso{a_n}} \text{ for all }N>N_1
\end{align*}
Now observe that for all $N>N_1$
\begin{align*}
  \abso{\sum_{n=0}^N a_n \Big(\sum_{k=N-n+1}^\infty b_k \Big)}\leq \sum_{n=0}^{N_0} \abso{a_n} \abso{\sum_{k=N-n+1}^{\infty}b_k}+ \sum_{n=N_0+1}^{N} \abso{a_n} \abso{\sum_{k=N-n+1}^{\infty}b_k}<\epsilon \vdone 
\end{align*}
\end{proof}
\begin{mdframed}
We first define the \textbf{limit superior} by 
\begin{align*}
\limsup_{n\to\infty} a_n\triangleq \lim_{n\to \infty} (\sup_{k\geq n}a_k)
\end{align*}
Note that $\limsup_{n\to\infty} a_n$ must exists because $(\sup_{k\geq n}a_k)_n$ is a decreasing sequence. 
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definition for Limit Superior)}
If we let $E$ be the set of subsequential limits of $a_n$
 \begin{align*}
E\triangleq \set{L\in\overline{\R}:L=\lim_{k\to \infty}a_{n_k}\text{ for some }n_k}
\end{align*}
The set $E$ is non-empty and 
\begin{align*}
\max E=\limsup_{n\to\infty} a_n
\end{align*}
\end{theorem}
\begin{proof}
Let $n_1\triangleq 1$. Recursively, because
\begin{align*}
\sup_{j\geq n_k}a_k\geq \limsup_{n\to\infty} a_n>\limsup_{n\to\infty} a_n - \frac{1}{k}\text{ for each }k
\end{align*}
We can let $n_{k+1}$ be the smallest number such that 
\begin{align*}
a_{n_{k+1}}>\limsup_{n\to\infty} a_n - \frac{1}{k}
\end{align*}
It is straightforward to check $a_{n_k}\to \limsup_{n\to\infty} a_n$ as $k\to \infty$. Note that no subsequence can converge to $\limsup_{n\to\infty} a_n+\epsilon $ because there exists $N$ such that  $\sup_{k\geq N}a_k<\limsup_{n\to\infty} a_n+\epsilon $. 
\end{proof}
\begin{mdframed}
  We can now state the \textbf{limit comparison test} as follows. Given a positive sequence $b_n$, 
\begin{align*}
  \limsup_{n\to\infty} \frac{\abso{z_n}}{b_n}\inr \text{ and }\sum b_n\text{ converge }&\implies \sum z_n\text{ absolutely converge } \\
 \liminf_{n\to\infty} \frac{b_n}{\abso{z_n}}>0\text{ and }\sum z_n \text{ diverge }&\implies \sum b_n\text{ diverge }
\end{align*}
\end{mdframed}
\begin{theorem}
\label{geometric series}
\textbf{(Geometric Series)} 
\begin{align*}
\abso{z}< 1 \implies \sum_{n=0}^{\infty} z^n= \frac{1}{1-z}
\end{align*}
\end{theorem}
\begin{proof}
The proof follows from noting  
\begin{align*}
  (1-z)\sum_{n=0}^N z^n=1-z^{N+1}\to 1\text{ as }N\to \infty
\end{align*}
\end{proof}

\begin{theorem}
\label{Ratio and Root Test}
\textbf{(Ratio and Root Test)} 
\begin{align*}
  \limsup_{n\to\infty} \sqrt[n]{\abso{z_n}}<1 \text{ or }\limsup_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}<1&\implies \sum z_n\text{ absolutely converge }\\
  \liminf_{n\to\infty} \sqrt[n]{\abso{z_n}}>1\text{ or }\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}>1 &\implies \sum  z_n\text{ diverge }
\end{align*}
\end{theorem}
\begin{proof}
The convergent part follows from comparison to an appropriate geometric series and the diverge part follows from noting $\abso{z_n}$ does not converge to $0$.
\end{proof}
\begin{theorem}
\textbf{(Root Test is Stronger Than Ratio Test)}
\begin{align*}
\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}\leq \liminf_{n\to\infty} \sqrt[n]{\abso{z_n}} \leq \limsup_{n\to\infty} \sqrt[n]{\abso{z_n}} \leq \limsup_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $ and WOLG suppose $\liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}>0$. We prove 
\begin{align*}
  \vi{\liminf_{n\to\infty} \sqrt[n]{\abso{z_n}}\geq \liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}-\epsilon  }
\end{align*}
Let $\alpha \inr $ satisfy 
\begin{align*}
\liminf_{n\to\infty}  \abso{\frac{z_{n+1}}{z_n}}-\epsilon <\alpha < \liminf_{n\to\infty} \abso{\frac{z_{n+1}}{z_n}}
\end{align*}
Let $N$ satisfy  
\begin{align*}
\text{ For all }n\geq N,\abso{\frac{z_{n+1}}{z_n}}>\alpha  
\end{align*}
We then see 
\begin{align*}
  \sqrt[N+n]{\abso{z_{N+n}}}\geq \sqrt[N+n]{\abso{z_N}\alpha^n}=\alpha \Big(\frac{\abso{z_N}^{\frac{1}{N+n}}}{\alpha ^{\frac{N}{N+n}}} \Big)\to \alpha \text{ as }n\to \infty \vdone
\end{align*}
The proof for the other side is similar.
\end{proof}
\begin{mdframed}
Lastly, we prove \customref{Cauchy's Condensation Test}{Cauchy's Condensation Test}, whose existence is almost solely for investigating \customref{p-Series}{p-Series}.
\end{mdframed}
\begin{theorem}
\label{Cauchy's Condensation Test}
\textbf{(Cauchy's Condensation Test)} Suppose $a_n\searrow 0$. We have 
\begin{align*}
\sum_{n=0}^{\infty} 2^na_{2^n}\text{ converge }\iff \sum_{n=1}^{\infty}a_n\text{ converge }
\end{align*}
\end{theorem}
\begin{proof}
Observe that for all $N\inn$ 
\begin{align*}
\sum_{n=0}^N 2^n a_{2^n}\geq \sum_{n=0}^N \sum_{k=1}^{2^n} a_{2^n+k-1} =\sum_{n=1}^{2^{N+1}-1}a_n
\end{align*}
and
\begin{align*}
 2\sum_{n=1}^{2^N-1} a_n= 2\sum_{n=1}^N \sum_{k=0}^{2^{n-1}-1}a_{2^{n-1}+k}\geq 2\sum_{n=1}^N 2^{n-1} a_{2^n}=\sum_{n=1}^N 2^na_{2^n}
\end{align*}
\end{proof}
\begin{theorem}
\label{p-Series}
\textbf{(p-Series)}
\begin{align*}
\sum_{n=1}^{\infty} \frac{1}{n^p}\text{ converge }\iff p>1
\end{align*}
\end{theorem}
\begin{proof}
Observe that
\begin{align*}
\sum_{n=0}^{\infty} 2^n \frac{1}{(2^n)^p}=\sum_{n=0}^{\infty}(2^{1-p})^n
\end{align*}
The result then follows from  \customref{Cauchy's Condensation Test}{Cauchy's Condensation Test} and \customref{geometric series}{geometric series}. 
\end{proof}
\section{Operator Norm}
\begin{abstract}
This section 
\end{abstract}
\begin{mdframed}
In this section, and particularly in functional analysis, we say a function $T$ between two metric space is a  \textbf{bounded operator} if $T$ always map bounded set to bounded set. In particular, if $T$ is a linear transformation between two normed space, we say $T$ is a \textbf{bounded linear operator}. Now, suppose $\mathcal{X},\mathcal{Y}$ are two normed space over $\R\text{ or }\C$. In space $L(\mathcal{X},\mathcal{Y})$, alternatively, we can define 
\begin{align*}
T\text{ is bounded } \overset{\triangle}{\iff} \exists M\inr,\forall x\in \mathcal{X}, \norm{Tx}\leq M\norm{x}
\end{align*}
The proof of equivalency is simple. For $(\longrightarrow )$, observe 
\begin{align*}
\norm{Tx}= \norm{x}\cdot\norm{T \frac{x}{\norm{x}}}\leq \Big(\sup \set{\norm{Ty}:\norm{y}=1} \Big)\norm{x}
\end{align*}
For $(\longleftarrow)$, observe 
\begin{align*}
\norm{Tx-Ty}=\norm{T(x-y)}\leq M \norm{x-y}
\end{align*}
We first show that \customref{LOB}{a linear transformation is continuous if and only if it is bounded}. 
\end{mdframed}
\begin{theorem}
\label{LOB}
\textbf{(Liner Operator is Bounded if and only if it is Continuous)} Given two normed space $\mathcal{X},\mathcal{Y}$ over $\R\text{ or }\C$ and  $T\in L(\mathcal{X},\mathcal{Y})$, we have 
\begin{align*}
T\text{ is a bounded operator }\iff T\text{ is continuous on $\mathcal{X}$}
\end{align*}
\end{theorem}
\begin{proof}
If $T$ is bounded, we see that $T$ is Lipschitz. 
\begin{align*}
\norm{Tx-Ty}\leq M \norm{x-y}
\end{align*}
Now, suppose $T$ is linear and continuous at $0$. Let $\epsilon $ satisfy 
\begin{align*}
\sup_{\norm{y}\leq \epsilon } \norm{Ty} \leq 1
\end{align*}
Observe that for all $x \in \mathcal{X}$, we have
\begin{align*}
\norm{Tx}= \frac{\norm{x}}{\epsilon } \bnorm{T \frac{\epsilon x}{\norm{x}}}\leq \frac{\norm{x}}{\epsilon}
\end{align*}
\end{proof}
\begin{mdframed}
Here, we introduce a new terminology, which shall later show its value. Given a set $X$, we say two metrics $d_1,d_2$ on $X$ are \textbf{equivalent}, and write $d_1\sim d_2$, if we have 
\begin{align*}
\exists m,M \inr^+, \forall x ,y\in X, md_1(x,y)\leq d_2(x,y) \leq Md_1(x,y)
\end{align*}
Now, given a fixed vector space $V$, naturally, we say two norms $\norm{\cdot}_1,\norm{\cdot}_2$ on $V$ are \textbf{equivalent} if 
\begin{align*}
\exists m,M \inr^+, \forall x\in X, m \norm{x}_1\leq \norm{x}_2 \leq M\norm{x}_1
\end{align*}
We say two metric $d_1,d_2$ on  $X$ are  \textbf{topologically equivalent} if the topology they induce on $X$ are identical.\\

A few properties can be immediately spotted.  
\begin{enumerate}[label=(\alph*)]
  \item Our definition of $\sim$ between metrics of a fixed $X$ is an equivalence relation.
  \item Our definition of $\sim$ between norms on a fixed $V$ is an equivalence relation.
  \item Equivalent norms induce equivalent metrics.
  \item Equivalent metrics are topologically equivalent. 
\end{enumerate}

We now prove \customref{ANoF}{if $V$ is finite-dimensional, then all norms on  $V$ are equivalent}. This property will later show its value, as used to prove \customref{LmoF}{linear map of finite-dimensional domain is always continuous} 
\end{mdframed}
\begin{theorem}
\label{ANoF}
\textbf{(All Norms on Finite-dimensional space are Equivalent)} Suppose $V$ is a finite-dimensional vector space over $\R\text{ or }\C$. Then 
\begin{align*}
\text{ all norms on $V$ are equivalent }
\end{align*}
\end{theorem}
\begin{proof}
Let $\set{e_1,\dots ,e_n}$ be a basis of $V$. Define $\infty$-norm $\norm{\cdot}_\infty$ on $V$ by 
\begin{align*}
\bnorm{\sum \alpha_i e_i}_{\infty}\triangleq  \max \abso{\alpha_i} 
\end{align*}
It is easily checked that $\norm{\cdot}_\infty$ is indeed a norm. Fix a norm $\norm{\cdot}$ on $V$. We reduce the problem into  
\begin{align*}
  \vi{\text{ finding $m,M\inr^+$ such that }m\norm{x}_\infty \leq \norm{x}\leq M\norm{x}_\infty}
\end{align*}
We first claim 
\begin{align*}
\blue{M=\sum \norm{e_i}\text{ suffices }}
\end{align*}
Compute 
\begin{align*}
\norm{x}= \bnorm{\sum \alpha_ie_i} \leq \sum \abso{\alpha _i} \norm{e_i} \leq \norm{x}_\infty \sum \norm{e_i}= M \norm{x}_\infty \bdone
\end{align*}
Note that reverse triangle inequality give us 
\begin{align}
\label{Lip1}
\Big|\norm{x}-\norm{y}\Big|\leq \norm{x-y} \leq M \norm{x-y}_\infty
\end{align}
Then we can check that 
\begin{enumerate}[label=(\alph*)]
  \item  $\norm{\cdot}:\Big(V,\norm{\cdot}_\infty \Big)\rightarrow \R$ is Lipschitz continuous because of \myref{Equation}{Lip1}.
  \item $S\triangleq \set{y\in V:\norm{y}_\infty=1}$ is sequentially compact in $\norm{\cdot}$ and non-empty. 
\end{enumerate}
Now, by EVT, we know $\min _{y \in S}\norm{y}$ exists. Note that $\min_{y \in S}\norm{y}>0$, since $0 \not\in S$. We claim 
\begin{align*}
  \olive{m= \min_{y \in S}\norm{y}\text{ suffices }}
\end{align*}
Fix $x \in V$ and compute 
\begin{align*}
m\norm{x}_\infty = \norm{x}_\infty (\min_{y \in S} \norm{y})\leq \norm{x}_\infty \cdot  \bnorm{ \frac{x}{\norm{x}_\infty}}=\norm{x}\odone \vdone
\end{align*}






\end{proof}
\begin{theorem}
\label{LmoF}
\textbf{(Linear map of Finite-dimensional Domain is always Continuous)} Given a finite-dimensional normed space $\mathcal{X}$ over $\R\text{ or }\C$, an arbitrary normed space  $\mathcal{Y}$ over $\R\text{ or }\C$ and a linear transformation  $T:\mathcal{X}\rightarrow \mathcal{Y}$, we have 
\begin{align*}
T\text{ is continuous }
\end{align*}
\end{theorem}
\begin{proof}
Fix $x\in \mathcal{X},\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find $\delta$ such that }\forall h\in \mathcal{X}: \norm{h}\leq \delta, \norm{T(x+h)-Tx}\leq \epsilon }
\end{align*}
Let $\set{e_1,\dots ,e_n}$ be a basis of $\mathcal{X}$. Note that $ \norm{ \sum \alpha_i e_i }_1\triangleq  \sum \abso{\alpha _i}$ is a norm. \customref{ANoF}{Because $\mathcal{X}$ is finite-dimensional, we know $\norm{\cdot}$ and $\norm{\cdot}_1$ are equivalent}. Then, we can fix $M\inr^+$ such that 
\begin{align*}
\hspace{2cm}\norm{x}_1 \leq M\norm{x}\hspace{0.5cm}(x \in V)
\end{align*}
We claim 
\begin{align*}
\vi{\delta= \frac{\epsilon }{M(\max \norm{Te_i} )}\text{ suffices }} 
\end{align*}
Fix $\norm{h}\leq \delta$ and express $h=\sum \alpha_i e_i$. Compute using linearity of $T$
\begin{align*}
  \norm{T(x+h)-Tx}&=\norm{\sum \alpha_i T e_i}\\
  &\leq \sum \abso{\alpha _i} \norm{Te_i}\\
  &\leq  \norm{h}_1 (\max \norm{Te_i} )\\
  &\leq M \norm{h}(\max \norm{Te_i})=\epsilon \vdone
\end{align*}
\end{proof}
\begin{mdframed}
We now see that, because \customref{LOB}{Linear transformation is bounded if and only if it is continuous} and \customref{LmoF}{Linear map of finite-dimensional domain is always continuous}, if $\mathcal{X}$ is finite-dimensional, then all linear map of domain $\mathcal{X}$ are bounded. A counter example to the generalization of this statement is followed. 
\begin{Example}{\textbf{(Differentiation is an Unbounded Linear Operator)}}{}
\begin{align*}
\mathcal{X}=\Big(\R[x]|_{[0,1]}, \norm{\cdot}_\infty \Big), D(P)\triangleq P'
\end{align*}
Note that $\set{x^n}_{n\inn}$ is bounded in $\mathcal{X}$ and $\set{D(x^n)}_{n\inn}$ is not.   
\end{Example}
Now, suppose $\mathcal{X},\mathcal{Y}$ are two fixed normed spaces over $\R$ or $\C$. We can easily check that the set $BL(\mathcal{X},\mathcal{Y})$ of bounded linear operators from $\mathcal{X}$ to $\mathcal{Y}$ form a vector space over whichever field $\mathcal{Y}$ is over.\\

Naturally, our definition of boundedness of linear operator derive us a norm on $BL(\mathcal{X},\mathcal{Y})$, as followed 
\begin{align}
\label{tnop}
\norm{T}_{\text{op}}\triangleq \inf \set{M\inr^+ :\forall x \in \mathcal{X}, \norm{Tx}\leq M\norm{x}}
\end{align}
Before we show that our definition is indeed a norm, we first give some equivalent definitions and prove their equivalency. 
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definitions of Operator Norm)} Given two fixed normed space $\mathcal{X},\mathcal{Y}$ over $\R$ or  $\C$, a bounded linear operator  $T:\mathcal{X}\rightarrow \mathcal{Y}$, and define $\norm{T}_{\text{op}}$ as in \myref{Equation}{tnop}, we have 
\begin{align*}
\norm{T}_{\text{op}}=\sup_{x\in \mathcal{X},x\neq 0} \frac{\norm{Tx}}{\norm{x}}
\end{align*}
\end{theorem}
\begin{proof}
Define $J\triangleq \set{M\inr^+:\forall x \in \mathcal{X},\norm{Tx}\leq M\norm{x}}$ and observe 
\begin{align*}
J&=\set{M\inr^+:M\geq \frac{\norm{Tx}}{\norm{x}},\forall x\neq 0\in \mathcal{X}}
\end{align*}
This let us conclude 
\begin{align*}
\sup_{x \in \mathcal{X},x\neq 0} \frac{\norm{Tx}}{\norm{x}}=\min  J= \norm{T}_{\text{op}}
\end{align*}
\end{proof}
\begin{mdframed}
It is now easy to see 
\begin{align}
  \norm{T}_{\text{op}}&=\sup_{x \in \mathcal{X},x\neq 0} \frac{\norm{Tx}}{\norm{x}}\label{equivdefopnorm1}\\
&=\sup_{x\in \mathcal{X},\norm{x}=1} \norm{Tx}\label{equivdefopnorm}
\end{align}
It is not all in vain to introduce the equivalent definitions. See that the verification of  $\norm{\cdot}_{\text{op}}$ being a norm on $BL(\mathcal{X},\mathcal{Y})$ become simple by utilizing the equivalent definitions. 
\begin{enumerate}[label=(\alph*)]
  \item For positive-definiteness, fix non-trivial $T$ and fix $x\in \mathcal{X}\setminus N(T)$. Use \myref{Equation}{equivdefopnorm1} to show $\norm{T}_{\text{op}}\geq \frac{\norm{Tx}}{\norm{x}}>0$. 
  \item For absolute homogenity, use \myref{Equation}{equivdefopnorm} and $\norm{Tcx}=\abso{c}\cdot \norm{Tx}$.
  \item For triangle inequality, use \myref{Equation}{equivdefopnorm} and $\norm{(T_1+T_2)x}\leq \norm{T_1x}+\norm{T_2x}$. 
\end{enumerate}
Naturally, and very very importantly, \myref{Equation}{equivdefopnorm1} give us 
\begin{align*}
\hspace{3cm}\norm{Tx}\leq \norm{T}_\text{op}\cdot \norm{x}\hspace{1.5cm}(x\in \mathcal{X})
\end{align*}
This inequality will later be the best tool to help analyze the derivatives of functions between Euclidean spaces, and perhaps better, it immediately give us 
\begin{align*}
 \frac{\norm{T_1T_2x}}{\norm{x}}\leq \frac{\norm{T_1}_\text{op}\cdot \norm{T_2}_\text{op}\cdot \norm{x}}{\norm{x}}=\norm{T_1}_\text{op}\cdot \norm{T_2}_{\text{op}}
\end{align*}
Then \myref{Equation}{equivdefopnorm1} give us  
\begin{align*}
\norm{T_1T_2}_\text{op}\leq \norm{T_1}_\text{op}\cdot \norm{T_2}_\text{op}
\end{align*}
\end{mdframed}
\section{Directional Derivative and Gradient}
\label{Directional Derivative and Gradient}
\begin{abstract}
This short section introduce the idea of directional derivative and gradient. It shall be noted that, although both gradient and directional derivative are defined for real-valued function in this section, the notion of directional derivative can be easily generalized to function between Euclidean space; while the notion of gradient, as the way we define it, is only for real-valued function. 
\end{abstract}
\begin{mdframed}
Given two normed space $\mathcal{X},\mathcal{Y}$, suppose $f$ maps an open neighborhood $O$ around $x$ in $\mathcal{X}$ into $\mathcal{Y}$. We say $f$ is \textbf{differentiable at} $x$ if there exists a bounded linear transformation $A_x:\mathcal{X}\rightarrow \mathcal{Y}$ (from now, $A_x$ will be denoted $df_x$) such that  
\begin{align}
\label{defdi}
  \lim_{h\to 0} \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}=0 
\end{align}
Immediately, we should check that the linear approximation is unique. Suppose $df_x$ and  $df_x'$ both satisfy \myref{Equation}{defdi}. We are required to show $(df_x-df_x')h=0$ for all $\norm{h}_\mathcal{X}=1$. Fix $h\in \mathcal{X}$ such that $\norm{h}_\mathcal{X}=1$. Note that 
\begin{align*}
 \frac{(df_x-df'_x)th}{t}\text{ is a constant in $t$ for $t\neq 0$ }
\end{align*}
This then reduced the problem into showing 
\begin{align}
\label{goaldib}
\frac{(df_x-df'_x)th}{t\norm{h}_{\mathcal{X}}}\to 0\text{ as $t\to 0$ }
\end{align}
Observe 
\begin{align*}
  (df_x-df'_x)th&= \Big( f(x+th)-f(x)-df'_x(th)\Big) - \Big(f(x+th)-f(x)-df_x(th) \Big)
\end{align*}
which implies 
\begin{align*}
\norm{(df_x-df_x')th}_\mathcal{Y}\leq \norm{f(x+th)-f(x)-df'_x(th)}_{\mathcal{Y}}+ \norm{f(x+th)-f(x)-df_x(th)}_\mathcal{Y}
\end{align*}
and thus implies \myref{Equation}{goaldib}.\\

It shall be quite clear that a function $f$ differentiable at $x$ must be continuous at $x$, by noting the nominator of \myref{Equation}{defdi} must tend to $0$. For clarity, we here specify the notation. By $\R$, we mean a field equipped with the usual norm $\norm{x}\triangleq \abso{x}$. By  $\R^n$ we mean the set of functions from $\set{1,\dots ,n}$ to $\R$ equipped with the usual vector addition, scalar multiplication, dot product and induced norm. 
\end{mdframed}
\begin{definition}
\label{DoDDoS}
\textbf{(Definition of Directional Derivative of Scalar function)} Given a normal vector $v \inr^n$ and a function $f$ that maps an open-neighborhood $E$ around  $x \inr^n$ into $\R$, by the \textbf{directional derivative} $\partial _v f(x)$ of $f$ with respect to $v$ at $x$, we mean
\begin{align*}
\partial_v f(x)\triangleq \lim_{t\to 0} \frac{f(x+tv)-f(x)}{t}\text{ if exists }
\end{align*}
\end{definition}
\begin{mdframed}
Something to note in our definition for directional derivative (\myref{Definition}{DoDDoS})
\begin{enumerate}[label=(\alph*)]
  \item The limit on the right hand side is done in $\Big(\R ,\abso{\cdot}\Big)$ 
  \item If $f$ is differentiable at $x$, then we have
    \begin{align}
    \label{pavfx}
    \partial_{av+bw} f(x)=df_x(av+bw)=adf_x(v)+bdf_x(w)=a\partial_v f(x)+b\partial_w f(x)
    \end{align}
\end{enumerate}
With what we observed, one can immediately see that if a function $f$ is differentiable at $x$, then $f$ has directional derivative with respect to any direction at $x$. The converse is not true. It is possible that  $f$ has directional derivative with respect to all directions, and yet $f$ is still not differentaible. Consider 
\end{mdframed}
\begin{Example}{\textbf{(Discontinuous function such that all directional derivatives exist)}}{}
\begin{align*}
f:\R^2 \rightarrow \R, f(x,y)=\begin{cases}
 \frac{x^2y}{x^4+y^2} & \text{ if $x^2+y^2\neq 0$ }\\
 0& \text{ if $x=y=0$ }
\end{cases}
\end{align*}
\end{Example}
\begin{definition}
\textbf{(Definition of Gradient of $\R^n\rightarrow \R$ function)} Given a point $x\inr^n$ with open neighborhood $E$, a function $f:E\rightarrow \R$ differentiable at $x$, we define the \textbf{gradient} $\nabla f(x)\inr^n$ of $f$ at $x$ to be the unique vector that satisfy
\begin{align*}
\nabla f(x)\cdot v=df_x(v)\text{ for all $v\inr^n$ }
\end{align*}
\end{definition}
\begin{mdframed}
We should immediately discuss whether our definition of gradient is well-defined. The proof of existence and uniqueness follows from generating an orthogonal basis $\set{v_1,\dots ,v_n}$ and noting $\nabla f(x)$ must equal to $\sum_{i=1}^n df_x(v_i)v_i$.\\

A few things you must know about gradient is as followed
\begin{enumerate}[label=(\alph*)]
  \item $\nabla f(x)$ is only defined when $f$ is differentiable at  $x$.
  \item gradient $\nabla f(x)$ "points toward" the direction at which $f:\R^n\rightarrow \R$ grow the fastest. Suppose $v$ is normal. See 
\begin{align*}
\nabla f(x)\cdot v = df_x(v)=\partial_v f(x)
\end{align*}
Using Cauchy-Schwarz Inequality, we see that $\partial_v f(x)$ is of largest value when $v=\frac{\nabla f(x)}{\abso{\nabla f(x)}}$. If $v=\frac{\nabla f(x)}{\abso{\nabla f(x)}}$, then 
\begin{align*}
\partial_v f(x)=\abso{\nabla f(x)}
\end{align*}
\item It is possible $\nabla f(x)=0$. This is true if and only if $df_x$ maps  $\R^n$ into $0$. This fact echos with the fact gradient points toward the fastest growing direction. See (b).  
\item Given an orthogonal basis $\set{v_1,\dots ,v_n}$, we have
  \begin{align*}
  \nabla f(x)=\sum_{i=1}^n df_x (v_i)v_i=\sum_{i=1}^n \partial_{v_i}f(x)v_i
  \end{align*}
This is how you compute $\nabla f(x)$ when you have to.
\end{enumerate}
\end{mdframed}
\section{MVT}
\begin{abstract}
  This is a short section introducing MVT. It is proved here because it will be used in \customref{Section of DT}{next section}. 
\end{abstract}
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering       
\includegraphics[height=7cm,width=10cm]{CMVT.png}
   \end{minipage}
\end{center}
\hspace{3cm}
\begin{theorem}
\textbf{()}
\end{theorem}
\begin{theorem}
\label{CMVT}
\textbf{(Cauchy's MVT)} Given a function $f:[a,b]\to \R$ such that  
\begin{enumerate}[label=(\alph*)]
  \item $f,g$ are  differentiable on $(a,b)$
  \item $f,g$ are continuous on $[a,b]$
\end{enumerate}
There exists $x \in (a,b)$ such that 
 \begin{align*}
   \big[f(b)-f(a) \big]g'(x)=\big[g(b)-g(a) \big]f'(x)
\end{align*}
\end{theorem}
\begin{proof}
Define $h$ on  $(a,b)$ by 
\begin{align*}
h(x)\triangleq \big[f(b)-f(a)\big]g'(x)-\big[g(b)-g(a)\big]f'(x)
\end{align*}
We reduced our problem into finding $x \in (a,b)$ such that 
\begin{align*}
\vi{h(x)=0}
\end{align*}
Because $f,g$ are both differentiable on  $(a,b)$, we know there exists an anti-derivative $H$ of  $h$ on  $(a,b)$ such that
\begin{align*}
H(x)=\big[f(b) -f(a)\big]g(x)-\big[g(b)-g(a) \big]f(x)
\end{align*}
This let us reduce our problem into 
\begin{align*}
  \vi{\text{ finding a local extremum of $H$ on  $(a,b)$ }}
\end{align*}
Because $f,g$ are both continuous  on  $[a,b]$, we know $H$ is continuous on $[a,b]$. Then by EVT, we know 
 \begin{align*}
\exists x \in [a,b] , H(x)=\max_{t \in [a,b]}H(t)\text{ and }\exists y \in [a,b], H(y)=\min_{t \in [a,b]}H(t)
\end{align*}
If such $x,y$ is in $(a,b)$, we are done. If not, says that $x,y$ both are on end points $a$ or  $b$. Compute that 
\begin{align*}
H(a)=f(b)g(a)-g(b)f(a)=H(b)
\end{align*}
We see $H$ is constant on  $[a,b]$. Then all points in $(a,b)$ are extremums. $\vdone$
\end{proof}
\begin{corollary}
\label{MVT}
\textbf{(Lagrange's MVT)} Given a function $f:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable on  $(a,b)$ 
  \item $f$ is continuous on  $[a,b]$
\end{enumerate}
Then there exists $x \in (a,b)$ such that 
\begin{align*}
f'(x)=\frac{f(b)-f(a)}{b-a}
\end{align*}
\end{corollary}
\begin{proof}
The proof is done by applying \customref{CMVT}{Cauchy's MVT}, where $g(x)\triangleq x$.
\end{proof}
\begin{mdframed}
There are two hypotheses in Lagrange's MVT 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable on $(a,b)$ 
  \item $f$ is continuous on $[a,b]$
\end{enumerate}
They are all necessary. The necessity of differentiability on $(a,b)$ is clear as shown by the canonical example using absolute value. The necessity of continuity on $[a,b]$ can be shown by the example 
\begin{align*}
f(x)=\begin{cases}
  1& \text{ if $a<x\leq b$ }\\
  0& \text{ if  }x=a
\end{cases}
\end{align*}
\end{mdframed}
\section{L'Hospital Rule}
\section{Differentiability Theorem}
\label{Chapter of DT}
\begin{abstract}
This section prove
\begin{enumerate}[label=(\alph*)]
  \item \customref{DiJ}{The matrix representation of derivative for function between Euclidean spaces}
  \item \customref{DT}{Differentiability Theorem}
\end{enumerate}
Note that the proof of \customref{DT}{Differentiability Theorem} use \customref{MVT}{MVT} and the fact \customref{ANoF}{that all norms on $\R^{k}$ are equivalent} where $k=nm$, and utilize the Frobenius norm. 
\end{abstract}
\begin{mdframed}
Given an orthonormal basis  $\set{q_1,\dots ,q_m}$ of $\R^m$ and a function $f:\R^n \rightarrow \R^m$, we let $f_j(x)$ be a real number
\begin{align*}
f_j(x)=f(x)\cdot q_j
\end{align*}
It shall be clear that 
\begin{align*}
f(x)=\sum_{j=1}^m f_j(x)q_j
\end{align*}
which explain why we require $\set{q_1,\dots ,q_m}$ to be orthonormal in the first place. For brevity of the statement of the next theorem (\myref{Theorem}{DiJ}), we introduce another notation. If we are provided a normal basis $\set{e_1,\dots ,e_n}$ of $\R^n$, we denote $\partial_{e_i} f_j(x)$ by $\partial_i f_j(x)$
\end{mdframed}
\begin{theorem}
\label{DiJ}
\textbf{(Derivative is Jacobian)} Suppose  $\alpha =\set{e_1,\dots ,e_n}$ is a basis of $\R^n$, and $\beta =\set{q_1,\dots ,q_m}$ is an orthonormal basis of  $\R^m$. Suppose  $f$ maps an open neighborhood $O$ around $x\inr^n$ to $\R^m$.  Then 
\begin{align*}
  f\text{ is differentiable at }x\implies \begin{cases}
    \partial_{i}f_j(x)\text{ exists for all }i,j\\
    [df_x]_{\alpha }^{\beta }=\begin{bmatrix}
      \partial_1f_1(x)&\cdots & \partial_nf_1(x)\\
      \vdots & \ddots & \vdots\\
      \partial_1f_m(x) & \cdots & \partial_nf_m(x)
    \end{bmatrix}
  \end{cases}
\end{align*}
\end{theorem}
\begin{proof}
Suppose $e_1,\dots ,e_n$ are all normal. Fix $i,j$. We wish to show 
 \begin{align*}
\vi{\partial_i f_j(x) \text{ exists }}
\end{align*}
Because $f$ is differentiable at $x$, by definition of $df_x$, we have
\begin{align*}
\lim_{t\to 0} \frac{\abso{f(x+te_i)-f(x)-df_x(te_i)}}{\abso{te_i}}=0
\end{align*}
Set $R_i:\R\rightarrow \R^m$ by $R_i(t)\triangleq f(x+te_i)-f(x)-df_x(te_i)$. We have 
\begin{align}
\label{Dij1}
  \lim_{t\to 0}\frac{\abso{R_i(t)}}{\abso{t}}=0
\end{align}
Compute
\begin{align*}
f_j(x+te_i)-f_j(x)&=\big(f(x+te_i)-f(x) \big)\cdot q_j\\
&=\big(R_i(t)+df_x(te_i) \big) \cdot q_j\\
&= R_i(t)\cdot q_j+ tdf_x(e_i)\cdot q_j
\end{align*}
This then give us 
\begin{align*}
\frac{f_j(x+te_i)-f_j(x)}{t}=\frac{R_i(t)\cdot q_j}{t}+ df_x(e_i)\cdot q_j
\end{align*}
and 
\begin{align*}
  df_x(e_i)\cdot q_j - \frac{\abso{R_i(t)\cdot q_j}}{\abso{t}} \leq \frac{f_j(x+te_i)-f_j(x)}{t}\leq df_x(e_i)\cdot q_j + \frac{\abso{R_i(t)\cdot q_j}}{\abso{t}}
\end{align*}
By Cauchy-Schwarz Inequality, we now have 
\begin{align*}
  df_x(e_i)\cdot q_j - \frac{\abso{R_i(t)}}{\abso{t}} \leq \frac{f_j(x+te_i)-f_j(x)}{t}\leq df_x(e_i)\cdot q_j + \frac{\abso{R_i(t)}}{\abso{t}}
\end{align*}
Now applying Squeeze Theorem and \myref{Equation}{Dij1}, we have  
\begin{align*}
\partial_i f_j(x)=\lim_{t\to 0} \frac{f_j(x+te_i)-f_j(x)}{t}=df_x(e_i)\cdot q_j \vdone
\end{align*}
Using the fact $\beta $ is orthonormal, we now have 
\begin{align*}
df_x(e_i)=\sum_{j=1}^m \Big(df_x(e_i)\cdot q_j \Big)q_j=\sum_{j=1}^m \partial_if_j(x)q_j
\end{align*}
and suggest the matrix representation. 
\end{proof}
\begin{mdframed}
Note that the converse is not always true. It is possible that a function $f$ has all partial derivatives with respect to a given basis, or even all directions, and yet $f$ is still discontinuous. We have given an example already in \customref{last section}{Directional Derivative and Gradient}. Consider a less trivial one. 
\end{mdframed}

\begin{Example}{\textbf{(Non-differentiable Continuous Funciton with Partial Derivative)}}{}
\begin{align*}
f:\R^2 \rightarrow \R\text{ and }f(x,y)=\begin{cases}
  \frac{x\abso{y}}{\sqrt{x^2+y^2} }& \text{ if $(x,y)\neq 0$ }\\
  0& \text{ if $(x,y)=0$ }
\end{cases}
\end{align*}
We have 
\begin{align*}
\partial_xf(0)=\partial_yf(0)=0
\end{align*}
By \myref{Theorem}{DiJ} (Derivative is Jacobian), if $f$ is differentiable at $0$, then  $df_0$ must be trivial. Yet  
\begin{align*}
\frac{\abso{f(h,h)-f(0)-df_0(h,h)}}{\abso{(h,h)}}=\frac{h }{2\abso{h} }\not\to 0 
\end{align*}
Note that $f$ is continuous at  $0$, by observing 
\begin{align*}
  x^2+y^2- 2\abso{xy}=\big(\abso{x}-\abso{y} \big)^2\geq 0 \implies \frac{x^2+y^2}{2}\geq \abso{xy}
\end{align*}
which implies 
\begin{align*}
\abso{f}\leq \frac{\sqrt{x^2+y^2} }{2}
\end{align*}
\end{Example}
\begin{mdframed}
We now introduce a property of function between normed space that are stronger than differentiability. Given two normed space $\mathcal{X},\mathcal{Y}$, and an open $E\subseteq \mathcal{X}$, we say $f:E\rightarrow \mathcal{Y}$ is \textbf{continuously differentiable } on $\mathcal{Y}$ if the map $D:(E,\norm{\cdot}_\mathcal{X})\rightarrow \Big(BL(\mathcal{X},\mathcal{Y}), \norm{\cdot}_\text{op} \Big)$ defined by 
\begin{align*}
D(x)=df_x
\end{align*}
is continuous. Note that the definition of the term "continuously differentiable" coincide when $\mathcal{X}=\mathcal{Y}=\R$ and $df_x$ is just $h\mapsto f'(x)h$. We now give proof to the  \customref{DT}{Differentiability Theorem}, which links between the continuity of total derivative and the continuity of partial derivatives.
\end{mdframed}
\begin{theorem}
\label{DT}
\textbf{(Differentiability Theorem)} Suppose  $\alpha =\set{e_1,\dots ,e_n}$ is an orthonormal basis of $\R^n$, and $\beta =\set{q_1,\dots ,q_m}$ is an orthonormal basis of  $\R^m$. Suppose  $f$ maps an open set $E\subseteq\R^n$ to $\R^m$.  Then  
\begin{align*}
f\text{ is continuously differentiable on $E$ }\iff \partial_if_j\text{ exists and is continuous on $E$ for all $i,j$ }
\end{align*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

Fix $i,j$. Because $f$ is differentiable on $E$, we know  $\partial_i f_j$ exists on $E$ by \myref{Theorem}{DiJ}. Fix $x \in E$. We only have to show 
\begin{align*}
  \vi{\partial_i f_j\text{ is continuous at $x$ }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find }\delta\text{ such that } \abso{\partial_i f_j(y)-\partial_i f_j(x)}\leq \epsilon \text{ for all $\abso{y-x}<\delta$ }}
\end{align*}
Because $f$ is continuously differentaible at $x$, we know there exists  $\delta$ such that 
\begin{align*}
\norm{df_y-df_x}_\text{op}< \epsilon \text{ for all $\abso{y-x}\leq \delta$ }
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such $\delta$ suffices }}
\end{align*}
By the \customref{DiJ}{the matrix representation}, we know
\begin{align*}
\partial_i f_j(y)-\partial_i f_j(x)=(df_y-df_x)e_i \cdot q_j
\end{align*}
Then by Cauchy-Inequality, we have 
\begin{align*}
  \abso{\partial_i f_j(y)-\partial_if_j(x)}&\leq \abso{(df_y-df_x)e_i}\\
  &\leq \norm{df_y-df_x}_\text{op}<\epsilon \vdone
\end{align*}

$(\longleftarrow)$\\

We first show 
\begin{align*}
\vi{f\text{ is differentiable on $E$ }}
\end{align*}
We first prove 
\begin{align*}
\olive{\forall j\in \set{1,\dots, m}, f_j:\R^n\rightarrow \R\text{ is differentiable on $E$ }\implies f\text{ is differentiable on $E$ }}
\end{align*}
Fix $x\in E$. We wish to prove 
\begin{align*}
\olive{\text{ $f$ is differentiable at $x$ }}
\end{align*}
Define $A:E\rightarrow \R^m$ by 
\begin{align*}
A(h)\triangleq \sum_{j=1}^m  (df_j)_x (h)q_j
\end{align*}
We claim 
\begin{align*}
\olive{A\text{ suffices to be the $df_x$ }}
\end{align*}
Using the fact $q_j$ are orthonormal, we have
\begin{align*}
f(x+h)-f(x)-A(h)=\sum_{j=1}^m \big(f_j(x+h)-f_j(x)-(df_j)_x(h)  \big)q_j
\end{align*}
This give us 
\begin{align*}
\lim_{h\to 0} \frac{\abso{f(x+h)-f(x)-A(h)}}{\abso{h}}&=\lim_{h\to 0} \frac{\abso{\sum_{j=1}^m \big(f_j(x+h)-f_j(x)-(df_j)_x (h) \big)q_j}}{\abso{h}}\\
&\leq \lim_{h\to 0} \frac{\sum_{j=1}^m \abso{f_j(x+h)-f_j(x)-(df_j)_x(h)}}{\abso{h}}=0 \odone
\end{align*}
Fix $j \in \set{1,\dots ,m}$. We can now reduce the problem into 
\begin{align*}
  \vi{f_j:\R^n\rightarrow \R\text{ is differentiable on $E$ }}
\end{align*}
Fix $x \in E$. We wish to prove 
\begin{align*}
\vi{f_j\text{ is differentiable at $x$ }}
\end{align*}
Express $h=\sum_{i=1}^n h_ie_i$. Define $B:E\rightarrow \R$ by
\begin{align*}
B(h)=\sum_{i=1}^n \partial_i f_j(x)h_i
\end{align*}
We claim 
\begin{align*}
\vi{B\text{ suffices to be $(df_j)_x$ }}
\end{align*}
By continuity of each $\partial_i f_j$ on $E$, we can let  $\delta$ satisfy 
\begin{align*}
\abso{\partial_i f_j(y)-\partial_i f_j(x)}<  \frac{\epsilon }{n}  \text{ for all $y\in B_\delta(x)$  }
\end{align*}
We claim 
\begin{align*}
  \vi{\frac{\abso{f_j(y)-f_j(x)-B(y-x)}}{\abso{y-x}}\leq \epsilon \text{ for all }y \in B_\delta (x)}
\end{align*}
Express $y-x=\sum_{k=1}^n h_ke_k$. Define $v_0,\dots,v_n\inr^n$ by
\begin{align*}
v_0\triangleq 0\text{ and }v_k\triangleq \sum_{i=1}^k h_ie_i\text{ for all $k\in \set{1,\dots ,n}$ }
\end{align*}
Now observe
\begin{align*}
\frac{\abso{f_j(y)-f_j(x)-B(y-x)}}{\abso{y-x}}&=\frac{\abso{f_j(x+v_n)-f_j(x)-B(\sum_{k=1}^n h_ke_k)}}{\abso{y-x}}\\
&=\frac{\abso{\big(\sum_{k=1}^nf_j(x+v_k)-f_j(x+v_{k-1}) \big)- \sum_{k=1}^n \partial_k f_j(x)h_k   }}{\abso{y-x}}\\
&=\frac{\abso{\sum_{k=1}^n f_j(x+v_k)-f_j(x+v_{k-1})-\partial_k f_j(x)h_k}}{\abso{y-x}}\\
&=\frac{\abso{\sum_{k=1}^n f_j(x+v_{k-1}+h_ke_k)-f_j(x+v_{k-1})-\partial_k f_j(x)h_k}}{\abso{y-x}}\\
&=\frac{\abso{\sum_{k=1}^n \partial_k f_j(x+v_{k-1}+t_ke_k)h_k-\partial_k f_j(x)h_k}}{\abso{y-x}}\\
&\leq \frac{\sum_{k=1}^n \abso{\big(\partial_k f_j(x+v_{k-1}+t_ke_k)-\partial_k f_j(x) \big)h_k}}{\abso{y-x}}\\
&< \frac{\sum_{k=1}^n \frac{\epsilon }{n} \abso{h_k}}{\abso{y-x}}\leq \epsilon \vdone
\end{align*}
We now prove
\begin{align*}
\blue{f\text{ is continuously differentiable on $E$ }}
\end{align*}
Fix $\epsilon $ and $x\in E$. We are required 
\begin{align*}
\blue{\text{ to find $\delta$ such that $\norm{df_y-df_x}_\text{op}\leq \epsilon $ for all $y\in B_\delta (x)$ }}
\end{align*}
Note that one can define a norm $\norm{\cdot}_F$ called "Forbenius Norm" on $BL(\R^n,\R^n)$ by 
\begin{align*}
\norm{A}_F\triangleq \sqrt{\sum_{i=1}^n \sum_{j=1}^n a_{i,j}^2}  \text{ where }[A]_\alpha ^\beta = \begin{bmatrix}
  a_{1,1}& \cdots & a_{n,1}\\
  \vdots & \ddots & \vdots \\
  a_{1,n} & \cdots & a_{n,n}  
\end{bmatrix}
\end{align*}
Because \customref{ANoF}{all norms on finite-dimensional real vector spaces are equivalent}, we know there exists $M$ such that for all $x\in E$, we have 
\begin{align*}
\norm{df_x}_\text{op}\leq  M\norm{df_x}_F
\end{align*}
Because the partial derivatives are all continuous by definition, we can let $\delta$ satisfy 
\begin{align*}
  \big(\partial_i f_j(x+h)\big)^2-\big(\partial_i f_j(x)\big)^2< \frac{\epsilon^2 }{M^2n^2}\text{ for all $h \in B_\delta (0)$ }
\end{align*}
We claim 
\begin{align*}
\blue{\text{ such }\delta\text{ suffices }}
\end{align*}
Let $\abso{y-x}<\delta$. We see 
\begin{align*}
\norm{df_y-df_x}_\text{op}\leq M\norm{df_y -df_x}_F< M \sqrt{\sum_{i=1}^n \sum_{j=1}^n\frac{\epsilon^2 }{M^2n^2}}=\epsilon \bdone
\end{align*}












\end{proof}

\section{Product Rule and Chain Rule}
\begin{abstract}
This section prove  \customref{CR}{Chain Rule} 
\end{abstract}
\begin{mdframed}
Although 
\end{mdframed}
\begin{theorem}
\textbf{(Product Rule)} Given two function  $f,g:\R^d\rightarrow \R$ differentiable at $x$, we have 
\begin{align*}
\nabla (fg)(x)= g(x) \nabla f(x)+ f(x)\nabla g(x)
\end{align*}
\end{theorem}
\begin{proof}
Note that 
\begin{align*}
  \lim_{h\to 0}f(x+h)\Big[\frac{g(x+h)-g(x)-\nabla g(x)\cdot h}{\abso{h}} \Big]&= 0 \\
  \text{ and }\lim_{h\to 0} g(x)\Big[ \frac{f(x+h)-f(x)-\nabla f(x)\cdot h}{\abso{h}}\Big]=0
\end{align*}
Adding these two equations together, we have 
\begin{align*}
\lim_{h\to 0}  \frac{f(x+h)g(x+h)-f(x)g(x)-[g(x)\nabla f(x)+f(x)\nabla g(x)]\cdot h}{\abso{h}}=0
\end{align*}
\end{proof}
\begin{mdframed}
We now deduce for $f,g:\R^d\rightarrow \R^n$ that 
\begin{align*}
\nabla (f\cdot g)=\nabla \Big(\sum_{j=1}^n f_j\cdot g_j\Big)=\sum_{j=1}^n \nabla (f_j\cdot g_j)&=\sum_{j=1}^n g_j(x)\nabla f_j(x)+f_j(x)\nabla g_j(x)\\
                                                                                                                 &=(df_x)^t(g(x))+(dg_x)^t(f(x))
\end{align*}
\end{mdframed}
\begin{mdframed}
We now prove the Chain Rule for function between normed space. 
\end{mdframed}
\begin{theorem}
\label{CR}
\textbf{(Chain Rule)} Given three normed space $\mathcal{X},\mathcal{Y},\mathcal{Z}$, a point $x \in \mathcal{X}$, a function $g$ that map an open set $U\subseteq \mathcal{Y}$ containing $f(x)$ into $\mathcal{Z}$, a function $f$ that map an open-neighborhood around $x$ into $U$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable at $x$
   \item $g$ is differentiable at $f(x)$
\end{enumerate}
we have 
\begin{align*}
d(g\circ f)_x=dg_{f(x)}\circ df_{x}
\end{align*}
\end{theorem}
\begin{proof}
For brevity, we use $F\triangleq g\circ f$. We wish to prove 
\begin{align*}
\vi{\lim_{h\to 0}\frac{\norm{F(x+h)-F(x)-dg_{f(x)}df_x(h)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}=0}
\end{align*}
Fix $k\triangleq f(x+h)-f(x)$. Observe 
\begin{align*}
 F(x+h)-F(x)-dg_{f(x)}df_x(h)=\Big(g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) \Big) + dg_{f(x)}\big(k-df_x(h)\big)
\end{align*}
This now implies 
\begin{align*}
&\frac{\norm{F(x+h)-F(x)-dg_{f(x)}df_x(h)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}\text{ is smaller than }
\end{align*}
\begin{align*}
\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z+ \norm{dg_{f(x)}\big(k-df_x(h)\big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}} 
\end{align*}
This let us reduce the problem into proving 
\begin{align*}
  &\olive{ \lim_{h\to 0}\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} =0}\\
  \text{ and }&\blue{\lim_{h\to 0} \frac{\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}=0}
\end{align*}
We first prove 
\begin{align*}
\olive{ \lim_{h\to 0}\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} =0}
\end{align*}
Note that if $\norm{k}_\mathcal{Y}=0$, we have 
\begin{align*}
\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} =0
\end{align*}
Now, observe that 
\begin{align*}
\frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{h}_\mathcal{X}} = \frac{\norm{g\big(f(x)+k\big)-g\big(f(x)\big)- dg_{f(x)}(k) }_Z }{\norm{k}_\mathcal{Y}}\cdot \frac{\norm{k}_\mathcal{Y}}{\norm{h}_\mathcal{X}} 
\end{align*}
Because $h \to 0 \implies k\to 0$, we can now reduce the problem into proving 
\begin{align*}
\olive{\limsup_{h\to 0} \frac{\norm{k}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\text{ exists }}
\end{align*}
Observe 
\begin{align*}
\frac{\norm{k}_\mathcal{Y}}{\norm{h}_\mathcal{X}}&= \frac{\norm{f(x+h)-f(x)-df_x(h)+df_x(h)}_\mathcal{X}}{\norm{h}_\mathcal{X}}\\
&\leq \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}+\frac{\norm{df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\\
&\leq \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}} + \norm{df_x}_\text{op} \odone
\end{align*}
We now prove 
\begin{align*}
\blue{\lim_{h\to 0} \frac{\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}=0}
\end{align*}
Note that if $f(x+h)-f(x)-df_x(h)=0$, then $\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}=0$. Now, observe 
\begin{align*}
\frac{\norm{dg_{f(x)}\big(k-df_x(h) \big)}_\mathcal{Z}}{\norm{h}_\mathcal{X}}&= \frac{\norm{dg_{f(x)}\big(f(x+h)-f(x)-df_x(h) \big)}_\mathcal{Z}}{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}\cdot \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\\
&\leq \norm{dg_{f(x)}}_\text{op}\cdot \frac{\norm{f(x+h)-f(x)-df_x(h)}_\mathcal{Y}}{\norm{h}_\mathcal{X}}\to 0 \bdone
\end{align*}
\end{proof}
\begin{mdframed}
Our general version of \customref{CR}{Chain Rule}, together with the \customref{DiJ}{matrix representation of derivative}, in fact give us a special type of chain rule, i.e. 
\begin{align*}
  \frac{\partial }{\partial x}u(y,z)= \frac{\partial u}{\partial y}\frac{\partial y}{\partial x}+ \frac{\partial u}{\partial z}\frac{ \partial z}{\partial x}
\end{align*}
which one can manually check. 
\end{mdframed}
\section{Smoothness}
\begin{mdframed}
Given the set of all real-valued functions on an open set $E$ of $\R^n$, we can define the differentaible class by saying a function $f:E\rightarrow \R$ is of class $C^k$ if 
\begin{align*}
\frac{\partial^kf}{(\partial x_1)^{\alpha_1}\cdots (\partial x_n)^{\alpha_n}}\text{ is continuous on $E$ for all $\alpha_1+\cdots + \alpha_n=k$  }
\end{align*}
Alternatively, we can say a function $f:E\rightarrow \R$ is of $C^2$ if the function $D:E\rightarrow \big(L(\R^n,\R),\norm{\cdot}_\text{op} \big)$ defined by 
 \begin{align*}
D(x)=df_x
\end{align*}
is again differentaible on $E$. 
\end{mdframed}
\begin{theorem}
\label{SoMPD}
\textbf{(Structure of Mixed Partial Derivative)} Given an open set $E\subseteq \R^2$, a point $p\in  E$, a basis $\set{e_1,e_2}$ of $\R^2$, a function $f:E\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $\partial_1 f$ exists on $E$
   \item $\partial_2 f$ exists on $E$
     \item $\partial_{21} f$ exists on $E$ and is continuous at $p$
\end{enumerate}
Then 
\begin{align*}
\partial_{12} f(p)=\partial_{21}f(p)
\end{align*}
\end{theorem}
\begin{proof}
Express elements of $E$ in the basis $\set{e_1,e_2}$, and express $p=(a,b)$. We are required to prove 
\begin{align*}
  \vi{\lim_{h\to 0}\frac{\partial_2f(a+h,b)-\partial_2 f(a,b)}{h}=\partial_{21}f(a,b)}
\end{align*}
Define $\Delta(h,k)$ on $E\setminus p$ by 
\begin{align*}
\Delta(h,k)\triangleq f(a+h,b+k)-f(a,b+k)-f(a+h,b)+f(a,b)
\end{align*}
Note that because $\partial_2f$ exists on $E$, for all $h\neq 0$, we have 
 \begin{align*}
\lim_{k\to 0} \frac{\Delta (h,k)}{hk}=\frac{\partial_2 f(a+h,b)-\partial_2 f(a,b)}{h}
\end{align*}
This let us reduce the problem into proving 
\begin{align*}
  \vi{\lim_{h\to 0}\lim_{k\to 0} \frac{\Delta (h,k)}{hk}=\partial_{21}f(a,b)}
\end{align*}
We first show 
\begin{align}
\label{oea}
  \olive{\frac{\Delta (h,k)}{hk}=\partial_{21}f(x,y)\text{ for some $(x,y):\abso{x-a}<h\text{ and }\abso{y-b}<k$ }}
\end{align}
Define $u(t)$ by 
\begin{align*}
u(t)\triangleq f(t,b+k)-f(t,b)
\end{align*}
Compute 
\begin{align*}
u'(t)=\partial_1 f(t,b+k)-\partial_1 f(t,b)
\end{align*}
Then we have
\begin{align*}
\Delta (h,k)&=u(a+h)-u(a)\\
&=hu'(x)\text{ for some $x\in (a,a+h)$ by MVT (\myref{Corollary}{MVT}) }\\
&=h\big(\partial_1 f(x,b+k)-\partial_1 f(x,b) \big)
\end{align*}
Define $v(t)$ by 
\begin{align*}
v(t)\triangleq \partial_1 f(x,t)
\end{align*}
Compute 
\begin{align*}
v'(t)=\lim_{h\to 0}\frac{\partial_1 f(xe_1+(t+h)e_2)-\partial_1 f(xe_1+te_2)}{h}=\partial_{21}f(x,t)
\end{align*}
Then we have 
\begin{align*}
\Delta (h,k)&=h(\partial_1 f(x,b+k)-f(x,b))\\
&=h\big(v(b+k)-v(b)\big)\\
&=hk v'(y)\text{ for some $y\in (b,b+k)$ }\\
&=hk\partial_{21}f(x,y)\odone
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
  \vi{\text{ to find some $\delta$ such that for all }h \in (-\delta,\delta)\setminus 0, \abso{\lim_{k\to 0}\frac{\Delta (h,k)}{hk}-\partial_{21}f(x,y)}\leq \epsilon }
\end{align*}
Because of \olive{\myref{Equation}{oea}} and $\partial_{21}f$ is continuous at $p$, we know there exists $\delta$ such that for all $h,k\in (-\delta,\delta)\setminus 0$ , we have 
\begin{align*}
  \abso{\frac{\Delta ( h,k)}{hk}-\partial_{21}f(a,b)}<\frac{\epsilon }{2} 
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such }\delta\text{ works }}
\end{align*}
Fix $h\in (-\delta,\delta)\setminus 0$. Note that $\lim_{k\to 0}\frac{\Delta (h,k)}{hk}=\frac{\partial_2 f(a+h,b)-\partial_2 f(a,b)}{h}$ exists, so we can find small enough $k'$ such that
\begin{align*}
  0<\abso{k'}<\delta \text{ and } \abso{\lim_{k\to 0} \frac{\Delta (h,k)}{hk}- \frac{\Delta (h,k')}{hk'}}<\frac{\epsilon}{2}
\end{align*}
Now observe 
\begin{align*}
\abso{\lim_{k\to 0}\frac{\Delta (h,k)}{hk}-\partial_{21}f(x,y)}\leq \abso{\lim_{k\to 0}\frac{\Delta (h,k)}{hk}-\frac{\Delta (h,k')}{hk'}}+ \abso{\frac{\Delta (h,k')}{hk'}-\partial_{21}f(a,b)}\leq \epsilon \vdone
\end{align*}





\end{proof}
\begin{corollary}
\textbf{(Clairaut's Theorem on equality of mixed partial)} Given a basis $\set{e_1,\dots ,e_n}$ of $\R^n$, an open set $E\subseteq \R^n$, a function $f:E\rightarrow \R$ such that 
\begin{align*}
\partial_{ij}f\text{ exist and is continuous on }E\text{ for all }i,j \in \set{1,\dots ,n}
\end{align*}
Then 
\begin{align*}
\partial_{ij}f=\partial_{ji}f\text{ on $E$ for all }i,j\in \set{1,\dots ,n}
\end{align*}
\end{corollary}
\begin{proof}
Fix $p\in E$, and fix $i,j \in \set{1,\dots ,n}$. We are required to prove 
\begin{align*}
\vi{\partial_{ij}f(p)=\partial_{ji}f(p)}
\end{align*}
Express $p$ in the form  $p=x_0e_i+y_0e_j+v$ where $v\in\text{span}\Big(\set{e_1,\dots ,e_n}\setminus \set{e_i,e_j}\Big)$.\\

Because $E$ is open, we can let $B_\epsilon (p)\subseteq \R^n$ be contained by $E$. Define $U\subseteq \R^2$ by   
\begin{align*}
U= B_{\epsilon }(x_0,y_0)
\end{align*}
Check that 
\begin{align*}
\set{xe_i+ye_j+v \inr^n :(x,y)\in U}\subseteq E
\end{align*}
Define $g:U\rightarrow \R$ by 
\begin{align*}
g(x,y)\triangleq f(xe_i+ye_j+v)
\end{align*}
Check that 
\begin{align*}
\partial_1 g(x,y)=\partial_i f(xe_i+ye_j+v)\text{ and }\partial_2g(x,y)=\partial_j f(xe_i+ye_j+v)
\end{align*}
We can now apply \myref{Theorem}{SoMPD} to $g$ and have 
 \begin{align*}
\partial_{ij}f(p)=\partial_{12}g(x_0,y_0)=\partial_{21}g(x_0,y_0)=\partial_{ji}f(p)\vdone
\end{align*}
\end{proof}
\begin{mdframed}

\end{mdframed}
\begin{theorem}
\textbf{(Composition of Smooth Map is Smooth)}
\end{theorem}
\section{Holomorphic Functions}
\begin{abstract}
This is a short section introducing the idea of holomorphic and prove some basic properties of holomorphic functions, i.e., \customref{Cauchy Riemann Criteria}{Cauchy Riemann Criteria} and \customref{Product and Quotient Rule for Holomorphic Function}{Product and Quotient Rule for Holomorphic Function}. 
\end{abstract}
\begin{mdframed}
Given a complex-valued function $f$ defined on some open subset of  $\C$ containing  $z$, we say  $f$ is \textbf{holomorphic at $z$} if there exists some complex number denoted by $f'(z)$ such that 
\begin{align*}
\frac{f(z+h)-f(z)-f'(z)h}{h}\to 0\text{ as }h\to 0;h\inc
\end{align*}
Immediately, one can see that a holomorphic function is differentiable when viewed as a function between $\R^2$ with derivative 
\begin{align}
\label{cd}
  \begin{bmatrix}
    a & -b \\
    b & a
  \end{bmatrix}\text{ where }f'(z)=a+b i
\end{align}
With the form of derivative in mind, one may conjecture that holomorphic is a 'stricter' condition than merely differentiable when regarded as function between $\R^2$. This is exactly true. Consider the following example.
\end{mdframed}
\begin{Example}{\textbf{(A not holomorphic function)}}{}
\begin{align*}
f(z)\triangleq \overline{z}
\end{align*}
This is a linear function with matrix representation 
\begin{align*}
\begin{bmatrix}
  1&0 \\
  0& -1
\end{bmatrix}
\end{align*}
which doesn't fit the necessary form in \myref{Equation}{cd}.
\end{Example}
\begin{theorem}
\label{Cauchy Riemann Criteria}
\textbf{(Cauchy Riemann Criteria)} Given a complex-valued function $f$ defined on some open subset $U$ of $\C$ containing  $z$, if we write 
 \begin{align*}
f(x+yi)=u(x,y)+iv(x,y)
\end{align*}
where $u,v:U\rightarrow \R$, then the following two statements are equivalent.
\begin{enumerate}[label=(\alph*)]
  \item $f$ is holomorphic at  $z$. 
  \item $u,v$ are differentiable at  $z$ and 
\begin{align*}
    \frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}\text{ and }\frac{\partial u}{\partial y}=\frac{-\partial v}{\partial x}\text{ at }z
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
(a) to (b) is an immediate result of \customref{CR}{Chain Rule} and \customref{DiJ}{matrix representation of $f'(z)$}. Suppose  (b) is true. Let $h=h_1+ih_2$ and $z=x+yi$. Because $u,v$ are differentiable at  $(x,y)$, by the \customref{DiJ}{matrix representation of derivative}, we have 
\begin{align*}
\frac{f(z+h)-f(z)}{h}&= \frac{u(x+h_1,y+h_2)-u(x,y)}{h_1+ih_2}+ i \frac{v(x+h_1,y+h_2)-v(x,y)}{h_1+ih_2}\\
&=\frac{h_1u_x+h_2u_y+ih_1v_x+ih_2v_y+o(\abso{h})}{h_1+ih_2}\\
&=\frac{(h_1+ih_2)u_x+i(h_1+ih_2)v_x+o(\abso{h})}{h_1+ih_2}\\
&=u_x+iv_x+\frac{o(\abso{h})}{h_1+ih_2}\to u_x+iv_x
\end{align*}
\end{proof}
\begin{theorem}
\label{Product and Quotient Rule for Holomorphic Function}
\textbf{(Product and Quotient Rule for Holomorphic Function)} Given two function $f,g$ holomorphic at  $z$, we have 
 \begin{align*}
   (fg)'(z)= f'(z)g(z)+f(z)g'(z)
\end{align*}
and if $g(z)\neq 0$, we also have 
\begin{align*}
\Big(\frac{f}{g}\Big)'(z)=\frac{f'(z)g(z)-f(z)g'(z)}{(g(z))^2}
\end{align*}
\end{theorem}
\begin{proof}
Observe
\begin{align*}
  &\frac{f(z+h)g(z+h)-f(z)g(z)}{h}\\
  =&f(z+h)\Big[\frac{g(z+h)-g(z)}{h} \Big] +g(z) \Big[\frac{f(z+h)-f(z)}{h} \Big]\to f'(z)g(z)+f(z)g'(z)
\end{align*}
and 
\begin{align*}
\frac{\frac{f(z+h)}{g(z+h)}-\frac{f(z)}{g(z)}}{h}&= \frac{f(z+h)g(z)-f(z)g(z+h)}{g(z+h)g(z)h}\\
&=\frac{1}{g(z+h)g(z)}\Big[ g(z)\Big(\frac{f(z+h)-f(z)}{h}\Big)-f(z)\Big(\frac{g(z+h)-g(z)}{h} \Big) \Big]\\
&\to \frac{f'(z)g(z)-f(z)g'(z)}{(g(z))^2}
\end{align*}
\end{proof}
\section{Uniform Convergence and Differentiation}
\begin{abstract}
This is a section discussing the relationship between uniform convergence and differentiation, which heavily rely on the usage of \customref{MVT}{MVT}, and is used to prove \customref{AfaS}{Analytic function is smooth}. 
\end{abstract}
\begin{mdframed}
Before stating \myref{Theorem}{UCaD}, let's see three examples why we don't (can't) use the hypothesis: $f_n \to f$ uniformly in our statement of \myref{Theorem}{UCaD} 
\begin{Example}{\textbf{(Differentiable functions are NOT closed under uniform convergence)}}{}
\begin{align*}
X=[-1,1]\text{ and }f(x)=\abso{x}
\end{align*}
By \customref{WaT}{Weierstrass approximation Theorem}, there is a sequence of polynomials (differentiable) that uniformly converge to $f$, which is not differentiable at  $0$. 
\end{Example}
\begin{Example}{\textbf{(Derivative won't necessarily converge to the right place)}}{}
\begin{align*}
X=\R \text{ and }f_n(x)=\frac{\sin nx}{\sqrt{n} }
\end{align*}
Compute 
\begin{align*}
f'(x)=0 \text{ and }f'_n(x)=\sqrt{n} \cos nx
\end{align*}
\end{Example}
\begin{Example}{\textbf{(Derivative won't necessarily converge to the right place)}}{}
\begin{align*}
X=\R \text{ and }f_n(x)=\frac{x}{1+nx^2}
\end{align*}
Compute 
\begin{align*}
f=\tilde{0} \text{ and }f'_n(0)=1
\end{align*}
\end{Example}
Informally speaking, these examples together with the fact \customref{RIFac}{Riemann integral are closed under uniform convergence} should give you some ideas that differentiation and integration although are operations inverse to each other, are NOT symmetric. There is a certain hierarchy on continuous functions on a fixed compact interval. Thus, we have the next Theorem in its form. Note that in application, the next Theorem only require us to prove $f'_n$ uniformly converge, and doesn't require us to prove to where does it converge. 
\end{mdframed}
\begin{theorem}
\label{UCaD}
\textbf{(Uniform Convergence and Differentiation)} Given a bounded interval $[a,b]$ and some sequence of function $f_n:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f'_n$ uniformly converge on  $(a,b)$
  \item $f_n$ are continuous on  $[a,b]$
  \item $f_n(x_0)\to L$ for some $x_0 \in [a,b]$
\end{enumerate}
Then 
\begin{enumerate}[label=(\alph*)]
  \item $f_n$ uniformly converge on  $[a,b]$ 
  \item and
\begin{align*}
\Big(\lim_{n\to \infty}f_n \Big)'(x_0)=\lim_{n\to \infty}f_n'(x_0)\text{ on $(a,b)$ }
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\begin{align}
\label{fnun}
\vi{f_n\text{ uniformly converge on $[a,b]$}}
\end{align}
Fix $\epsilon $. We wish  
\begin{align*}
  \vi{\text{ to find $N$ such that $\norm{f_n-f_m}_\infty \leq \epsilon $ for all $n,m>N$}}
\end{align*}
Because $f_n(x_0)$ converge, and $f'_n$ uniformly converge, we know there exists $N$ such that 
 \begin{align}
\label{UCD1}
\begin{cases}
 \abso{f_n(x_0)-f_m(x_0)}<\frac{\epsilon}{2} \\
\norm{f_n'-f_m'}_\infty <\frac{\epsilon }{2(b-a)}
\end{cases}\text{ for all $n,m>N$ }
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Fix $x \in [a,b]$ and $n,m>N$. We first show
\begin{align*}
\olive{\abso{\big(f_n-f_m\big)(x)-\big(f_n-f_m \big)(x_0)}\leq \frac{\epsilon}{2}}
\end{align*}
Because $(f_n-f_m)'=f_n'-f_m'$, by \customref{MVT}{MVT} and \myref{Equation}{UCD1}, we can deduce 
\begin{align*}
 \abso{\big(f_n-f_m\big)(x)-\big(f_n-f_m \big)(x_0)}&=\Big|\big[(f_n-f_m)'(t)\big](x-x_0)\Big|\text{ for some $t$ between $x,x_0$ }\\
 &< \frac{\epsilon}{2(b-a)}\cdot \abso{x-x_0}\\
 &\leq \frac{\epsilon }{2(b-a)}\cdot (b-a)=\frac{\epsilon}{2}\hspace{0.3cm}\big(\because x,x_0  \in [a,b]\big)\odone
\end{align*}
Now, by \myref{Equation}{UCD1}, we have 
\begin{align*}
  \abso{\big(f_n-f_m \big)(x)}&\leq \abso{\big(f_n-f_m \big)(x)-\big(f_n-f_m \big)(x_0)}+\abso{\big(f_n-f_m \big)(x_0)}\\
&<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon \vdone
\end{align*}
Let $f:[a,b]\rightarrow \R$ be the limit of $f_n$. It remains to prove
\begin{align}
\label{UCAC2}
\blue{f'(x)=\lim_{n\to \infty}f_n'(x)\text{ on }(a,b)}
\end{align}
Fix $x\in (a,b)$ and define $\phi ,\phi_n:[a,b]\setminus x\rightarrow \R$ by 
\begin{align*}
\phi (t)\triangleq \frac{f(t)-f(x)}{t-x}\text{ and }\phi_n(t)\triangleq \frac{f_n(t)-f_n(x)}{t-x}
\end{align*}
It is clear that $\phi_n\to \phi$ pointwise on $[a,b]\setminus x$. We now show 
\begin{align*}
\olive{\phi_n\to \phi\text{ uniformly on }[a,b]\setminus x}
\end{align*}
Fix $\epsilon $. We have
\begin{align*}
  \olive{\text{ to find $N$ such that  $\abso{\phi_n(t)-\phi _m(t)}\leq \epsilon $ for all $n,m>N$ and  $t\in [a,b]\setminus x$ }}
\end{align*}
Because $f_n'$ uniformly converge on  $(a,b)$, we know there exists $N$ such that 
 \begin{align}
\label{CUaC1}
\norm{f_n'-f_m'}_\infty\leq \epsilon \text{ for all $n,m>N$ }
\end{align}
We claim 
\begin{align*}
\olive{\text{ such }N\text{ works }}
\end{align*}
Fix $n,m>N$ and $t \in [a,b]\setminus x$. We wish to prove 
\begin{align*}
\olive{\abso{\phi_n(t)-\phi_m(t)}\leq \epsilon }
\end{align*}
Because $(f_n-f_m)'=f_n'-f'_m$, by \customref{MVT}{MVT} and \myref{Equation}{CUaC1}, we can deduce 
\begin{align*}
  \abso{\phi_n(t)-\phi_m(t)}&\leq \abso{\frac{f_n(t)-f_n(x)}{t-x}-\frac{f_m(t)-f_m(x)}{t-x}}\\
                            &=\abso{\frac{\big(f_n-f_m\big)(t)-\big(f_n-f_m\big)(x)}{t-x}}\\
 &=\abso{\big(f'_n-f'_m\big)(t_0)}\text{ for some $t_0$ between $t,x$  }\\
&\leq \epsilon \odone
\end{align*}
Note that 
\begin{align*}
\lim_{n\to \infty}\lim_{t\to x}\phi_n(t)=\lim_{n\to \infty}f'(x)\text{ exists }
\end{align*}
We can now \customref{COLO}{exchange the limit} and see that the derivative of $f$ at $x$ exists. 
\begin{align*}
f'(x)=\lim_{t\to x}\phi (t)&=\lim_{t\to x}\lim_{n\to \infty}\phi_n(t)\\
&=\lim_{n\to \infty}\lim_{t\to x}\phi_n(t)=\lim_{n\to \infty}f'_n(x) \bdone
\end{align*}
\end{proof}
\begin{theorem}
\label{UCaH}
\textbf{(Uniform Convergence and Holomorphic)} Given a real number $r$ and some sequence of function $f_n:\overline{D_r(z_0)}\rightarrow \C$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f'_n$ uniformly converge on  $D_r(z_0)$
  \item $f_n$ are continuous on  $\overline{D_r(z_0)}$
  \item $f_n(v)\to L$ for some $v \in \overline{D_r(z_0)}$
\end{enumerate}
Then 
\begin{enumerate}[label=(\alph*)]
  \item $f_n$ uniformly converge on  $\overline{D_r(z_0)}$ 
  \item and
\begin{align*}
\Big(\lim_{n\to \infty}f_n \Big)'(z)=\lim_{n\to \infty}f_n'(z)\text{ on $D_r(z_0)$ }
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\begin{align}
\label{fnun}
\vi{f_n\text{ uniformly converge on $\overline{D_r(z_0)}$}}
\end{align}
Fix $\epsilon $. We wish  
\begin{align*}
  \vi{\text{ to find $N$ such that $\norm{f_n-f_m}_\infty \leq \epsilon $ for all $n,m>N$}}
\end{align*}
Because $f_n(v)$ converge, and $f'_n$ uniformly converge, we know there exists $N$ such that 
 \begin{align}
\label{UCD3}
\begin{cases}
 \abso{f_n(v)-f_m(v)}<\frac{\epsilon}{2} \\
\norm{f_n'-f_m'}_\infty <\frac{\epsilon }{8r}
\end{cases}\text{ for all $n,m>N$ }
\end{align}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Fix $z \in \overline{D_r(z_0)}$ and $n,m>N$. We first show
\begin{align*}
\olive{\abso{\big(f_n-f_m\big)(z)-\big(f_n-f_m \big)(z_0)}\leq \frac{\epsilon}{2}}
\end{align*}
Denote $f_n-f_m:\overline{D_r(z_0)}\rightarrow \C$ by $g$. Because 
\begin{align*}
\abso{g(z)-g(z_0)}\leq \abso{\text{Re}\Big(g(z)-g(z_0) \Big)}+ \abso{\text{Im}\Big(g(z)-g(z_0)\Big)}
\end{align*}
WOLG, we only have to prove 
\begin{align*}
\olive{\abso{\text{Re}\Big(g(z)-g(z_0) \Big)}\leq \frac{\epsilon}{4}}
\end{align*}
Because $\overline{D_r(z_0)}$ is convex, we can define $h:[0,1]\rightarrow \R$ by
\begin{align*}
h(t)\triangleq \text{Re}\Big(g(tz+(1-t)z_0) \Big)
\end{align*}
By \customref{CR}{Chain Rule} and \customref{DiJ}{matrix representation of derivative}, we see that for all $t\in (0,1)$ 
\begin{align*}
  h'(t)= ac-bd&\text{ where }z_0-z=a+b i \\
&\text{ and } g'(tz+(1-t)(z_0))=c+di
\end{align*}
Because $\abso{a+b i}\leq r$ and $\abso{c+di}\leq \frac{\epsilon}{8r}$ by  \myref{Equation}{UCD3}, if we use  \customref{MVT}{MVT}, we see that 
\begin{align*}
 \abso{\text{Re}\Big(g(z)-g(z_0) \Big)}=\abso{h(1)-h(0)}&=\abso{h(t)}\text{ for some $t\in (0,1)$ }\\
 &=\abso{ac}+\abso{bd}\leq \frac{\epsilon }{4}\odone
\end{align*}
Now, by \myref{Equation}{UCD3}, we have 
\begin{align*}
  \abso{\big(f_n-f_m \big)(z)}&\leq \abso{\big(f_n-f_m \big)(z)-\big(f_n-f_m \big)(v)}+\abso{\big(f_n-f_m \big)(v)}\\
&<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon \vdone
\end{align*}
Let $f:\overline{D_r(z_0)}\rightarrow \C$ be the limit of $f_n$. It remains to prove
\begin{align}
\label{UCAC4}
\blue{f'(z)=\lim_{n\to \infty}f_n'(z)\text{ on }D_r(z_0)}
\end{align}
Fix $z\in D_r(z_0)$ and define $\phi ,\phi_n:\overline{D_r(z_0)}\setminus z\rightarrow \R$ by 
\begin{align*}
\phi (u)\triangleq \frac{f(u)-f(z)}{u-z}\text{ and }\phi_n(u)\triangleq \frac{f_n(u)-f_n(z)}{u-z}
\end{align*}
It is clear that $\phi_n\to \phi$ pointwise on $\overline{D_r(z_0)}\setminus z$. We now show 
\begin{align*}
\olive{\phi_n\to \phi\text{ uniformly on }\overline{D_r(z_0)}\setminus z}
\end{align*}
Fix $\epsilon $. We have
\begin{align*}
  \olive{\text{ to find $N$ such that  $\abso{\phi_n(t)-\phi _m(t)}\leq \epsilon $ for all $n,m>N$ and  $t\in \overline{D_r(z_0)}\setminus z$ }}
\end{align*}
Because $f_n'$ uniformly converge on  $D_r(z_0)$, we know there exists $N$ such that 
 \begin{align}
\label{CUaC2}
\norm{f_n'-f_m'}_\infty\leq \frac{\epsilon}{4} \text{ for all $n,m>N$ }
\end{align}
We claim 
\begin{align*}
\olive{\text{ such }N\text{ works }}
\end{align*}
Fix $n,m>N$ and $u \in \overline{D_r(z_0)}$. We wish to prove 
\begin{align*}
\olive{\abso{\phi_n(u)-\phi_m(u)}\leq \epsilon }
\end{align*}
Denote $f_n-f_m:\overline{D_r(z_0)}\rightarrow \C$ by $g$. Because 
\begin{align*}
\abso{\phi_n(u)-\phi_m(u)}=\abso{\frac{g(u)-g(z)}{u-z}}\leq \frac{\abso{\text{Re}\Big(g(u)-g(z) \Big)}}{\abso{u-z}}+ \frac{\abso{\text{Re}\Big(g(u)-g(z) \Big)}}{\abso{u-z}}
\end{align*}
WOLG, we only have to prove 
\begin{align*}
\olive{\frac{\abso{\text{Re}\Big(g(u)-g(z) \Big)}}{\abso{u-z}}\leq \frac{\epsilon}{2}}
\end{align*}
Again, define $h:[0,1]\rightarrow \R$ by 
\begin{align*}
h(t)\triangleq \text{Re}\Big(g(tu+(1-t)z_0) \Big)
\end{align*}
Then by \customref{CR}{Chain Rule} and \customref{DiJ}{matrix representation of derivative}, we see that for all $t\in (0,1)$ 
\begin{align*}
  h'(t)= ac-bd&\text{ where }u-z=a+b i \\
&\text{ and } g'(tu+(1-t)(z))=c+di
\end{align*}
Now, by \customref{MVT}{MVT} and \myref{Equation}{CUaC2}, we can deduce 
\begin{align*}
  \frac{\abso{\text{Re}\Big(g(u)-g(z) \Big)}}{\abso{u-z}}=\frac{\abso{h(1)-h(0)}}{\abso{u-z}}&=\frac{\abso{h'(t)}}{\abso{a+b i}}\text{ for some }t \in (0,1)\\
&= \frac{\abso{ac}+\abso{bd}}{\abso{a+b i}}\leq \abso{c}+\abso{d}\leq \frac{\epsilon}{2} \odone
\end{align*}
Note that 
\begin{align*}
\lim_{n\to \infty}\lim_{t\to x}\phi_n(t)=\lim_{n\to \infty}f'(x)\text{ exists }
\end{align*}
We can now \customref{COLO}{exchange the limit} and see that the derivative of $f$ at $x$ exists. 
\begin{align*}
f'(x)=\lim_{t\to x}\phi (t)&=\lim_{t\to x}\lim_{n\to \infty}\phi_n(t)\\
&=\lim_{n\to \infty}\lim_{t\to x}\phi_n(t)=\lim_{n\to \infty}f'_n(x) \bdone
\end{align*}
\end{proof}
\section{Analytic Functions}
\begin{abstract}

\end{abstract}
\begin{mdframed}
In this section, by a  \textbf{power series}, we mean a pair $(z_0,c_n)$ where $z_0 \inc$ is called the \textbf{center} of power series, and $c_n\inz$ are the coefficients sequence. By \textbf{radius of convergence}, we mean a unique  $R \in \R^+_0 \cup  \infty$ such that 
 \begin{align*}
\sum_{n=0}^\infty c_n(z-z_0)^n
\begin{cases}
  \text{ converge absolutely }& \text{ if  }\abso{z-z_0}<R\\
  \text{ diverge }& \text{ if  }\abso{z-z_0}>R\\
\end{cases}
\end{align*}
Such $R$ always exist (and is unique, the uniqueness can be checked without computing the actual value of $R$) and is exactly 
\begin{align}
\label{Cauchy-Hadamard}
R=\frac{1}{\limsup_{n\to\infty} \sqrt[n]{c_n} }
\end{align}
This result is called \textbf{Cauchy-Hadamard Theorem} and is proved by applying \customref{Ratio and Root Test}{Root Test} to $\sum c_n(z-z_0)^n$. Note that Cauchy-Hadamard Theorem does not tell us whether a power series converges at points of boundary of disk of convergence. It require extra works to determine if the power series converge at boundary. 
\end{mdframed}
\begin{theorem}
\label{Abel's Test for Power Series}
\textbf{(Abel's Test for Power Series)} Suppose $a_n\to0$ monotonically and $\sum a_nz^n$ has radius of convergence $R$ . 
\begin{align*}
  \text{ The power series $\sum a_nz^n$ at least converge on $\overline{D_R(0)}\setminus \set{R}$ }
\end{align*}
\end{theorem}
\begin{proof}
Note that 
\begin{align*}
\sum \frac{a_n}{R^n}z^n\text{ has radius of convergence }R
\end{align*}
Fix $z\in \overline{D_R(0)}\setminus \set{R}$. Note that 
\begin{align*}
\abso{\sum_{n=0}\frac{z^n}{R^n}}= \abso{\frac{1-(\frac{z}{R})^{N+1}}{1-\frac{z}{R}}} \leq \frac{2}{\abso{1-\frac{z}{R}}}\text{ for all }N
\end{align*}
It then follows from  \customref{Dirichlet's Test}{Dirichlet's Test} that $\sum a_n (\frac{z}{R})^n$ converge.
\end{proof}
\begin{Example}{\textbf{(Discussion of Convergence on Boundary)}}{}
\begin{align*}
f_q(z)=\sum_{n=0}^\infty n^q z^n \text{ provided $q\inr$ }
\end{align*}
It is clear that  $f_q$ has convergence radius  $1$ for all  $q\inr$. For boundary, we have
\begin{align*}
\begin{cases}
  q<-1 \implies f_q\text{ converge on $S^1$ }\\
  -1\leq q<0 \implies f_q\text{ converge on }S^1\setminus \set{1}\\
  0 \leq q \implies f_q \text{ diverge on }S^1
\end{cases}
\end{align*}
Note that
\begin{enumerate}[label=(\alph*)]
  \item At $z=1$, the discussion is just \customref{p-Series}{p-Series}.  
  \item $n^q\searrow 0$ if and only if  $q<0$; and if  $n^q\searrow 0$, then the series converge by  \customref{Abel's Test for Power Series}{Abel's Test for Power Series}.  
  \item If $q\geq 0$, $n^qz^n$ does not converge to  $0$ on  $S^1\setminus \set{1}$
\end{enumerate}
\end{Example}

\begin{mdframed}
Notice that the fact $\sum c_n(z-z_0)^n$ absolutely converge in $D_R(z_0)$ implies the convergence is uniform on all $\overline{D_{R-\epsilon }(z_0)}$ by \customref{WM-t}{M-Test}. However, on $D_R(z_0)$, the convergence is not always uniform. 
\begin{Example}{\textbf{(Failure of Uniform Convergence on $D_R(z_0)$)}}{}
\begin{align*}
f(z)=\sum_{n=0}^\infty z^n
\end{align*}
Note $R=1$. Use \customref{geometric series}{Geometric Series Formula} to show $f(z)=\frac{1}{1-z}$ on $D_1(0)$. It is then clear that $f$ is unbounded on  $D_1(0)$ while all partial sums $\sum_{k=0}^n z^k$ is bounded on $D_1(0)$. 
\end{Example}

We now introduce some terminologies. We say a complex function $f$ is \textbf{analytic at} $z_0\inc$ if there exists a power series $(z_0,c_n)$ such that $f$ agrees with $\sum_{n=0}^\infty c_n(z-z_0)^n$ on $D_R(z_0)$ for some $R$  (of course, such $R$ must not be strictly greater than the radius of convergence of $(a,c_n)$).\\

It shall be quite clear that if $f,g$ are both analytic at  $z\inc$ with radius $R_f\leq R_g$, then $f+g$ and $fg$ are both analytic at  $z$ with radius at least $R_f$. (the fact $fg$ is analytic at $z$ with radius at least $R_f$ is an immediate consequence of  \customref{Merten Cau}{Merten's Theorem for Cauchy Product}). We now investigate deeper into analytic functions. We first prove that \customref{AfaS}{analytic functions are smooth}.  
\end{mdframed}
\begin{theorem}
\label{AfaS}
\textbf{(Term by Term Differentiation)} Given a power series $(z_0,c_n)$ of convergence radius $R$, if we define $f:D_R(z_0)\rightarrow \C$ by
\begin{align*}
f(z)\triangleq \sum _{n=0}^{\infty}c_n(z-z_0)^n 
\end{align*}
Then $f$ is holomorphic on $D_{R}(z_0)$ and its derivative is also a power series with radius of convergence at least $R$.
 \begin{align*}
f'(z)= \sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n
\end{align*}
\end{theorem}
\begin{proof}
We first prove that 
\begin{align*}
\vi{\text{ The power series }\sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n\text{ has radius of convergence at least }R}
\end{align*}
Note that 
\begin{align*}
  \sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n=\sum_{n=1}^{\infty}nc_n(z-z_0)^{n-1}
\end{align*}
Fix $z \in D_R(z_0)$. We are required to prove 
\begin{align*}
  \vi{\sum_{n=1}^{\infty} n \abso{c_n(z-z_0)^{n-1}}\text{ converge }}
\end{align*}
Let $r<R$ satisfy $z\in D_r(z_0)$. \customref{Cauchy-Hadamard}{Cauchy-Hadamard} tell us that 
\begin{align*}
\sum_{n=0}^{\infty} c_nr^n\text{ absolutely converge }
\end{align*}
which implies 
\begin{align*}
K\triangleq \sum_{n=0}^{\infty}\abso{c_n}r^{n-1}=\frac{\sum_{n=0}^{\infty}\abso{c_n}r^n}{r}\inr
\end{align*}
Now observe 
\begin{align*}
\sum_{n=1}^{\infty}n\abso{c_n(z-z_0)^{n-1}}&=\sum_{n=1}^{\infty} \abso{\frac{z-z_0}{r}}^{n-1}n\abso{c_n}r^{n-1}\\
&\leq K\sum_{n=1}^{\infty} nq^{n-1}\text{ where }q= \abso{\frac{z-z_0}{r}}<1
\end{align*}
The proof then follows from noting $\sum_{n=1}^{\infty} nq^{n-1}$ converge by \customref{Ratio and Root Test}{Root Test}. $\vdone$\\

We now prove 
\begin{align*}
\blue{f'(z)=\sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n\text{ on }D_R(z_0)}
\end{align*}
Define $f_m:D_R(z_0)\rightarrow \C$ by
 \begin{align*}
f_m(z)\triangleq \sum_{n=0}^{m} c_n(z-z_0)^n
\end{align*}
Observe 
\begin{enumerate}[label=(\alph*)]
  \item $f_m\to f$ pointwise on $D_R(a)$ 
  \item $f'_m(z)=\sum_{n=0}^{m-1}(n+1)c_{n+1}(z-z_0)^n$ for all $m$
\end{enumerate}
Fix $z\in D_R(z_0)$. Proposition (b) allow us to reduce the problem into proving
\begin{align}
\label{an1}
  \blue{f'(z)=\lim_{m\to \infty}f'_m(z)\text{ on $D_R(a)$ }}
\end{align}
Let $z\in D_r(z_0)$ where $r<R$. With proposition (a) in mind, to show \myref{Equation}{an1}, by \myref{Theorem}{UCaH}, we only have to prove $f'_m$ uniformly converge on $D_r(z_0)$, which follows from \customref{WM-t}{M-Test} and the fact that \vi{$\sum_{n=0}^{\infty}(n+1)c_{n+1}(z-z_0)^n$ absolutely converge on $D_R(z_0)$}. $\bdone$ 
\end{proof}
\begin{mdframed}



Now by \myref{Theorem}{AfaS}, if we are given a real function $f$ analytic at $a$, the power series representation $(a,c_n):\sum_{n=0}^\infty c_n(z-a)^n=f$ must be unique, since $f$ is proved to be infinitely differentiable at $a$ and proved to satisfy $c_k=\frac{f^{(k)}(a)}{k!}$.\\

Notice that the arguments above are all based on the hypothesis that $f$ is analytic, and that smoothness does not imply analytic. See the following examples.
\end{mdframed}
\begin{Example}{\textbf{(Smooth but not Analytic Function)}}{}
\begin{align*}
f(x)=\begin{cases}
  e^{\frac{-1}{x^2}}& \text{ if $x\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}
\end{align*}
Use induction to show that 
\begin{align*}
f^{(k)}(x)=P_k(\frac{1}{x})e^{-(\frac{1}{x})^2}\hspace{0.5cm}\exists P_k\inr[x^{-1}],\forall k\inz^+_0,\forall x\inr^*
\end{align*}
and again use induction to show that  
\begin{align*}
f^{(k)}(0)=0\hspace{0.5cm}\forall k\inz_0^+
\end{align*}
The trick to show $f^{(k)}(0)=0$ is let $u=\frac{1}{x}$.\\

Now, with \myref{Theorem}{AfaS}, we see that $f$ is not analytic at $0$. 
\end{Example}
\begin{Example}{\textbf{(Bump Function)}}{}
\begin{align*}
f(x)=\begin{cases}
  e^{\frac{-1}{1-x^2}}& \text{ if $\abso{x}<1$ }\\
  0& \text{ otherwise }
\end{cases}
\end{align*}
Use the same trick (but more advanced) to show $f$ is smooth, and note that $f$ is not analytic at $\pm 1$. 
\end{Example}
\begin{mdframed}
\end{mdframed}
\begin{mdframed}
Now, it comes an interesting question. Given a real function $f$ analytic at $a$ with radius  $R$, and suppose $b \in (a-R,a+R)$.
\begin{enumerate}[label=(\alph*)]
  \item Is $f$ also analytic at $b$?
   \item What do we know about the radius of convergence of $f$ at $b$?
   \item Suppose $f$ is indeed analytic at $b$. It is trivial to see that the power series $(a,c_{a;n})$ and $(b,c_{b;n})$ must agree on the common convergence interval, and because $f$ is given, we by \myref{Theorem}{AfaS}, have already known the value of $c_{b;n}$. Can we verify that the power series  $(a,c_{a;n})$ and $(a,c_{b;n})$ do indeed agree with each other on the common convergence interval?
\end{enumerate}
\myref{Theorem}{TT} (Taylor's Theorem) give satisfying answers to these problems. 
\end{mdframed}
\begin{theorem}
\label{TT}
  \textbf{(Taylor's Theorem)} Given a real function $f$ analytic at $a$ with radius $R$, and suppose $b\in (a-R,a+R)$. Then 
\begin{align*}
f(x)=\sum_{k=0}^\infty \frac{f^{(k)}(b)}{k!}(x-b)^k\text{ on $\abso{x-b}<R-\abso{b-a}$ }
\end{align*}
\end{theorem}
\begin{proof}
WOLG, let $a=0$. Suppose $x$ satisfy $\abso{x-b}<R-\abso{b}$. Compute 
 \begin{align*}
f(x)&=\sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}x^k\\
&=\sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}(x-b+b)^k\\
&=\sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}\sum_{n=0}^k \binom{k}{n} (x-b)^n b^{k-n}\\
&=\sum_{k=0}^\infty \sum_{n=0}^k \frac{f^{(k)}(a)}{k!}\binom{k}{n}(x-b)^n b^{k-n}
\end{align*}
Note that 
\begin{align*}
  \sum_{k=0}^\infty \abso{\sum_{n=0}^\infty \frac{f^{(k)}(a)}{k!}\binom{k}{n}(x-b)^n b^{k-n}}&\leq \sum_{k=0}^\infty \sum_{n=0}^\infty \abso{\frac{f^{(k)}(a)}{k!}}\binom{k}{n}\abso{x-b}^n \cdot \abso{b}^{k-n}\\
  &=\sum_{k=0}^{\infty} \abso{\frac{f^{(k)}(a)}{k!}}\sum_{n=0}^{\infty}\binom{k}{n}\abso{x-b}^{n}\cdot \abso{b}^{k-n}\\
  &=\sum_{k=0}^\infty \abso{\frac{f^{(k)}(a)}{k!}} \big(\abso{x-b}+\abso{b} \big)^k
\end{align*}
converge, by Cauchy-Hadamard Theorem and $\abso{x-b}+\abso{b}<R$.\\

Now, using Fubini's Theorem for Infinite Series, we have 
\begin{align*}
\sum_{k=0}^\infty \sum_{n=0}^{k} \frac{f^{(k)}(a)}{k!}\binom{k}{n}(x-b)^{n}b^{k-n}&=\sum_{k=0}^\infty \sum_{n=0}^{\infty} \frac{f^{(k)}(a)}{k!} \binom{k}{n}(x-b)^n b^{k-n}\\
&=\sum_{n=0}^\infty \sum_{k=0}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}(x-b)^n b^{k-n}\\
&=\sum_{n=0}^\infty \sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}(x-b)^n b^{k-n}\\
&=\sum_{n=0}^\infty \Big[ \sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}b^{k-n}\Big] (x-b)^n\\
\end{align*}
We have reduced the problem into proving 
\begin{align*}
\vi{\sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!} \binom{k}{n}b^{k-n}=\frac{f^{(n)}(b)}{n!}}
\end{align*}
Using the formula in \myref{Theorem}{AfaS}, because $b$ is in $(a-R,a+R)$, we can compute 
\begin{align*}
f^{(n)}(b)&=\sum_{k=n}^{\infty}\frac{k!}{(k-n)!}c_{a;k}(b)^{k-n}\\
&=\sum_{k=n}^\infty \frac{k!}{(k-n)!} \cdot \frac{f^{(k)}(a)}{k!} \cdot b^{k-n}\\
&=\sum _{k=n}^{\infty}\frac{f^{(k)}(a)}{(k-n)!}b^{k-n}
\end{align*}
This now implies 
\begin{align*}
\frac{f^{(n)}(b)}{n!}&=\sum_{k=n}^\infty \frac{f^{(k)}(a)}{n!(k-n)!}b^{k-n}\\
&=\sum_{k=n}^\infty \frac{f^{(k)}(a)}{k!}\binom{k}{n}b^{k-n}\vdone
\end{align*}

\end{proof}
\section{Abel's Theorem and its application}
\begin{mdframed}
In this section, we use the notation $\mathbb{S}_M(R)$ to denote \textbf{stolz region}
\begin{align*}
\mathbb{S}_M(R)\triangleq \set{z\inc: \frac{\abso{R-z}}{R-\abso{z}}\in (0,M)}
\end{align*}
\end{mdframed}
\begin{theorem}
\textbf{(Abel's Theorem for Power Series)} Given a complex Maclaurin series $f(z)=\sum_{n=0}^{\infty} c_nz^n$ of convergence radius $R$ such that  
\begin{align*}
\sum_{n=0}^{\infty}c_nR^n\text{ converge }
\end{align*}
Then for all $M>1$, we have  
\begin{align*}
f|_{\mathbb{S}_M(R)}(z)\to \sum_{n=0}^{\infty}c_nR^n=f(R)\text{ as }z\to R 
\end{align*}
\end{theorem}
\begin{proof}
We first 
\begin{align*}
\vi{\text{ prove when }R=1}
\end{align*}
Fix $\epsilon $. We wish  
\begin{align*}
\vi{\text{ to find $\delta$ such that }\abso{\sum_{n=0}^{\infty}c_nz^n-c_n}<\epsilon \text{ for all }z \in \mathbb{S}_M(1)\cap D_\delta(1)}
\end{align*}
To use summation by part, we first fix 
\begin{align*}
s_n\triangleq \sum_{k=0}^{n}c_k\text{ and }s\triangleq \lim_{n\to \infty}s_n
\end{align*}
Now Use summation by part 
\begin{align*}
\sum_{n=0}^k c_nz^n&=\sum_{n=0}^{k}(s_n-s_{n-1})z^n\\
&=\sum_{n=0}^k s_nz^n - \sum_{n=0}^{k-1} s_nz^{n+1}\\
&=s_kz^k+(1-z)\sum_{n=0}^{k-1} s_nz^n
\end{align*}
Note that 
\begin{align*}
  \hspace{1cm}(1-z)\sum_{n=0}^{\infty}z^n=1\hspace{0.5cm}(\abso{z}<1)
\end{align*}
This give us 
\begin{align*}
\lim_{z\to 1^-}\Big( \sum_{n=0}^{\infty}c_nz^n- \sum_{n=0}^\infty c_n \Big)&= \lim_{z\to 1^-} \Big( \lim_{k\to \infty} s_kz^k  + (1-z)\sum_{n=0}^{k-1} s_nz^n - s\Big)\\
&=\lim_{z\to 1^-}  (1-z)\sum_{n=0}^{\infty} (s_n-s)z^n\hspace{0.5cm}(\because \forall z\inc:\abso{z}<1, \lim_{k\to \infty}s_kz^k=0)
\end{align*}
We reduce the problem into 
\begin{align*}
  \vi{\text{ finding }\delta\text{ such that }\abso{(1-z)\sum_{n=0}^{\infty}(s_n-s)z^n}\leq \epsilon \text{ for all }z \in \mathbb{S}_M(1)\cap D_\delta(1)}
\end{align*}
Because $s_n \to s$, we know there exists $N$ such that $\abso{s_n-s}<\frac{1}{2M}$ for all $n>N$. We claim 
\begin{align*}
\vi{\delta= \frac{\epsilon }{2\sum_{n=0}^N \abso{s_n-s}}\text{ suffices }}
\end{align*}
Note that $\sum_{n=0}^{\infty}(s_n-s)z^n$ absolutely converges by direct comparison test. Then we can deduce
\begin{align*}
  \abso{(1-z)\sum_{n=0}^{\infty}(s_n-s)z^n}&=  \abso{1-z}\cdot \abso{\sum_{n=0}^{N}(s_n-s)z^n+\sum_{n=N+1}^{\infty}(s_n-s)z^n}\\
 &\leq \abso{1-z}\Big(\sum_{n=0}^N \abso{s_n-s}+ \frac{\epsilon }{2M} \sum_{n=N+1}^\infty \abso{z}^n \Big)\\
 &\leq  \frac{\epsilon}{2}+ \frac{\epsilon }{2M} \cdot \frac{\abso{1-z}}{1-\abso{z}} \cdot \abso{z}^{N+1}\leq \epsilon \hspace{0.5cm}(\because \abso{z}<1\text{ and }\frac{\abso{1-z}}{1-\abso{z}}<M)\vdone
\end{align*}
We now prove 
\begin{align*}
\blue{\text{ when $R\inr^+$ }}
\end{align*}
Fix $\epsilon $. We wish 
 \begin{align*}
   \blue{\text{ to find }\delta\text{ such that }\abso{\sum_{n=0}^{\infty}c_nz^n-c_nR^n}<\epsilon \text{ for all }z \in \mathbb{S}_M(R)\cap D_\delta(R)}
\end{align*}
Fix  
\begin{align*}
a_n=c_nR^n\text{ and }g(z)\triangleq \sum_{n=0}^{\infty}a_n z^n=\sum_{n=0}^{\infty}c_nR^nz^n\hspace{0.5cm}(\abso{z}<1)
\end{align*}
By premise and \vi{our result}, we know 
\begin{align*}
g(1)\text{ exists and there exists $\delta'$ such that }\abso{g(z)-g(1)}<\epsilon \text{ for all $z \in\mathbb{S}_M(1)\cap D_{\delta'}(1)$ }
\end{align*}
We claim 
\begin{align*}
  \blue{\delta=R\delta'\text{ suffices }}
\end{align*}
First note that 
\begin{align*}
\frac{\abso{R-z}}{R-\abso{z}}\in (0,M)\implies \frac{\abso{1-\frac{z}{R}}}{1-\abso{\frac{z}{R}}}\in (0,M)
\end{align*}
This tell us 
\begin{align*}
z\in \mathbb{S}_M(R) \implies \frac{z}{R}\in \mathbb{S}_M(1)
\end{align*}
Fix $z\in \mathbb{S}_M(R)\cap D_\delta(R)$. We now have
\begin{align*}
\frac{z}{R}\in \mathbb{S}_M(1)\cap D_{\delta'}(1)
\end{align*}
This then let us conclude
\begin{align*}
\abso{\sum_{n=0}^{\infty}c_nz^n-c_nR^n}&=\abso{g(\frac{z}{R})-g(1)}<\epsilon \bdone
\end{align*}
\end{proof}
\begin{mdframed}
\begin{Example}{\textbf{(Identity of $\ln$ derived from Abel's Theorem)}}{}
\begin{align*}
  \ln (1+x)=\sum_{n=1}^{\infty} (-1)^{n-1} \frac{x^n}{n}\text{ for all $x \in (-1,1]$ }
\end{align*}
Check that both side satisfy $y'=\frac{1}{1+x}$, and $y(0)=0$. This tell us that two sides equal on  $(-1,1)$. Now using Abel's Theorem and the continuity of $\ln$, we have 
\begin{align*}
\ln 2= \sum_{n=1}^{\infty} (-1)^{n-1} \frac{1}{n}
\end{align*}
\end{Example}
\begin{Example}{\textbf{($1-1+1-1+\cdots=\frac{1}{2}$)}}{}
\begin{align*}
1-1+1-1+\cdots = \frac{1}{2}\text{ is WRONG!!! }
\end{align*}
When people say: "$1-1+1-1+\cdots =\frac{1}{2}$", they mean the sum of the series in the sense of Abel. Compute the Macularin series of $\frac{1}{1+r}$
\begin{align*}
\frac{1}{1+r}=\sum_{n=0}^{\infty}(-1)^nr^n
\end{align*}
Check both side do equal on $(-1,1)$ by direct computation. Apply Abel's Theorem to see the magic. 
\end{Example}
\end{mdframed}
\section{Identity Theorem and Bump Functions}
\begin{mdframed}
By a \textbf{region} $D$ in $\C$, we mean an open connected subset of  $\C$. 
\end{mdframed}
\begin{theorem}
\textbf{(Identity Theorem)} Given two analytic $f,g:D\rightarrow \C$, if $f=g$ on some set  $S \subseteq D$ such that $S$ has a limit point in $D$, then  $f=g$ on  $D$.
\end{theorem}
\begin{mdframed}
By a \textbf{bump function}, we mean a real-valued function $f$  on $\R^n$ such that  $f$ is smooth and compactly supported. The simplest bump function $\Psi:\R\rightarrow \R$ is perhaps that 
\begin{align*}
\Psi (x)\triangleq \begin{cases}
  e^{\frac{-1}{1-x^2}}& \text{ if $\abso{x}<1$ }\\
  0& \text{ if $\abso{x}\geq 1$ }
\end{cases}
\end{align*}
To put in more tools in the toolbox for constructing bump functions, consider the smooth function $f:\R\rightarrow \R$ defined by 
\begin{align*}
f(x)\triangleq \begin{cases}
  e^{\frac{-1}{x}}& \text{ if $x>0$ }\\
  0& \text{ if $x\leq 0$ }
\end{cases}
\end{align*}
and  $g:\R\rightarrow \R$ defined by 
\begin{align*}
g(x)\triangleq \frac{f(x)}{f(x)+f(1-x)}
\end{align*}
which is a smooth increasing function such that  
\begin{align*}
g(x)=\begin{cases}
  0& \text{ if $x\leq 0$ }\\
  1& \text{ if $x\geq 1$ }
\end{cases}
\end{align*}
Lastly, if we let $r\inr^+$ and define $h:\R\rightarrow \R$ by
\begin{align*}
h(x)\triangleq g(1+r+x)g(1+r-x)
\end{align*}
We see that $h$ is a bump function and 
\begin{align*}
h(x)=\begin{cases}
  1& \text{ if $\abso{x}\leq r$ }\\
  0& \text{ if $\abso{x}\geq r+1$ }
\end{cases}
\end{align*}
A $n$-dimensional generalization of this bump function $h$ is  $H:\R^n\rightarrow \R$ 
\begin{align*}
H(x_1,\dots ,x_n)\triangleq  h(x_1)\cdots h(x_n)
\end{align*}
\end{mdframed}
\chapter{Measure Theory}

\section{Sigma-Algebra} 
\begin{abstract}
In this section, we first discuss properties of $\sigma$-algebra and some of its substructure for better understanding of a slightly generalized version of \customref{caratheodory_extension_theorem}{Carathéodory's extension theorem}. Note that in this section, the terms 'ring', 'field', or 'algebra' do not refer to algebraic structures like the integer ring. 
\end{abstract}
\begin{mdframed}
Given a set $X$ and an non-empty set $R$ of subsets of $X$, we say  $R$ is a \textbf{semi-ring}, if for each $A,B \in R$, we have 
\begin{enumerate}[label=(\alph*)]
  \item $A\cap B \in R$ (closed under finite intersection)
  \item  $A\setminus B=\bigsqcup_{i=1}^n K_i$ for some disjoint $K_1,\dots ,K_n \in R$. (relative complements can be written as finite disjoint union)
\end{enumerate}
and we say $R$ is a \textbf{ring}, if for each $A,B \in R$, we have 
\begin{enumerate}[label=(\alph*)]
  \item $A\cup B \in R$ (closed under finite union)
  \item $A\setminus  B \in R$ (closed under relative complement)
\end{enumerate}
One should check 
\begin{enumerate}[label=(\alph*)]
\label{c_ring}
  \item Semi-ring always contain the empty set.
\item Since $A\cap B=A\setminus (A\setminus B)$, \underline{closure under relative complement} implies \underline{closure under} \underline{finite intersection}. Thus, a ring, or any collection closed under relative complement, is always a semi-ring.
\item Note that $A\cup B=(A\setminus B)\sqcup (A\cap B) \sqcup  (B\setminus A)$. This implies we can replace  \underline{closure under finite union} with \underline{closure under finite disjoint union} as definition for ring. 
  \item Given a family $S$ of subsets of $X$, there exists smallest ring  $R(S)$ containing $S$. Such ring $R(S)$ is called \textbf{the ring generated by $S$}. 
\end{enumerate}
\end{mdframed}
\begin{theorem}
\label{RGbS}
\textbf{(Ring Generated by Semi-Ring)} If $S$ is a semi-ring, then 
\begin{align*}
R(S)&=\set{A:A\text{ is the union of some finite pair-wise disjoint sub-family $S'$ of $S $}}\\
&=\set{A:A\text{ is the union of some finite sub-family $S'$ of $S$ }}
\end{align*}
\end{theorem}
\begin{proof}
  Let $R\triangleq \set{A:A\text{ is the union of some finite pair-wise disjoint sub-family $S'$ of $S$}}$. $R'\triangleq \set{A:A\text{ is the union of some finite sub-family $S'$ of $S$ }}$. We first prove  \vi{$R=R(S)$}.\\


  Clearly, the problem can be reduced into proving \vi{$R$ is a ring}. Because $R$ is clearly closed under finite disjoint union, by \customref{c_ring}{property (c) of the ring}, we can reduce the problem into proving \vi{$R$ is closed under relative complement}.\\

We first show \olive{$R$ is closed under finite intersection}. Given $\bigsqcup E_i,\bigsqcup F_j \in R$, we see 
\begin{align*}
  \Big(\bigsqcup_i E_i\Big)\cap \Big(\bigsqcup_j F_j\Big)=\bigsqcup_{i,j} E_i\cap F_j \in R \odone
\end{align*}

Now observe
\begin{align*}
\Big(\bigsqcup_i E_i \Big)\setminus \Big(\bigsqcup_j F_j \Big)&=\bigcap_j \Big(\bigsqcup_i (E_i\setminus F_j) \Big)\\
&=\bigcap_j A_j  \text{ for some $A_j \in R$ }\vdone
\end{align*}
We now prove \blue{$R=R'$}. It is clear that $R\subseteq R'$. We only have to prove  \blue{$R'\subseteq R$}. This is trivially true, since any finite sub-family $S'$ of $S$ is a finite sub-family of $R$ and  $R$ is closed under finite union. $\bdone$
\end{proof}
\begin{mdframed}
We now give definition to the most important structure in this section. Given a family $\Sigma$ of subsets of $X$,  we say $\Sigma$ is a \textbf{$\sigma$-algebra} (or sometimes \textbf{$\sigma$-field}) on $X$ and say $(X,\Sigma)$ is a \textbf{measurable space}, if $\Sigma$ is ring and for each sequence $(A_n)_{n\inn}$ of elements of $\Sigma$ we have
\begin{enumerate}[label=(\alph*)]
  \item $X \in \Sigma$
  \item  $\bigcup_{n\inn} A_n \in \Sigma$ (Closed under countable union)
\end{enumerate}
Similarly, one should check 
\begin{enumerate}[label=(\alph*)]
\label{pop_sig}
\item Because $\bigcap_{n=1}^{\infty}A_n = A_1 \setminus (\bigcup_{n=2}^{\infty}(A_1\setminus A_n))$, we see  $\sigma$-algebra is \underline{closed under countable} \underline{intersection}. 
\item Because $\bigcup_{n=1}^{\infty}A_n = \bigsqcup_{n=1}^{\infty} (A_n \setminus (\bigcup_{k=n+1}^{\infty} A_k))$, we can replace \underline{closure under countable} \underline{union} with \underline{closure under countable disjoint union}.  
\item Because $\bigcup_{n\inn}A_n = \bigcup_{n\inn} (\bigcup_{k=1}^n A_k)$, we can replace \underline{closure under countable union} with the condition \underline{for all  $(A_n)\subseteq \Sigma$ such that $\forall n,A_n \subseteq A_{n+1}$, we have $\bigcup_n A_n \in \Sigma$}.
\item Given a family $S$ of subsets of $X$, there exists smallest sigma-algebra $\sigma(S)$ containing $S$. Such $\sigma$-algebra $\sigma(S)$ is called \textbf{the sigma-algebra generated by $S$}.
\end{enumerate}
Now, given a family $S$ of subsets of $X$, there in fact exists an explicit expression of $\sigma (S)$, albeit infamous. Let $\omega_1$ be the smallest uncountable ordinal, and let $\mathbf{\Sigma}^0_1\triangleq S$. For each ordinal $\alpha <\omega_1$, we recursively define $\mathbf{\Pi}^0_\alpha \triangleq  \set{X\setminus A: A \in \mathbf{\Sigma}^0_\alpha}$ and $\mathbf{\Sigma}^0_\alpha \triangleq \set{\bigcup_{n\inn}A_n: (A_n)\subseteq \bigcup_{1\leq \gamma <\alpha }\mathbf{\Pi}^0_\gamma }$. One can use transfinite induction to check that $\sigma (S)= \bigcup_{\alpha <\omega_1}\mathbf{\Sigma}^0_\alpha$.
\end{mdframed}
\section{Carathéodory's Extension Theorem}
\label{caratheodory_extension_theorem}
\begin{abstract}
In this section, we introduce a general process to construct measure on $X$. The process involve first inducing an outer measure from a pre-measure on some weaker structure $S$, and then restricting the outer measure onto a subfamily that contain exactly all the subset that "sharply cut" all the other subsets of $X$. The subfamily, as we shall prove, is a sigma-algebra. This extending-restricting process is known mostly by the name of Carathéodory Extension Theorem, and the rigorous  definition of "sharply cut" is known by the name of Carathéodory criterion. The selected 'some weaker structure' $S$, which we begin out extension from, is a semi-ring. Although as long as $S$ contain the empty set, the process works, in the sense that one can generate a measure with pre-measure $\mu:S\rightarrow [0,\infty]$, to have the generated measure agree with  $\mu$ on $S$, some necessary condition needs to be satisfied, and the axioms of semi-ring, not equivalent to some other popular choices that also suffice, e.g., ring or quasi-semi-ring, suffices to be a set of necessary conditions.\\

Note that in this section, if we write $\mu (A)$ without specifying whether $A$ is in the domain of  $\mu$, we mean that the statement always hold true as long as $A$ is in the domain of $\mu$, and note that the difference between the term \textbf{measure space} and measurable space lies in that the latter is not equipped with a measure yet. 
\end{abstract}
\begin{mdframed}
Given a collection $S$ of subsets of  $X$ containing the empty set, we say  $\mu:S\rightarrow [0,\infty]$ is a \textbf{pre-measure} (or \textbf{content}) on $(X,S)$ if 
\begin{enumerate}[label=(\alph*)] 
  \item $\mu(\varnothing)=0$ (null empty set) 
  \item $A \subseteq B \implies \mu (A) \leq \mu (B)$ (monotone)
  \item $\mu\big(\bigsqcup_{n\inn} E_n \big)=\sum_{n\inn} \mu (E_n)$ (countably additive, or $\sigma$-additive)
\end{enumerate}
and say $\mu$ is a \textbf{measure} if $S$ is a $\sigma$-algebra on $X$. Note that if $S$ is closed under relative complement, e.g., $S$ is a ring or any stronger structure, then monotone is implied by countable additive. Now, with the hints given below, one can check straightforward that if $S$ is a semi-ring, we have 
\begin{enumerate}[label=(\alph*)]
\label{pop_meas}
  \item $\mu (A_1 \sqcup   \cdots \sqcup   A_n)= \mu (A_1) + \cdots + \mu (A_n)$ (finitely additive)
  \item $\mu \big(\bigcup_{n \in J} A_n\big)\leq \sum_{n\in J} \mu (A_n)$, for each finite or countable $J$.   \item $A_n \nearrow A \implies \mu (A_n)\nearrow \mu \big(A\big)$.  
  \item $A_n\searrow A\text{ and }\mu (A_1)<\infty \text{ and }S\text{ is a $\sigma$-algebra }\implies \mu (A_n)\searrow \mu \big(A\big)$  
\end{enumerate}
Hints: Properties (b) and (c) are proved by letting $B_n\triangleq A_n \setminus (A_{n-1}\cup \cdots \cup  A_1)$; and property (d) is proved by letting $B_n\triangleq A_1 \setminus A_n,\forall n\inn$ and the equation 
\begin{align*}
\mu (A_1)-\lim_{n\to \infty} \mu (A_n)&=\lim_{n\to \infty}\mu (A_1)-\mu (A_n)\\
&=\lim_{n\to \infty} \mu (B_n)\\
(\text{property (c) is used here})\hspace{0.5cm}&=\mu (\bigcup_{n=1}^{\infty}B_n)=\mu (A_1 \setminus \bigcap_{n=1}^{\infty}A_n)=\mu (A_1)- \mu (\bigcap_{n=1}^{\infty}A_n)
\end{align*}
Note that our proof for property (d) require $A_1$ to be of finite measure in last step. 
\end{mdframed}
\begin{mdframed}
Now, suppose $S$ is a semi-ring and $\mu:S\rightarrow [0,\infty]$ is a pre-measure on $(X,S)$. This section address the question: Is there a unique extension of $\mu$ onto $\sigma (S)$? The answer is indeed affirmative: The extension always exists, and if $\mu$ is $\sigma$-finite, the extension is unique. 
While extensions of pre-measures can be constructed from structures weaker than a semi-ring, we focus on semi-rings here, as they are the starting points in common applications, e.g., Lebesgue-Stieltjes measure.\\

We first extend $\mu$ from $S$ onto $R(S)$. Since each element of $R(S)$ is a union of some finite pair-wise disjoint sub-family of $S$, as \customref{RGbS}{we proved before}, for each $A=\bigsqcup_{j=1}^{n_a}A_j \in R(S)$, we can assign $\mu(A)\triangleq \sum_{j=1}^{n_a}\mu (A_j)$. Such assignment is well-defined, as one can check using  
\begin{align*}
\bigsqcup_{j=1}^{n_a}A_j = \bigsqcup _{k=1}^{n_b}B_k \implies A=\bigsqcup_{j,k} A_j \cap B_k \text{ where }A_j\cap B_k \in S 
\end{align*}
At this point, one should check $\mu$ remains countably additive after the extension onto $R(S)$ by dissecting $A=\bigsqcup A_n \in R(S)$ into a countable disjoint union of element in $S$. \\



We now give definition to \textbf{outer measure}, and shows that \customref{Piom}{given any pre-measure $\mu$ on some semi-ring $S$ of subsets of $X$, there exists outer measure  $\mu^*$ on $X$ such that  $\mu^*\text{ and }\mu$ agrees on $S$}.
\end{mdframed}
\begin{definition}
\textbf{(Definition of outer measure)} Given a set $X$, by an  \textbf{outer measure}, we mean a function $\nu :2^X\rightarrow [0,\infty]$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $ \nu  (\varnothing)=0$ (null empty set)
  \item $A\subseteq \bigcup_{n\inn} A_n\implies \nu  (A)\leq  \sum \nu (A_n)$ (countably subadditive)
\end{enumerate}
\end{definition}
\begin{mdframed}
Equivalently, one can replace countably subadditive with the following two axioms   
\begin{enumerate}[label=(\alph*)]
  \item $A \subseteq B \implies \nu (A) \leq \nu  (B)$  (monotone)
  \item $\nu  (\bigcup_{n\inn} A_n )\leq \sum \nu (A_n)$  
\end{enumerate}

\end{mdframed}
\begin{theorem}
\label{Piom}
\textbf{(Pre-measure on semi-ring induces outer measure)} Given a pre-measure $\mu$ on some semi-ring $S$ of subsets of $X$, if we define $\mu^*:2^X\rightarrow [0,\infty]$ by
\begin{align*}
\mu^*(E)\triangleq \inf \bset{\sum_n \mu (T_n):E \subseteq \bigcup _n T_n\text{ and }T_1,T_2,\dots \in S}\text{ where $\inf \varnothing= \infty$ }
\end{align*}
Then 
\begin{align*}
  \mu^*\text{ is an outer measure agreeing with $\mu$ on $R(S)$}
\end{align*}
\end{theorem}
\begin{proof}
  It is clear $\mu^*(\varnothing)=0\text{ and }A\subseteq B \implies \mu^*(A)\leq \mu^*(B)$. It remains to prove that for arbitrary $B_n$ we have 
\begin{align*}
  \vi{\mu^* \Big(\bigcup_n B_n \Big)  \leq \sum_n \mu^* (B_n)}
\end{align*}
If  $\sum_n \mu^*(B_n)=\infty$, the proof is trivial. We from now suppose $\sum_n \mu^*(B_n)< \infty$. Fix $\epsilon $. We prove 
\begin{align*}
  \vi{\mu^*\Big(\bigcup _n B_n \Big)\leq \sum_n \mu^* (B_n)+\epsilon }
\end{align*}
Because $\mu^*(B_n)<\infty$ for each $n\inn$, we know for each $n\inn$ there exists an countable cover $(T_{n,k})_{k\inn}\subseteq R$ of $B_n$ such that 
\begin{align*}
\sum_k \mu (T_{n,k}) \leq \mu^*(B_n)+\epsilon (2^{-n}) 
\end{align*}
It is clear that  $\set{T_{n,k}:n,k \inn}$ is a countable cover of $\bigcup_n B_n$, we now see 
\begin{align*}
  \mu^*\Big(\bigcup_n B_n \Big)\leq  \sum_{n,k} \mu (T_{n,k})&= \sum_n \sum_k \mu(T_{n,k})\hspace{0.5cm}(\because \text{\customref{FTfDS}{Fubini's Theorem for Double Series}})\\
&\leq \sum_n \mu^*(B_n)+ \epsilon (2^{-n})= \sum_n \mu^*(B_n)+ \epsilon \vdone
\end{align*}
At this point, one should check that even if we first extend $\mu$ onto $R(S)$ before doing the same procedure, we would still have the same outer measure. Now, because for each $T \in R(S)$, we clearly have $\mu^*(T)\leq \mu (T)$, to finish the proof it only remains to see for each cover $T_n \subseteq R(S)$ of $T$, we have
\begin{align*}
\mu (T)=\sum_n \mu (T\cap T_n)\leq \sum_n \mu (T_n)
\end{align*}
\end{proof}
\begin{mdframed}
  So far, we have proved that given a semi-ring $S$ and a pre-measure $\mu:S\rightarrow [0,\infty]$, there exist some outer measure agree with $\mu$ on $R(S)$. One may wish to ask if such outer measure, an extension of the pre-measure $\mu$, is unique? The answer is negative even in the most trivial case, as the \customref{Nou}{example below} shows. In fact, the outer measure induced in \myref{Theorem}{Piom} is called the \textbf{maximal outer extension}, in the sense that if $\nu$ is an outer measure agreeing with  $\mu$ on $R(S)$, then  $\nu(E)\leq \mu^*(E),\forall E\subseteq X$, as one can check straightforwardly. Also, one can check that it make no difference if we first extend $\mu$ from $S$ onto  $R(S)$ or not, before we extend $\mu$ to the maximal outer measure $\mu^*$. 
\end{mdframed}
\begin{Example}{\textbf{(Non-uniqueness of outer extension)}}{}
\label{Nou}
\begin{align*}
X\triangleq \set{1,2}\text{ and }R\triangleq \set{\varnothing,X}
\end{align*}
Define the pre-measure $\mu:R\rightarrow [0,\infty]$ and an outer measure $\nu:\power{X}\rightarrow [0,\infty]$ agreeing with $\mu$ on $S$ by 
\begin{align*}
\mu (A)\triangleq \begin{cases}
  0& \text{ if $A=\varnothing$ }\\
  1& \text{ if $A=X$ }
\end{cases}\text{ and }\nu(A)\triangleq \begin{cases}
  \mu (A)& \text{ if $A\in R$ }\\
  \frac{1}{2}& \text{ if $A\not\in R$ }
\end{cases}
\end{align*}
One can check that the maximal outer extension $\mu^*$ disagree with $\nu$ on $\power{X}\setminus R$.
\end{Example}
\begin{mdframed}
It is important to note that the final step, \myref{Theorem}{Omi}, inducing measure from outer measure, is a general Theorem and operate independently of \myref{Theorem}{Piom}. This distinction is crucial because many constructions of measures, such as the Hausdorff measure, begin by defining an outer measure explicitly, rather than inducing it from a weaker structure like a semi-ring.
\end{mdframed}
\begin{theorem}
\label{Omi}
\textbf{(Outer measure induce measure)} Given an outer measure $\mu^*$ on $X$, if we let 
\begin{align*}
  \mathcal{A}\triangleq \set{A \subseteq X:\mu^*(E)=\mu^*(E\cap A)+ \mu^*(E\setminus A)\text{ for all $E\subseteq X$ }} 
\end{align*}
then $\mathcal{A}$ is a sigma-algebra on $X$ and  $\mu^*|_{\mathcal{A}}:\mathcal{A}\rightarrow [0,\infty]$ is a measure. 
\end{theorem}
\begin{proof}
Because of the following facts 
\begin{enumerate}[label=(\alph*)]
  \item $A\setminus B=A\cap B^c$
  \item $A\cup B=(A^c \cap B^c)^c$ 
  \item \customref{pop_sig}{property (c) of sigma-algebra}
\end{enumerate}
we can reduce the problem into proving the following propositions 
\begin{enumerate}[label=(\roman*)]
  \item $\mathcal{A}$ is closed under complement. 
  \item $X\in \mathcal{A}$.
  \item $\mathcal{A}$ is closed under finite intersection. 
  \item $\mu^*|_{\mathcal{A}}$ is countably additive. (Thus at least form a pre-measure)
  \item $\mathcal{A}$ is closed under countable disjoint union. 
\end{enumerate}
We will prove the propositions sequentially, as the proof of each subsequent proposition may rely on the proofs of the preceding ones. The first two are straightforward to check. We now prove $\vi{\mathcal{A}\text{ is closed under finite intersection}}$. Fix $A,B \in \mathcal{A}\text{ and }E\subseteq X$, we wish to show 
\begin{align*}
\vi{\mu^*(E)=\mu^*(E\cap A\cap B)+\mu^*\big(E\setminus (A\cap B)\big)}
\end{align*}
Because $B \in \mathcal{A}$, we can "sharply cut $E\setminus (A\cap B)$ by $B$"; that is 
 \begin{align}
\label{sharp}
\mu^*(E\setminus (A\cap B))=\mu^*\big((E\cap B)\setminus A\big)+\mu^*(E\setminus B)
\end{align}
\myref{Equation}{sharp} together with $A,B \in \mathcal{A}$ then give us 
\begin{align*}
\mu^*(E\cap A\cap B)+\mu^*\big(E\setminus (A\cap B)\big)&=\mu^*(E\cap A\cap B)+\mu^*\big((E\cap B)\setminus A \big)+ \mu^*(E\setminus B)\\
&=\mu^*(E\cap B)+\mu^*(E\setminus B)=\mu^*(E)\vdone
\end{align*}
We now prove the \olive{claim: For each pairwise disjoint sequence $(A_n)\subseteq \mathcal{A}$ and $E\subseteq X$, we have the equality} 
\begin{align*}
\olive{\mu^*\Big(E\cap \bigsqcup_n A_n \Big)=\sum_n \mu^*(E\cap A_n)}
\end{align*}
The countably subadditivty of $\mu^*$ trivially implies the inequality  
\begin{align*}
\mu^*\Big(E\cap \bigsqcup_n A_n\Big)\leq \sum_n \mu^*(E\cap A_n)
\end{align*}
Using induction and the fact $(A_n)\subseteq \mathcal{A}$, we see that 
\begin{align*}
\mu^*(E\cap \bigsqcup_n A_n)= \sum_{n=1}^N \mu^*(E\cap A_n) + \mu^*\Big(E\cap \bigsqcup_{n=N+1}^{\infty}A_n \Big)\text{ for all $N \inn$ }
\end{align*}
Then since $\mu^*$ has codomain $[0,\infty]$, we see 
\begin{align*}
\mu^*(E\cap \bigsqcup_n A_n)\geq \sum_{n=1}^N \mu^*(E\cap A_n)\text{ for all $N \inn$ }
\end{align*}
This implies the desired inequality $\mu^*(E\cap \bigsqcup_n A_n)\geq \sum_{n=1}^{\infty}\mu^*(E\cap A_n)$. $\odone$\\

Using $E\triangleq \bigsqcup_n A_n$, one see our \olive{claim} implies that $\mu^*|_{\mathcal{A}}$ is countably additive. Lastly, we prove $\blue{\mathcal{A}\text{ is closed under countable disjoint union}}$. Fix a pairwise disjoint sequence $(A_n)\subseteq \mathcal{A}$ and $E\subseteq X$. We wish to prove
\begin{align*}
\blue{\mu^*(E)\geq \mu^*(E\cap \bigsqcup_n A_n)+\mu^*(E\setminus \bigsqcup_n A_n)}
\end{align*}
 Using induction and the fact $(A_n)\subseteq \mathcal{A}$, we see that 
\begin{align*}
\mu^*(E\cap \bigsqcup_{n=1}^N A_n)=\sum_{n=1}^N \mu^*(E\cap A_n)\text{ for all $N\inn$ }
\end{align*}
Then our \olive{claim} give us 
\begin{align}
\label{mue}
\mu^*(E\cap \bigsqcup_{n=1}^NA_n) \to \mu^*(E\cap \bigsqcup_n A_n)\text{ as $N \to \infty$ }
\end{align}
Now, because of the identity $F\cup G=(F^c\cap G^c)^c$, proposition (i) and (iii) have shown $\mathcal{A}$ is closed under finite union. This implies $\bigsqcup_{n=1}^N A_n\in \mathcal{A}$ for all $N\inn$, which, together with monotone of $\mu^*$, give 
\begin{align}
  \mu^*(E\cap \bigsqcup_{n=1}^N A_n)+\mu^*(E\setminus \bigsqcup_n A_n)&\leq \mu^*(E\cap \bigsqcup_{n=1}^{N}A_n)+ \mu^*(E\setminus \bigsqcup_{n=1}^N A_n)\notag\\
&\leq \mu^*(E) \text{ for all $N\inn$ }\label{mue2}
\end{align}
\myref{Equation}{mue} and \myref{Equation}{mue2} gives the desired inequality. $\bdone$
\end{proof}
\begin{mdframed}
\myref{Theorem}{Piom} together with \myref{Theorem}{Omi} shows that for each pre-measure $\mu$ on semi-ring $S$, we can induce a measure $\mu^*|_{\mathcal{A}}$ agreeing with $\mu$ on $S$. Although this result is correct, it doesn't show $S \subseteq \mathcal{A}$, which is necessary to refer to $\mu^*|_{\mathcal{A}}$ as an extension. However, it is straightforward to verify that $S \subseteq \mathcal{A}$ using the definition of a semi-ring and the property that $\mu^*(T) = \mu(T)$ for all $T \in S$.\\

Moving forward, here are some additional concepts we will utilize in subsequent sections: Given a measurable space $(X,\Sigma,\mu)$, we define a measurable set $N \in \Sigma$ as a \textbf{null set} if $\mu(N) = 0$. Moreover, we say that $\mu$ is a \textbf{complete measure} if every subset of a null set is also measurable. It is important to note that every measure induced by an outer measure is complete, as one can readily verify.
\end{mdframed}
\begin{mdframed}
Lastly, we wish to ask: Given a sigma-algebra $\Sigma$ containing $S$ and contained by $\mathcal{A}$, under what condition, is Carathéodory the only extension of $\mu$ onto $\Sigma$ ? \\

This question turns out to have direct connection with the notion named '$\sigma$-finite'. Given a pre-measure space $(X,S,\mu)$, we say $\mu$ is \textbf{$\sigma$-finite} if there exists a countable cover $(A_n)\subseteq S$ of $E$ such that  $\mu (A_n)<\infty$ for all $n\inn$. It is clear that   \begin{enumerate}[label=(\alph*)]  
  \item $\mu$ is $\sigma$-finite only if $S$ form a cover of $X$. 
  \item If $\mu:S\rightarrow [0,\infty]$ is $\sigma$-finite and $\nu $ is a pre-measure defined on a class larger than $S$, such that $\nu $ agree with $\mu$ on $S$, then  $\nu $ is also $\sigma$-finite.
\end{enumerate}
\end{mdframed}
\begin{theorem}
\textbf{(Uniqueness of Extension)} Suppose 
\begin{enumerate}[label=(\alph*)]
  \item $(X,S,\mu)$ is a pre-measure space, and $S$ is a semi-ring.
  \item $\mathcal{A}$ is the induced sigma-algebra in \myref{Theorem}{Omi}  
  \item $\Sigma\subseteq \mathcal{A}$ is a sigma-algebra containing $S$  
  \item $\nu :\Sigma \rightarrow [0,\infty]$ is a measure agreeing with $\mu$ on $S$
\end{enumerate}
We have 
\begin{align}
\label{vmu1}
\nu (A)\leq \mu^*(A)\text{ for all $A\in \Sigma$ }
\end{align}
and, if $\mu$ is $\sigma$-finite, we have
\begin{align}
\label{vmu2}
\nu (A)=\mu^*(A)\text{ for all $A \in \Sigma$  }
\end{align}
\end{theorem}
\begin{proof}
The \myref{inequality}{vmu1} follows from the greatest-lower-bound definition of induced outer-measure,  \customref{pop_meas}{property (b) of measure} and monotone of measure.\\

From now on, we suppose $\mu$ is $\sigma$-finite. Before we prove \myref{Equation}{vmu2}, we first prove the \olive{claim: for each  $A\in \Sigma$, there exists a pairwise disjoint sequence $(D_n)\subseteq R(S)\subseteq \Sigma$ such that $A\subseteq \bigsqcup_n D_n$ and $\nu (D_n)=\mu^*(D_n)<\infty$ for all $n\inn$}.\\

Because $\mu$ is $\sigma$-finite, there exists a sequence $(A_n)\subseteq S$ such that $A\subseteq \bigcup_n A_n$ and $\mu (A_n)<\infty,\forall n\inn$. Define $D_1\triangleq A_1$ and  $D_n\triangleq A_n \setminus (A_1 \cup  \cdots \cup A_{n-1})$ for all  $n>2$. Noting the
\customref{RGbS}{structure of $R(S)$}, it is clear that $D_n$ is a pairwise disjoint sequence  in $R(S)$. It is also clear that $A\subseteq \bigcup A_n = \bigsqcup  D_n$. Fix $n\inn$. It remains to prove 
\begin{align*}
\olive{\nu  (D_n)=\mu^*(D_n)<\infty}
\end{align*}
The inequality $\mu^*(D_n)<\infty$ follows from $\mu^*(D_n)\leq \mu (A_n)<\infty$, and the equation $\nu (D_n)=\mu^*(D_n)$ follows from $R(S)\subseteq \Sigma$ and $R(S)\subseteq \mathcal{A}$. $\odone$\\  

Note that for all $n\inn$, $A\cap D_n \in \Sigma \subseteq \mathcal{A}$. Now, since 
\begin{align*}
\nu  (A)=\sum_{n=1}^{\infty} \nu (A\cap D_n)\text{ and }\mu^*(A)=\sum_{n=1}^{\infty} \mu^*(A\cap D_n)
\end{align*}
To prove \myref{Equation}{vmu2}, it only remains to prove $\vi{\nu  (A\cap D_n)=\mu^*(A\cap D_n),\forall n\inn}$.\\

Because $\nu $ is a measure and $A\cap D_n \in \mathcal{A}$, we have the equations set 
\begin{align}
\begin{cases}
  \nu  (A\cap D_n)=\nu  (D_n)-\nu  (D_n\setminus A)\\  
\mu^*(A\cap D_n)=\mu^*(D_n)-\mu^*(D_n\setminus A)
\end{cases}
\end{align}
The proof then follows from the facts 
\begin{enumerate}[label=(\alph*)]
  \item $\nu (D_n)=\mu^*(D_n)<\infty$ 
  \item $\nu  (A\cap D_n)\leq \mu^*(A\cap D_n)\text{ and }\nu (D_n\setminus A)\leq \mu^*(D_n\setminus A)$ $\vdone$
\end{enumerate}
Note that fact (b) can be checked straightforwardly. 
\end{proof}
\section{Equivalent Definition of Lebesgue Measure}
\begin{abstract}

\end{abstract}
\begin{mdframed}
It is clear that the collection $S$ of half-open interval $\prod [a_j,b_j)$ form a semi-ring. If we define a volume function $\mu : S\rightarrow [0,\infty]$ on  $S$ by 
 \begin{align*}
\mu \Big( \prod_{j=1}^d [a_j,b_j) \Big)\triangleq  \prod_{j=1}^d (b_j-a_j)
\end{align*}
we see that the empty set is indeed null and $\mu$ is indeed monotone. To check that $\mu$ form a pre-measure, it remains to prove 
\begin{align*}
\prod_{j=1}^d [a_j,b_j)= \bigsqcup_{n=1}^{\infty} \prod_{j=1}^d [a_{j,n},b_{j,n}) \implies \mu \Big(\prod_{j=1}^d [a_j,b_j) \Big)= \sum_{n=1}^{\infty} \mu \Big(\prod_{j=1}^d [a_{j,n},b_{j,n}) \Big)
\end{align*}
To check 
\begin{align}
\mu \Big(\prod_{j=1}^d [a_j,b_j) \Big) \geq \sum_{n=1}^{\infty} \mu \Big(\prod_{j=1}^d [a_{j,n},b_{j,n}) \Big)
\end{align}
One fix arbitrary $N$ and cut  $\prod_{j=1}^d [a_j,b_j)$ into finite amount of grids to see  
\begin{align*}
\mu \Big(\prod_{j=1}^d [a_j,b_j) \Big) \geq \sum_{n=1}^{N} \mu \Big(\prod_{j=1}^d [a_{j,n},b_{j,n}) \Big)
\end{align*}
To check 
\begin{align}
\label{58}
\mu \Big(\prod_{j=1}^d [a_j,b_j) \Big)\leq \sum_{n=1}^{\infty} \mu \Big(\prod _{j=1}^d [a_{j,n},b_{j,n}) \Big)
\end{align}
One fix $\epsilon$ and have each $\epsilon _n$ satisfy 
\begin{align*}
  \mu \Big(\prod_{j=1}^d [a_{j,n}-\epsilon_n , b_{j,n}+\epsilon_n) \Big) \leq (1+\epsilon )\mu  \Big(\prod_{j=1}^d [a_{j,n},b_{j,n}) \Big)
\end{align*}
Then because  
\begin{align*}
\bset{\prod_{j=1}^d (a_{j,n}-\epsilon_n , b_{j,n}+\epsilon_n )\subseteq \R^d:n\inn}\text{ form an open cover for compact }\prod_{j=1}^d [a_j,b_j]
\end{align*}
there exists finite subset $I \subseteq \N$ such that 
\begin{align*}
\prod_{j=1}^d [a_j,b_j]\subseteq \bigcup_{n \in I} \prod_{j=1}^d [a_{j,n}-\epsilon _n, b_{j,n}+\epsilon _n)
\end{align*}
This then give us 
\begin{align*}
\mu \Big(\prod_{j=1}^d [a_j,b_j)\Big)\leq \sum_{n \in I}\mu \Big(\prod_{j=1}^d [a_{j,n}-\epsilon _n, b_{j,n}+\epsilon _n) \Big) \leq (1+\epsilon )\sum_{n=1}^{\infty} \mu \Big(\prod_{j=1}^d [a_{j,n},b_{j,n}) \Big)
\end{align*}
which give us \myref{Equation}{58}. Having proved that the volume function $\mu : S\rightarrow [0,\infty]$ is a pre-measure, we can \customref{Piom}{induce a outer measure on $\R^d$ by }
\begin{align*}
\abso{E}_e \triangleq \inf \bset{ \mu (T_n) : E \subseteq \bigcup_ n T_n \text{ and }T_1,T_2,\dots  \in S}
\end{align*}
and \customref{Omi}{restrict the outer measure into a measure by letting the collection $\mathcal{L}(\R^d)$ of Lebesgue measurable set to be} 
\begin{align*}
\mathcal{L}(\R^d)\triangleq \set{A \subseteq \R^d: \forall E \subseteq \R^d, \abso{E}_e= \abso{E\cap A}_e + \abso{E\cap A^c}_e}
\end{align*}
Notably, if we define the class $K_n$ of half-open dyadic cubes by 
\begin{align*}
K_n \triangleq \bset{\prod_{j=1}^{d} [\frac{m_j}{2^n}, \frac{m_j+1}{2^n})\subseteq \R^d: m_j \inz \text{ for all }j}
\end{align*}
We see that for each $p$ in some open set $U$, there exists some small enough half-open dyadic cube $Q\in K_n$ (for some $n$) such that $p \in Q \subseteq U$. Then, since the collection $\bigcup K_n$ of half-open dyadic cubes is countable, we see $U$ is in the sigma-algebra  $\mathcal{L}(\R^d)$. 
\end{mdframed}
\begin{theorem}
\textbf{(Equivalent Definition of Lebesgue Measurability)} The following statements are equivalent. 
\begin{enumerate}[label=(\alph*)]
  \item $E\in \mathcal{L}(\R^d)$. 
  \item For all $\epsilon $, there exists some open $O$ containing $E$ such that  $\abso{O\setminus E}_e< \epsilon $. 
  \item For all $\epsilon $, there exists some closed $F$ contained by $E$ such that $\abso{E\setminus F}_e<\epsilon $. 
  \item $E= H \setminus Z$ for some null $Z$ and some $H \in G_\delta$ with same measure as  $E$. 
  \item $E=H \cup Z$ for some null $Z$ and some $H\in F_\sigma$ with same measure as $E$. 
\end{enumerate}
\end{theorem}
\begin{proof}
Since $\R^d$ is  $\sigma$-finite, to prove from (a) to (b), we can WOLG suppose $\abso{E}<\infty$; the proof then follows from the definition of $\abso{E}_e$, the trick of enlarging the cover to appropriate size and that open set is measurable. The proof for $(\text{b})\implies (\text{d})\implies (\text{a})$ is straightforward. The proof for $(\text{b})\implies (\text{c})$ is observation of $O \setminus E^c= E\setminus O^c$, and the proof for  $(\text{c})\implies (\text{e})\implies (\text{a})$ is again straightforward.
\end{proof}
\begin{mdframed}
It is worth pointing out that every open set can be expressed as a countable disjoint union of half-open dyadic cubes by an algorithmic construction, since if we write  
\begin{align*}
U_1\triangleq \bigsqcup_n Q_{n,1}\text{ and }U_2 \triangleq \bigsqcup_n Q_{n,2}
\end{align*}
we have 
\begin{align*}
\abso{U_1\times U_2}=\left|\bigsqcup_{n,k}Q_{n,1}\times Q_{k,2}\right|=\sum_{n,k} \abso{Q_{n,1}} \abso{Q_{k,2}}= \sum_n \abso{Q_{n,1}}\sum_{k} \abso{Q_{k,2}}= \abso{U_1}\abso{U_2}
\end{align*}
which in tern tell us that the product of measurable set is indeed measurable. (proved using an open set approximation from outside and the fact $\R^d$ is  $\sigma$-finite) \\


\end{mdframed}

\chapter{Riemann Calculus}
\section{Riemann-Stieltjes Integral}

\section{Riemann-Stieltjes on Computation}
\begin{theorem}
\label{CoV}
\textbf{(Change of Variable)} Given two functions $g,\beta  :[A,B]\rightarrow \R$, a function $\phi: [A,B]\rightarrow [a,b]$ and two functions $f,\alpha :[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $g = f \circ  \phi $ for all $x \in [a,b]$
  \item $\beta = \alpha  \circ \phi $ for all $x \in [a,b]$ 
  \item $\alpha , \beta  $ increase respectively on $[a,b]$ and $[A,B]$ 
  \item $\phi:[A,B]\rightarrow [a,b]$ is a homeomorphism 
  \item $\int_a^b fd\alpha $ exist
\end{enumerate}
Then 
\begin{align*}
\int_A^B g d\beta  =\int_a^b fd\alpha \text{   (This implies $\int_A^B gd\beta $ exists)}
\end{align*}
\end{theorem}          
\begin{proof}
Fix $\epsilon $. We only wish 
\begin{align*}
  \vi{\text{ to find a partition $Q$ of  $[A,B]$ &such that $U(Q,g,\beta )-L(Q,g,\beta )<\epsilon  $  }}\\
  \vi{\text{ and such that $\int_a^b fd\alpha \in \Big[L(Q,g,\beta ),U(Q,g,\beta ) \Big]$  }}
\end{align*}
Because $\int_a^b fd\alpha $ exists, we know 
\begin{align}
\label{CR1}
\text{ there exists a partition $P$ of  $[a,b]$ such that }U(P,f,\alpha )-L(P,f,\alpha )<\epsilon 
\end{align}
where, of course, $\int_a^b fd\alpha  \in \Big[L(P,f,\alpha ),U(P,f,\alpha )\Big]$.\\

Let $P=\set{a=x_0,x_1,\dots ,x_n=b}$. Because $\phi$ is a homeomorphism, we can let $\phi$ be strictly increasing WOLG.\\ 

Define a partition $Q$ on  $[A,B]$ by 
\begin{align*}
Q=\phi^{-1}[P]=\set{A=\phi^{-1}(x_0),\phi^{-1}(x_1),\dots ,\phi^{-1}(x_n)=B}
\end{align*}
Now, because $\beta =\alpha \circ  \phi$ and $g=f\circ \phi$  for all $x\in [a,b]$ by premise, and because $\phi$ is a homeomorphism, we have 
\begin{align}
  U(Q,g,\beta )&=\sum_{k=1}^n \big[  \sup_{t \in [\phi^{-1}(x_{k-1}),\phi^{-1}(x_k)]} g(t)\big] \big[\beta (\phi^{-1}(x_{k})) - \beta  (\phi^{-1}(x_{k-1}))\big]\notag\\
  &=\sum_{k=1}^n  \big[  \sup_{t \in [\phi^{-1}(x_{k-1}),\phi^{-1}(x_k)]} f \circ  \phi (t)\big] \big[\alpha \circ \phi  \big(\phi^{-1}(x_{k})\big) - \alpha \circ \phi \big(\phi^{-1}(x_{k-1})\big)\big]\notag\\
  &=\sum_{k=1}^n \big[\sup_{t \in [x_{k-1},x_k]}f(t) \big] \big(\alpha (x_k)-\alpha (x_{k-1}) \big)=U(P,f,\alpha )
\label{CR2}
\end{align}
Similarly, we can deduce $L(Q,g,\beta )=L(P,f,\alpha )$. Now, from \myref{Equation}{CR2} and by definition of $P$ (\myref{Equation}{CR1}), we see 
\begin{align*}
&U(Q,g,\beta )-L(Q,g,\beta )=U(P,f,\alpha )-L(P,f,\alpha )<\epsilon \\
  \text{ and }&\int_a^b fd\alpha \in \Big[L(P,f,\alpha ),U(P,f,\alpha ) \Big]=\Big[ L(Q,g,\beta ),U(Q,g,\beta ) \Big]\vdone
\end{align*}



\end{proof}
\begin{theorem}
\label{RRSI}
\textbf{(Reduction of Riemann-Stieltjes Integral: Part 1)} Given two functions $f,\alpha :[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $\alpha $ increase on $[a,b]$ 
  \item $\alpha $ is differentiable on $(a,b)$ 
  \item $\lim_{x\to b^-}\frac{\alpha (x)-\alpha (b)}{x-b}$ exists and $\lim_{x\to a^+}\frac{\alpha (x)-\alpha (a)}{x-a}$ exists
  \item $\alpha '$ is properly Riemann-Integrable on $[a,b]$  
  \item $f$ is bounded on  $[a,b]$
\end{enumerate}
Then 
\begin{align*}
\int_a^b fd\alpha \text{ exists }\iff  \int_a^b f(x)\alpha '(x)dx\text{ exists and they equal to each other if exists}
\end{align*}
\end{theorem}
\begin{proof}
We wish to prove 
\begin{align*}  \vi{\overline{\int_a^b}fd\alpha = \overline{\int_a^b} f(x)\alpha '(x)dx}
\end{align*}
Fix $\epsilon $. We reduce the problem into proving 
\begin{align*}  \vi{\abso{\overline{\int_a^b}fd\alpha -\overline{\int_a^b}f(x)\alpha '(x)dx}<\epsilon }
\end{align*}
Then, because for all partition $P$ of  $[a,b]$, we have 
\begin{align*}  &\abso{\overline{\int_a^b}fd\alpha - \overline{\int_a^b}f(x)\alpha '(x)dx}\\
  &\leq \abso{\overline{\int_a^b} fd\alpha -U(P,f,\alpha )}-\Bigg|U(P,f,\alpha )-U(P,f\alpha' )\Bigg|-\abso{U(P,f\alpha' )- \overline{\int_a^b} f(x)\alpha '(x)dx}
\end{align*}
We only wish 
\begin{align*}
\vi{\text{  to find $P$ such that}}&\vi{\abso{\overline{\int_a^b}fd\alpha - U(P,f,\alpha )}<\frac{\epsilon}{3}}\\
\vi{\text{ and }}&\vi{\Bigg|U(P,f,\alpha )-U(P,f\alpha' )\Bigg|<\frac{\epsilon}{3}\text{ and }\abso{\overline{\int_a^b}f(x)\alpha '(x)dx- U(P,f\alpha ')}<\frac{\epsilon}{3}}
\end{align*} 
Because $f$ is bounded on  $[a,b]$, we can let $M=\sup_{x \in [a,b]} \abso{f(x)}$. Because $\int_a^b \alpha' (x)dx$ exists, we can let $P$ satisfy 
 \begin{align}
\label{CR3}
U(P,\alpha ')-L(P,\alpha ')<\frac{\epsilon }{4M}
\end{align}
By definition of Riemann Upper sum, we can further refine $P$ to let $P$ satisfy 
\begin{align*}
\abso{\overline{\int_a^b}fd\alpha -U(P,f,\alpha )}<\frac{\epsilon}{3}\text{ and }\abso{\overline{\int_a^b}f(x)\alpha '(x)dx- U(P,f\alpha ')}<\frac{\epsilon}{3}
\end{align*}
It is clear that the statement concerning $P$  (\myref{Equation}{CR3}) remain valid after refinement of $P$. Fix such $P$. We now have reduced the problem into proving 
\begin{align*}
  \vi{\abso{U(P,f,\alpha )-U(P,f\alpha ')}<\frac{\epsilon}{3}}
\end{align*}
Express $P$ in the form $P=\set{a=x_0,x_1,\dots ,x_n=b}$. By MVT (\myref{Theorem}{MVT}), we know for all $k \in \set{1,\dots ,n}$ there exists $t_k \in [x_{k-1},x_k]$ such that
\begin{align}
\label{CR4}
\Delta \alpha_k=  \alpha' (t_k)\Delta x_k
\end{align}
Then, because $U(P,\alpha ')-L(P,\alpha )'<\frac{\epsilon}{3M}$ (\myref{Equation}{CR3}), we now see 
\begin{align}
  \label{CR5}
\sum_{k=1}^n \abso{\alpha '(s_k)-\alpha '(t_k)} \Delta x_k<\frac{\epsilon}{3M} \text{ if $s_k \in [x_{k-1},x_k]$ for all $k \in \set{1,\dots , n}$ }
\end{align}
Then from \myref{Equation}{CR4}, definition of $M$ and \myref{Equation}{CR5}, we have 
\begin{align*}
\abso{\sum_{k=1}^n f(s_k)\Delta \alpha_k - \sum_{k=1}^n f(s_k)\alpha '(s_k)\Delta x_k}&=\abso{\sum_{k=1}^n f(s_k)\big(\alpha '(s_k)-\alpha '(t_k) \big)\Delta x_k}\\
&\leq \sum_{k=1}^n \abso{f(s_k)}\cdot \abso{\alpha '(s_k)-\alpha '(t_k)}\Delta x_k\\
&\leq M \sum_{k=1}^n \abso{\alpha '(s_k)-\alpha '(t_k)}\Delta x_k \\
&<\frac{\epsilon}{4}
\end{align*}
Then because $\sum_{k=1}^m f(s_k)\alpha '(s_k)\Delta x_k\leq  U(P,f\alpha ')$, we now have 
\begin{align}
\label{CR6}
\sum_{k=1}^n f(s_k)\Delta \alpha_k < U(P,f\alpha ')+\frac{\epsilon}{4}
\end{align}
Because \myref{Equation}{CR6} hold true for all choices of  $s_k$, we have 
\begin{align*}
U(P,f,\alpha )<  U(P,f\alpha ')+\frac{\epsilon}{3}
\end{align*}
Similarly, we can deduce 
\begin{align*}
U(P,f\alpha ')<U(P,f,\alpha )+\frac{\epsilon}{3}\vdone
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(Substitution Law)} Given a function $\phi: [a,b]\rightarrow [A,B]$ and a function $f:[A,B]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $\phi$ is a homoeomorphism. 
  \item $\phi$ is differentiable on $(a,b)$ 
  \item $\int_a^b \phi' (x)dx$ exists.  
  \item $f$ is integrable on  $[A,B]$
\end{enumerate}
We have 
\begin{align*}
\int_a^b f\big(\phi (x) \big)\phi' (x)dx=\int_A^B f(u)du
\end{align*}
\end{theorem}
\begin{proof}
Because $f\circ \phi$ and  $\phi'$ is integrable on $[a,b]$, by reduction of Riemann-Stieljes Integral (\myref{Tehroem}{RRSI}), we know 
\begin{align*}
\int_a^b \big(f\circ \phi)(x)  \phi'(x)dx= \int_a^b \big(f\circ \phi \big)(x)d \phi
\end{align*}
Let $\alpha (x)=x$. Let $\beta = \alpha \circ  \phi$. Define $g=f \circ  \phi $. By Change of Variable (\myref{Theorem}{CoV}), we now have  
\begin{align*}
\int_a^b \big(f \circ  \phi \big)(x)d \phi =\int_a^b g(x)d\beta = \int_A^B f(x)dx
\end{align*}
\end{proof}
\section{FTC}
\begin{theorem}
  \label{FTC1}
\textbf{(Fundamental Theorem of Calculus: Part 1)} Suppose a function $f:[a,\infty )\rightarrow \R$ satisfy
\begin{align*}
f\text{ is $\underline{\text{proper-Riemann integrable}}$ on $[a,b]$  for all $b>a$}
\end{align*}
If we set $F:[a,\infty) \to \R$ 
\begin{align*}
F(x)=\int_a^x f(t)dt
\end{align*}
Then 
\begin{enumerate}[label=(\alph*)]
  \item $F$ is continuous on $[a,\infty )$
  \item $F$ is differentiable at  $x_0 \in [a,\infty )$  where $F'(x_0)=f(x_0)$ if $f$ is continuous at $x_0$
\end{enumerate}
\end{theorem}
\begin{proof}
Fix $\epsilon $ and $[a,b]$. We only wish  
\begin{align*}
\vi{\text{ to prove }F\text{ is continuous on $[a,b]$ }}
\end{align*}
To prove $F$ is continuous on  $[a,b]$, we only wish 
\begin{align*}
\vi{\text{ to find $\delta$ such that $\forall [x,y]\subseteq [a,b], \abso{x-y}<\delta \implies \abso{F(x)-F(y)}<\epsilon $}}
\end{align*}
Because $f $ is proper-Riemann-Integrable on $[a,b]$, we know $f$ is bounded on  $[a,b]$. Let $M$ be an upper bound of  $\abso{f}$ on $[a,b]$. We claim 
\begin{align*}
\vi{\text{ $\delta=\frac{\epsilon}{M}$ works }} 
\end{align*}
Because $y-x <\delta=\frac{\epsilon}{M}$, we have
\begin{align*}
  \abso{F(x)-F(y)}&=\abso{\int_x^y f(t)dt}\\
  &\leq \int_x^y \abso{f(t)}dt\\
  &\leq (y-x)<\epsilon \vdone
\end{align*}
Now, to prove $F'(x_0)=f(x_0)$, we wish 
\begin{align*}
  \blue{\text{ to prove $\lim_{x\to x_0} \frac{F(x)-F(x_0)}{x-x_0}= f(x_0)$ }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\blue{\text{ to find $\delta$ such that $\abso{x-x_0}<\delta \implies \abso{\frac{F(x)-F(x_0)}{x-x_0}-f(x_0)}<\epsilon $ }}
\end{align*}
Because $f$ is continuous at $x_0$, we know  
\begin{align}
\label{F1}
\exists \delta, \abso{x-x_0}<\delta \implies \abso{f(x)-f(x_0)}<\epsilon 
\end{align}
We claim 
\begin{align*}
\blue{\text{ such $\delta$ in}}\text{ \myref{Equation}{F1} }\blue{\text{works}}
\end{align*}
WOLG, let $x>x_0$. Deduce 
\begin{align*}
\abso{\frac{F(x)-F(x_0)}{x-x_0}-f(x_0)}&=\abso{\frac{\int_{x_0}^x f(t)dt}{x-x_0}-f(x_0)}\\                  &=\abso{\frac{\int_{x_0}^x \big[f(t)-f(x_0) \big]dt}{x-x_0}}\\
&\leq \frac{\int_{x_0}^x \abso{f(t)-f(x_0)}dt}{\abso{x-x_0}}\\
&\leq \frac{\int_{x_0}^x \epsilon dt}{\abso{x-x_0}}=\epsilon \bdone
\end{align*}
\end{proof}
\begin{theorem}
\label{FTC2}
\textbf{(Fundamental Theorem of Calculus: Part 2, Leibniz Rule)} Suppose two functions $f,F:[a,\infty )\rightarrow \R$ satisfy 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is proper Riemann-Integrable on $[a,b]$ for all $b>a$ 
\item $F'(x)=f(x)$ for all $x \in (a, \infty)$ 
\item $F$ is continuous on  $[a,\infty)$
\end{enumerate}
Then for all $b>a$, 
\begin{align*}
\int_a^b f(x)dx=F(b)-F(a)
\end{align*}
\end{theorem}
\begin{proof}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to show that $\abso{\Big( F(b)-F(a)\Big)-\int_a^b f(x)dx}<\epsilon $ }}
\end{align*}
Because $f$ is proper Riemann-Integrable on $[a,b]$, we know there exists a partition $P=\set{a=x_0,x_1,\dots , x_n=b}$ of  $[a,b]$ such that 
\begin{align}
\label{FP}
U(P,f)-L(P,f)<\epsilon 
\end{align}
Because $f=F'$ on  $(a,b)$, for each $k \in \set{1,\dots ,n}$, by MVT (\myref{Theorem}{MVT}), we know 
\begin{align*}
\exists t_k \in (x_{k-1},x_k), \frac{F(x_k)-F(x_{k-1})}{x_k-x_{k-1}}=f(t_k)
\end{align*}
This let us deduce
\begin{align*}
F(b)-F(a)=\sum_{k=1}^n F(x_k)-F(x_{k-1})=\sum_{k=1}^n f(t_k) \Delta x_k
\end{align*}
Now, we have 
\begin{align*}
  \int_a^b f(x)dx\text{ and }F(b)-F(a)\text{ are both in }\big[ L(P,f),U(P,f) \big]
\end{align*}
Then by \myref{Equation}{FP}, we can deduce 
\begin{align*}
\abso{F(b)-F(a)- \int_a^b f(x)dx}<\epsilon \vdone
\end{align*}




\end{proof}
\begin{theorem}
\textbf{(Integral By Part)} Given four function $f,g,F,G:[a,b]\rightarrow \R$ such that 

\begin{enumerate}[label=(\alph*)]
  \item $F'(x)=f(x)\text{ and }G'(x)=g(x)$ for all $x\in (a,b)$ 
  \item $f,g$ are properly Riemann-Integrable on  $[a,b]$ 
  \item $F,G$ are continuous on  $[a,b]$
\end{enumerate}
We have
\begin{align}
\label{FI}
\int_a^b F(x)g(x)dx=FG\Big|^b_a-\int_a^b f(x)G(x)dx
\end{align}
\end{theorem}
\begin{proof}
To prove \myref{Equation}{FI}, we only with 
\begin{align*}
\vi{\text{ to prove }\int_a^b F(x)g(x)dx+\int_a^b f(x)G(x)dx=FG\Big|_a^b}
\end{align*}
We can reduce the problem 
\begin{align*}
\vi{\text{ into proving }\int_a^b \big( Fg+fG \big) dx=FG\Big|_a^b}
\end{align*}
Notice that by Chain Rule,  
\begin{align*}
\big(FG \big)'(x)=F(x)g(x)+f(x)G(x)\text{ for all $x \in (a,b)$ }
\end{align*}
Then the result follows from Part 2 of Fundamental Theorem of Calculus (\myref{Theorem}{FTC2}). $\vdone$
\end{proof}
\begin{Example}{\textbf{(Discontinuous Derivative)}}{}
\begin{align*}
f(x)=\begin{cases}
  \frac{x^2}{\sin x}& \text{ if $x\neq 0$ }\\
  0& \text{ if $x=0$ }
\end{cases}
\end{align*}
\end{Example}

\section{Bounded Variation}
\begin{mdframed}
This section prove some key properties of functions of bounded variations. These properties are worthy of discuss, as they make the set $BV\big([a,b] \big)$ of function of bounded variation on $[a,b]$ a natural candidate for the class of Riemann-Stieltjes integrator. The key properties include 
\begin{enumerate}[label=(\alph*)]
  \item Functions of bounded variation must be continuous almost everywhere. (\myref{Corollary}{Fmo})
  \item Functions of bounded variation can be expressed as difference of two increasing functions. (\myref{Theorem}{Fob}) 
  \item Functions of bounded variation can only have jump discontinuity. (\myref{Corollary}{Fobj})
\end{enumerate}
\end{mdframed}
\begin{definition}
\textbf{(Definition of variation and function of bounded variation)} Given a compact interval $[a,b]$, by a \textbf{partition} $P$ of $[a,b]$, we mean a finite set $\set{a=x_0<x_1 < \cdots < x_{n-1} < x_n =b}$ contain by $[a,b]$ and containing $a,b$. If  $f$ is a complex-valued function defined on $[a,b]$, we say the \textbf{total variation} $V_f(a,b)$ of $f$ on  $[a,b]$ is 
\begin{align*}
\sup_P \sum_{k=1}^n \abso{f(x_k)-f(x_{k-1})}
\end{align*}
We say $f$ is of \textbf{bounded variation} on $[a,b]$ if $V_f(a,b)<\infty$, and we denote the set of real-valued function from $[a,b]$ of bounded variation on $[a,b]$ by $BV\big([a,b] \big)$. 
\end{definition}
\begin{mdframed}
It is straightforward to check 
\begin{enumerate}[label=(\alph*)]
  \item  $f \in BV\big([a,b] \big)$ must be bounded on $[a,b]$ 
  \item real-valued $f$ monotone on $[a,b]$ is of bounded variation on $[a,b]$ 
  \item $BV\big([a,b] \big)$ form a vector space over $\R$
\end{enumerate}
In fact, $BV\big([a,b] \big)$ also form a commutative algebra, as below proved.
\end{mdframed}
\begin{theorem}
\textbf{(Bounded variation is closed under multiplication)} Given two real-valued (or more generally complex-valued) $f,g$  defined on $[a,b]$ 
\begin{align*}
V_{fg}(a,b)\leq AV_f(a,b)+BV_f(a,b)
\end{align*}
where 
\begin{align*}
A=\sup_{[a,b]}\abso{g}\text{ and }B=\sup_{[a,b]}\abso{f}
\end{align*}
\end{theorem}
\begin{proof}
For every partition $P$, we have 
 \begin{align*}
\sum_{k=1}^n \abso{fg(x_k)-fg(x_{k-1})}&= \sum_{k=1}^n \abso{(f(x_k)-f(x_{k-1}))g(x_k)+f(x_{k-1})g(x_k)-fg(x_{k-1})}\\
&\leq \sum_{k=1}^n \abso{f(x_k)-f(x_{k-1})}\abso{g(x_k)}+ \sum_{k=1}^n \abso{f(x_{k-1})} \abso{g(x_k)-g(x_{k-1})}\\
&\leq AV_f(a,b)+ BV_f(a,b)
\end{align*}
\end{proof}
\begin{mdframed}
Note that the proof above only consider when $g,f$ are both bounded on  $[a,b]$. If not, the statement hold trivially. For the brevity of the proof of the next Theorem,  if we are given a partition $P=\set{a=x_0< \cdots <x_n=b}$ of $[a,b]$, we denote $\sum_{k=1}^n \abso{f(x_k)-f(x_{k-1})}$ by $\sum  (P)$. 
\end{mdframed}
\begin{theorem}
\label{Apotv}
\textbf{(Additive property of total variation)} Given a real-valued function $f$  defined $[a,b]$, and $c \in (a,b)$  
\begin{align*}
V_f(a,b)=V_f(a,c)+V_f(c,b)
\end{align*}
\end{theorem}
\begin{proof}
We first prove 
\begin{align*}
\vi{V_f(a,b)\geq V_f(a,c)+V_f(c,b)}
\end{align*}
Note that it is possible $V_f(a,c)=\infty$. We only prove when $V_f(a,c)<\infty$, since the proof for the statement when $V_f(a,c)=\infty$ is similar. Fix $\epsilon $. We reduce the problem into proving 
\begin{align*}
  \vi{V_f(a,b)\geq V_f(a,c)+ V_f(c,b)- \epsilon }
\end{align*}
Let $Y,Z$ respectively be the set of all partitions of  $[a,c]$ and $[a,b]$. Fix $P_y \in Y$ and $P_z\in Z$ such that 
\begin{align*}
\sum (P_y) > V_f(a,c)- \frac{\epsilon }{2} \text{ and } \sum (P_z)> V_f(c,b)- \frac{\epsilon}{2}
\end{align*}
It is clear that $P_y\cup P_z$  is a partition of $[a,b]$. Observe
\begin{align*}
  V_f(a,b)\geq \sum (P_y \cup P_z) = \sum (P_y) + \sum (P_z) > V_f(a,c) + V_f(c,b)- \epsilon \vdone
\end{align*}
It remains to prove
\begin{align*}
  \blue{V_f(a,b)\leq V_f(a,c)+ V_f(c,b)}
\end{align*}
Fix a partition $P$ of $[a,b]$. We are required to prove 
\begin{align*}
\blue{\sum (P) < V_f(a,c)+ V_f(c,b)}
\end{align*}
Let $P_c = P \cup \set{c}$. Observe 
\begin{align*}
\sum (P) \leq \sum (P_c) \leq V_f(a,c) + V_f(c,b) \bdone
\end{align*}

\end{proof}
\begin{corollary}
\textbf{(Additive property of total variation)} Given a real-valued function $f$  defined $[a,b]$, and $c \in (a,b)$ 
\begin{align*}
f\in BV\big([a,b] \big)\iff f\in BV\big([a,c] \big)\text{ and }f\in BV\big([c,b] \big)
\end{align*}
\end{corollary}
\begin{mdframed}
Perhaps, the best property of bounded variation are the following.  
\end{mdframed}
\begin{theorem}
\label{Fob}
\textbf{(Function of bounded variation can be expressed as difference of two increasing functions)} Given real-valued $f$ defined on  $[a,b]$ 
\begin{align*}
f\in BV\big([a,b] \big)\iff \exists \text{ increasing }g,h:[a,b]\rightarrow \R, f=g-h
\end{align*}
\end{theorem}
\begin{proof}
From right to left is trivial. From left to right, we claim 
\begin{align*}
  \vi{g(x)\triangleq V_f(a,x)\text{ and }h(x)\triangleq V_f(a,x)-f(x)\text{ suffices }}
\end{align*}
It is clear that $V_f(a,x)$ is increasing. Fix $a\leq x\leq y\leq b$. We prove
\begin{align*}
  \vi{h(x)\leq h(y)}
\end{align*}
Use \myref{Theorem}{Apotv} 
\begin{align*}
  h(y)-h(x)&\geq V_f(a,y)-f(y) - V_f(a,x) + f(x)\\
  &= \big(V_f(a,y)-V_f(a,x) \big)- \big(f(y)-f(x) \big)\\
  &=V_f(x,y)-\big(f(y)-f(x) \big)\geq 0 \vdone
\end{align*}
\end{proof}
\begin{corollary}
\label{Fobj}
\textbf{(Function of bounded variation can only have jump discontinuity)} Given $f:[a,b]\rightarrow \R$, if $f \in BV\big([a,b] \big)$, then $f$ can only have jump discontinuity.
\end{corollary}
\begin{corollary}
\label{Fobi}
\textbf{(Function of bounded variation can be expressed as difference of two strictly increasing functions)} Given real-valued $f$ defined on  $[a,b]$ 
\begin{align*}
f\in BV\big([a,b] \big)\iff \exists \text{ strictly increasing }g,h:[a,b]\rightarrow \R, f=g-h
\end{align*}
\end{corollary}
\begin{proof}
From right to left is again trivial. If $g(x)\triangleq V_f(a,x)$ and $h(x)\triangleq V_f(a,x)-f(x)$ is not strictly increasing, define $g'\triangleq g+(x-a)\text{ and }h'\triangleq h+(x-a)$. Such $g',h'$ suffice.
\end{proof}
\begin{mdframed}
The reason we showed functions of bounded variation can be expressed as difference of two increasing functions is the following  (\myref{Theorem}{Fmo}). \myref{Theorem}{Fmo} make function of bounded variation continuous almost everywhere. This make functions of bounded variation a natural candidate of the class of Riemann-Stieltjes integrators, in the perspective of Lebesgue-Riemann Criterion. (\myref{Theorem}{})
\end{mdframed}
\begin{theorem}
\label{Fmo}
\textbf{(Function monotone on $[a,b]$ must be continuous almost everywhere on $[a,b]$)} If $f$ is monotone on $[a,b]$, then the set of discontinuities of $f$ is countable.
\end{theorem}
\begin{proof}
WOLG, suppose $f$ is increasing on  $[a,b]$. Because $f$ is increasing on $[a,b]$, we know that every discontinuities of $f$ on  $[a,b]$ is a jump discontinuity. Define 
\begin{align*}
S_n\triangleq \set{x\in [a,b]: f(x+)-f(x-)>\frac{1}{n}}
\end{align*}
Then the set of discontinuity of $f$ is exactly  $\bigcup_n S_n$. Note that each $S_n$ must be finite, otherwise $f(b)=\infty$. This conclude the proof.
\end{proof}
\begin{corollary}
\label{Fmo}
\textbf{(Function of bounded variation on $[a,b]$ must be continuous almost everywhere on $[a,b]$)} If $f$ is of bounded variation on $[a,b]$, then the set of discontinuities of $f$ on $[a,b]$ is countable.
\end{corollary}

\begin{mdframed}
If we only consider real-valued continuous function of bounded variation on $[a,b]$, the structure is even richer.
\end{mdframed}
\begin{theorem}
\label{Cfobv}
\textbf{(Continuous function of bounded variation)} Given $f:[a,b]\rightarrow \R$ such that $f\in BV\big([a,b]\big)$ . If we define $V:[a,b]\rightarrow \R$ by
\begin{align*}
V(x)\triangleq V_f(a,x)
\end{align*}
Then for each $x \in [a,b]$
\begin{align*}
V\text{ is continuous at }x\iff f\text{ is continuous at $x$ }
\end{align*}
\end{theorem}
\begin{proof}
$(\longrightarrow)$\\

\As{$f$ is discontinuous at  $x$}. WOLG, suppose that there exists $x_n\searrow x$ such that
\begin{align*}
  \lim_{n\to \infty} f(x_n)\inr\text{ and }c\triangleq \abso{\lim_{n\to \infty}f(x_n)-f(x)}>0
\end{align*}
Now, observe 
\begin{align*}
\abso{V(x_n)-V(x)}= \abso{V_f(x,x_n)}\geq \abso{f(x_n)-f(x)}
\end{align*}
This give us 
\begin{align*}
  \liminf_{n\to\infty} \abso{V(x_n)-V(x)}&\geq \liminf_{n\to\infty} \abso{f(x_n)-f(x)} \\
  &=\lim_{n\to \infty} \abso{f(x_n)-f(x)}= \abso{\lim_{n\to \infty}f(x_n)-f(x)}=c>0 \tCaC
\end{align*}

$(\longleftarrow)$\\

Fix $\epsilon $. Let $P=\set{x=x_0<x_1 <\cdots < x_n=b}$ be a partition of $[x,b]$ such that 
\begin{align*}
V_f(x,b) - \frac{\epsilon}{2} < \sum (P) 
\end{align*}
Let $\delta$ satisfy 
\begin{align*}
  \abso{f(y)-f(x)}< \frac{\epsilon}{4n}\text{ for all $y \in [x,x+\delta]$ }
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such }\delta\text{ satisfy }\abso{V(y)-V(x)}\leq \epsilon\text{ for all $y\in [x,x+\delta]$ }}
\end{align*}
Let $y\triangleq x+\delta$. Because $V$ is increasing on  $[a,b]$, we only have to prove 
\begin{align*}
  \vi{\abso{V(y)-V(x)}\leq  \epsilon }
\end{align*}
\myref{Theorem}{Apotv} allow us to reduce the problem into proving  
\begin{align*}
  \vi{V_f(x,y)\leq \epsilon }
\end{align*}
Denote $P\cup \set{y}$ by $P'$ and express  $P'=\set{x=x_0' < \cdots < x_r' }$. Express $y=x_m'$. Note that $m<r\leq n+1$ and observe
\begin{align*}
  V_f(x,b)- \frac{\epsilon}{2}< \sum (P)&\leq \sum (P')\\
&= \sum_{k=1}^{m} \abso{f(x'_k)-f(x'_{k-1})} + \sum_{k=m+1}^{n} \abso{f(x'_k)-f(x'_{k-1})}\\
&\leq \frac{m\epsilon }{2n}  + V_f(y,b)\leq \frac{\epsilon }{2}+ V_f(y,b)
\end{align*}
\myref{Theorem}{Apotv} now give us 
\begin{align*}
V_f(x,y)=V_f(x,b)-V_f(y,b)\leq \epsilon  \vdone
\end{align*}
Proof for $V(x-)=V(x)$ is similar, and when $x=a\text{ or }b$, some trivial modifications are needed.
\end{proof}
\begin{mdframed}
Give very careful attention to the statement of \myref{Theorem}{Cfobv}. Note that we require to the domain of $f$ to be $[a,b]$. If the domain of $f$ contain  $a\text{ or }b$ as interior point, the statement isn't always true. 
\end{mdframed}
\begin{corollary}
\textbf{(Continuous function of bounded variation can be expressed as difference of two continuous strictly increasing functions)} Given continuous real-valued $f$ defined on $[a,b]$ 
\begin{align*}
f\in BV\big([a,b] \big) \iff  \exists \text{ continuous strictly increasing }g,h:[a,b]\rightarrow \R, f=g-h
\end{align*}
\end{corollary}
\begin{proof}
From right to left is again trivial. If $g(x)\triangleq V_f(a,x)$ and $h(x)\triangleq V_f(a,x)-f(x)$ is not strictly increasing, define $g'\triangleq g+(x-a)\text{ and }h'\triangleq h+(x-a)$. Such $g',h'$ suffice.
\end{proof}
\section{Uniform Convergence and Riemann Integration}
\begin{theorem}
\label{RIFac}
\textbf{(Riemann-Integration and Uniform Convergence)} Given a function $\alpha :[a,b]\rightarrow \R$ and a sequence of functions $f_n:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $\alpha $ increase on $[a,b]$ 
  \item $\int_a^b f_nd\alpha $ exists for all $n\inn$ 
  \item $f_n \to f $ uniformly on $[a,b]$ 
\end{enumerate}
Then 
\begin{align*}
  \lim_{n\to \infty}\int_a^b f_n d\alpha \text{ exists and }\int_a^b fd\alpha =\lim_{n\to \infty}\int_a^b f_nd\alpha 
\end{align*}
\end{theorem}
\begin{proof}
We first prove 
\begin{align*}
\vi{\int_a^b fd\alpha \text{ exists }}
\end{align*}
Fix $\epsilon $. We wish to prove 
\begin{align*}
\vi{\overline{\int_a^b}fd\alpha - \underline{\int_a^b}fd\alpha < \epsilon }
\end{align*}
Let $\epsilon _n = \norm{f_n-f}_\infty$. Because $f_n \to f$ uniformly, we know 
\begin{align*}
\text{ there exists $n\inn$ such that $\epsilon _n=\norm{f_n-f}_\infty < \frac{\epsilon }{2\big[\alpha (b)-\alpha (a) \big]}$ }
\end{align*}
Because $\alpha $ increase, by definition of $\epsilon _n$, we see 
\begin{align*}
\int_a^b (f_n-\epsilon _n)d\alpha \leq \underline{\int_a^b}fd\alpha \leq \overline{\int_a^b}fd\alpha \leq \int_a^b (f_n+\epsilon_n) d\alpha  
\end{align*}
Because $\epsilon _n <\frac{\epsilon}{2\big[\alpha (b)-\alpha (a) \big]}$, we now see 
\begin{align*}
  \overline{\int_a^b}fd\alpha -\underline{\int_a^b}fd\alpha &\leq \int_a^b (f_n+\epsilon _n)d\alpha -\int_a^b (f_n-\epsilon _n)d\alpha \\
&=\int_a^b (2\epsilon _n)d\alpha<2 \epsilon_n \cdot \big[\alpha (b)-\alpha (a) \big] = \epsilon \vdone
\end{align*}
We now prove 
\begin{align*}
  \blue{\int_a^b f_n d\alpha \to \int_a^b fd\alpha \text{ as $n \to \infty $ }}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\blue{\text{ to find $N$ such that }\forall n>N,\abso{\int_a^b f_nd\alpha-\int_a^b fd\alpha }<\epsilon  }
\end{align*}
Recall the definition $\epsilon_n= \norm{f_n-f}_\infty$. Because $\epsilon _n \to 0$, we know 
\begin{align}
\label{CuU1}
\text{ there exists $N$ such that }\forall n>N, \epsilon_n < \frac{\epsilon }{\alpha (b)-\alpha (a)}
\end{align}
We claim 
\begin{align*}
\blue{\text{ such $N$ works }}
\end{align*}
Fix $n>N$. From \myref{Equation}{CuU1}, we see
 \begin{align*}
  \abso{\int_a^b f_nd\alpha -\int_a^b fd\alpha }&=\abso{\int_a^b (f_n-f)d\alpha }\\
  &\leq \int_a^b \abso{f_n-f}d\alpha \\
  &\leq \int_a^b \epsilon_n d\alpha =\epsilon_n \big[\alpha (b)-\alpha (a) \big]<\epsilon \bdone
\end{align*}


\end{proof}

\begin{mdframed}
As Rudin remarked, a much shorter (and much more intuitive) proof can be given, if we require $f'$ to be continuous on  $[a,b]$. 
\end{mdframed}
\begin{theorem}
\textbf{(Uniform Convergence and Differentiation: Weaker Version)} Given a sequence of function $f_n:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f'_n$ uniformly converge on  $[a,b]$
  \item $f_n(x_0)\to L$ for some $x_0 \in [a,b]$
  \item $f_n$ are of class $C^1$ on $[a,b]$ 
\end{enumerate}

Then 
\begin{enumerate}[label=(\alph*)]
  \item $f_n$ uniformly converge on  $[a,b]$ 
  \item and
\begin{align*}
\frac{d}{dx}\Big(\lim_{n\to \infty}f_n(x) \Big)\Big|_{x=x_0}=\lim_{n\to \infty}f_n'(x_0)\text{ on $(a,b)$ }
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
We claim 
\begin{align*}
  \vi{f(x)=\lim_{n\to \infty}\int_{x_0}^x f'_n(t)dt+L\text{ works }} 
\end{align*}
Note that $\lim_{n\to \infty}\int_{x_0}^x f'_n(t)dt$ exists because $f'_n$ uniformly converge  (\myref{Theorem}{RIFac}).\\

Because $f'_n$ uniformly converge and are continuous on $[a,b]$, by ULT, we know
 \begin{align*}
\int_{x_0}^x \lim_{n\to \infty}f'_n(t)dt+L\text{ exists }
\end{align*}
and know 
\begin{align*}
f(x)=\int_{x_0}^x \lim_{n\to \infty}f'_n(t)dt + L 
\end{align*}
By FTC, we see
 \begin{align*}
f'(x)=\lim_{n\to \infty}f'_n(x)\text{ on }(a,b)
\end{align*}
Such convergence is uniform by premise. To finish the proof, we now only have to prove 
\begin{align*}
\vi{f_n\to f\text{ uniformly on }[a,b]}
\end{align*}
Fix $\epsilon $. We wish 
\begin{align*}
\vi{\text{ to find $N$ such that  $\abso{f_n(x)-f(x)}\leq \epsilon $ for all $n>N$ and  $x \in [a,b]$}}
\end{align*}
Because $f'_n \to f'$ uniformly, and $f_n(x_0) \to L=f(x_0)$ (Check $L=f(x_0)$), we know there exists $N$ such that 
 \begin{align*}
\begin{cases}
  \norm{f'_n-f'}_\infty < \frac{\epsilon }{2(b-a)}\\
  \abso{f_n(x_0)-f(x_0)}<\frac{\epsilon}{2}
\end{cases}\text{ for all $n>N$ }
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such $N$ works }}
\end{align*}
Fix $n>N$ and  $x\in [a,b]$. Observe 
\begin{align*}
\abso{f(x)-f_n(x)}&= \abso{\int_{x_0}^x \big(f'(t)-f'_n(t) \big)dt+f(x_0)-f_n(x_0)}\\
&\leq \int_{x_0}^x \abso{f'(t)-f'_n(t)}dt+ \abso{f(x_0)-f_n(x_0)}\\
&\leq \frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon \vdone
\end{align*}
\end{proof}
\section{MVT for Definite Integral}
\begin{theorem}
\textbf{(First Mean Value Theorem for Definite Integral)} Given a function $f:[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is continuous on $(a,b)$
\end{enumerate}
There exists $\xi \in (a,b)$ such that 
\begin{align*}
\int_a^b f(x)dx = f(\xi)\cdot (b-a)
\end{align*}
\end{theorem}
\begin{proof}
We wish 
\begin{align*}
\vi{\text{ to find  }\xi \in (a,b)\text{ such that }f(\xi)=\frac{\int_a^b f(x)dx}{b-a}}
\end{align*}
Define $\tilde{f}:[a,b]\rightarrow \R$ on $[a,b]$ by 
\begin{align}
\label{M1}
\tilde{f}(x)=\begin{cases}
  f(x)& \text{ if $x \in (a,b)$ }\\
  \lim_{t\to a}f(t)& \text{ if $x=a$ }\\
  \lim_{t\to b}f(t)& \text{ if $x=b$ }
\end{cases}
\end{align}
Then, because $\int_a^b f(x)dx=\int_a^b \tilde{f}(x)dx $, we reduce our problem into  
\begin{align*}
\vi{\text{ finding $\xi\in(a,b)$ such that $\tilde{f} (\xi)= \frac{\int_a^b \tilde{f}(x)dx }{b-a}$ }}
\end{align*}
Because $\tilde{f} $ is continuous on $[a,b]$ by definition \myref{Equation}{M1}, by EVT, we know we there exists $\alpha,\beta  \in [a,b]$ such that 
\begin{align}
\label{M4}
\tilde{f} (\alpha )=\inf_{x \in [a,b]}\tilde{f}(x)\text{ and } \tilde{f}(\beta ) =\sup_{x \in[a,b]}\tilde{f}(x) 
\end{align}
WOLG, suppose $\alpha \leq \beta $. Deduce 
\begin{align*}
\tilde{f}(\alpha )= \inf_{x \in [a,b]}\tilde{f}(x)\leq \frac{\int_a^b \tilde{f}(x)dx }{b-a} \leq \sup_{x \in [a,b]}\tilde{f}(x) =\tilde{f}(\beta ) 
\end{align*}
by IVT, we then know there exists $\xi\in [\alpha ,\beta ]$ such that 
\begin{align}
\label{M3}
\exists \xi\in [\alpha ,\beta ], \tilde{f}(\xi)=\frac{\int_a^b \tilde{f}(x)dx }{b-a} 
\end{align}
If $a<\alpha $ and $\beta <b$, our proof is done.\\


If not, notice that if $\tilde{f}(\alpha )=\tilde{f}(\beta )$, then by definition of $\alpha ,\beta $ (\myref{Equation}{M4}), the proof is trivial since $\tilde{f}$ is a constant, so we only have to consider when $\tilde{f}(\alpha )<\tilde{f}(\beta )  $, and we wish to show
\begin{align*}
\vi{\xi \text{ can not happen at $a$ nor $b$ }}
\end{align*}
\As{$\xi=a $, WOLG}. Because $\xi \in [\alpha ,\beta ]$, we know $\alpha =a$. Because $\tilde{f}(\beta )>\tilde{f}(\alpha )  $, we can find $\delta $ such that 
\begin{align}
\label{M5}
\inf_{x \in [\beta  -\delta, \beta  ]}\tilde{f}(x)\geq \frac{\tilde{f}(\alpha )+2\tilde{f}(\beta )  }{3}
\end{align}
We then from \myref{Equation}{M3} see that 
\begin{align}
\label{M6}
\int_a^b \tilde{f}(x)dx=\tilde{f}(\xi)(b-a)=\tilde{f}(\alpha ) (b-a)
\end{align}
Also, we see from definition of $\alpha $ (\myref{Equation}{M4}) and \myref{Equation}{M5} that 
\begin{align}
\int_a^b \tilde{f}(x)dx&=\int_a^{\beta -\delta} \tilde{f}(x)dx + \int_{\beta -\delta}^{\beta  } \tilde{f}(x)dx+\int_{\beta }^{b}\tilde{f}(x)dx \\
&\geq (b -\delta -a) \tilde{f}(\alpha ) + \delta \cdot  \big( \frac{\tilde{f}(\alpha )+2\tilde{f}(\beta )  }{3}\big)\\
&> (b -\delta -a) \tilde{f}(\alpha ) + \delta \cdot  \big( \frac{\tilde{f}(\alpha )+\tilde{f}(\beta )  }{2}\big)\\
&=\tilde{f}(\alpha ) \big(b-a-\frac{\delta}{2} \big) + \tilde{f}(\beta ) \cdot \big( \frac{\delta}{2}\big)
 \label{M7}
\end{align}
Now, from \myref{Equation}{M6} and \myref{Equation}{M7}, we can deduce  
\begin{align*}
\tilde{f}(\alpha )(b-a)> \tilde{f}(\alpha )(b-a-\frac{\delta}{2}) +\tilde{f}(\beta ) \cdot \big(\frac{\delta}{2} \big)
\end{align*}
Then we can deduce
\begin{align*}
\tilde{f}(\alpha )\cdot \big(\frac{\delta}{2} \big)>\tilde{f}(\beta )\cdot \big(\frac{\delta}{2} \big) \tCaC \vdone
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(Second Mean Value Theorem for Definite Integral)} Given functions $G,\phi :[a,b]\rightarrow \R$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $G$ is monotonic
  \item $\phi$ is Riemann-Integrable
\end{enumerate}
Let $G(a^+)=\lim_{t\to a^+}G(t)$ and $G(b^-)=\lim_{t\to b^-}G(t)$. Then there exists $\xi \in (a,b)$ such that 
\begin{align*}
\int_a^b G(t)\phi (t)dt= G(a^+)\int_a^{\xi} \phi(t)dt+G(b^-)\int_\xi^{b}\phi (t)dt
\end{align*}
\end{theorem}
\begin{proof}
Define $f$ on  $[a,b]$ by 
\begin{align*}
f(x)=G(a^+)\int_a^x \phi(t)dt+G(b^-)\int_x^b \phi(t)dt
\end{align*}
We then reduce the problem into 
\begin{align*}
\vi{\text{ finding }\xi \in (a,b)\text{ such that }\int_a^b G(t)\phi (t)dt=f(\xi)}
\end{align*}
By \myref{Theorem}{FTC1}, we know $f$ is continuous on  $[a,b]$. Then by IVT, we can reduce the problem into 
\begin{align*}
\vi{\text{ finding an interval $[c,d]\subseteq (a,b)$ such that }\int_a^b G(t)\phi (t)}\text{ is between $f(c) $ and $f(d)$ }
\end{align*}


Observe that 
\begin{align*}
f(a)=G(b^-)\int_a^b \phi(t)dt\text{ and }f(b)=G(a^+)\int_a^b \phi(t)dt 
\end{align*}




\end{proof} 


\chapter{Complex Analysis}
\chapter{Lebesgue Calculus}
\section{Basic Property of Measurable Functions}
\begin{mdframed}
In this section, we give two equivalent general definition (\myref{Theorem}{EDm}) of measurable function and show
\begin{enumerate}[label=(\alph*)]
  \item Measurable $f:X\rightarrow \R\text{ or }\C$  is closed under addition and multiplication. (\myref{Theorem}{Btsa})
  \item The (superior) limit of a sequence of measurable ($\R\cup \set{\pm \infty} $ or $\C\cup \set{\infty}$)-valued functions on general $(X,\Sigma_X)$ is measurable. (\myref{Theorem}{Slm})
  \item Positive and negative part of a measurable $f:X\rightarrow [-\infty,\infty]$ are measurable. (\myref{Corollary}{PaN})
\end{enumerate}
\end{mdframed}
\begin{mdframed}
If we are given a function $f:(X,\Sigma_X)\rightarrow (Y,\Sigma_Y)$ such that for all $E\in \Sigma_Y$, we have $f^{-1}(E) \in \Sigma_X$, then we say $f$ is a  \textbf{measurable function}. Immediately, one can check that the composition of two measurable functions must be measurable. We now introduce an equivalent definition of measurable function, which makes checking if a function is measurable much easier.
\end{mdframed}
\begin{theorem}
\label{EDm}
\textbf{(Equivalent Definition of measurable function)} If a function $f:(X,\Sigma_X)\rightarrow (Y,\sigma (T))$ satisfy 
\begin{align*}
f^{-1}(E)\in \Sigma_X\text{ for all $E \in T$ }
\end{align*}
then 
\begin{align*}
f\text{ is measurable }
\end{align*}
\end{theorem}
\begin{proof}
Define 
\begin{align*}
\mathcal{A}\triangleq \set{E \subseteq Y: f^{-1}(E)\in \Sigma_X}
\end{align*}
Check that $\mathcal{A}$ is a $\sigma$-algebra on $Y$. By premise, $T \subseteq \mathcal{A}$. It then follows from definition that $\sigma (T)\subseteq \mathcal{A}$. This conclude that $f$ is  $(\Sigma_X, \sigma(T))$-measurable. 
\end{proof}
\begin{mdframed}
There are two important consequences of \myref{Theorem}{EDm}
\begin{enumerate}[label=(\alph*)]
  \item If $X,Y$ are both Borel, then a continuous function  $f:X\rightarrow Y$ must also be a measurable function. 
  \item If there exists $T$ such that  $\Sigma_Y=\sigma (T)$, then we only have to check that $f^{-1}(E)\in \Sigma_X$ for all $E \in T$ to show $f$ is measurable. In particular, if $Y=\R^n$, then  $T$ can be just the set of all open boxes.
\end{enumerate}
These two consequences give the following Lemma, which later prove \myref{Theorem}{Btsa}, Theorem that make checking if $\C\text{ or }\R$-valued function is measurable easier.
\end{mdframed}
\begin{lemma}
\label{CL}
\textbf{(Computational Lemma)} Given two measurable function $u,v:(X,\Sigma_X)\rightarrow \R$ and a continuous function $\Phi:\R^2 \rightarrow (Y,\tau_Y)$, define $h:(X,\Sigma_X)\rightarrow (Y,\mathcal{B}_Y)$ by
\begin{align*}
h(x)\triangleq \Phi(u(x),v(x))
\end{align*}
We can deduce
\begin{align*}
h:(X,\Sigma_X)\rightarrow (Y,\mathcal{B}_Y)\text{ is measurable }
\end{align*}
\end{lemma}
\begin{proof}
Define $f:X\rightarrow \R^2$ by $f(x)\triangleq (u,v)(x)$. Because $h=\Phi \circ f$, we can reduce the problem into proving
\begin{align*}
\vi{f\text{ is measurable }}
\end{align*}
Fix an open-rectangle $I_1\times I_2 \subseteq \R^2$. Because $\mathcal{B}_{\R^2}$ can be generated by the set of all open rectangles, we can reduce the problem into proving 
\begin{align*}
  \vi{f^{-1}(I_1\times I_2) \in \Sigma_X}
\end{align*}
Because $u,v$ are measurable, we know
\begin{align*}
f^{-1}(I_1\times I_2)=u^{-1}(I_1)\cap v^{-1}(I_2) \in \Sigma_X \vdone
\end{align*}
\end{proof}
\begin{theorem}
\label{Btsa}
\textbf{(Basic tools to show a real/complex-valued function is measurable)} Given a measurable set $X$, and two real-valued function $u,v:X\rightarrow \R$
\begin{enumerate}[label=(\alph*)]
  \item  $u+iv:X\rightarrow \C$ is measurable if and only if $u,v$ are measurable.
  \item If  $f,g:X\rightarrow \R\text{ or }\C$ are measurable, so are $f+g,fg\text{ and }\abso{f}$.
  \item If $f:X\rightarrow \R\text{ or }\C$ is measurable and $g:X\rightarrow \R\text{ or }\C$ isn't, then $f+g$ is not measurable.
  \item If $f:X\rightarrow \C$ is measurable, then there exists $\alpha :X\rightarrow \C$ such that $\abso{\alpha }=1$ and $f=\alpha \abso{f}$
\end{enumerate}
\end{theorem}
\begin{proof}
(a) follows from \myref{Lemma}{CL} and noting $u=\text{Re}(u+iv),v=\text{Im}(u+iv)$, since Re,Im $:\C\rightarrow \R$ are continuous.\\

(b) follows from the fact $\R,\C$ are topological field and \myref{Lemma}{CL}. \\

It is clear that the set of all function from  $X$ to $\R\text{ or }\C$ form a group under addition. (b) shows that the set of measurable functions from a subgroup, thus giving (c).\\

It remains to prove (d). Define 
\begin{align*}
E\triangleq \set{x \in X: f(x)=0}\text{ and }\phi(z)\triangleq  \frac{z}{\abso{z}}
\end{align*}
We claim 
\begin{align*}
  \vi{\alpha \triangleq \phi\circ (f+\textbf{1}_E)\text{ suffices }}
\end{align*}
Because $f$ is measurable, we know $E$ is measurable. This implies that $\textbf{1}_E:X\rightarrow \C$ is measurable. It follows that $f+\textbf{1}_E$ is $(X,\mathcal{B}_\C)$-measurable. Note that $f+\textbf{1}_E$ is never $0$ on $X$.\\


Now by \myref{Theorem}{}, we see $f+\textbf{1}_E$ is $(X,\mathcal{B}_{\C^*})$-measurable. It then follows from the fact  $\phi:\C^*\rightarrow \C$ is continuous that $\alpha $ is $(X,\mathcal{B}_\C)$ measurable.\\

Observe that $\alpha$ maps $E$ into $\set{1}$, and when $x\not\in E$, we have $\alpha (x)= \frac{f(x)}{\abso{f(x)}}$. $\vdone$
\end{proof}
\begin{mdframed}
Note that \myref{Theorem}{Btsa} does not consider function whose range include $\infty$. This will be later addressed using approximation of simple functions.
\end{mdframed}
\begin{theorem}
\label{Slm}
\textbf{(Superior limit of measurable $f_n:X\rightarrow [-\infty,\infty]$ is measurable)} Given a sequence $f_n:X\rightarrow [-\infty, \infty]$ of measurable functions
\begin{align*}
g\triangleq \sup f_n \text{ and }f\triangleq \limsup_{n\to\infty} f_n\text{ are both measurable }
\end{align*}
\end{theorem}
\begin{proof}
It is straightforward to check 
\begin{align*}
  g^{-1}(\alpha ,\infty] = \bigcup_n f_n^{-1} (\alpha ,\infty]
\end{align*}
It is straightforward to check
\begin{align*}
  \mathcal{B}_{[-\infty,\infty]}=\sigma \Big( \set{(\alpha ,\infty] \subseteq [-\infty,\infty]: \alpha \inr} \Big)
\end{align*}
These two facts and \myref{Theorem}{EDm} shows $g$ is measurable. The same arguments shows that $\inf g_k$ is measurable if $g_k$  are all measurable.\\

It is straight forward to check 
\begin{align*}
f= \inf_{n\geq 1} \sup_{k\geq n} f_k
\end{align*}
It then follows $f$ is measurable.
\end{proof}
\begin{corollary}
\label{Plm}
\textbf{(Pointwise limit of measurable $f_n:X\rightarrow [-\infty,\infty]$ is measurable)} If the sequence $f_n:X\rightarrow [-\infty,\infty]$ pointwise converge to $f:X\rightarrow [-\infty,\infty]$,  then $f$ is measurable. 
\end{corollary}
\begin{corollary}
\label{PaN}
\textbf{(Positive and Negative parts of a measurable function are measurable)} If we are given measurable $f:X\rightarrow [-\infty,\infty]$, and we define the \textbf{positive and negative part} $f^+,f^-:X\rightarrow [0,\infty]$ \textbf{of} $f$ by 
\begin{align*}
f^+\triangleq \max \set{f,0}\text{ and }f^- \triangleq -\min \set{f,0}
\end{align*}
Then 
\begin{align*}
f^+\text{ and }f^-\text{ are measurable }
\end{align*}
\end{corollary}
\begin{proof}
Define  
\begin{align*}
h_n\triangleq \begin{cases}
  f& \text{ if $n$ is odd }\\
  0& \text{ if $n$ is even }
\end{cases}
\end{align*}
then we have 
\begin{align*}
\limsup_{n\to\infty} h_n=f^+\text{ and }\liminf_{n\to\infty} h_n=-f^-
\end{align*}
Now, because $0$ is a measurable function, by \myref{Theorem}{Slm}, $f^+$ is measurable. Moreover, because $-1$ is a measurable function, by  \myref{Theorem}{Btsa}, $f^-$ is measurable.
\end{proof}
\section{Egorov's and Lusin's Theorem}
\section{Abstract integration} 
\begin{mdframed}
We say a function is a \textbf{simple function} if its range is of finite cardinality. Simple function is the cornerstone of the development of Lebesgue Integral Theory, as we shall see.\\

Suppose $s$ is a simple function defined on some measurable $E\subseteq (X,\Sigma_X)$ with range $\set{c_1,\cdots ,c_n}\subseteq [0,\infty]$. Define 
\begin{align*}
E_j\triangleq \set{x \in X: s(x)=c_j}
\end{align*}
It is clear that $\set{E_j}$ is a finite disjoint decomposition of $E$, and $s$ is measurable if and only if all $E_j$ are measurable.\\

We can write
\begin{align*}
s= \sum_{j=1}^n c_i \textbf{1}_{E_j}
\end{align*}
Then if $s$ is measurable, we can define
\begin{align*}
\int_E s d\mu \triangleq \sum_{j=1}^n c_j \mu (E_j)
\end{align*}
and if $s$ is defined on some larger domain while $s|_E$ is measurable, we can define 
\begin{align*}
\int_E s d\mu \triangleq \sum_{j=1}^n c_j\mu (E_j\cap E)
\end{align*}
Note that if $s$ is defined on some measurable $F$ containing  $E$ and  $s$ is measurable on $F$, then $s$ is surely measurable on $E$. It is straightforward to verify our definition so far is consistent.
\end{mdframed}


\begin{mdframed}
We now expand our definitions to the class of all measurable functions range in $[0,\infty]$.\\

Given a function $f$ range in $[0,\infty]$ and measurable on $E$, we define 
\begin{align*}
\int_E fd\mu \triangleq \sup_{s\leq f\text{ on }E} \int_E s d\mu
\end{align*}
If $f$ is defined on some large measurable domain $F$, we see that 
 \begin{align*}
\int_Efd\mu= \int_F f\textbf{1}_Ed\mu
\end{align*}
It is at this point one should verify our definition is so far consistent. That is, for each simple $s$, we have 
\begin{align*}
\int_Esd\mu=\sup_{s'\leq s\text{ on }E}\int_Es'd\mu
\end{align*}
The trick is to decompose each $E_j'$ into $\bigcup E_j'\cap E_i$. 
\end{mdframed}

\begin{mdframed}
We now expand our definitions to the class of all measurable functions range in $[-\infty,\infty]$, but before such, we have to first introduce the idea of \textbf{Lebesgue integrable}. For either $s$ or  $f$, range in either $[0,\infty)$ or $[0,\infty]$, it is always possible that $\int_E s\text{ or }fd\mu=\infty$.\\ 

If we have $\int_Efd\mu=\infty$, we say $f$ is \textbf{not Lebesgue integrable}, and if $\int_Efd\mu<\infty$, we say $f$ is \textbf{Lebesgue integrable on }$E$. Because $\R$ is complete in order, we know $f$ is either Lebesgue integrable or not Lebesgue integrable on $E$.\\

Given a function range in $[-\infty,\infty]$ and measurable on $E$, if both $f^+,f^-$ are Lebesgue integrable on $E$, we say  $f$ is Lebesgue integrable on $E$, and write 
\begin{align*}
\int_E fd\mu= \int_E f^+d\mu- \int_E f^-d\mu
\end{align*}
If either  $f^+$ or  $f^-$ are not Lebesgue integrable on $E$, we say $f$ is not Lebesgue integrable on $E$.\\

Notice that 
\begin{align*}
f\text{ is Lebesgue integrable on }E\iff \int_E \abso{f}d\mu<\infty
\end{align*}
It is clear that our definition is again so far consistent.
\end{mdframed}
\begin{mdframed}
We now 
\end{mdframed}

\section{Basic property of abstract integration}
\begin{mdframed}
This section prove some basic properties of Lebesgue integral over general measure space $(X,\Sigma_X,\mu)$. From now when we use the notation $X$, it shall be understood $X$ is equipped with an $\sigma$-algebra $\Sigma_X$ and a measure $\mu$. We will prove 
\begin{enumerate}[label=(\alph*)]
  \item Lebesgue Monotone Convergence Theorem (\myref{Theorem}{LMCT})
  \item Fatou's Lemma (\myref{Theorem}{Fatou})
  \item Reverse Fatou's Lemma (\myref{Theorem}{RFL}) 
  \item Dominated Convergence Theorem (\myref{Theorem}{LDCT})
\end{enumerate}
\end{mdframed}
\begin{theorem}
\label{LMCT}
\textbf{(Lebesgue Monotone Convergence Theorem)} Given a sequence of measurable $f_n:X\rightarrow [0,\infty]$ such that $\set{f_n(x)}_{n\inn}$ is an increasing sequence for each $x\in X$, then 
\begin{align*}
\int_X fd\mu = \lim_{n\to \infty}\int_X f_nd\mu
\end{align*}
\end{theorem}
\begin{proof}
$f$ is measurable by \myref{Corollary}{Plm}. Because  $f_n \nearrow f$ on $X$, we know 
\begin{align*}
 \lim_{n\to \infty}\int_X f_nd\mu = \sup_n \int_X f_nd\mu \leq \int_X fd\mu 
\end{align*}
It remains to prove 
\begin{align*}
\vi{\int_X fd\mu \leq \lim_{n\to \infty}\int_X f_nd\mu}
\end{align*}
Fix simple $0\leq s\leq f$ on $X$. We reduce the problem into proving 
\begin{align*}
  \vi{\int_X sd\mu \leq \lim_{n\to \infty}\int_X f_nd\mu}
\end{align*}
Fix $c \in (0,1)$. We reduce the problem into proving 
\begin{align*}
  \vi{c\int_X sd\mu \leq \lim_{n\to \infty}\int_X f_nd\mu}
\end{align*}
Define 
\begin{align*}
E_n \triangleq \set{x\in X: f_n(x)\geq cs(x)}
\end{align*}
$E_n$ are measurable because $f_n-cs$ are measurable. Now because $f_n$ are non-negative on  $X$, we have 
\begin{align*}
\int_X f_n d\mu \geq \int_{E_n}f_n d\mu \geq c \int_{E_n} sd\mu
\end{align*}
Taking limit, we see
\begin{align*}
\lim_{n\to \infty} \int_X f_n d\mu \geq \lim_{n\to \infty} c \int_{E_n}sd\mu 
\end{align*}
It is straightforward to check $E_n$ is increasing and $\bigcup E_n =X$. Then if we decompose $s=\sum_{j} c_j \textbf{1}_{F_j}$, by \myref{Theorem}{Bppm}, we can take limit 
\begin{align*}
\lim_{n\to \infty}\mu (F_j\cap E_n)=\mu (F_j)
\end{align*}
It then follows that 
\begin{align*}
\lim_{n\to \infty}\int_X f_nd\mu \geq \lim_{n\to \infty}c \int_{E_n}sd\mu= c\int_X sd\mu \vdone
\end{align*}
\end{proof}
\begin{mdframed}
It is worth pointing out in our proof for Lebesgue Monotone Convergence Theorem, instead of proving $\int_X sd\mu \leq \lim_{n\to \infty}\int_X f_nd\mu$, we proved $c\int_X sd\mu \leq \lim_{n\to \infty}\int_X f_nd\mu$. Multiplying $\int_X sd\mu$ with $c\in (0,1)$ is not just a random limit technique. Our action play a much more profound role. Consider the Example.
\end{mdframed}
\begin{Example}{\textbf{(Why we take $c\int_X sd\mu$ ?)}}{}
\begin{align*}
X=[0,1]\text{ and }f_n=1-\frac{1}{n}
\end{align*}
We can take $s=f$, and see $E_n=\varnothing$ for all $n$, which renders our proceeding proof invalid. 
\end{Example}
\begin{corollary}
\textbf{(Monotone Convergence Theorem for general functions)} Given a sequence of measurable $f_n:X\rightarrow [0,\infty]$ such that
\begin{enumerate}[label=(\alph*)]
  \item $\set{f_n(x)}_{n\inn}$ is an increasing sequence on $N^c$
  \item $f:X\rightarrow [0,\infty]$ is the limit of $f_n$ on $N^c$ 
  \item $\mu (N)=0$
  \item $\mu$ is complete
\end{enumerate}
We have 
\begin{align*}
\int_X fd\mu = \lim_{n\to \infty}\int_X f_nd\mu
\end{align*}
\end{corollary}
\begin{proof}
Let
\begin{align*}
g(x)\triangleq \begin{cases}
  f(x)& \text{ if $x \in N^c$ }\\
  0& \text{ if $x\in N$ }
\end{cases}
\end{align*}
Note that 



\begin{align*}
\int_X fd\mu = \int_{N^c}fd\mu = \lim_{n\to \infty}\int_{N^c} fd\mu = \lim_{n\to \infty} \int_X fd\mu
\end{align*}
\end{proof}
\begin{theorem}
\label{Fatou}
\textbf{(Fatou's Lemma)} Given measurable $f_n:X\rightarrow [0,\infty]$ 
\begin{align*}
\int_X \liminf_{n\to\infty} f_nd\mu \leq \liminf_{n\to\infty} \int_X f_n d\mu
\end{align*}
\end{theorem}
\begin{proof}
Since $\inf_{k\geq n}f_k\leq f_n$ for each $n,x$, we see 
 \begin{align*}
\int_X \inf_{k\geq n}f_k d\mu \leq \int_X f_nd\mu\text{ for all $n$ }
\end{align*}
Because $\inf_{k\geq n}f_k\nearrow  \liminf_{m\to\infty} f_m$ as $n\to \infty$, by (\myref{Theorem}{LMCT}: Lebesgue Monotone Convergence Theorem), we can take limit 
\begin{align*}
\int_X \liminf_{n\to\infty} f_n d\mu = \lim_{n\to \infty}\int_X \inf_{k\geq n}f_k d\mu\leq \liminf_{n\to\infty} \int_X f_nd\mu
\end{align*}
\end{proof}
\begin{Example}{\textbf{(Fatou's Lemma strict inequality)}}{}
\begin{align*}
f_{2k}(x)=\begin{cases}
 0& \text{ if $x\in [0,\frac{1}{2}]$ } \\
 1& \text{ if $x\in (\frac{1}{2},1]$ }
\end{cases}\text{ and }f_{2k+1}(x)=\begin{cases}
  1& \text{ if $x\in [0,\frac{1}{2})$ }\\
  0& \text{ if $x\in [\frac{1}{2},1]$ }
\end{cases}
\end{align*}
\end{Example}
\begin{mdframed}
  From now, we use $L^1(\mu)$ to denote the set of all function defined and $\mu$-Lebesgue-integrable on $X$, and we say a sequence of function  $f_n:X\rightarrow [0,\infty]$ is  \textbf{dominated} by $g$, if $g$ is a $[0,\infty]$-valued function defined on $X$ such that 
\begin{align*}
\sup_n \abso{f_n(x)} \leq g(x)\text{ for all $x\in X$ }
\end{align*}
\end{mdframed}
\begin{theorem}
\label{RFL}
\textbf{(Reverse Fatou's Lemma)} Given measurable $f_n:X\rightarrow [0,\infty]$ dominated by some $g \in L^1(\mu)$ 
\begin{align*}
\limsup_{n\to\infty} \int_X f_nd\mu \leq \int_X \limsup_{n\to\infty} f_nd\mu
\end{align*}
\end{theorem}
\begin{proof}
By (\myref{Theorem}{Fatou}: Fatou Lemma)
\begin{align*}
\int_X \liminf_{n\to\infty} (g-f_n) d\mu \leq \liminf_{n\to\infty} \int_X (g-f_n)d\mu 
\end{align*}
Multiplying both side with $-1$, we have 
\begin{align*}
\limsup_{n\to\infty} \int_X (f_n-g)d\mu \leq \int_X \limsup_{n\to\infty} (f_n-g)d\mu
\end{align*}
Then adding both side the constant $\int_X gd\mu$, we reach to the conclusion.
\end{proof}
\begin{mdframed}
Note that in our proof above, when we "pull" the negative sign out from $\liminf_{n\to\infty} $, it changed to $\limsup_{n\to\infty} $. This is a standard technique, which can be justified using the sub-sequence definition of limit superior.
\end{mdframed}
\begin{theorem}
\label{LDCT}
\textbf{(Dominate Convergence Theorem)} Given a sequence $f_n:X\rightarrow \C\cup \set{\infty}$ of measurable function such that 
\begin{align*}
f\triangleq \lim_{n\to \infty}f_n\text{ exists on $X$ }
\end{align*}
If there exists $g \in L^1(\mu)$ dominating $f_n$, then 
\begin{align*}
\lim_{n \to \infty} \int_X \abso{f_n-f}d\mu=0\text{ and }\int_X fd\mu = \lim_{n\to \infty} \int_X f_n d\mu\text{ exists in $\C\cup  \set{\infty}$ }
\end{align*}
\end{theorem}
\begin{proof}
Because $f$ is measurable by \myref{Corollary}{Plm} and  $\abso{f}\leq g$ on $X$, $f \in L^1(\mu)$.\\

We first prove 
\begin{align*}
  \vi{\lim_{n\to \infty}\int_X \abso{f_n-f}d\mu=0}
\end{align*}
We relax the problem into proving 
\begin{align*}
  \vi{\limsup_{n\to\infty} \int_X \abso{f_n-f}d\mu=0}
\end{align*}
Note that $\abso{f_n-f}\leq 2g$. We can now apply (\myref{Theorem}{Fatou}: Fatou lemma) to $2g- \abso{f_n-f}$ and see 
\begin{align*}
\int_X 2gd\mu&=   \int_X \lim_{n\to \infty}(2g- \abso{f_n-f})d\mu \\
&\leq \liminf_{n\to\infty} \int_X (2g-\abso{f_n-f})d\mu\\
&= \int_X 2gd\mu + \liminf_{n\to\infty} \Big(- \int_X \abso{f_n-f}d\mu \Big)\\
&=\int_X 2gd\mu - \limsup_{n\to\infty} \int_X \abso{f_n-f}d\mu
\end{align*}
Then because $g \in L^1(\mu)$, we can subtract it and obtain 
\begin{align*}
\limsup_{n\to\infty}\int_X  \abso{f_n-f}d\mu=0 \vdone
\end{align*}
It then follows that 
\begin{align*}
  \limsup_{n\to\infty} \abso{\int_X (f_n-f)d\mu} \leq \limsup_{n\to\infty} \int_X \abso{f_n-f}d\mu =0
\end{align*}
which implies 
\begin{align*}
\lim_{n\to \infty} \int_X (f_n-f)d\mu =0
\end{align*}
and because $f\in L^1(\mu)$, we have
\begin{align*}
\lim_{n\to \infty}\int_X f_nd\mu = \int_X fd\mu
\end{align*}
\end{proof}
\begin{Example}{\textbf{(Counterexample for Dominate Convergence Theorem)}}{}
\begin{align*}
f_n(x)=\begin{cases}
  \frac{1}{n}& \text{ if $\abso{x}<n$ }\\
  0& \text{ if $\abso{x}\geq n$ }
\end{cases}
\end{align*}
\end{Example}
\section{Equivalent Definitions of Lebesgue Measurable Functions and Integral}
\chapter{Harmonic Analysis}
\section{Weierstrass approximation Theorem: $[a,b]\rightarrow \R$}
\begin{theorem}
\label{Bernoulli's Inequality}
\textbf{(Bernoulli's Inequality)} Given $r,x \in\R$, suppose 
\begin{enumerate}[label=(\alph*)]
  \item $r\geq 1$ 
  \item $x\geq -1$
\end{enumerate}
Then
\begin{align*}
  (1+x)^r\geq 1+rx
\end{align*}
\end{theorem}
\begin{proof}
Fix $r\geq 1$. We wish 
\begin{align*}
\vi{\text{ to prove }(1+x)^r\geq 1+rx\text{ for all  }x \geq -1}
\end{align*}
Define $f:[-1,\infty)\rightarrow \R$ by 
 \begin{align}
\label{Bere1}
f(x)=(1+x)^r-(1+rx)
\end{align}
We reduced the problem into  
\begin{align*}
\vi{\text{ proving }f(x)\geq 0\text{ for all } x\geq -1}
\end{align*}
Because $r\geq 1$ by premise, by definition of $f(x)$  (\myref{Equation}{Bere1}), we see that 
\begin{align*}
f(0)=0\text{, and }f(-1)=r-1\geq 0
\end{align*}
Notice that by definition of $f$  (\myref{Equation}{Bere1}),  $f(x)$ is clearly differentiable on $(-1,\infty)$.\\

Then, by MVT (\myref{Theorem}{MVT}), to prove $f(x)\geq 0$ on $(-1,\infty)$, we only wish 
\begin{align*}
\vi{\text{ to prove }f'(x)\geq 0\text{ for all }x> 0\text{ and }f'(x)\leq 0\text{ for all $x \in (-1,0)$ }}
\end{align*}
Compute $f'$
 \begin{align*}
f'(x)&=r(1+x)^{r-1}-r\\
&=r\Big((1+x)^{r-1}-1 \Big)
\end{align*}
Because $r\geq 1$, we can deduce 
\begin{align*}
x>0 \implies (1+x)^{r-1}\geq 1 \implies f'(x)=r\Big((1+x)^{r-1}-1 \Big)\geq 0
\end{align*}
and deduce 
\begin{align*}
x \in (-1,0) \implies 1+x \in (0,1) \implies (1+x)^{r-1}\leq 1 \implies f'(x)=r\Big((1+x)^{r-1}-1 \Big)\leq  0
\end{align*}
$\vdone$
\end{proof}
\begin{mdframed}
In this section, notation  $\mathcal{C}\big([a,b] \big)$ means the set of \textbf{real-valued continuous function on $[a,b]$}.
\end{mdframed}
\begin{theorem}
\label{WaT}
\textbf{(Weierstrass approximation Theorem: $[a,b]\rightarrow \R$)} Let $\R[x]\big|_{[a,b]}$ be the space of polynomials on $[a,b]$ with real coefficient. We have 
\begin{align*}
\text{ $\R[x]\big|_{[a,b]}$ is dense in $\Big(\mathcal{C}\big([a,b] \big),\norm{\cdot}_{\infty} \Big)$ }
\end{align*}
\end{theorem}
\begin{proof}
WOLG, we can let $[a,b]=[0,1]$. The reason we can assume such is explained at last. Now, let $f:[0,1]\rightarrow \R$ be a continuous function. Fix $\epsilon $. We only wish 
\begin{align*}
\vi{\text{ to find $P \in \R[x]\big|_{[0,1]}$ such that $\norm{f-P}_{\infty}<\epsilon $}}
\end{align*}
Define $\tilde{f} \in \mathcal{C}\big([0,1] \big)$ by 
\begin{align}
  \label{tse1}
\tilde{f}(x)= f(x)-f(0)-x \big[f(1)-f(0) \big] 
\end{align}
It is easy to check $\tilde{f}$ is continuous. We first prove that 
\begin{align*}
\blue{\big( \tilde{f}(x)-f(x)\big)\in \R[x]\big|_{[0,1]}  }
\end{align*}
By definition of $\tilde{f} $ (\myref{Equation}{tse1}), we see 
\begin{align*}
\tilde{f}(x)-f(x)=(f(0)-f(1))x- f(0)\in \R[x]\big|_{[0,1]}\bdone
\end{align*}
This reduce our problem into 
\begin{align*}
\vi{\text{ finding }P\in \R[x]\big|_{[0,1]}\text{ such that $\norm{\tilde{f}-P}_{\infty}<\epsilon $ }}
\end{align*}
Notice that by definition of $\tilde{f}$ (\myref{Equation}{tse1}), we have 
\begin{align*}
\tilde{f}(0)=0=\tilde{f}(1) 
\end{align*}
Then, we can expand the definition of $\tilde{f} $  by
\begin{align}
\label{tse2}  
\tilde{f}(x)=\begin{cases}
  \tilde{f} (x)& \text{ if $x\in [0,1]$ }\\
  0& \text{ if $x \not \in [0,1]$ }
\end{cases}
\end{align}
This makes $\tilde{f}$ uniformly continuous on $\R$, since  $\tilde{f}$ is uniformly continuous on $[0,1]$ and $[0,1]^c$. Now, for each $n\inn$, define $Q_n \in \R[x]$ by 
\begin{align}
\label{tse3}
Q_n=c_n(1-x^2)^n\text{ where $c_n$ is chosen to satisfy }\int_{-1}^1 Q_n(x)dx=1
\end{align}
Define $P_n:[0,1]\rightarrow \R$ by 
\begin{align*}
P_n(x)=\int_{-1}^1 \tilde{f} (x+t)Q_n(t)dt
\end{align*}
We now prove 
\begin{align*}
\olive{P_n \in \R[x]\big|_{[0,1]}}
\end{align*}
Because $\tilde{f}(x)=0$ for all $x \not \in (0,1)$ by definition of $\tilde{f} $ (\myref{Equation}{tse2}), we see that 
\begin{align}
\label{tse4}
P_n(x)=\int_{-x}^{1-x}\tilde{f}(x+t)Q_n(t)dt \text{ for all }x \in [0,1]
\end{align}
Fix $x \in [0,1]$. Now, by change of variable, we see 
\begin{align*}
P_n(x)=\int_{-x}^{1-x} \tilde{f}(x+t)Q_n(t)dt=\int_{0}^1 \tilde{f}(u)Q_n(u-x)du  
\end{align*}
Because $Q_n$ is a polynomial by definition (\myref{Equation}{tse3}), we can express $Q_n(u-x)$ by 
\begin{align*}
Q_n(u-x)=\sum_{k=0}^{m} a_k x^k\text{ for some $\set{a_0,\dots ,a_m}$ depending on $u$}
\end{align*}
Then we see 
\begin{align*}
  P_n(x)=\int_0^1 \tilde{f}(u)Q_n(u-x)du= \sum_{k=0}^m x^k \Big( 
\int_0^1 \tilde{f}(u) a_kdu
  \Big)  
\end{align*}
This shows that $P_n \inr[x]\big|_{[0,1]}$

$\odone$\\


Now, because $\tilde{f}$ is uniformly continuous on $\R$, we can fix $\delta<1$ such that 
\begin{align}
\label{tse5}
\forall x,y \inr, \abso{x-y}<\delta \implies \abso{\tilde{f}(x)-\tilde{f}(y)  }<\frac{\epsilon}{2}
\end{align}
By definition of $\tilde{f}$ (\myref{Equation}{tse2}), we know $\tilde{f} $ is a bounded function. Then we can set $M$ by 
\begin{align*}
M=\sup_{x \inr} \abso{f(x)}
\end{align*}
Let $n$ satisfy 
 \begin{align}
  \label{tse5}
4M \sqrt{n} (1-\delta^2)^n < \frac{\epsilon}{2} 
\end{align}
Such $n$ exists, because  $\delta<1 \implies  \sqrt{n}(1-\delta^2)^n \to 0 $. We claim 
\begin{align*}
\vi{\text{ $P_n$ satisfy $\norm{\tilde{f}-P_n}_{\infty}<\epsilon $}}
\end{align*}
We first prove 
\begin{align*}
\blue{c_n< \sqrt{n} }
\end{align*}
By Bernoulli's Inequality (\myref{Theorem}{Bernoulli's Inequality}). Compute 
\begin{align*}
1=\int_{-1}^1 Q_n(x)dx&=  c_n\int_{-1}^1 (1-x^2)^n dx \\
&=2c_n\int_0^1 (1-x^2)^n dx\\
&\geq 2c_n\int_0^{\frac{1}{\sqrt{n} }}(1-x^2)^n dx\\
&\geq 2c_n \int_0^{\frac{1}{\sqrt{n} }} 1-nx^2dx=c_n\big(\frac{4}{3\sqrt{n} } \big)> c_n (\frac{1}{\sqrt{n} })
\end{align*}


This implies 
\begin{align*}
\sqrt{n}>c_n \bdone
\end{align*}
Because $\sqrt{n}>c_n $, by definition of $Q_n$  (\myref{Equation}{tse3}), we have 
\begin{align*}
Q_n(x)<\sqrt{n}(1-x^2)^n \leq \sqrt{n}(1-\delta^2)^n \text{ for all $x$ such that  $\delta \leq \abso{x}\leq 1$ }
\end{align*}
Fix $x \in [0,1]$. Finally, because 
\begin{enumerate}[label=(\alph*)]
  \item $\int_{-1}^1 Q_n(x)dx=1$ by definition of  $Q_n$  (\myref{Equation}{tse3})
  \item $Q_n(x)=c_n(1-x^2)^n\geq 0$ for all $x \in [-1,1]$ 
  \item $\abso{\tilde{f}(x+t)-\tilde{f}(x)}<\frac{\epsilon}{2} $ for all $t$ such that $\abso{t}<\delta $, by definition of $\delta $ (\myref{Equation}{tse5})
  \item $Q_n(x)\leq \sqrt{n}(1-\delta^2)^n $ for all $x$ such that  $\delta\leq \abso{x}\leq 1$
  \item $4M\sqrt{n}(1-\delta^2)^n<\frac{\epsilon}{2} $ by definition of $n$  (\myref{Equation}{tse5})
\end{enumerate}
we have
\begin{align*}
\abso{P_n(x)-\tilde{f}(x)}&=\abso{\int_{-1}^1 \tilde{f}(x+t)Q_n(t)dt- \tilde{f}(x)}\\
&=\abso{\int_{-1}^1 \tilde{f}(x+t)Q_n(t)dt- \tilde{f}(x)\int_{-1}^1 Q_n(t)dt }\\
&=\abso{\int_{-1}^1 \tilde{f}(x+t)Q_n(t)dt -\int_{-1}^1 \tilde{f}(x)Q_n(t)dt}\\
&=\abso{\int_{-1}^1 \big[\tilde{f}(x+t)-\tilde{f}(x)   \big]Q_n(t)dt}\\
&\leq \int_{-1}^1 \abso{\big[ \tilde{f}(x+t)-\tilde{f}(x)\big]Q_n(t)}dt\\
&=\int_{-1}^1 \abso{\tilde{f}(x+t)-\tilde{f}(x)  }Q_n(t)dt\\
&\leq \int_{-1}^{-\delta} 2MQ_n(t)dt +\int_{-\delta}^{\delta} \abso{\tilde{f}(x+t)-\tilde{f}(x)  }Q_n(t)dt+\int_{\delta}^1 2M Q_n(t)dt\\
&\leq 2M\Big(\int_{-1}^{-\delta}Q_n(t)dt+\int_{\delta}^1 Q_n(t)dt  \Big)+ \int_{-\delta}^\delta \big(\frac{\epsilon}{2} \big)Q_n(t)dt\\
&\leq 4M(1-\delta) \sqrt{n}(1-\delta^2)^n+ \frac{\epsilon}{2} \\
&\leq 4M\sqrt{n}(1-\delta^2)^n+\frac{\epsilon}{2} <\epsilon  
\end{align*}
Because $x$ is arbitrarily picked from  $[0,1]$, we now have $\norm{P_n-\tilde{f} }_{\infty}<\epsilon \vdone$\\

Lastly, we show 
\begin{align*}
\olive{\text{ our result can be transplanted to arbitrary $\mathcal{C}\big([a,b] \big)$ }}
\end{align*}
Let $[a,b]$ be arbitrary. Fix $\epsilon $ and $f \in \mathcal{C}\big([a,b] \big)$. We wish 
\begin{align*}
\olive{\text{ to find $P \in \R[x]\big|_{[a,b]}$ such that $\norm{f-P}_\infty\leq \epsilon $ }}
\end{align*}
Define $g:[0,1]\rightarrow \R$ by 
\begin{align}
g(x)\triangleq f(a+(b-a)x)
\end{align}
We know there exists $P_n:[0,1]\rightarrow \R$ such that 
\begin{align*}
\norm{P_n-g}_{\infty}<\epsilon 
\end{align*}
Define $H_n:[a,b]\rightarrow \R$ by 
\begin{align*}
H_n(x)=P_n\big(\frac{x-a}{b-a} \big)
\end{align*}
Because $P_n$ is a real polynomial on  $[0,1]$, we know $H_n$ is a real polynomial on $[a,b]$. We now claim 
\begin{align*}
  \olive{\text{ such $H_n\text{ works }$ }}
\end{align*}
Fix $x \in [a,b]$. Observe 
\begin{align*}
\big|f(x)-H_n(x)\big|&=\abso{f(x)-P_n\big(\frac{x-a}{b-a} \big)}\\
&=\abso{g\big(\frac{x-a}{b-a} \big)-P_n\big(\frac{x-a}{b-a} \big)}< \epsilon \odone
\end{align*}
\end{proof}
\begin{mdframed}
It is at now, we will show that every real-valued continuous functions on $[a,b]$ can be approximated by polynomials with rational coefficient. This fact enable our computer to more easily approximate real-valued continuous function on $[a,b]$.\\

Note that since $\mathcal{C}\big([a,b] \big)$ is a separable metric space, we can show that $\mathcal{C}\big([a,b] \big)$ has cardinality of at most continuum $\mathfrak{c}$ . 
\end{mdframed}
\begin{theorem}
\textbf{(The space $\Q[x]|_{[a,b]}$ is dense in $\Big( \mathcal{C}\big([a,b] \big),\norm{\cdot}_{\infty}\Big)$, thus $\mathcal{C}\big([a,b] \big)$ is separable)} 
\begin{align*}
\Big( \mathcal{C}\big([a,b] \big),\norm{\cdot}_{\infty}\Big)\text{ is separable }
\end{align*}
\end{theorem}
\begin{proof}
Because $\Q[x]\big|_{[a,b]}$ is countable, to show $\mathcal{C}\big([a,b] \big)$ is separable, we only wish to show 
\begin{align*}
\vi{\Q [x]\big|_{[a,b]}\text{ is dense in }\mathcal{C}\big([a,b] \big)}
\end{align*}
Because $\R[x]\big|_{[a,b]}$ is dense in $\mathcal{C}\big([a,b] \big)$, we reduce our problem into proving 
\begin{align*}
\vi{\Q[x]\big|_{[a,b]}\text{ is dense in }\R[x]\big|_{[a,b]}}
\end{align*}
Fix $\epsilon \text{ and } P \in \R[x]\big|_{[a,b]}$. We must
\begin{align*}
\vi{\text{ find $Q \in \Q[x]\big|_{[a,b]}$ such that $\norm{Q-P}_\infty \leq \epsilon $}}
\end{align*}
Express $P(x)=\sum_{k=0}^n r_kx^k$. Let $M> \max \set{\abso{a},\abso{b}}$. Because $\Q$ is dense in $\R$, we know there exists $c_k \inq$ such that  $\abso{c_k-r_k}< \frac{\epsilon }{(n+1)M^n}$. We claim 
\begin{align*}
\vi{Q(x)=\sum_{k=0}^n c_kx^k\text{ works }}
\end{align*}
Fix $x \in [a,b]$. See
\begin{align*}
\abso{P(x)-Q(x)}&=\abso{\sum_{k=0}^n (c_k-r_k)x^k}\\
&\leq \sum_{k=0}^n \abso{c_k-r_k}\cdot \abso{x}^k\\
&\leq \sum_{k=0}^n \abso{c_k-r_k}\cdot M^k\\
&\leq (M^n)\sum_{k=0}^n \abso{c_k-r_k}\\
&< M^n (n+1)\big( \frac{\epsilon }{(n+1)M^n}\big)=\epsilon \vdone
\end{align*}
\end{proof}
\section{The Stone-Weierstrass Theorem}
\begin{mdframed}
Recall that a \textbf{vector space over a field $\F$} is a set $V$ equipped with \textbf{vector addition} $+:V\times V\to V$  and \textbf{scalar multiplication}  such that 
\begin{enumerate}[label=(\alph*)]
  \item $(V,+)$ is an abelian group. 
  \item Scalar multiplication is compatible with field multiplication:   $\Big((ab)v=a(bv) \Big)$
  \item Scalar multiplication is distributive: $\Big((a+b)v=av+bv\text{ and }a(v+w)=av+aw \Big)$
\end{enumerate}

There are many ways to define the term \textbf{algebra over a field $\F$}. One can exhaust all the laws an algebra should obey. In short, an \textbf{algebra over a field $\F$} (or \textbf{$\F$-algebra}) is a vector space $V$ equipped with a vector multiplication such that  
\begin{enumerate}[label=(\roman*)]
  \item Multiplication is $\underline{\text{distributive}}$ to addition. 
   \item Scalar and vector multiplications are compatible: $(av)\cdot (bw)=ab(v\cdot w)$
\end{enumerate}
Given an arbitrary set $E$ and a field $\F$, let $A$ be the set of all functions from  $E$ to $\F$. The following is a list of some algebra 
\begin{enumerate}[label=(\alph*)]
  \item $(\R^3,\text{cross product})$  over $\R$ 
  \item $(\C,\text{complex multiplication})$ over $\C$ 
  \item $(\Q[x],\text{function multiplication})$ over $\Q$
  \item $(\text{Functions from }E\text{ to }\F,\text{function multiplication})$ over $\F$
  \item $(\text{Continuous functions from $(E,\tau)$ to $\C$ },\text{function multiplication})$ over $\C$  
  \item $(\text{Linear transformation from $V$ to $V$, composition})$ over $\F$ where  $V$ is over $\F$
  \item $\big(M_n(\F),\text{matrix multiplication}\big)$ over $\F$
\end{enumerate}
Note that $B=(\text{continuous functions from }\C\text{ to }\C,\text{composition})$ over $\C$ is not an algebra, even though $B$ is both a vector space and a ring. ($\because$ scalar multiplication and multiplication are not compatible).\\

It is at here we shall introduce some general terminologies. Given an arbitrary set $E$, a field  $\F$ and a point $x \in E$, we say a family $\mathcal{F}$ of functions  from $E$ to  $\F$  \textbf{vanish at $x$} if for all $f \in \mathcal{F}$, we have $f(x)=0$. We say $\mathcal{F}$ \textbf{separate points} in $E$ if for all  $x_2\neq x_1 \in E$, there exists $f\in \mathcal{F}$ such that $f(x_2)\neq f(x_1)$. 
\end{mdframed}

\chapter{Calculus in Euclidean Space}
\section{Inverse Function Theorem}
\begin{mdframed}
Interestingly, if $f:\Big(\R,\norm{\cdot}_2 \Big)\rightarrow \Big(\R^n , \norm{\cdot}_2\Big)$ is a curve in $\R^n$ 
\begin{align*}
f(t)=(f_1(t),\cdots , f_n(t))
\end{align*}
and we define 
\begin{align*}
f'(t)\triangleq (f_1'(t),\cdots ,f_n'(t))
\end{align*}
We have 
\begin{align*}
\abso{f'(t)}= \norm{df_t}_{\text{op}}
\end{align*}
This give us the following expected result (\myref{Corollary}{BPoD}). 
\end{mdframed}
\begin{theorem}
\label{BPoDB}
\textbf{(Basic Property of Derivative)} Suppose $f$ maps a convex open set $E\subseteq \Big(\R^n,\norm{\cdot}_2 \Big)$ into $\Big(\R^m,\norm{\cdot}_2 \Big)$,  $f$ is differentiable on $E$, and there exists $M\inr$ such that 
\begin{align*}
\hspace{2.5cm}\norm{df_x}_\text{op}\leq M\hspace{1.5cm}(x \in E)
\end{align*}
Then for all $a,b \in E$, we have
\begin{align*}
\abso{f(b)-f(a)}\leq M \abso{b-a}
\end{align*}
\end{theorem}
\begin{proof}
Define $\gamma:[0,1]\rightarrow E$ by 
\begin{align*}
\gamma (t)\triangleq a+(b-a)t
\end{align*}
Now, note that 
\begin{align*}
\abso{f(b)-f(a)}&= \abso{(f\circ \gamma )(1)-(f\circ \gamma )(0)}\\
&= \abso{\int_0^ 1(f\circ \gamma )'(t)dt}\\
&\leq \int_0^1 \abso{(f\circ \gamma )'(t)}dt\\
&=\int_0^1 \norm{d(f\circ \gamma )_t}_\text{op}dt\\
&\leq \int_0^1 \norm{df_{\gamma (t)}}_\text{op}\cdot \norm{d\gamma_t}_\text{op}dt\\
&\leq \int_0^1 M\cdot \abso{b-a}dt=M\abso{b-a}
\end{align*}
\end{proof}
\begin{corollary}
\label{BPoD}
\textbf{(Basic Property of Derivative)} Suppose $f$ maps a convex open set $E\subseteq \Big(\R^n,\norm{\cdot}_2 \Big)$ into $\Big(\R^m,\norm{\cdot}_2 \Big)$, $f$ is differentiable on $E$, and $df_x=0$ for all $x\in E$, then
\begin{align*}
f\text{ stay constant on $E$ }
\end{align*}
\end{corollary}



\begin{mdframed}
In this section, we will give a local statement and the proof for Inverse Function Theorem in  $\R^n$ (\myref{Theorem}{IFT}).  Let $L(\R^n)$ be the set of linear transformation that maps $\R^n$ into itself, and let $\Omega$ be the set of all invertibles in $L(\R^n)$. We will first prove that  $\Omega$ is open (\myref{Theorem}{SCftI}). 
\end{mdframed}
\begin{theorem}
\label{SCftI}
\textbf{($\Omega$ is Open)} Suppose $A\in \Omega$. If we define $\epsilon \triangleq \frac{1}{\norm{A^{-1}}_\text{op}}$, then  
\begin{align*}
B_\epsilon (A) \overset{\text{def}}{=}\set{T \in L(\R^n): \norm{T-A}_\text{op}< \epsilon }\subseteq \Omega
\end{align*}
\end{theorem}
\begin{proof}
Fix $T \in B_\epsilon (A)$ and $x\neq 0\inr^n$. We are required to show 
\begin{align*}
\vi{\abso{Tx}>0}
\end{align*}
If $T=A$, then the proof is trivial. We therefore suppose $T\neq A$. Define 
\begin{align*}
\beta \triangleq \norm{T-A}_\text{op}
\end{align*}
Note that $T\neq A\in B_\epsilon (A)$ implies $0<\beta < \epsilon  $. We claim 
\begin{align*}
  \vi{(\epsilon  -\beta )\abso{x}\leq \abso{Tx}}
\end{align*}
Observe 
\begin{align}
\label{Obe1}
\epsilon  \abso{x}= \epsilon  \abso{A^{-1}Ax}\leq \abso{Ax}
\end{align}
Observe 
\begin{align}
\label{Obe2}
\abso{Ax}\leq \abso{(A-T)x}+\abso{Tx}\leq \beta \abso{x}+ \abso{Tx}
\end{align}
\myref{Equation}{Obe1} and \myref{Equation}{Obe2} implies 
\begin{align*}
\epsilon  \abso{x}\leq \beta \abso{x}+ \abso{Tx}
\end{align*}
which implies 
\begin{align*}
  (\epsilon  -\beta  )\abso{x}\leq \abso{Tx} \vdone
\end{align*}
\end{proof}
\begin{mdframed}
\myref{Theorem}{SCftI} is essential for proving Inverse Function Theorem (\myref{Theorem}{IFT}). Think about what happen if $f$ is linear. If $f$ is linear, then $f^{-1}$ is linear, and we will have 
\begin{align*}
df^{-1}=f^{-1}=(f)^{-1}=(df)^{-1}
\end{align*}
Because derivative is unique, it is reasonable to guess that if $f$ is not linear and $f^{-1}$ is differentiable, we would have 
\begin{align*}
df^{-1}=(df)^{-1}
\end{align*}
Now, if we wish $df^{-1}$ to exists everywhere on $f(U)$, we must guarantee that $df$ is invertible on  $U$, and this is when \myref{Theorem}{SCftI} kick in. Note that in our proof of Inverse Function Theorem (\myref{Theorem}{IFT}), our selection of $U$ guarantee  $df_x$ is invertible for all $x \in U$, by \myref{Theorem}{SCftI}.\\

The rest of the proof boiled down to a fixed point argument  (\myref{Theorem}{BFPT}) to show $f$ is one-to-one in $U$ and $f(U)$ is open. 
\end{mdframed}
\begin{theorem}
\label{IFT}
\textbf{(Inverse Function Theorem)} Given a function $f$ that maps an open neighborhood $E\subseteq \R^n$ around $a$  into $\R^n$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable on $E$  
  \item $df_a$ is invertible 
   \item $f$ is continuously differentiable at  $a$
\end{enumerate}
Then there exists open $U \subseteq E$ containing $a$ such that
\begin{enumerate}[label=(\alph*)]
  \item $f$ is one-to-one in $U$
   \item $f(U)$ is open 
    \item The inverse of $f|_U$ is differentaible at $f(a)$. 
\end{enumerate}
\end{theorem}
\begin{proof}
Fix 
 \begin{align}
\label{ldteq}
\ld   \triangleq  \frac{1}{2\norm{(df_a)^{-1}}_\text{op}}
\end{align}
Because $f$ is continuously differentaible at  $a$, we know there exists  $\delta$ such that
\begin{align}
\label{fcond}
\hspace{3cm}\norm{df_x-df_a}_\text{op}< \ld  \hspace{1.5cm}(x\in B_\delta (a))
\end{align}
We claim 
\begin{align*}
  \vi{U\triangleq B_\delta(a)\text{ suffices }}
\end{align*}
For each $y\inr^n$, define $\phi_y:U\rightarrow \R^n$ by 
\begin{align*}
\phi_y(x)\triangleq x+ (df_a)^{-1}(y-f(x))
\end{align*}
Before anything, we first prove
\begin{align*}
  \olive{\text{ for all }y\inr^n, \phi_y:U\rightarrow \R^n\text{ is a contraction of $U$}}
\end{align*}
Fix $y \inr^n$, and $x_1,x_2 \in U$. We claim 
\begin{align}
\label{phiy12}
\olive{\abso{\phi_y(x_1)-\phi_y (x_2)}\leq \frac{1}{2}\abso{x_1-x_2}}
\end{align}
Because $U$ is convex, \myref{Theorem}{BPoDB} allow us to reduce the problem into proving 
\begin{align*}
\olive{\norm{d(\phi_y)_x}_\text{op}\leq \frac{1}{2}\text{ for all $x \in U$ }}
\end{align*}
Fix $x \in U$. Using Chain Rule (\myref{Theorem}{CR}) and the fact that the derivative of a bounded linear transformation is itself, we can compute $d(\phi_y)_x$ 
\begin{align*}
d(\phi_y)_x&=I+(df_a)^{-1}(-df_x)\\
&=(df_a)^{-1}(df_a-df_x)
\end{align*}

This together with \myref{Equation}{ldteq} and \myref{Equation}{fcond} give us 
\begin{align*}
\norm{d(\phi_y)_x}_\text{op}\leq \norm{(df_a)^{-1}}_\text{op}\norm{df_a-df_x}_\text{op}<\frac{1}{2}\odone
\end{align*}

We now prove 
\begin{align*}
\blue{f\text{ is one-to-one in $U$ }}
\end{align*}
Fix $y$ in $f(U)$. We wish to show 
\begin{align*}
\blue{\text{ there exists at most one $x\in U$ such that }f(x)=y}
\end{align*}
Because $f(x)=y \iff x\text{ is a fixed point of }\phi_y$, we can reduce the problem into
\begin{align*}
  \blue{\phi_y\text{ has at most one fixed point }}
\end{align*}
Because $\phi_y$ \olive{is a contraction of $U$}, Banach Fixed Point Theorem  (\myref{Theorem}{BFPT}) tell us $\phi_y$ has at most one fixed point. $\bdone$\\  

We now prove 
\begin{align*}
\brown{f(U)\text{ is open in $\R^n$ }}
\end{align*}
Fix $y_0 \in f(U)$. Let $x_0= f^{-1}(y_0)$. Because $U$ is open, we know there exists $r$ such that 
\begin{align*}
  \overline{B_r(x_0)}\subseteq U
\end{align*}
We claim 
\begin{align*}
  \brown{B_{\ld r}(y_0)\subseteq f(U)}
\end{align*}
Fix $y\in B_{\ld r}(y_0)$. We are required to prove 
\begin{align*}
  \brown{y\in f(U)}
\end{align*}
Because 
\begin{align*}
y=f(x)\iff  x\text{ is a fixed point of }\phi_y
\end{align*}
We then can use Banach Fixed Point Theorem (\myref{Theorem}{BFPT}) to reduce the problem into proving 
\begin{align*}
\brown{\phi_y\text{ is a contraction that maps some complete subset of $U$ into  itself}}
\end{align*}
We claim 
\begin{align*}
\brown{\overline{B_r(x_0)}\text{ suffices }}
\end{align*}
We have already known $\phi_y$ is a contraction on $U$, and it is clear that $\overline{B_r(x_0)}$ is complete. We reduce the problem into proving 
\begin{align*}
\brown{\phi_y\big(\overline{B_r(x_0)} \big)\subseteq \overline{B_r(x_0)}}
\end{align*}
Using
\begin{enumerate}[label=(\alph*)]
  \item definition of $\phi_y$
  \item $\abso{y-y_0}<\ld r$ 
  \item $\norm{(df_a)^{-1}}_\text{op}=\frac{1}{2\ld }$
\end{enumerate}
We can deduce 
\begin{align*}
\abso{\phi_y (x_0)-x_0}&= \abso{(df_a)^{-1}(y-f(x_0))}\\
&\leq \norm{(df_a)^{-1}}_\text{op} \abso{y-y_0}<\frac{r}{2}
\end{align*}
Fix $x\in \overline{B_r(x_0)}$. We can now deduce 
\begin{align*}
  \abso{\phi_y (x)-x_0}&\leq \abso{\phi_y (x_0)-\phi_y (x)}+\abso{x_0-\phi_y (x_0)}\\
&\leq \frac{1}{2}\abso{x_0-x}+\frac{r}{2}\leq r \bodone
\end{align*}
We now prove 
\begin{align*}
\blue{df_x\text{ is invertible for all }x \in U}
\end{align*}
Fix $x\in U$.  
\begin{align*}
\norm{df_x-df_a}_\text{op}\cdot \norm{(df_a)^{-1}}_\text{op}<\frac{1}{2}
\end{align*}
\myref{Theorem}{SCftI} now implies $df_x$ is invertible. $\bdone$\\

Lastly, it remains to prove 
\begin{align*}
  \vi{f^{-1}:f(U)\rightarrow U\text{ is differentiable on $f(U)$}}
\end{align*}
Fix $y\in f(U)$, and $x\triangleq f^{-1}(y)$. We are required to prove 
\begin{align*}
  \vi{\lim_{k\to 0} \frac{\abso{f^{-1}(y+k)-x-(df_x)^{-1}k}}{\abso{k}}=0}
\end{align*}
Fix $h(k)\triangleq f^{-1}(y+k)-f(x)$. In other words, $h\inr^n$ is fixed to be the unique vector such that 
\begin{align*}
f(x+h)=y+k
\end{align*}
We now see 
\begin{align*}
f^{-1}(y+k)-x-(df_x)^{-1}k&= h-(df_x)^{-1}k\\
&=-(df_x^{-1})(f(x+h)-f(x)-df_xh)
\end{align*}
and see 
\begin{align*}
\abso{f^{-1}(y+k)-x-(df_x)^{-1}k}\leq \norm{(df_x)^{-1}}_\text{op} \abso{f(x+h)-f(x)-df_xh}
\end{align*}
which give us 
\begin{align*}
\frac{\abso{f^{-1}(y+k)-x-(df_x)^{-1}k}}{\abso{k}}\leq \norm{(df_x)^{-1}}_\text{op} \frac{\abso{f(x+h)-f(x)-df_xh}}{\abso{h}} \cdot \frac{\abso{h}}{\abso{k}}
\end{align*}
This allow us to reduce the problem into proving 
\begin{align*}
\vi{\limsup_{k\to 0} \frac{\abso{h}}{\abso{k}}\inr}
\end{align*}
We claim 
\begin{align*}
  \vi{\frac{\abso{h}}{\abso{k}}\leq \ld ^{-1}\text{ for all $k$ such that $y+k \in f(U)$ }}
\end{align*}
Compute 
\begin{align*}
\phi_y (x+h)-\phi_y (x)=h- (df_a)^{-1}k
\end{align*}
\olive{\myref{Equation}{phiy12}} let us deduce 
\begin{align*}
\abso{h-(df_a)^{-1}k}= \abso{\phi_y (x+h)-\phi_y (x)}\leq \frac{\abso{h}}{2}
\end{align*}
This with triangle inequality implies  
\begin{align*}
 \norm{(df_a)^{-1}}_\text{op} \abso{k} \geq \abso{(df_a)^{-1}k}\geq  \frac{\abso{h}}{2} \vdone
\end{align*}
\end{proof}
\begin{mdframed}
The following is a technical recap of our proof for the Inverse Function Theorem (\myref{Theorem}{IFT}). 
\begin{enumerate}[label=\arabic*:]
  \item Let $\ld \triangleq \frac{1}{2 \norm{(df_a)^{-1}}_{\text{op}}}$ 
  \item Claim $B_\delta(a)$ suffices to be $U$, where $\norm{df_x-df_a}_\text{op}<\ld $ 
  \item For each $y\inr^n$, define $\phi_y:U\rightarrow \R^n$ by $\phi_y(x)\triangleq x+(df_a)^{-1}(y)$. 
  \item Prove that $\phi_y$ is a contraction of $U$ by taking derivative and utilize step 1 and 2.
  \item Prove that $\phi_y$ fix $x\iff f(x)=y$ 
  \item Prove $f$ is one-to-one in $U$ using step 4,5.
  \item Prove $f(U)$ is open by proving $B_{\ld r}(y_0)\subseteq f(U)$, while $\overline{B_r(x_0)}\subseteq U$. The proof use step 4,5, some computation and ultimately claim that $\phi_y$ admits a fixed point as $\phi_y$ maps $\overline{B_r(x_0)}$ into itself. 
  \item Prove $df_x$ is invertible in $U$ by \myref{Theorem}{SCftI}
  \item Algebraically prove $df^{-1}=(df)^{-1}$, using $\abso{h-(df_a)^{-1}k}=\abso{\phi_y (x+h)-\phi_y (x)}\leq \frac{\abso{h}}{2}$
\end{enumerate}
\end{mdframed}
\begin{theorem}
\textbf{(Inversion is Continuous)} 
\begin{align*}
\text{ The mapping }A \to A^{-1}\text{ is continuous on $\Omega$ }
\end{align*}
\end{theorem}
\begin{proof}
Fix $A\in \Omega$ and let $T \in \Omega$. We are required to prove 
\begin{align*}
  \vi{\lim_{T\to A} \norm{T^{-1}-A^{-1}}_\text{op}=0}
\end{align*}
We know 
\begin{align*}
T^{-1}-A^{-1}=T^{-1}(A-T)A^{-1}
\end{align*}
This implies 
\begin{align*}
\norm{T^{-1}-A^{-1}}_\text{op}\leq \norm{T^{-1}}_\text{op} \norm{A-T}_\text{op} \norm{A^{-1}}_\text{op}
\end{align*}
This allow us to reduce the problem into proving 
\begin{align*}
  \vi{\limsup_{T\to A} \norm{T^{-1}}_\text{op}\inr}
\end{align*}
Fix  $\epsilon \triangleq \frac{1}{\norm{A^{-1}}_\text{op}}$, $T \in B_\epsilon (A)$ and $\beta \triangleq \norm{T-A}_\text{op}< \epsilon $. We claim 
\begin{align*}
  \vi{\norm{T^{-1}}_\text{op} \leq  (\epsilon -\beta )^{-1}}
\end{align*}
Following the proof of \myref{Theorem}{SCftI}, we have 
\begin{align*}
  (\epsilon - \beta )\abso{x}\leq \abso{Tx}\text{ for all $x\inr^n$ }
\end{align*}
This implies 
\begin{align*}
  \frac{\abso{T^{-1}x}}{\abso{x}}\leq (\epsilon -\beta )^{-1}\text{ for all $x\neq 0 \inr^n$ }\vdone
\end{align*}
\end{proof}
\begin{corollary}
\textbf{(Continuously Differentaible Version of Inverse Function Theorem)} Given a function $f$ that maps open $E\subseteq \R^n$ containing $a$  into $\R^n$ such that 
\begin{enumerate}[label=(\alph*)]
  \item $f$ is differentiable on $E$  
  \item $df_a$ is invertible 
   \item $f$ is continuously differentiable on $E$
\end{enumerate}
Then there exists open $U\subseteq E$ containing $a$ such that 
\begin{align*}
f(U)\text{ is open and }f|_U:U\to f(U)\text{ is a diffeomorphism }
\end{align*}
\end{corollary}

\section{Implicit Function Theorem}
\begin{mdframed}
Some notations first. If $A \in L(\R^{n+m}, \R^k)$. We define $A|_{\R^n}:L(\R^n,\R^k)$ by
\begin{align*}
A|_{\R^n}(x)\triangleq A(x,0)
\end{align*}
\end{mdframed}
\begin{theorem}
\label{IpFT}
\textbf{(Implicit Function Theorem)} Suppose a function $f$ that maps open $E\subseteq \R^n \times \R^m$ containing $(a,b)$ into $\R^n$ satisfy 
\begin{enumerate}[label=(\alph*)]
  \item $f(a,b)=0$ 
  \item $(df_{(a,b)})|_{\R^n}$ is invertible 
  \item $f$ is continuously differentiable on $E$
\end{enumerate}
Then there exists open $U \subseteq E$ containing $(a,b)$ and open $W\subseteq \R^m$ containing $b$ such that there exists a unique function $g$ from $W$ to  $\R^n$ such that
\begin{enumerate}[label=(\alph*)]
  \item $\big(g(y),y \big) \in U$ for all $y \in W$
  \item $f(g(y),y)=0$ for all $y \in W$  
\end{enumerate}
Moreover, $g$ satisfy 
\begin{enumerate}[label=(\alph*)]
  \item $g$ is continuously differentiable on $W$
   \item $g$ satisfy $dg_b=-(df_{(a,b)}|_{\R^n})^{-1}\circ df_{(a,b)}|_{\R^m}$
\end{enumerate}
\end{theorem}
\begin{proof}
Define $F:E\rightarrow \R^n \times \R^m$ by 
\begin{align*}
F(x,y)\triangleq \Big(f(x,y),y \Big)
\end{align*}
Because $f$ is continuously differentiable on $E$, using Differentaibility Theorem (\myref{Theorem}{DT}), we can deduce 
\begin{align*}
F\text{ is continuously differentiable on $E$ }
\end{align*}
Again using Differentiability Theorem  (\myref{Theorem}{DiJ}), we can write down $dF_{(a,b)}$ in the matrix form with respect to standard basis
\begin{align*}
[dF_{(a,b)}]= \begin{bmatrix}
  df_{(a,b)}|_{\R^n} & O \\
  df_{(a,b)}|_{\R^m} & I
\end{bmatrix}
\end{align*}
Now, because $df_{(a,b)}|_{\R^n}$ is invertible, we know $dF_{(a,b)}$ is invertible.\\

We can now apply Inverse Function Theorem (\myref{Theorem}{IFT}) to $F:E\rightarrow \R^n \times \R^m$. This give us 
\begin{enumerate}[label=(\alph*)]
  \item an open $U\subseteq E\subseteq \R^n \times \R^m$ containing $(a,b)$
  \item open $V\triangleq F(U)\subseteq \R^n \times \R^m$ containing $(0,b)$
  \item $F|_U:U\rightarrow V$ is a diffeomorphism.
\end{enumerate}
Define $W$ by 
\begin{align*}
W\triangleq \set{y\inr^m : (0,y)\in V}
\end{align*}
We claim 
\begin{align*}
\vi{\text{ such }U,W\text{ suffices }}
\end{align*}
Note that it is easy to check $W$ is open, utilizing $V$ is open and the same $\epsilon $.\\

Now, because $F|_U:U\rightarrow V$ is bijective, we know for each $y\in W$, there exists unique $(x,y)\in U$ such that 
\begin{align*}
F(x,y)=(0,y)
\end{align*}
We can now well define a function $g:W\rightarrow \R^n$ such that 
\begin{align*}
\Big(g(y),y \Big)\in U\text{ and }f(g(y),y)=0\text{ for all $y \in W$ }
\end{align*}
It remains to show 
\begin{enumerate}[label=(\alph*)]
  \item \blue{$g$ is continuously differnetiable on  $W$}
  \item  \olive{$dg_b=-(df_{(a,b)})|_{\R^n}^{-1}(df_{(a,b)})|_{\R^m}$}
\end{enumerate}
Fix $i \in \set{1,\dots ,n}$ and $j \in \set{1,\dots ,m}$. We wish to prove 
\begin{align*}
\blue{\partial_i g_j\text{ exists and is continuous on $W$ }}
\end{align*}
Express 
\begin{align*}
g(y_1,\dots ,y_m)=\Big(g_1(y_1,\dots ,y_m),\dots , g_n(y_1,\dots ,y_m) \Big)
\end{align*}
Express 
\begin{align*}
F^{-1}(z_1,\dots ,z_{n+m})= \Big(F^{-1}_1(z_1,\dots ,z_{n+m}), \dots , F^{-1}_{n+m}(z_1,\dots ,z_{n+m}) \Big)
\end{align*}
Because $F^{-1}$ is continuously differentiable on $V$, we reduce the problem into proving 
\begin{align*}
  \blue{\partial_i g_j (y)=\partial_{n+i} F^{-1}_j (0,y)\text{ for all $y\in W$ }}
\end{align*}
Because $F^{-1}(0,y)=(g(y),y)$ for all $y \in W$, we know  
\begin{align}
\label{F-1g}
F^{-1}_j (0,\dots , 0 ,y_1,\dots ,y_m)=g_j(y_1, \dots ,y_m) \text{ for all $y \in W$ }
\end{align}
Fix arbitrary $y=(y_1,\dots ,y_m)\in W$. Because $W$ is open, we can see from \myref{Equation}{F-1g} that 
\begin{align*}
\partial_i g_j(y)&=\lim_{t\to 0} \frac{g_j(y_1,\dots ,y_i +t, \dots ,y_m)}{t}\\
&=\lim_{t\to 0} \frac{F^{-1}_j(0,\dots ,0, y_1,\dots, y_i+t,\dots ,y_m)}{t}\\
&=\partial_{n+i}F_j^{-1}(y) \bdone
\end{align*}
Define $\Phi: W \rightarrow U$ by 
\begin{align*}
\Phi (y)=\Big(g(y),y \Big)
\end{align*}
By definition of $g$, we have 
 \begin{align*}
f\circ \Phi =0 \text{ on $W$ }
\end{align*}
This by Chain Rule give us 
\begin{align*}
df_{\Phi(y)}\circ d\Phi_y=0\text{ on $W$ }
\end{align*}
In particular 
\begin{align*}
df_{(a,b)}\circ d\Phi_b=0
\end{align*}
Now, compute 
\begin{align*}
d\Phi_b= \begin{bmatrix}
  dg_b \\
  I
\end{bmatrix}\text{ and }df_{(a,b)}=\begin{bmatrix}
df_{(a,b)}|_{\R^n} & df_{(a,b)}|_{\R^m}
\end{bmatrix}
\end{align*}
This then give us 
\begin{align*}
df_{(a,b)}|_{\R^n} \circ dg_b+ df_{(a,b)}|_{\R^m}=0
\end{align*}
and of course 
\begin{align*}
dg_b= - \big(df_{(a,b)}|_{\R^n} \big)^{-1} \circ df_{(a,b)}|_{\R^m} \odone \vdone
\end{align*}
\end{proof}
\begin{Example}{\textbf{(Unit Circle Example)}}{}
\begin{align*}
f(x,y)\triangleq x^2+y^2-1 \text{ and }(a,b)\triangleq (1,1)
\end{align*}
We have 
\begin{align*}
g(y)=\sqrt{2-y^2}\text{ on } y \in (1-\epsilon ,1+\epsilon )
\end{align*}
Compute 
\begin{align*}
df_{(a,b)}=\begin{bmatrix}
  2 & 2
\end{bmatrix}\text{ and }dg_1 =\begin{bmatrix}
-1
\end{bmatrix}
\end{align*}
This established 
\begin{align*}
dg_a= -(df_{(a,b)}|_{\R^1})^{-1}\circ df_{(a,b)}|_{\R^1}
\end{align*}
\end{Example}
\begin{Example}{\textbf{(Implicit Function Theorem Implies Inverse Function Theorem)}}{}
\begin{align*}
\text{ Given continuously differentaible }h:E\overset{\text{open}}{\subseteq}\R^n\ni a\rightarrow \R^n\text{ such that }dh_a\text{ is invertible }
\end{align*}
Define $f:E\times \R^n\rightarrow \R^n$ by 
\begin{align*}
f(x,y)\triangleq h(x)-y
\end{align*}
It is easily checked that $f(x,h(x))=0$  and the rest of the condition is satisfied. Now by Implicit Function Theorem (\myref{Theorem}{IpFT}), we see that there exists $g:W\overset{\text{open}}{\subseteq}\R^n \rightarrow E$ such that 
\begin{align*}
f(g(y),y)=0\text{ for all $y\in W$ }
\end{align*}
In other words, 
\begin{align*}
h(g(y))=y\text{ for all $y \in W$ }
\end{align*}
\end{Example}
\section{Feynman's Trick}
\begin{mdframed}
In this section
\end{mdframed}
\begin{theorem}
\textbf{(Feynman's Trick)} Given a real-valued function $f (x,t)$ defined on $[a,b]\times [c,d]$, and an real-valued function $\alpha $ of bounded variation on $[a,b]$, such that 
\begin{enumerate}[label=(\alph*)]
  \item $\partial_2f(x,t)$ exists on $[a,b]\times [c,d]$
  \item For all $t\in [c,d]$, the integral $\int_a^b f (x,t)d \alpha (x)$ exists. 
  \item For all $s \in [c,d]$ and $\epsilon $, there exists $\delta$ such that 
    \begin{align*}
    \abso{\partial_2f (x,t)- \partial_2f(x,s)} <\epsilon \text{ for all $x\in [a,b]$ and all $t \in (s-\delta, s+\delta)$ }
    \end{align*}

\end{enumerate}
Then we have 
\begin{align*}
\frac{d}{dt}\int_a^b f(x,t)d\alpha (x)= \int_a^b \frac{\partial }{\partial t}f(x,t)d\alpha (x)
\end{align*}
In other words, if we define $g(t)\triangleq \int_a^b f(x,t)d\alpha (x)$, then we have
\begin{align*}
g'(t)=\int_a^b \partial_2 f(x,t)d\alpha (x)
\end{align*}
\end{theorem}
\begin{proof}
Fix $s \in [c,d]$. We are required to prove 
\begin{align*}
\vi{g'(s)=\int_a^b \partial_2 f(x,s)dx}
\end{align*}
Note that, for all $t \neq s \in [c,d]$, we have
\begin{align*}
  \frac{g(t)-g(s)}{t-s}=\int_a^b \frac{f(x,t)-f(x,s)}{t-s}d\alpha (x)
\end{align*}
This allow us to reduce the problem into proving 
\begin{align*}
  \vi{\frac{f(x,t)-f(x,s)}{t-s}\to \partial_2 f(x,s)\text{ uniformly for all $x \in [a,b]$ as }t\to s}
\end{align*}
By MVT (\myref{Corollary}{MVT}), we know for all $t\neq s \in [c,d]$, there exists  $u_t$ between $t$ and $s$ such that 
\begin{align*}
\frac{f(x,t)-f(x,s)}{t-s}= \partial_2 f(x,u_t)
\end{align*}
The proof now follows from (c). $\vdone$
\end{proof}
\begin{Example}{\textbf{(Introductive application of Feynman's Trick)}}{}
\begin{align*}
  \text{ What is the value of }\int_0^1 \frac{x-1}{\ln x}dx\text{ ? }
\end{align*}
Define $f(x,t)\triangleq \frac{x^t-1}{\ln x}$ on $[0,1]\times [0,1]$. Observe $\partial_2 f(x,t)=x^t$, and observe 
\begin{align*}
\int_0^1 f(x,0)dx=0\text{ and }\int_0^1 \partial_2 f(x,t)dx= \frac{1}{t}
\end{align*}
We can con compute 
\begin{align*}
\int_0^1 \frac{x-1}{\ln x}dx&= \int_0^1 f(x,1)dx\\
&=\int_0^1 f(x,0)dx+\int_0^1 \Big(\int_0^1 \partial_2 f(x,t)dx \Big) dt=0
\end{align*}
\end{Example}
\begin{Example}{\textbf{(Dirichlet's Integral)}}{}
\begin{align*}
\text{ What is the value of }\int_0^{\infty} \frac{\sin t}{t}dt\text{ ? }
\end{align*}
Define the Laplace transformation 
\begin{align*}
f(s,t)\triangleq e^{-st} \frac{\sin t}{t}\text{ on $\R_0^+\times\R^+$ }
\end{align*}
Observe  
\begin{align*}
\partial_1 f(s,t)=-e^{-st}\sin t
\end{align*}
Now compute 
\begin{align*}
  \int_0^{\infty} -e^{-st}\sin tdt &= \frac{1}{-2i}\int_0^{\infty}e^{-st}(e^{it}-e^{-it})dt\\
  &=\frac{1}{-2i} \Big( \frac{e^{t(i-s)}}{i-s}- \frac{e^{t(-i-s)}}{-i-s} \Big) \Big|_{t=0}^{\infty}\\
  &= \frac{-1}{1+s^2}= \frac{d}{ds}(-\arctan s)
\end{align*}
It is clear that 
\begin{align*}
\lim_{s\to \infty} \int_0^{\infty}f(s,t)dt=0
\end{align*}
We now have 
\begin{align*}
\int_0^{\infty}\frac{\sin t}{t}dt&= \int_0^{\infty} f(0,t)dt\\
&=\lim_{s\to \infty}\int_0^{\infty}f(s,t)dt- \int_0^{\infty} \int_0^{\infty} \partial_1 f(s,t)dt ds\\
&=\int_0^{\infty} \frac{1}{1+s^2}ds= \frac{\pi}{2}
\end{align*}
\end{Example}


\section{Appendix: Linear Algebra}
\begin{mdframed}
This section contains
\begin{enumerate}[label=(\alph*)]
  \item definition and basic properties of the term \textbf{norm}
  \item definition and basic properties of the term \textbf{inner product}
  \item definition and basic properties of the term \textbf{positive semi-definite Hermitian form}
  \item full statement and proof of \textbf{Cauchy Schwarz Inequality} for both inner product space and positive semi-definite Hermitian form  
  \item statement and proof of \textbf{SVD} (singular value decomposition). 
\end{enumerate}
\end{mdframed}
\textbf{(Norm Axiom Part)}
\begin{mdframed}
Recall that by a \textbf{normed space} $V$, we mean a vector space over a sub-field $\F$ of $\C$ equipped with  $\norm{\cdot}:V\to \R_0^+$ satisfying the following $\underline{\text{axioms}}$: 
\begin{enumerate}[label=(\alph*)]
  \item $\norm{x}=0 \implies x=0$ (positive-definiteness)
  \item $\norm{sx}=\abso{s}\cdot \norm{x}$ for all $s \in \F$ and $x\in V$ (absolute-homogenity)
  \item $\norm{x+y}\leq \norm{x}+\norm{y}$ for all $x,y \in V$ (triangle inequality)
\end{enumerate}
Observe
\begin{align*}
\norm{0}=\norm{0+x}\leq \norm{0}+\norm{x}\text{ for all $x\in V$ }
\end{align*}
This shows that $\norm{x}\geq 0$ for all $x\in V$. Also observe 
\begin{align*}
\norm{0}=\norm{0(x)}=\abso{0}\cdot \norm{x}=0
\end{align*}

We can now rewrite the normed space axioms into
\begin{enumerate}[label=(\alph*)]
  \item $\norm{x}=0\iff x=0$ (positive-definiteness)
  \item $\norm{sx}=\abso{s}\cdot \norm{x}$ for all $s \in \F$ and $x\in V$ (absolute-homogeneity)
  \item $\norm{x+y}\leq \norm{x}+\norm{y}$ for all $x,y \in V$ (triangle inequality)
  \item $\norm{x}\geq 0$ for all $x \in V$ (non-negativity)
\end{enumerate}
\end{mdframed}
\textbf{(Inner Product Axiom Part)}
\begin{mdframed}
Recall that by an \textbf{inner product space} $V$, we mean a vector space over  $\R$ or $\C$ equipped with  $\langle \cdot,\cdot\rangle : V^2 \rightarrow \R\text{ or }\C$ satisfying the following $\underline{\text{axioms}}$
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle >0$ for all $x\neq 0$ (Positive-definiteness)
  \item $\langle x,y\rangle =\overline{\langle y,x\rangle }$ (Conjugate symmetry)
  \item $\langle x+y,z\rangle =\langle x,z\rangle +\langle y,z\rangle $ and $\langle cx,z\rangle=c\langle x,z\rangle $ (Linearity in the first argument)
\end{enumerate}
Note that conjugate symmetry let us deduce
\begin{align*}
\langle x,x\rangle =\overline{\langle x,x\rangle }\implies \langle x,x\rangle \inr
\end{align*}
Also, one can easily use linearity in first argument to deduce 
\begin{align*}
\langle 0,0\rangle =2\langle 0,0\rangle \implies \langle 0,0\rangle =0
\end{align*}
This now let us rewrite the inner product space over $\C$ axioms into 
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle \geq 0$ for all $x\in V$ (non-negativity) 
  \item $\langle x,x\rangle =0 \iff x=0$ (positive-definiteness)
  \item $\langle x,y\rangle =\overline{\langle y,x\rangle }$ (conjugate symmetry)
  \item $\langle cx+y,z\rangle =c\langle x,z\rangle +\langle y,z\rangle $ and $\langle x,cy+z\rangle =\overline{c}\langle x,y\rangle +\langle x,z\rangle $ (Linearity)
\end{enumerate}
Note that using $c=1$ and $y=0$, ($\because \langle 0,z\rangle =0\langle x,z\rangle=0 $) one can check that the latter expression of linearity implies the first expression.
\end{mdframed}
\begin{mdframed}
If the scalar field is $\R$, then conjugate symmetry is just symmetry and we also have linearity in the second argument.\\

This now let us rewrite the inner product space over $\R$ axioms into 
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle \geq 0$ for all $x\in V$ (non-negativity) 
  \item $\langle x,x\rangle =0 \iff x=0$ (positive-definiteness)
  \item $\langle x,y\rangle =\langle y,x\rangle $ (symmetry)
  \item Linearity in both arguments
\end{enumerate}
\end{mdframed}
\begin{mdframed}
If we do not require $\langle \cdot,\cdot\rangle $ to be positive-definite, but only non-negative, i.e. $\langle x,x\rangle \geq 0$ for all $x\in V$, then we have a \textbf{positive semi-definite Hermitian form}. Formally speaking, a positive semi-definite Hermitian form $\langle\cdot,\cdot \rangle :V^2 \rightarrow \R\text{ or }\C$ satisfy the following axioms
\begin{enumerate}[label=(\alph*)]
  \item $\langle x,x\rangle \geq 0$ for all $x\in V$ (non-negativity)
  \item $\langle x,y\rangle =\overline{\langle y,x\rangle }$ (conjugate symmetry)
  \item $\langle x+y,z\rangle =\langle x,z\rangle +\langle y,z\rangle $ and $\langle cx,z\rangle=c\langle x,z\rangle $ (Linearity in the first argument)
\end{enumerate}
\begin{Example}{\textbf{(Example of Positive semi-definite Hermitian form)}}{}
\begin{align*}
\text{ arbitrary }V\text{ over $\R$ or  $\C$ }\hspace{0.5cm}\langle x,y\rangle \triangleq 0\text{ for all $x,y$ }\hspace{1cm}
\end{align*}
\end{Example}
\end{mdframed}
\textbf{(Norm Induce Part)}
\begin{mdframed}
Given a vector space $V$ over $\R$ or  $\C$, one can check that if  $V$ is equipped with an inner product  $\langle \cdot,\cdot\rangle :V^2\rightarrow \R\text{ or }\C$, then we can induce a norm on $V$ by 
\begin{align*}
\hspace{3cm}\norm{x}\triangleq \sqrt{\langle x,x\rangle }\hspace{1.5cm}(x \in V)
\end{align*}
Note that 
\begin{align*}
\norm{x}=0\iff \langle x,x\rangle =0 
\end{align*}
This implies that if $\langle \cdot,\cdot\rangle $ is an inner product (satisfy positive-definiteness), then $\norm{\cdot}$ is also positive-definite. And if $\langle \cdot,\cdot\rangle $ is not positive-definite, then there exists  $x\neq 0\in V$ such that $\norm{x}=0$, which make $\norm{\cdot}$ a \textbf{semi-norm}.\\

Absolute homogeneity follows from the linearity of inner product.\\

To check triangle inequality, we first have to prove Cauchy-Schwarz inequality.
\end{mdframed}
\begin{theorem}
\label{BPoP}
\textbf{(Basic Property of Positive semi-definite Hermitian form)} Given a positive semi-definite Hermitian form $\langle \cdot,\cdot\rangle:V^2 \rightarrow \R\text{ or }\C $ and $x,y \in V$, we have 
\begin{align*}
\langle x,x\rangle =0 \implies \langle x,y\rangle =0
\end{align*}
\end{theorem}
\begin{proof}
\As{$\langle x,y\rangle\neq 0 $}. Fix $t> \frac{\norm{y}^2 }{2\abso{\langle x,y\rangle }^2}$. Compute 
\begin{align*}
\norm{y-t\langle y,x\rangle x}^2 &= \norm{y}^2 + \norm{(-t)\langle y,x\rangle x }^2  + \langle -t\langle y,x\rangle x,y \rangle + \langle y,-t\langle y,x\rangle x\rangle \\
&=\norm{y}^2 + t^2 \abso{\langle x,y\rangle }^2 \norm{x}^2 -t \langle y,x\rangle \langle x,y\rangle - t \langle x,y\rangle \langle y,x\rangle  \\
&= \norm{y}^2- 2t \abso{\langle x,y\rangle }^2 <0 \tCaC
\end{align*}
\end{proof}
\begin{theorem}
\textbf{(Cauchy-Schwarz Inequality)} Given a positive semi-definite Hermitian form $\langle \cdot,\cdot\rangle :V^2\rightarrow \C$ on vector space $V$ over $\C$, we have 
\begin{enumerate}[label=(\alph*)]
  \item $\abso{\langle x,y\rangle}\leq  \norm{x}\cdot\norm{y}\hspace{0.5cm}(x,y \in V)$ 
  \item the equality hold true if $x,y$ are linearly dependent
  \item the equality hold true if and only if $x,y$ are linearly dependent (provided $\langle \cdot,\cdot\rangle $ is an inner product)
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove 
\begin{align*}
  \vi{\hspace{3cm}\abso{\langle x,y\rangle }\leq \norm{x}\cdot\norm{y}\hspace{1.5cm}(x,y \in V)}
\end{align*}
Fix $x,y \in V$. \myref{Theorem}{BPoP} tell us $\norm{x}=0 \implies  \langle x,y\rangle =0$. Then we can reduce the problem into proving 
\begin{align*}
\vi{\frac{\abso{\langle x,y\rangle }^2}{\norm{x}^2}\leq\norm{y}^2  }
\end{align*}
Set $z\triangleq y-\frac{\langle y,x\rangle }{\norm{x}^2}x$. We then have 
\begin{align*}
\langle z,x\rangle =\langle y-\frac{\langle y,x\rangle }{\norm{x}^2}x,x \rangle =\langle y,x\rangle - \frac{\langle y,x\rangle }{\norm{x}^2}\langle x,x\rangle =0
\end{align*}
Then from $y=z+\frac{\langle y,x\rangle }{\norm{x}^2}x$, we can now deduce
\begin{align*}
\langle y,y\rangle &= \langle z+\frac{\langle y,x\rangle }{\norm{x}^2}x,z+ \frac{\langle y,x\rangle }{\norm{x}^2}x\rangle  \\
&=\langle z,z\rangle + \abso{\frac{\langle y,x\rangle }{\langle x,x\rangle }}^2 \langle x,x\rangle \\
&=\langle z,z\rangle + \frac{\abso{\langle x,y\rangle }^2}{\langle x,x\rangle }
\end{align*}
Because $\langle z,z\rangle\geq 0 $, we now have
\begin{align*}
\langle y,y\rangle = \langle z,z\rangle + \frac{\abso{\langle x,y\rangle }^2}{\langle x,x\rangle }\geq  \frac{\abso{\langle x,y\rangle }^2}{\langle x,x\rangle }\vdone
\end{align*}
The equality hold true if and only if $\langle z,z\rangle =0$. This explains the other two statements regarding the equality. 
\end{proof}
\begin{mdframed}
The proof is clearly geometrical. If one wish to remember the proof, one should see the trick we use is exactly 
\begin{align*}
z\triangleq y-\abso{y}(\cos \theta )\hat{x}\text{ is the projection of $y$ onto $x^{\perp}$ }
\end{align*}
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
       \includegraphics[height=5cm,width=12cm]{CSp.jpeg}
   \end{minipage}
\end{center}
Then all we do rest is just expanding $\abso{y}^2=\abso{z+\tilde{x}}^2$, where $\tilde{x}=y-z=\abso{y}(\cos \theta)\hat{x}$, which give the answer and is easy to compute since $z\cdot \tilde{x}=0$.\\

Now, with Cauchy-Schwarz Inequality, we can check the triangle inequality 
\begin{align*}
  \norm{x+y}^2&=\langle x+y,x+y\rangle \\
&=\langle x,x\rangle +\langle x,y\rangle +\langle y,x\rangle +\langle y,y\rangle \\
&= \langle x,x\rangle + \langle y,y\rangle +2\text{ Re }\langle x,y\rangle \\
&\leq \norm{x}^2 + \norm{y}^2 + 2\abso{\langle x,y\rangle }\\
&\leq \norm{x}^2 + \norm{y}^2 + 2\norm{x}\cdot \norm{y}=\Big(\norm{x}+\norm{y}\Big)^2
\end{align*}
\end{mdframed}
\textbf{(Euclidean Space Abstract Part)} 
\begin{mdframed}
  By a \textbf{concrete Euclidean Space}, we mean some space of $n$-tuple  $(x_1,\dots ,x_n)$ over $\R$, equipped with inner product  $\langle \cdot,\cdot\rangle_E$ defined by 
\begin{align*}
  \langle  (x_1,\dots,x_n)  ,(y_1,\dots ,y_n)\rangle_E= \sqrt{\sum_{k=1}^n (y_k-x_k)^2} 
\end{align*}
By an \textbf{Euclidean Space}, we simply mean a finite dimensional vector space $V$ over $\R$,  equipped with an inner product $\langle \cdot,\cdot\rangle $ such that there exists a concrete Euclidean space $E$ and an isomorphism  $\phi:V\to E$ such that 
\begin{align*}
\hspace{3cm}\langle x,y\rangle= \langle \phi (x),\phi (y)\rangle_E \hspace{1.5cm}(x,y \in V)
\end{align*}
Note that if you define $\langle \cdot,\cdot\rangle $ on the space of $n$-tuples $(x_1,\dots ,x_n)$ over $\R$ by 
\begin{align*}
  \langle  (x_1,\dots,x_n)  ,(y_1,\dots ,y_n)\rangle=2 \sqrt{\sum_{k=1}^n (y_k-x_k)^2} 
\end{align*}
Then, the space of $n$-tuple is clearly not a concrete Euclidean space, and clearly an Euclidean space. 

\end{mdframed}
\textbf{(SVD)}

\chapter{Differential Geometry}
\section{Invariance of Domain}
\begin{theorem}
\textbf{(Invariance of Domain)} If $U$ is an open subset of $\R^n$ and  $f:U\rightarrow \R^n$ is one-to-one and continuous, then 
\begin{align*}
f(U)\text{ is open and }f\text{ is a homeomorphism between }U\text{ and }f(U)
\end{align*}
\end{theorem}
\begin{theorem}
\textbf{(Invariance of Dimension)} If $U$ is a non-empty open subset of $\R^n$ and  $V$ is a non-empty subset of $\R^m$ homeomorphic to  $U$, then  $n=m$.
\end{theorem}
\section{Differentiable Manifold}
\begin{abstract}
This section give an example of a differentiable manifold.
\end{abstract}
\begin{mdframed}
Given a topological space $(X,\mathscr{T })$ and fix $n$, if $:U\rightarrow \R^n$ is a homeomorphism between some subspace $U$ of $X$ and some open subspace of $\R^n$, we say $(\Phi,U)$ is a \textbf{chart} on $X$, and if there exists a collection $A=\set{(\Phi_i, U_i)}_{i \in I}$ of chart cover the whole $X$, we say $A$ is an  \textbf{atlas} and  $X$ is  \textbf{locally Euclidean}. An atlas $A$ is said to be a \textbf{smooth atlas}, if for each two charts $(\Phi_i,U_i),(\Phi_j,U_j)$ in $A$, 
\begin{align*}
\text{ The function }\Phi_i\circ \Phi_j^{-1}:\Phi_j(U_i\cap U_j)\to \Phi_i(U_i\cap U_j)\text{ is a smooth diffeomorphism }
\end{align*}
It is easily checked that for each two chart $\Phi_i,\Phi_j$, the \textbf{transition map} $\Phi_i\circ \Phi_j^{-1}$ is a homeomorphism. Now, if the union of two smooth atlas $A_1,A_2$ is again differentiable, we say $A_1,A_2$ are \textbf{compatible}. With some effort, one can check that compatibility is an equivalence relation on the collection of all possible atlas on $X$. Thus, it make sense for us to define the \textbf{smooth structure}, i.e., a maximal smooth atlas on $X$. We now give definition to the term \textbf{topological manifold}, which we will use throughout this chapter.  
\end{mdframed}
\begin{definition}
\textbf{(Definition of Topological Manifold)} We say a Topological space $X$ is a \textbf{topological manifold} if 
\begin{enumerate}[label=(\alph*)]
  \item $X$ is locally Euclidean. 
  \item $X$ is Hausdorff. 
  \item $X$ is second countable.
\end{enumerate}
\end{definition}
\begin{mdframed}
Immediately, one should check that all three conditions are necessary: There are locally Euclidean space that is not Hausdorff, \customref{Bug-Eyed Line}{Bug-Eyed Line} for example. There also are locally Euclidean space that is not second countable, \customref{Long Line}{Long Line} for example. Also, one can check that any open subspace or finite products of topological manifold is still topological manifold. 
Now, by a \textbf{smooth manifold}, we merely mean a manifold equipped with a maximal smooth atlas, and given a function $F:M\rightarrow N$ that maps a smooth manifold $M$ into another smooth manifold  $N$, we say  $F$ is \textbf{smooth at $p$} if $F$ is continuous at $p$ and there exists some charts $(U,\phi),(V,\psi)$ respectively at $p,F(p)$ such that 
\begin{enumerate}[label=(\roman*)]
  \item $U \subseteq F^{-1}(V)$. 
  \item $\psi \circ F\circ \phi^{-1} : \phi (U) \subseteq \R^m \rightarrow \psi (V)\subseteq \R^n\text{  is smooth at }\phi (p)$. 
\end{enumerate}
Note that $F$ is required to be continuous at $p$ in the first place to guarantee that for all $(V,\psi)$ at $F(p)$ there exists some $(U,\phi)$ at $p$ such that $F(U)\subseteq V$. Immediately, one can check that if $F$ is smooth at  $p$, then 
\begin{enumerate}[label=(\alph*)]
  \item For all charts $(U,\phi),(V,\psi)$ at $p,F(p)$, the function $\psi \circ F\circ \phi ^{-1}:\phi(F^{-1}(V)\cap U )\subseteq \R^m\rightarrow \psi  (V )\subseteq \R^n$ is smooth at $p$. 
  \item If $G:N\rightarrow R$ is another map smooth at $F(p)$, then $G\circ F:M\rightarrow R$ is smooth at $p$.  
\end{enumerate}
When $F$ is a $\R^n$-valued function on $M$, we define the smoothness of  $F$ by considering  $\R^n$ as a manifold with the standard atlas  $\set{(\R^n,\textbf{id})}$. Now, let $C^{\infty}(M,\R^n)$ be the set of smooth function from $M$ to $\R^n$. We know  $C^{\infty}(M,\R^n)$ form an algebra as one can check.
\end{mdframed}
\section{Lie Group}
\begin{mdframed}
By a \textbf{Lie group}, we mean a smooth manifold equipped with a group structure such that the inversion and group addition are both smooth map, or equivalently, that 
\begin{align*}
M^2\rightarrow  M; (g,h)\mapsto gh^{-1}
\end{align*}
is smooth. 
\end{mdframed}
\section{Partition of Unity}
\begin{mdframed}
For each chart $(U,\varphi)$, we can let $k:\R^m\rightarrow \R$ be a bump function whose support lies inside $\phi(U)$
\end{mdframed}
\chapter{Beauty}
\section{Fundamental Theorem of Algebra}
\begin{theorem}
\textbf{(Fundamental Theorem of Algebra)}
\end{theorem}
\section{Euler's Formula}
\begin{mdframed}
Suppose that we define 
\begin{align*}
  \hspace{2cm}\exp(z)&\triangleq \sum_{n=0}^\infty \frac{z^n}{n!}\hspace{1.5cm}(z\inc)\\
  \hspace{2cm}\sin (z)&\triangleq \sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)!}z^{2n+1}\hspace{1.5cm}(z\inc)\\
  \hspace{2cm}\cos (z)&\triangleq \sum_{n=0}^\infty \frac{(-1)^n}{(2n)!}z^{2n}\hspace{1.5cm}(z\inc)
\end{align*}
Some properties we are familiar with is now easily seen using basic technique we learned in Chapter 3. 
\begin{enumerate}[label=(\alph*)]
  \item $\exp(z),\sin(z),\cos(z)$ are well defined on $\C$ by \myref{Cauchy-Hadamard}{Cauchy-Hadamard}.  
  \item $\begin{cases}
    \exp(x)\\
    \sin(x)\\
    \cos(x)
  \end{cases}\inr$ provided $x\inr$. 
  \item $\begin{cases}
    \exp(0)=1 \\
    \sin(0)=0\\
    \cos (0)=1
  \end{cases}$
  \item $\exp(x)$ is strictly increasing on $\R\hspace{0.5cm}(\because \abso{y^n-x^n}\leq \abso{y^n}+\abso{x^n})$
  \item $\exp(x)\nearrow \infty$ as $x\to \infty\hspace{0.5cm}(x\inr)$ 
  \item $\exp(x)\cdot \exp(y)=\exp(x+y)\hspace{0.5cm}(x\inc)$ by \myref{Merten's Theorem of Cauchy Product}{Merten Cau}
  \item $\exp (x)\searrow 0$ as $x \to -\infty\hspace{0.5cm}(x\inr)$
  \item $\begin{cases}
\frac{d}{dz}\exp(z)=\exp(z)\\
  \frac{d}{dz}\sin (z)=\cos (z)\\
  \frac{d}{dz}\cos (z)=- \sin (z)
  \end{cases}(z\inc)$, using \myref{Term-by-Term Differentiation}{AfaS}. 
  \item $\exp (x)$ is convex on $\R\hspace{0.5cm}(\because (e^x)''=e^x>0)$  
  \item $\exp (nz)=\big(\exp (z) \big)^n\hspace{0.5cm}(z\inc,n\inz)$, by induction and \myref{Merten's Theorem of Cauchy Product}{Merten Cau}. 
\end{enumerate}
In particular, we have \textbf{Euler's Formula}. 
\end{mdframed}
\begin{theorem}
\textbf{(Euler's Formula)}  
\begin{align*}
\hspace{2cm}\exp(iz)=\cos (z) + i \sin (z)\hspace{1.5cm}(z\inc)
\end{align*}
\end{theorem}
\begin{proof}
Define 
\begin{align*}
I(n)\triangleq \begin{cases}
  1& \text{ if $n \equiv 0$ (mod $4$) }\\
  i& \text{ if $n \equiv 1$ (mod $4$) }\\
  -1& \text{ if $n \equiv 2$ (mod $4$) }\\
  -i& \text{ if $n \equiv 3$ (mod $4$) }
\end{cases}
\end{align*}
Compute 
\begin{align}
\label{expiz}
\exp(iz)&=\sum_{n=0}^\infty \frac{(iz)^n}{n!}\notag\\ 
&=\sum_{n=0}^{\infty} \frac{I(n)z^n}{n!}\\
&=\sum_{n=0}^{\infty} \frac{I(2n)}{(2n)!}z^{2n}+\frac{I(2n+1)}{(2n+1)!}z^{2n+1}\hspace{0.5cm}\big(\because \text{ this is a sub-sequence of \myref{(}{expiz})}\big)\notag\\
&= \sum_{n=0}^{\infty} \frac{(-1)^{n}}{(2n)!}z^{2n}+ i\cdot \frac{(-1)^n}{(2n+1)!}z^{2n+1} \notag
\end{align}
Now, we can conclude 
\begin{align*}
\cos (z)+i \sin (z)&= \sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n)!}z^{2n} + i \cdot \sum_{n=0}^{\infty} \frac{(-1)^{n}}{(2n+1)!}z^{2n+1}\\
&=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n)!}z^{2n} +  \sum_{n=0}^{\infty} i\cdot  \frac{(-1)^{n}}{(2n+1)!}z^{2n+1} \\
&=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n)!}z^{2n} +  i\cdot  \frac{(-1)^{n}}{(2n+1)!}z^{2n+1} =\exp (iz)
\end{align*}
\end{proof}


\section{Equivalent Definitions of Exponential Functions}
\begin{theorem}
\textbf{(First Characterization)} for all $x\inr$, the sequence $\set{(1+\frac{x}{n})^n}_{n\inn}$ has limit 
\begin{align*}
\lim_{n\to \infty} \Big(1+\frac{x}{n}\Big)^n = \sum_{k=0}^\infty \frac{x^k}{k!}
\end{align*}
\end{theorem}
\begin{proof}
The proof is trivial if $x=0$. First suppose  $x\inr^+$. Set 
\begin{align*}
t_n\triangleq \Big(1+\frac{x}{n}\Big)^n\text{ and }s_n\triangleq \sum_{k=0}^n \frac{x^k}{k!}
\end{align*}
We wish to show 
\begin{align*}
\vi{\limsup_{n\to\infty} t_n\leq \lim_{n\to \infty}s_n\leq \liminf_{n\to\infty} t_n}
\end{align*}
We first prove 
\begin{align*}
\blue{\limsup_{n\to\infty} t_n\leq \lim_{n\to \infty}s_n}
\end{align*}
Use Binomial Theorem to compute 
\begin{align*}
t_n=\Big(1+\frac{x}{n} \Big)^n&= \sum_{k=0}^n \binom{n}{k} \Big(\frac{x}{n} \Big)^k\\
&=\sum_{k=0}^n \frac{x^k}{k!} \cdot \frac{n(n-1)\cdots (n-k+1)}{n^k} \leq \sum_{k=0}^n \frac{x^k}{k!}=s_n \bdone
\end{align*}
We now prove 
\begin{align*}
\olive{\lim_{n\to \infty}s_n\leq \liminf_{n\to\infty} t_n}
\end{align*}
Fix $\epsilon $. Because $s_n\nearrow $, we know there exists  $m$ such that 
 \begin{align*}
s_m> \lim_{n\to \infty}s_n -\epsilon 
\end{align*}
Fix such $m$. Observe 
 \begin{align*}
\hspace{1.5cm}t_n=\sum _{k=0}^n \binom{n}{k}\Big(\frac{x}{n} \Big)^k\geq \sum_{k=0}^m \frac{x^k}{k!}\cdot \frac{n(n-1)\cdots (n-k+1)}{n^k}\hspace{1.5cm}(n\geq m)
\end{align*}
Clearly, there exists $N$ such that 
\begin{align*}
  \forall n>N, \abso{\Big( \sum_{k=0}^m \frac{x^k}{k!}\cdot \frac{n(n-1)\cdots (n-k+1)}{n^k}\Big) - s_m }\leq  s_m-(\lim_{n\to \infty}s_n-\epsilon )
\end{align*}
Then, we see for all $n>\max \set{N,m}$
\begin{align*}
  t_n\geq \sum_{k=0}^m \frac{x^k}{k!}\cdot \frac{n(n-1)\cdots (n-k+1)}{n^k} &\geq s_m - (s_m-\lim_{n\to \infty}s_n -\epsilon )\\
  &=\lim_{n\to \infty}s_n -\epsilon \odone \vdone
\end{align*}
For negative real, we only wish to show 
\begin{align*}
\vi{\sum_{n=0}^\infty \frac{(-x)^n}{n!}=\Big(\sum_{n=0}^\infty \frac{x^n}{n!} \Big)^{-1}}\text{ and }\blue{\lim_{n\to \infty}\Big(1-\frac{x}{n} \Big)^{n}=\Bigg(\lim_{n\to\infty    }\Big(1+\frac{x}{n} \Big)^{n} \Bigg)^{-1}}
\end{align*}
Because the convergence is absolute, we can use Merten's Theorem for Cauchy product (\myref{Theorem}{Merten Cau})to compute 
\begin{align*}
\Big( \sum_{n=0}^{\infty} \frac{(-x)^n}{n!}\Big)\cdot \Big( \sum_{n=0}^\infty \frac{x^n}{n!} \Big)&= \sum_{n=0}^\infty \Big(\sum_{k=0}^n \binom{n}{k}(-1)^{n-k} \Big) \frac{x^n}{n!}\\
&=\sum_{n=0}^\infty (1-1)^{n} \frac{x^n}{n!}=1 \vdone
\end{align*}
We first prove 
\begin{align*}
\olive{\lim_{n\to \infty}\Big(1-\frac{x^2}{n^2} \Big)^n=1}
\end{align*}
Computing term by term, it is clear that 
\begin{align*}
\limsup_{n\to\infty} \Big(1-\frac{x^2}{n^2} \Big)^n \leq 1
\end{align*}
Using Bernoulli's Inequality (\myref{Theorem}{Bernoulli's Inequality}), we see that for large enough $n$, we have 
\begin{align*}
  \Big(1-\frac{x^2}{n^2} \Big)^n\geq 1-\frac{x^2}{n} 
\end{align*}
This then implies 
\begin{align*}
\liminf_{n\to\infty} \Big(1-\frac{x^2}{n^2} \Big)^n \geq 1\odone
\end{align*}
Compute
\begin{align*}
\lim_{n\to \infty}\Big(1-\frac{x}{n} \Big)^{n} &= \lim_{n\to \infty}\Bigg(\frac{1-\frac{x^2}{n^2}}{1+\frac{x}{n}}\Bigg)^n \\
&=\frac{\lim_{n \to \infty} (1-\frac{x^2}{n^2})^n}{\lim_{n\to \infty}(1+\frac{x}{n})^n}=\frac{1}{\lim_{n\to \infty}(1+\frac{x}{n})^n}\bdone
\end{align*}





\end{proof}
\begin{mdframed}
\begin{align*}
\ln (x)\triangleq \int_1^x \frac{1}{t}dt
\end{align*}
By FTC (\myref{Theorem}{FTC1}), it is easy to see that 
 \begin{align*}
\hspace{1.5cm}\frac{d}{dx}\ln (x)=\frac{1}{x}\hspace{1.5cm}(x\inr^+)
\end{align*}
To see 
\begin{align*}
\ln (xy)=\ln (x)+ \ln (y)
\end{align*}
Fix $y\inr^+$ and set 
\begin{align*}
f(x)\triangleq \ln (x)\text{ and }g(x)\triangleq \ln(xy)
\end{align*}
Conclude $f'(x)=g'(x)$, and use FTC (\myref{Theorem}{FTC2}) to conclude $f-g$ is some fixed constant $k$. Now, see that 
\begin{align*}
g(1)=f(1)+k \implies k=\ln(y)
\end{align*}
Then, we have 
\begin{align*}
\ln(xy)=g(x)=f(x)+k=\ln (x)+ \ln (y)
\end{align*}
Using induction, it is now easy to see 
\begin{align*}
  \hspace{3cm}\ln (x^n)=n \ln (x)\hspace{2.5cm}(n\inz_0^+)
\end{align*}
\end{mdframed}
\begin{theorem}
\textbf{(Second Characterization)}
\end{theorem}
\section{Equivalent Definitions of Trigonometric Functions}
\section{Equivalent Definitions of Gamma and Beta Functions}
\section{Prime Number Theorem}


\chapter{and the Beast}
\section{Topologist's Sine Curve}
\label{Topologist's Sine Curve}
\section{Long Line}
\label{Long Line}
\section{Bug-Eyed Line}
\label{Bug-Eyed Line}
\begin{abstract}
This section introduce Bug-Eyed Line, which is a second-countable locally Euclidean space that is not Hausdorff.
\end{abstract}
\begin{mdframed}
Let $a,b$ be distinct real numbers and let $\R'$ be the quotient space of $\R\times \set{a,b}$ where 
\begin{align*}
  (x,a)\sim (x,b)\text{ for all $x\neq 0$ }
\end{align*}
The space $\R'$ is commonly referred as the  \textbf{Bug-Eyed Line}. Now by definition of quotient topology, given a subset $E$ of  $\R'$, $E $ is open if and only if 
\begin{align*}
  \set{x\inr:[(x,a)]\in E}\text{ and }\set{x\inr:[(x,b)]\in E}\text{ are both open in }\R
\end{align*}
With this in mind, it is clear that $\R'$ is not Hausdorff, since  $[(0,a)]$ and $[(0,b)]$ can not be distinguished, and it is straightforward to check $\R'$ is locally Euclidean.\\


Now, suppose we write some subset $A$ of $\R\times \set{a,b}$ in the form  
\begin{align*}
\set{(x,a):x \in A_1}\cup  \set{(x,b):x \in A_2}
\end{align*}
We can deduce
\begin{align*}
\pi^{-1}(\pi (A))=\bset{(x,a): x\in A_1 \cup (A_2\setminus \set{0})}\cup  \bset{(x,b):x \in A_2 \cup  (A_1\setminus \set{0})}
\end{align*}
It is then clear that $\pi$ is an open mapping, thus \customref{Basis and Quotient}{$\R'$ is second countable}.
\end{mdframed}

\section{Weierstrass Function}
\section{Fabius Function}
\section{Vitali Set}
\label{Vitali Set}
\begin{abstract}
This section construct the Vitali Set in $\R^d$ for reference.
\end{abstract}
\begin{mdframed}
Fix $p>m>0\inr$. Define an equivalence relation in $[0,m]^d$ by 
\begin{align*}
\textbf{x}\sim  \textbf{y}\overset{\triangle}{\iff }\textbf{x}-\textbf{y}\inq^d
\end{align*}
Using axiom of choice, we can let $E\subseteq [0,m]^d$ contain exactly one element in each equivalence class. Enumerate $([0,p]\cap \Q)^d$ by $\textbf{z}_n$. For each $n\inn$, define 
\begin{align*}
E+\textbf{z}_n \triangleq \set{\textbf{x}+\textbf{z}_n\inr^d: \textbf{x}\in E}
\end{align*}
It is straightforward to check from definition of $E$ that 
\begin{enumerate}[label=(\alph*)]
  \item $E+\textbf{z}_n$ are pairwise disjoint. 
  \item $[0,m]^d \subseteq \bigsqcup_n (E+\textbf{z}_n)\subseteq [0,m+p]^d$. 
\end{enumerate}
Now, if $E$ is really measurable, we see that 
\begin{align}
\label{con}
0<m^d\leq \sum_n \abso{E}\leq (m+p)^d
\end{align}
since 
\begin{align*}
\abso{\bigsqcup  E+\textbf{z}_n}= \sum_n \abso{E+\textbf{z}_n}=\sum_n \abso{E}
\end{align*}
Observing that \myref{Equation}{con} is impossible implies $E$ is not measurable.
\end{mdframed}

\section{Cantor Set}
\label{Cantor Set}
\begin{abstract}
This section construct the classical ternary Cantor set and some of its variant, and prove they are uncountable and perfect. 
\end{abstract}
\begin{mdframed}
By the term \textbf{Classical Ternary Cantor Set} $\mathcal{C}$, one usually mean
\begin{align*}
\mathcal{C}\triangleq \bigcap_{n\inn}\mathcal{C}_n
\end{align*}
where $\mathcal{C}_0 \triangleq [0,1]$, and $\mathcal{C}_{k+1}$ is the result of deleting the open middle third of each connected component -- which are clearly compact -- of $\mathcal{C}_k$. Immediately, one can see that  
\begin{enumerate}[label=(\alph*)]
  \item $\mathcal{C}_n$ has $2^n$ amount of connected components, which are all compact with the length of  $\frac{1}{3^n}$.
  \item $\mathcal{C}$ has zero measure, since $\abso{\mathcal{C}_{n+1}}=\abso{\mathcal{C}_n} - \frac{2^n}{3^{n+1}}$. 
  \item $\mathcal{C}$ is totally disconnected, since if $a,b \in \mathcal{C}$ are connected then $[a,b]\subseteq \mathcal{C}$, which is impossible by proposition (a). 
  \item $\mathcal{C}$ is perfect, since for each $\epsilon $ and $x\in\mathcal{C}$, there exists large enough $n$ such that the length of each connected component of $\mathcal{C}_n$ is smaller than $\epsilon $, and the end point of the connected component -- in which $x$ lies -- that isn't $x$ would belong to $\mathcal{C}$ and be $\epsilon $-close to $x$.  
  \item The endpoints $\set{x\in \mathcal{C}:x\text{ is the endpoint of some connected component of some }\mathcal{C}_n }$ are countable and dense in $\mathcal{C}$. 
\end{enumerate} 
With some tedious effort, one can see that $x\in\mathcal{C}$ if and only if 
\begin{align*}
x=\sum_{n=1}^{\infty} \frac{a_n}{3^n}\text{ for some }a_n \in \set{0,2}
\end{align*}
With base $3$ representation, one can use a diagonal argument to show $\mathcal{C}$ is uncountable. Another approach to show $\mathcal{C}$ is uncountable is to show \customref{NSP}{non-singleton perfect sets in $\R^n$ are uncountable}.
\end{mdframed}
\begin{theorem}
\label{NSP}
\textbf{(Non-Singleton Perfect Set in $\R^n$ is Uncountable)} Given a perfect set $E\subseteq \R^n$, if $E$ contain more than one element, then  $E$ must be uncountable.
\end{theorem}
\begin{proof}

\end{proof}
\begin{mdframed}
Notably, a variant of the Cantor set includes the \textbf{Fat Cantor Set}, which is constructed similarly to the classical ternary Cantor set, except that the removed open middle intervals at $n$stage is each of length $\delta^n$, where $0 < \delta < 3$. Note that the construction cannot be done if $\delta > 3$ and that the Fat Cantor Sets all have positive measure, are perfect and totally disconnected.  
\end{mdframed}
\section{Cantor-Lebesgue Function}
\begin{abstract}
This section construct the Cantor-Lebesgue Function for reference.
\end{abstract}
\begin{mdframed}
Consider the \customref{Cantor Set}{Classical Ternary Cantor Set $\mathcal{C}$}. Let 
\begin{align*}
\mathcal{D}_n \triangleq [0,1]\setminus \mathcal{C}_n\text{ for all }n
\end{align*}
For example,
\begin{align*}
\mathcal{D}_1=(\frac{1}{3},\frac{2}{3})\text{ and }\mathcal{D}_2= (\frac{1}{9},\frac{2}{9})\cup (\frac{1}{3},\frac{1}{2})\cup  (\frac{7}{9},\frac{8}{9})
\end{align*}
Because $\mathcal{C}_n$ has $2^n$ amount of connected components, we know  $\mathcal{D}_n$ has $2^n-1$ amount of connected components. Order these connected components by  $\set{I^n_j: 1\leq j\leq 2^n-1}$. We now define a sequence of function $f_n:[0,1]\rightarrow [0,1]$ by letting 
\begin{align*}
f_n(x)\triangleq \begin{cases}
0& \text{ if $x=0$ }\\
\frac{j}{2^n}& \text{ if $x\in \overline{I_j^n}$ }\\
1& \text{ if $x=1$ }
\end{cases}
\end{align*}
where $f_n$ is linear on  $\overline{\mathcal{D}_n}$. 
\end{mdframed}



\begin{mdframed}
For each $x\in [0,1]$ there exists a (not always unique) base 3 representation 
\begin{align*}
x=\sum_{n=1}^{\infty} \frac{a_n}{3^n}\text{ for some }a_n \in \set{0,1,2}
\end{align*}
The Cantor-Lebesgue Function $f:[0,1]\rightarrow [0,1]$ is defined as follows. Given $x= \sum_{n=1}^{\infty} \frac{a_n}{3^n}\in [0,1]$, if there exists some smallest $n$ such that $a_n=1$, define 
\begin{align*}
b_k\triangleq \begin{cases}
  0& \text{ if $k<n$ and $a_k=0$ }\\
  1& \text{ if $k<n$ and  $a_k=2$ } \\
  1& \text{ if $k=n$ }\\
  0& \text{ if $k>n$ }
\end{cases}
\end{align*}
It $1\not\in \set{a_n: n\inn}$, then we simply define 
\begin{align*}
b_n\triangleq \begin{cases}
  0& \text{ if $a_n=0$ }\\
 1& \text{ if $a_n=2$ } 
\end{cases}
\end{align*}
We then define 
\begin{align*}
f(x)\triangleq \sum_{n=1}^{\infty} \frac{b_n}{2^n}
\end{align*}
Note that if the base 3 representation of $x\in (0,1)$ has the trailing  $0$ or $2$, then the representation must not be unique, yet the procedure described above does give the same value $f(x)$ while the base 2 representation can be different. Some tedious effort can now be applied to show that 
\begin{enumerate}[label=(\alph*)]
  \item $f(\mathcal{C})=[0,1]$ where \customref{Cantor Set}{$\mathcal{C}$ is the classical ternary set}.  
  \item $f:[0,1]\rightarrow [0,1]$ is increasing on $[0,1]$.
\end{enumerate}
\end{mdframed}
\section{Volterra's Function}
\section{Peano Space-filling Curve}



\end{document}
