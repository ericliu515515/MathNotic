\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}


% Suppress tags for all equations
\mathtoolsset{showonlyrefs}

\title{\Huge{NCKU 112.1}\\Probability}
\author{\huge{Eric Liu}}
\date{}
\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak
\setcounter{chapter}{-1}

\chapter{Probability Space}
\section{Basic Definition for Background Space}
\begin{definition}
\label{0.1.1}
\textbf{(Definition of $\sigma$-Algebra)} We say  $(\Omega,\mathcal{G}\subseteq\power{\Omega})$ is a $\sigma$-algebra if 
\begin{equation}
\Omega \in\mathcal{G}
\end{equation}
\begin{equation}
X \in \mathcal{G}\implies \Omega \setminus X\in \mathcal{G} \text{ (Closed under complement) }
\end{equation}
\begin{equation}
\mathcal{A} \subseteq  \mathcal{G}\text{ and }\abso{\mathcal{A}}\leq \abso{\N}\implies \bigcup \mathcal{A}\in \mathcal{G}\text{ (Closed under countable union) }
\end{equation}
From now, we denote $\Omega\setminus X$ by $X^c$. We say $\mathcal{G}=\set{\varnothing,\Omega}$ is the trivial $\sigma$-algebra on $\Omega$. 
\end{definition}
\begin{theorem}
\label{0.1.2}
\textbf{(Basic Property of $\sigma$-Algebra)} Let $(\Omega,\mathcal{G})$ be a $\sigma$-algebra. Then we have
\begin{equation}
\varnothing \in \mathcal{G}
\end{equation}
\begin{equation}
\mathcal{A}\subseteq \mathcal{G}\implies \bigcap \mathcal{A} \in \mathcal{G}
\end{equation}
\begin{equation}
A,B\in \mathcal{G}\implies A\setminus B \in \mathcal{G}
\end{equation}
\end{theorem}
\begin{proof}
Observe $\varnothing=\Omega^c$, and observe $\bigcap \mathcal{A}=(\bigcup_{X \in \mathcal{A}}X^c)^c$, and observe $A\setminus B=A\cup B^c$
\end{proof}
\begin{lemma}
\label{0.1.3}
\textbf{(Intersection of $\sigma$-Algebras is a $\sigma$-Algebra)} Let $S$ be a set of $\sigma$-algebra over $\Omega$, then $\bigcap S$ is a $\sigma$-algebra. 
\end{lemma}
\begin{proof}
  $\Omega\in \bigcap S$ is trivial. Observe $A\in \bigcap S\implies  \forall \mathcal{G}\in S, A\in \mathcal{G}\implies \forall \mathcal{G}\in S, A^c\in \mathcal{G}\implies A^c \in\bigcap S$. Observe $\mathcal{A}\subseteq \bigcap S\text{ and }\abso{\mathcal{A}}\leq \abso{\N}\implies \forall\mathcal{G}\in S, \mathcal{A}\subseteq \mathcal{G}\implies \forall \mathcal{G}\in S, \bigcup \mathcal{A} \in\mathcal{G}\implies \bigcup A\in \bigcap S$.
\end{proof}
\begin{definition}
\label{0.1.4}
\textbf{(Definition of Generating $\sigma$-Algebra)} Let $\mathcal{F}\subseteq \power{\Omega}$. The $\sigma$-algebra generated by $\mathcal{F}$ is defined to be the smallest  $\sigma$-algebra that contain $\mathcal{F}$
\end{definition}
\fbox{\begin{minipage}{39em}
We have now gave enough tools to generate $\sigma$-algebra. 
\end{minipage}}
\fbox{\begin{minipage}{39em}
The following concern a class of measure space, called probability space.
\end{minipage}}
\begin{definition}
\label{0.1.5}
  \textbf{(Definition of Measure)} Let $\mathcal{G}$ be a $\sigma$-algebra over $\Omega$. Function $\mu:\mathcal{G}\rightarrow \R$ is called a measure if
\begin{equation}
\forall E\in \mathcal{G}, \mu(E)\geq 0\text{ (Nonnegative) }
\end{equation}  
\begin{equation}
\mu(\varnothing)=0
\end{equation}
\begin{equation}
\mathcal{F}\subseteq \mathcal{G}\text{ and }\abso{\mathcal{F}}\leq \abso{\N}\text{ and }\mathcal{F}\text{ is disjoint }\implies \mu(\bigcup  \mathcal{F})=\sum_{X\in \mathcal{F}} \mu(X)\text{ ($\sigma$ additivity) }
\end{equation}
\end{definition}
\begin{theorem}
\label{0.1.6}
\textbf{(Intended Property of Measure)} We have
\begin{equation}
A\subseteq B\in \mathcal{F}\implies \mu(A)\leq  \mu(B)
\end{equation}
\end{theorem}
\begin{proof}
Observe $\mu(B)=\mu(A\cup (B\setminus A))=\mu(A)+\mu (B\setminus A)\implies \mu(B)-\mu(A)=\mu(B\setminus A)\geq 0$
\end{proof}
\begin{definition}
\label{0.1.7}
\textbf{(Definition of Probability Measure)} A probability measure $P$ is a measure that satisfy $P(\Omega)=1$ 
\end{definition}
\begin{definition}
\label{0.1.8}
\textbf{(Definition of Probability Space)} A probability space is a triple $(\Omega,\mathcal{\mathcal{G}},P)$ where 
\begin{equation}
\Omega \text{ is a set called \textit{sample space} }
\end{equation}
\begin{equation}
\mathcal{G} \text{ is a $\sigma$-algebra over $\Omega$ called event space}
\end{equation}
\begin{equation}
P:\mathcal{G}\rightarrow [0,1]\text{ is a measure called probability measure }
\end{equation}
\end{definition}
\fbox{\begin{minipage}{39em}
Basically, probability measure "measure" the probability an event will happen. Normally we don't measure the probability a sample will happen, since it is undoable in some cases, for instance, shooting a dart to an infinite set. We only measure the probability that the outcome sample is in a set of certain samples, where the assignment of measure is outside of the system.   
\end{minipage}}


\fbox{\begin{minipage}{39em}
A simple example of a $\sigma$-algebra is
\begin{equation}
\Omega_2=\set{HH,HT,TH,TT},\mathcal{G}=\set{\varnothing,X,\set{HT,HH},\set{TH,TT}}
\end{equation}

Notice in this example, $\Omega$ is ought to be interpreted as tossing two coins and $\mathcal{G}$ is to observe the first coin is head or tail.\\

To expand the first example, we have another simple example:
\begin{equation}
\Omega_3=\set{HHH,HHT,HTH,HTT,THH,THT,TTH,TTT}
\end{equation}
Define
\begin{equation}
A_H:=\set{HHH,HHT,HTH,HTT}\text{ and }A_T:=\set{THH,THT,TTH,TTT}
\end{equation}
which is the information of tossing head or tail on first try.\\

Notice $A_T=A_H^c$. Define
\begin{equation}
A_{HH}:=\set{HHH,HHT}\text{ and }A_{HT}:=\set{HTH,HTT}
\end{equation}
\begin{equation}
A_{TH}:=\set{THH,THT}\text{ and }A_{TT}:=\set{TTH,TTT}
\end{equation}
so we have
\begin{equation}
A_H=A_{HH}\cup A_{HT}\text{ and }A_T=A_{TH}\cup A_{TT}
\end{equation}
Then we can define 
\begin{equation}
\mathcal{G}:= \set{\bigcup N: N\in \power{M}}
\end{equation}
where  $M=\set{A_{HH},A_{HT},A_{TH},A_{TT}}$\\

Notice we can define four $\sigma$-algebras by
\begin{equation}
\mathcal{F}_0=\set{\varnothing,\Omega},\mathcal{F}_1=\set{\varnothing,\Omega,A_T,A_H},\mathcal{F}_2=\mathcal{G},\mathcal{F}_3=\power{\Omega}
\end{equation}
then we have
\begin{equation}
\mathcal{F}_0\subset \mathcal{F}_1\subset \mathcal{F}_2 \subset \mathcal{F}_3
\end{equation}

The following concern Borel $\sigma$-algebra

$M=\set{X^{-1}[x]:x \in R(X)}$
\end{minipage}}
\begin{definition}
\label{0.1.9}
\textbf{(Definition of a Random Variable)} We say the function from $\Omega$ to $\R$ is a random variable, and often a random variable is defined on a fixed probability space, since the usage of random variable are mostly associated with a probability measure. 
\end{definition}
\fbox{\begin{minipage}{39em}
We now define 3 random variable for example from the last example of $\sigma$-algebra.\\

Let $S_0,u,d\inr^+$ and let $d<1<u$. We define three random variables $S_1,S_2,S_3$ on $\Omega_3$
 \begin{equation}
S_1(\omega)= \begin{cases}
  uS_0& \text{ if $\omega \in A_H$ }\\
  dS_0& \text{ if $\omega\inA_T$ }
\end{cases}S_2(\omega)=\begin{cases}
  u^2S_0& \text{ if $\omega\in A_{HH}$ }\\
  udS_0& \text{ if $\omega\in A_{HT}\cup A_{TH}$ }\\
  d^2S_0& \text{ if $\omega\in A_{TT}$ }
\end{cases}
\end{equation}
\begin{equation}
S_3(\omega)=\begin{cases}
  u^3S_0& \text{ if $\omega\in \set{HHH}$ }\\
  u^2dS_0& \text{ if $\omega\in \set{HHT,HTH,THH}$ }\\
  ud^2S_0& \text{ if $\omega\in \set{HTT,THT,TTH}$ }\\
  d^3S_0& \text{ if $\omega\in \set{TTT}$ }
\end{cases}
\end{equation}
Often, we just use $S$ to denote  $S(\omega)$. 
\end{minipage}}
\begin{definition}
\label{0.1.10}
\textbf{(Definition of Borel-Algebra)} The Borel-Algebra on $\R$, which we denote  $\mathcal{B}(\R)$ is the $\sigma$-algebra generated by all open interval of $\R$.\\

Some members of $\mathcal{B}(\R):$ 
 \begin{equation}
   (b,a),(a,\infty),\R
\end{equation}
\begin{equation}
  (b,a]=(b,\infty)\setminus (a,\infty)
\end{equation}
\begin{equation}
[a,\infty)=\R\setminus (-\infty, a) 
\end{equation}
\begin{equation}
[a,b]=[a,\infty)\setminus (b,\infty) 
\end{equation}
\begin{equation}
\set{a}=\R \setminus (-\infty, a)\cup (a,\infty)
\end{equation}
\end{definition}
\begin{theorem}
\label{0.1.11}
\textbf{(Construct  $\sigma$-Algebra with Random Variable)} Let $X$ be a random variable on  $\Omega$. We define
\begin{equation}
X^{-1}[B]=\set{\omega \in \Omega : X(\omega)\in B}
\end{equation}
and define the $\sigma$-algebra $\sigma(X)$ by
\begin{equation}
\sigma(X)=\set{X^{-1}[B]:B\in\mathcal{B}(\R)}
\end{equation}
We can verify $\sigma(X)$ is a $\sigma$-algebra.
\end{theorem}
\begin{proof}
Notice $\Omega= X^{-1}[\R]\in \sigma(X)$. Observe $(X^{-1}[B])^c=X^{-1}[B^c]\in \sigma (X)$. Let $C=\set{X^{-1}[B]:B\in \cc}\subseteq \sigma$ be countable. Observe  $\bigcup_{B\in C}X^{-1}[B]=X^{-1}[\bigcup_{B\in\cc }B]\in \sigma(X)$ 
\end{proof}
\fbox{\begin{minipage}{39em}
Notice $\sigma(S_1)=\mathcal{F}_1,\sigma(S_2)\neq \mathcal{F}_2,\sigma(S_3)\neq \mathcal{F}_3$.\\

Normally, we write $P(X=x_i)$ to denote $P(X^{-1}[x_i])$
\end{minipage}}
\begin{theorem}
\label{0.1.12}
\textbf{(Induce Measure on Borel Algebra by a Random Variable and a Probability Space)} Let $(\Omega,\mathcal{F},P:\mathcal{F}\rightarrow \R)$ be a probability space and $X:\Omega\rightarrow \R$ be a random variable. We can induce a measure on  $\mathcal{B}(\R)$ (we say this measure is induced by $X$ and  $P$)
\begin{equation}
  \mathcal{L}_X:\mathcal{B}(\R)\rightarrow \R, B\mapsto P(X^{-1}[B])
\end{equation}
\end{theorem}
\begin{proof}
Because $P$ is nonnegative, we know  $\mathcal{L}_X$ is nonnegative. Notice that $X^{-1}[\varnothing]=\varnothing$, so we know $\mathcal{L}_X(\varnothing)=P(\varnothing)=0$. Let $\mathcal{A}\subseteq \mathcal{B}(\R)$ and $\mathcal{A}$ be countable and disjoint. We have
\begin{align}
\mathcal{L}_X(\bigcup \mathcal{A})&=P(X^{-1}[\bigcup \mathcal{A}])\\
&=P(\bigcup \mathcal{C})\text{ where $\cc=\set{X^{-1}[a]:a \in \mathcal{A}}$ }\\
&=\sum_{c \in \cc} P(c)\\
&=\sum_{a \in \mathcal{A}}P(X^{-1}[a])=\sum_{a\in\mathcal{A}}\mathcal{L}_X(a)
\end{align} 
Notice that $\cc$ is countable because $\mathcal{A}$ is countable, and $\cc$ is disjoint because $x\in X^{-1}[a]\cap X^{-1}[b]\implies X(x)\in a\cap b$.
\end{proof}
\begin{definition}
\label{0.1.13}
\textbf{(Definition of Cumulative Distribution Function)} We say $F_X:\R\rightarrow \R$ is the cumulative distribution function of  $X$ if
 \begin{equation}
   F_X(x)=\mathcal{L}_X((-\infty, x])=P(X^{-1}[(-\infty,x]])
\end{equation}
\end{definition}
\fbox{\begin{minipage}{39em}

Distribution function is a function on the image of a random variable $X$, that map a real number to the probability the random variable $X$ is the real number $x_1$. How? Probability is the measure of the set that can be mapped to $x_1$. \\

In a more formal language, the random variable doesn't specify the probability of each event, but the one does, probability measure is often implicitly given by the random variable.
\end{minipage}}
\\
\fbox{\begin{minipage}{39em}
We now use the random variable $S_3:\Omega \rightarrow \R $ on $(\Omega,\mathcal{F},P:\power{\Omega}\rightarrow \R)$ to serve as an example of induced measure and cumulative distribution function.\\

Notice that the range of $S_3$ is $\set{d^3S_0<d^2uS_0<du^2S_0<u^3S_0}$. We have 
\begin{equation} F_{S_3}(d^3S_0)=\mathcal{L}_{S_3}((-\infty,d^3S_0])=P(S_3^{-1}[(-\infty,d^3S_0]])=P(\set{TTT})
\end{equation}
\begin{equation} F_{S_3}(d^2uS_0)=\mathcal{L}_{S_3}((-\infty,d^2uS_0])=P(S_3^{-1}[(-\infty,d^2uS_0]])=P(\set{TTT,TTH,THT,HTT})
\end{equation}
\begin{equation} F_{S_3}(du^2S_0)=\mathcal{L}_{S_3}((-\infty,du^2S_0])=P(S_3^{-1}[(-\infty,du^2S_0]])=P(\Omega \setminus \set{HHH} )
\end{equation}
\begin{equation} F_{S_3}(u^3S_0)=\mathcal{L}_{S_3}((-\infty, u^3S_0])=P(S_3^{-1}[(-\infty,u^3S_0]])=P(\Omega)
\end{equation}
With above deduction, we can precisely deduce
\begin{equation}
F_{S_3}(x)=\begin{cases}
  0& \text{ if  }x<d^3S_0\\
  P(\set{TTT})& \text{ if  }d^3S_0\leq x<d^2uS_0\\
  P(\set{TTT,TTH,THT,HTT})& \text{ if  }d^2uS_0\leq x<du^2S_0\\
  P(\Omega \setminus \set{HHH})& \text{ if  }du^2S_0\leq x<u^3S_0\\
  P(\Omega)& \text{ if  }u^3S_0\leq x
\end{cases}
\end{equation}
Notice that we can define measure $P$ in multiple ways, and then we will have different induced measures $\mathcal{L}_{S_3}$ and different cumulative distribution function $F_{S_3}(x)$. For instance, one can check that $P:\power{\Omega}=\mathcal{F}_3\rightarrow \R,X\mapsto \frac{\abso{X}}{8}$ is a measure, and $P:\mathcal{F}_3\rightarrow \R,X\mapsto \sum_{x\in X}f(x)$ where $f$ is defined by $\omega_1\omega_2\omega_3\mapsto \frac{2^i}{3^3}: i=\abso{\set{\omega_j:\omega_j=T,j=1,2,3}}$ are all measures, but the former is 
\begin{equation}
F_{S_3}(x)=\begin{cases}
  0& \text{ if  }x<d^3S_0\\
  \frac{1}{8}& \text{ if  }d^3S_0\leq x<d^2uS_0\\
  \frac{1}{2}& \text{ if  }d^2uS_0\leq x<du^2S_0\\
  \frac{7}{8}& \text{ if  }du^2S_0\leq x<u^3S_0\\
  1& \text{ if  }u^3S_0\leq x
\end{cases}
\end{equation}
and the latter is
\begin{equation}
F_{S_3}(x)=\begin{cases}
  0& \text{ if  }x<d^3S_0\\
  \frac{8}{27}& \text{ if  }d^3S_0\leq x<d^2uS_0\\
  \frac{8}{27}+\binom{3}{1}\frac{4}{27}& \text{ if  }d^2uS_0\leq x<du^2S_0\\
  \frac{8}{27}+\binom{3}{1}\frac{4}{27}+\binom{3}{2}\frac{2}{27}& \text{ if  }du^2S_0\leq x<u^3S_0\\
  1& \text{ if  }u^3S_0\leq x
\end{cases}
\end{equation}
\end{minipage}}
\section{Binomial Asset Pricing Model}
\begin{definition}
\label{0.2.1}
\textbf{(Definition of Stochastic Process)} Given a common probability space $(\Omega,\mathcal{F},P:\mathcal{F}\rightarrow \R)$, we say an indexed family of random variables defined on our given probability space is a stochastic process, denoted
\begin{equation}
\set{X_t:t\in \Lambda,\omega \in \Omega}
\end{equation}
where the index set $\Lambda$ is called time horizon. So, if someone say $X$ is a stochastic process, we can say $X_t$ is a random variable, and we can say  $X(t,\omega)=X_t(\omega)\in \R$.\\

Additionally, we say $\Omega$ is "background space" and $\Lambda \times \R$, where we can draw a trajectory to record the sequence of outcome, is "foreground space" or "configuration space", and we say $\Theta=\set{X(t,\omega):t \in \Lambda, \omega \in \Omega}$ is the "state space" that contains "state of process"\\

The "frequency" of a state of the process $X(t,\omega)$ is ?
\end{definition}
\fbox{\begin{minipage}{39em}
We now give an example of stochastic process. Let $\Omega=\set{H,T}$ and $\Lambda = [0,2]$. The stochastic process if the indexed set $\set{X_t:t\in \Lambda}$ of random variables specified by
\begin{equation}
X_t(H)=\sin(2\pi t)\text{ and }X_t(T)=\sin(4\pi t)
\end{equation}
Then 
\begin{equation}
\Theta=[-1,1]
\end{equation}
Another example is: Let $\Lambda=\set{1,2,3,4,5}$ and let $\Omega=\set{(\omega_1,\omega_2,\omega_3,\omega_4,\omega_5):\forall i\in \Lambda, \omega_i\in \set{H,T}}$. The configuration space is then $\set{1,2,3,4,5}\times \R$, and we can specify the stochastic process $\set{S_t:t \in \Lambda}$ by
\begin{equation}
S_t((\omega_1,\omega_2,\omega_3,\omega_4,\omega_5))=u^id^{t-i}S_0: i=\abso{\set{\omega_j: 1\leq j\leq t,\omega_j= H}}
\end{equation}
Then 
\begin{equation}
\Theta= \set{u^id^jS_0:i,j\in \N\cup \set{0},1\leq i+j\leq 5} 
\end{equation}
\end{minipage}}
\\
\fbox{\begin{minipage}{39em}
To buy an option from someone means acquiring the right, but not the obligation, to purchase a stock at a predetermined price, $K$. This comes with an associated cost called the ``risk premium'' or $V_0$, which is paid at the time of the transaction.\\

Notice when we say net earn or profit, we are comparing his money with and without making the deal.\\

If the stock ends up at price $S_1(\omega)$  lower than $K$, the buyer won't buy the stock at the price $K$, so for seller, he profit $V_0$.\\

If the stock ends up at price $S_1(\omega)$ higher than  $K$, then the seller net earn:  $-(S_1(\omega)-K)+V_0$\\

Now, we consider the profit we can make for depositing money into bank and get interest. The interest rate is $r$.\\


If the stock ends up at price $S_1(\omega)$ lower than $K$, the buyer won't buy the stock at the price $K$, so for seller, he profit $(1+r)V_0$.\\

If the stock ends up at price $S_1(\omega)$ higher than  $K$, then the seller net earn:  $-(S_1(\omega)-K)+(1+r)V_0$\\

Now, we wish to lower the risk so we buy $\Delta_0$ share of the stock we short at price $S_0$ to form a portfolio.\\

If the stock ends up at price $S_1(\omega)$ lower than $K$, the buyer won't buy the stock at the price $K$, so for seller, he profit $(1+r)V_0+\Delta_0(S_1(\omega)-(1+r)S_0)$.\\

If the stock ends up at price $S_1(\omega)$ higher than  $K$, then the seller net earn:  $-(S_1(\omega)-K)+(1+r)V_0+\Delta_0(S_1(\omega)-(1+r)S_0)$\\

\end{minipage}}

\fbox{\begin{minipage}{39em}
From now, we denote the value of the option at time $t$
\begin{equation}
V_t(\omega):=\max \set{S_t(\omega)-K,0}
\end{equation}
So we can say the value of our portfolio $X_t(\omega)$, our action of selling the option and buy the stock, at time $1$ is 
\begin{equation}
X_1(\omega)=-V_1(\omega)+(1+r)V_0+\Delta_0(S_1(\omega)-(1+r)S_0)
\end{equation}
Now we wish, for all $\omega$, we have $X_1(\omega)\geq 0$, by negotiating with the buyer on $V_0$ and deciding how many share  $\Delta_0$ we want to buy.\\

Notice that if we negotiate $V_0$ at
 \begin{equation}
V_0:=\frac{1}{1+r}[\frac{1+r-d}{u-d}V_1(H)+\frac{u-(1+r)}{u-d}V_1(T)]
\end{equation}
and buy share $\Delta_0$
\begin{equation}
\Delta_0:= \frac{V_1(H)-V_1(T)}{S_1(H)-S_1(T)}
\end{equation}
We see 
\begin{multline}
X_1(H)=-V_1(H)+(1+r)\frac{1}{1+r}[\frac{1+r-d}{u-d}V_1(H)+\frac{u-(1+r)}{u-d}V_1(T)]\\ 
+\frac{V_1(H)-V_1(T)}{S_1(H)-S_1(T)}(S_1(H)-(1+r)S_0)
\end{multline}
equals $0$ by substituting  $S_1(H)\text{ and }S_1(T)$ with $S_0u\text{ and }S_0d$
\end{minipage}}
\chapter{Discrete Random Variable}
\section{Basic Discrete Random Variable and Their Expected Values and Variance}
\begin{definition}
\label{1.1.1}
\textbf{(Definition of Discrete Random Variable and Probability Mass Function)} Let $(\Omega,\mathcal{F},P:\mathcal{F}\rightarrow \R)$ be a probability space. We say a random variable $X:\Omega\rightarrow \R $ is a discrete random variable, if 
\begin{equation}
  X[\Omega]\text{ if countable }
\end{equation}
and we define the probability mass function $p_X$ as
\begin{equation}
p_X(r)=P(X=r)
\end{equation}
\end{definition}
\fbox{\begin{minipage}{39em}
In this section, $X$ is always a discrete random variable.
\end{minipage}}
\begin{theorem}
\label{1.1.2}
\textbf{(Property of Probability Mass Function)} Let $X[\Omega]=\set{x_1, \dots }$. We have
\begin{equation}
P(a\leq X\leq b)=\sum_{x_i\in [a,b]}p_X(x_i)
\end{equation}
This reads: The probability $X$ is in  $[a,b]$ equals the sum of the probability $X$ is a number in  $[a,b]$
\end{theorem}
\begin{theorem}
\label{1.1.3}
\textbf{(Property of Probability Mass Function)} We have
\begin{equation}
\sum_{x_i \in \R} p_X(x_i)=1
\end{equation}
\end{theorem}
\fbox{\begin{minipage}{39em}
Now we introduce expected value. 
\end{minipage}}
\begin{definition}
\label{1.1.4}
\textbf{(Definition of Expected Value)} We say the expected value of $X$ is 
 \begin{equation}
E(X)=\sum x_ip(x_i)
\end{equation}
If possible, normally we write
\begin{equation}
\mu = E(X)
\end{equation}
\end{definition}
\begin{theorem}
\label{1.1.5}
\textbf{(Expected Value)} For function $g:\R\rightarrow \R$, we have
\begin{equation}
E(g\circ X)=\sum g(x_i)p(x_i)
\end{equation}
\end{theorem}
\begin{theorem}
\label{1.1.6}
\textbf{(Linearity of Expected Value of Discrete Random Variable)} We have
\begin{equation}
E(\alpha _1 X_1 +\alpha_2 X_2)=\alpha_1 E(X_1)+\alpha _2 E(X_2)
\end{equation}
\end{theorem}
\fbox{\begin{minipage}{39em}
We now introduce variance.
\end{minipage}}
\begin{definition}
\label{1.1.7}
\textbf{(Definition of Variance)} We define variance of $X$ to be
\begin{equation}
\var (X):=E((X-\mu)^2)
\end{equation}
If possible, normally we write
\begin{equation}
  \sigma(X):=\sqrt{\var (X)}
\end{equation}
\end{definition}
\begin{theorem}
\label{1.1.8}
\textbf{(Calculation of Variance)} We have
\begin{equation}
\var (X)=E(X^2)-\mu^2
\end{equation}
\end{theorem}
\begin{proof}
\begin{align}
\var (X)&=E[(X-\mu^2)]\\
&=E[X^2-2\mu X +\mu^2]\\
&=E[X^2]-2\mu E[X]+\mu^2\\
&=E[X^2]-\mu^2
\end{align}
\end{proof}
\begin{theorem}
\label{1.1.9}
\textbf{(Variance)} We have
\begin{equation}
\var [\alpha_1 X_1]=\alpha _1^2\var [X_1]
\end{equation}
\end{theorem}
\begin{proof}
\begin{align}
\var [\alpha _1 X_1]&=E[(\alpha _1X_1)^2]-E[\alpha _1X_1]^2\\
&=\alpha _1^2E[X_1^2]-\alpha _1^2E[X_1]^2\\
&=\alpha _1^2 (E[X_1^2]-E[X_1]^2)=\alpha _1^2 \var [X_1]
\end{align}
\end{proof}
\fbox{\begin{minipage}{39em}
We now introduce Bernoulli random variable.
\end{minipage}}
\begin{definition}
\label{1.1.10}
\textbf{(Definition of Bernoulli Random Variable)} Let $A$ be an event and $P(A)=p$. We define
\begin{equation}
I_A(\omega)=\begin{cases}
  1& \text{ if  }\omega \in A\\
  0& \text{ if  }\omega\not\in A
\end{cases}
\end{equation}
\end{definition}
\begin{theorem}
\label{1.1.11}
\textbf{(Properties of Bernoulli Random Variable)} We have
\begin{gather}
P(I_A=1)=p \text{ and }P(I_A=0)=1-p\\ 
E[I_A]=p\\
\var [I_A]=p(1-p)
\end{gather}
\end{theorem}
\begin{proof}
\begin{equation}
\var [I_A]=E[I_A^2]-E[I_A]^2=1^2p-p^2=p-p^2=p(1-p)
\end{equation}
\end{proof}
\begin{definition}
\label{1.1.12}
\textbf{(Definition of Binomial Random Variable)} We say the binomial random variable $B_{n,p}$ is the number of success in $n$ independent trials, each of which is a success with probability $p$. We have
\begin{equation}
P(B_{n,p}=x)=\binom{n}{x}p^x (1-p)^{n-x}
\end{equation}
\end{definition}
\begin{theorem}
\label{1.1.13}
\textbf{(Properties of Binomial Random Variable)} We have
\begin{equation}
E[B_{n,p}]=np \text{ and }\var [B_{n,p}]= np(1-p)
\end{equation}
\end{theorem}
\begin{proof} 
\begin{equation}
E[B_{n,p}]=E[\sum I_A]=\sum E[I_A]=np
\end{equation}
That of variance is left unproved. 
\end{proof}
\fbox{\begin{minipage}{39em}
Now we introduce the notion of Poisson random variable.
\end{minipage}}
\begin{definition}
\label{1.1.14}
  \textbf{(Definition of Poisson Random Variable)} For $\ld >0$, we define Poisson random variable $X_{\ld }$ by
  \begin{equation}
  P(X_\ld =i)=\frac{\ld ^i}{i!}e^{-\ld }
  \end{equation}
\end{definition}
\begin{theorem}
\label{1.1.15}
\textbf{(Verification of Poisson Random Variable)} Let $X$ be a Poisson random variable with parameter $\ld $. We verify
\begin{equation}
\sum_{k=0}^\infty P(X=k)=1
\end{equation}
\end{theorem}
\begin{proof}
\begin{equation}
\sum _{k=0}^{\infty}P(X=k)= \sum_{k=0}^{\infty}\frac{\ld ^k}{k!}e^{-\ld }=e^{-\ld }\sum _{k=0}^{\infty}\frac{\ld ^k}{k!}=e^{-\ld }e^{\ld }=1
\end{equation}
\end{proof}
\begin{theorem}
\label{1.1.16}
\textbf{(Expectation Value of Poisson Random Variable)} Let $X$ be a Poisson random variable with parameter $\ld $. We have
\begin{equation}
E[X]=\ld 
\end{equation}
\end{theorem}
\begin{proof}
Compute
\begin{align}
E[X]&=\sum_{k=0}^\infty kP(x=k)\\
&=\sum_{k=0}^\infty ke^{-\ld }\frac{\ld ^k}{k!}\\
&=e^{-\ld }\sum_{k=0}^\infty k \frac{\ld ^k}{k!}\\
&=e^{-\ld}\sum_{k=1}^\infty k\frac{\ld ^k}{k!}\\
&=e^{\ld }\sum_{k=1}^\infty \ld  \frac{\ld ^{k-1}}{(k-1)!}\\
&=e^{\ld }\ld\sum_{u=0}^\infty \frac{\ld ^u}{u! }\\
&=\ld e^{\ld }e^{-\ld }=\ld 
\end{align}
\end{proof}
\begin{theorem}
\label{1.1.17}
\textbf{(Variance of Poisson Random Variable)} Let $X$ be a Poisson random variable with parameter $\ld $. We have
\begin{equation}
\var [X]=\ld 
\end{equation}
\end{theorem}
\begin{proof}
Compute 
\begin{align}
E[X^2]&=\sum_{k=0}^\infty k^2P(x=k)\\
&=\sum_{k=0}^\infty k^2e^{-\ld }\frac{\ld^k}{k!}\\
&=e^{-\ld } \sum_{k=1}^\infty k^2 \frac{\ld ^k}{k!}\\
&=\ld e^{-\ld }\sum_{k=1}^\infty k \frac{\ld^{k-1}}{(k-1)!}\\
&=\ld e^{-\ld }[\sum_{k=1}^\infty (k-1)\frac{\ld ^{k-1}}{(k-1)!}+\sum_{k=1}^\infty \frac{\ld ^{k-1}}{(k-1)!}]\\
&=\ld  e^{-\ld}[\sum_{k=2}^\infty (k-1) \frac{\ld ^{k-1}}{(k-1)!}+\sum_{k'=0}^\infty \frac{\ld ^{k'}}{k'!}]\\
&=\ld e^{-\ld }[\sum_{k=2}^\infty \ld \frac{\ld ^{k-2}}{(k-2)!}+e^{\ld }]\\
&=\ld e^{-\ld }[\ld \sum_{k=0}^\infty \frac{\ld ^k}{k!}+e^\ld ]\\
&=\ld e^{-\ld }[\ld e^{\ld }+e^{\ld }]\\
&=\ld (\ld +1)
\end{align}
So we have
\begin{equation}
\var [X]=E[X^2]-E[X]^2=\ld ^2+\ld -\ld ^2=\ld 
\end{equation}
\end{proof}
\fbox{\begin{minipage}{39em}
Now we introduce Poisson Approximation of Binomial Random Variable
\end{minipage}}
\begin{theorem}
\label{1.1.18}
\textbf{(Poisson Approximation of Binomial Random Variable)} Let $X$ be a binomial $B_{n,p}$ and let $\ld =np$. We have
\begin{equation}
  \lim_{n\to\infty}P(X=k)=e^{-\ld }\frac{\ld ^k}{k!}
\end{equation}
\end{theorem}
\begin{proof}
Compute
\begin{align}
P(X=k)&=\binom{n}{k}p^k (1-p)^{n-k}\\
&=\binom{n}{k}(\frac{\ld }{n})^k(1-\frac{\ld }{n})^{n-k}\\
&=\frac{n(n-1)\cdots (n-k+1)}{k!}(\frac{\ld ^k}{n^k})(1-\frac{\ld }{n})^n(1-\frac{\ld }{n})^{-k}\\
&=\frac{\ld ^k}{k!}(1-\frac{\ld }{n})^n(\frac{n(n-1)\cdots (n-k+1)}{n^k})(1-\frac{\ld }{n})^{-k}
\end{align}
So we have
\begin{align}
  \lim_{n\to\infty}P(X=k)&=\frac{\ld ^k}{k!}\lim_{n\to\infty}(1-\frac{\ld}{n})^n\lim_{n\to\infty}\frac{n(n-1)\cdots (n-k+1)}{n^k}\lim_{n\to\infty}(1-\frac{\ld}{n})^{-k}\\
&=\frac{\ld ^k}{k!}e^{-\ld }(1)(1)=e^{-\ld }\frac{\ld ^k}{k!}
\end{align}
\end{proof}
\fbox{\begin{minipage}{39em}
Now we introduce the geometric random variable. 
\end{minipage}}
\begin{definition}
\label{1.1.19}
\textbf{(Definition of Geometric Random Variable)} A geometric random variable counts the number of trials required for the first success in independent trials with success probability $p$. Let  $X$ be a geometric random variable. We have
\begin{equation}
P(X=k)=(1-p)^{k-1}p
\end{equation}
\end{definition}
\begin{theorem}
\label{1.1.20}
\textbf{(Verification of Geometric Random Variable)} Let $X$ be a geometric random variable with parameter $p$. We have
\begin{equation}
\sum_{k=1}^\infty P(X=k)=1
\end{equation}
\end{theorem}
\begin{proof}
\begin{align}
\sum_{k=1}^\infty P(X=k)&=\sum_{k=1}^\infty (1-p)^{k-1}p\\
&=p\sum_{k=1}^\infty (1-p)^{k-1}\\
&=p\sum_{k=0}^\infty (1-p)^k\\
&=p(\frac{1}{p})=1
\end{align}
\end{proof}
\begin{theorem}
\label{1.1.21}
\textbf{(Expected Value of Geometric Random Variable)} Let $X$ be a geometric random variable with parameter $p$. We have
 \begin{equation}
E[X]=\frac{1}{p}
\end{equation}
\end{theorem}
\begin{proof}
  Notice that the trials are independent. We have
  \begin{equation}
  E[X]=(p)(1)+(1-p)(E[X]+1)
  \end{equation}
  So we have
  \begin{gather}
 p(E[X])=p+1-p\\
 \implies E[X]=\frac{1}{p} 
  \end{gather}
\end{proof}
\begin{theorem}
\label{1.1.22}
\textbf{(Variance of Geometric Random Variable)} Let $X$ be a geometric random variable with parameter $p$. We have
\begin{equation}
\var [X]=\frac{1-p}{p^2}
\end{equation}
\end{theorem}
\begin{proof}
  Denote $1-p$ by $q$. First notice
\begin{equation}
\sum_{k=1}^\infty kq^{k-1}p=E[X]=\frac{1}{p}
\end{equation}
Then deduce
\begin{align}
E[X^2]&=\sum_{k=1}^\infty k^2q^{k-1}p\\
&=\sum_{k=1}^\infty (k-1+1)^2q^{k-1}p\\
&=\sum_{k=1}^\infty (k-1)^2q^{k-1}p+\sum_{k=1}^\infty 2(k-1)q^{k-1}p+\sum_{k=1}^\infty q^{k-1}p\\
&=q\sum_{k=1}^\infty (k-1)^2q^{k-2}p+2\sum kq^{k-1}p-\sum_{k=1}^\infty q^{k-1}p\\
&=q\sum_{u=1}^\infty u^2q^{u-1}p+2E[X]-p\sum_{k=1}^\infty q^{k-1}\\
&=qE[X^2]+2E[X]-\frac{p}{1-q}
\end{align}
Then we can deduce
\begin{equation}
 p(E[X^2])= (1-q)E[X^2]=2E[X]-\frac{p}{1-q}=\frac{2}{p}-\frac{p}{p}=\frac{2-p}{p}
\end{equation}
So in summary we have
\begin{equation}
\var [X]=E[X^2]-E[X]^2=\frac{2-p}{p^2}-(\frac{1}{p})^2=\frac{1-p}{p^2}
\end{equation}
\end{proof}
\begin{theorem}
\label{1.1.23}
\textbf{(Geometric Random Variable)} Let $X$ be a geometric random variable with parameter $p$. We have
\begin{equation}
P(X>n)=(1-p)^n
\end{equation}
\end{theorem}
\begin{proof}
Let $q=1-p$
\begin{align}
P(X>n)&=1-\sum_{k=1}^n P(X=k)\\
&=1-\sum_{k=1}^n q^{k-1}p\\
&=1-(1-q)\sum_{k=1}^n q^{k-1}\\
&=1-\sum_{k=1}^n q^{k-1}+\sum_{k=1}^n q^{k}\\
&=1-\sum_{k=0}^{n-1} q^k+\sum_{k=1}^n q^k\\
&=1+(q^n-1)=q^n
\end{align}
\end{proof}
\begin{corollary}
\label{1.1.24}
We have
\begin{equation}
  P(X>n+k\text{ while }X>n)=\frac{q^{n+k}}{q^{n}}=q^k
\end{equation}
\end{corollary}
\begin{theorem}
\label{1.1.25}
\textbf{(Summary)} We have
\begin{gather}
\text{ A Bernoulli random variable $I$ is a building block}\\
\text{ Parameter $p$ is the probability it happens }\\
E[I]=p \text{ and }\var [X]=p-p^2
\end{gather}\\

\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
         A Binomial random variable $B_{n,p}$ counts the success of multiple independent Bernoulli random variable (trials) of parameter $p$. \\
         Parameter $n$ is the number of trials and parameter $p$ is the success probability.\\
         $E[B_{n,p}]=np \text{ and }\var [B_{n,p}]=np(1-p)$
    \end{minipage}
\end{center}\\
\begin{gather}

\end{gather}
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
        A Poisson random variable  $X_{\ld }$ is a random variable that counts the number of success in a given time unit when it is expected $\ld $ amount of success would occur.\\

        Parameter $\ld $ is the expected amount of success in the given time unit.\\

        $E[X_\ld ]=\ld \text{ and }\var [X_\ld ]=\ld $
    \end{minipage}
\end{center}\\
\begin{gather}

\end{gather}
\begin{center}
    \begin{minipage}{0.9\linewidth}  
        \centering
        A geometric random variable $X_p$ is a random variable that counts the counts the number of trials required for the first success.\\
        
        Parameter  $p$ is the success rate of each independent trials.\\

        $E[X_p]=\frac{1}{p}\text{ and }\var [X_p]=\frac{1-p}{p^2}$
    \end{minipage}
\end{center}
\end{theorem}
\fbox{\begin{minipage}{39em}
$P$ measure is specified by the natural condition of experiment (if the coin is fair). $X$ is specified by what do you want to observe in the experiment.
\end{minipage}}
\chapter{Continuous Random Variable}
\section{Basic Knowledge}
\begin{definition}
\label{2.1.1}
\textbf{(Definition of Continuous Random Variable)} A continuous random variable $X:(\Omega,\mathcal{F},P)\rightarrow (\R,\mathcal{B}(\R))$ is a random variable such that 
\begin{equation}
X[\Omega]\text{ is uncountable }
\end{equation}
\end{definition}
\begin{definition}
\label{2.1.2}
\textbf{(Definition of Cumulative Distribution Function)} The cumulative distribution function $F_X$ is 
 \begin{equation}
F_X(x)=P(X\leq x)
\end{equation}
\end{definition}
\begin{definition}
\label{2.1.3}
\textbf{(Definition of Absolutely Continuous Random Variable)} By an absolutely continuous random variable, we mean a continuous random variable $X:(\Omega,\mathcal{F},P)\rightarrow (\R,\mathcal{B}(\R))$ such that
\begin{equation}
F_X\text{ is differentiable everywhere }
\end{equation}
\end{definition}
\begin{definition}
\label{2.1.4}
\textbf{(Definition of Probability Density Function)} Let $X$ be an absolutely continuous random variable. The probability density function $f_X$ for $X$ is defined to be
\begin{equation}
f_X(x)=F'_X(x)
\end{equation}
\end{definition}
\fbox{\begin{minipage}{39em}
Above is how we begin in our argument, we now develop some simple yet crucial tool. These tool enable us too calculate the probability of $X$ taking value in some Borel set $B$ by taking 
 \begin{equation}
P(X\in B)=\int_B f_X(s)ds
\end{equation}
\end{minipage}}
\begin{theorem}
\label{2.1.5}
\textbf{(Fundamental Theorem of Calculus)} We have
\begin{equation}
P(a\leq X\leq b)=\int_a^b f_X(s)ds
\end{equation}
\end{theorem}
\begin{proof}
\begin{align}
  P(a\leq X\leq b)&=P(X\leq b)-P(X<a)\\
&=P(X\leq b)-P(X\leq a)\\
&=F_X(b)-F_X(a)\\
&=\int_a^b F_X'(s)ds\\
&=\int_a^b f_X(x)ds
\end{align}
\end{proof}
\begin{corollary}
\label{2.1.6}
\textbf{(Fundamental Theorem of Calculus)} We have
\begin{equation}
P(a\leq X)=\int_a^\infty f_X(s)ds
\end{equation}
\end{corollary}
\begin{proof}
Because measure is countable additive, we have
\begin{align}
P(a\leq X)&=\lim_{n\to\infty}\sum_{k=0}^n P(a+k\leq X<a+k+1)?\\
&=\lim_{n\to\infty}\sum_{k=0}^n \int_{a+k}^{a+k+1}f_X(s)ds\\
&=\lim_{n\to\infty}\int_a^{a+n+1} f_X(s)ds\\
&=\int_a^\infty f_X(s)ds
\end{align}
\end{proof}
\begin{corollary}
\label{2.1.7}
\textbf{(Fundamental Theorem of Calculus)} We have
\begin{equation}
P(X\leq a)=\int_{-\infty}^a f_X(s)ds
\end{equation}
\end{corollary}
\begin{proof}
Because measure is countable additive, we have
\begin{align}
  P(X\leq a)&=\lim_{n\to\infty}\sum_{k=0}^n P(a-k-1<X\leq a-k)\\
&=\lim_{n\to\infty} \sum_{k=0}^n \int_{a-k-1}^{a-k} f_X(s)ds\\
&=\lim_{n\to\infty} \int^a_{a-n-1}f_X(s)ds\\
&=\int^a_{-\infty}f_X(s)ds
\end{align}
\end{proof}
\fbox{\begin{minipage}{39em}
The definition of expected value and variance of continuous random variable present.
\end{minipage}}
\begin{definition}
\label{2.1.8}
\textbf{(Definition of Expected Value)} Let $X$ be an absolutely continuous random variable. We define
 \begin{equation}
E[X]:=\int_{-\infty}^\infty s\times f_X(s)ds
\end{equation}
\end{definition}
\begin{definition}
\label{2.1.9}
\textbf{(Definition of Variance)} Let $X$ be an absolutely continuous random variable. We define
\begin{equation}
\var [X]=\int^\infty_{-\infty} (s-E[X])^2\times f_X(s)ds
\end{equation}
\end{definition}
\begin{theorem}
\label{2.1.10}
\textbf{(Formula for Variance)}
\begin{equation}
\var [X]=E[X^2]-E^2[X]
\end{equation}
\end{theorem}
\begin{proof}
\begin{align}
\var [X]&= \int^\infty_{-\infty}(s-E[X])^2 \times f_X(s)ds\\
&=\int^\infty_{-\infty} s^2 \times f_X(s) ds-2E[X]\int^\infty_{-\infty} s\times f_X(s)ds+\int_{-\infty}^\infty E^2[X]\times f_X(s)ds\\
&=E[X^2]-2E^2[X]+E^2[X]\\
&=E[X^2]-E^2[X]
\end{align}
\end{proof}
\fbox{\begin{minipage}{39em}
The below show case when a variable $Y$ depend on a random variable $X$ (thus $Y$ is a random variable): what is the correct way to compute $f_Y$ from $f_X$. Notice that \myref{Theorem}{2.1.11} is inspired by \myref{Theorem}{2.1.12}
\end{minipage}}
\begin{theorem}
\label{2.1.11}
\textbf{(Example)} Define
\begin{equation}
f_X(x):=\begin{cases}
  3x^2& \text{ if $x\in [0,1]$ }\\
  0& \text{ if otherwise }
\end{cases}\text{ and }g(x):=1-x^4\text{ and }Y=g\circ X
\end{equation}
We don't have
\begin{equation}
f_Y(y)=g^{-1}(f_X(g^{-1}(y)))
\end{equation}
\end{theorem}
\begin{proof}
We have
\begin{align}
F_Y(y)&=P(Y\leq y)\\
&=P(1-X^4\leq y)\\
&=P(1-y\leq X^4)\\
&=P(X\geq (1-y)^{\frac{1}{4}})\\
&=\int_{(1-y)^{\frac{1}{4}}}^\infty f_X(s)ds\\
&=\int_{(1-y)^{\frac{1}{4}}}^1 3s^2ds\\
&= \at{s^3}_{s=(1-y)^{\frac{1}{4}}}^1\\
&=1-(1-y)^{\frac{3}{4}}
\end{align}
This give us
\begin{align}
f_Y(y)&=\frac{d}{dy}F_Y(y)\\
&=\frac{d}{dy}1-(1-y)^{\frac{3}{4}}\\
&=\frac{3}{4}(1-y)^{\frac{-1}{4}}
\end{align}
While 
\begin{align}
g^{-1}(f_X(g^{-1}(y)))&= g^{-1}(f_X((1-y)^{\frac{1}{4}}))\\
&=g^{-1}(3(1-y)^{\frac{1}{2}})\\
&=(1-3(1-y)^{\frac{1}{2}})^{\frac{1}{4}}
\end{align}

\end{proof}
\begin{theorem}
\label{2.1.12}
\textbf{(Example)} Define 
\begin{equation}
f_X(x):=\begin{cases}
  1& \text{ if $x\in [0,1]$ }\\
  0& \text{ if elsewhere }
\end{cases}\text{ and }g(x):=3x\text{ and }Y=g\circ X
\end{equation}
\end{theorem}
\begin{proof}
We see 
\begin{align}
F_Y(y)&=P(Y\leq y)\\
&=P(3X\leq y)\\
&=P(X\leq \frac{y}{3})\\
&=F_X(\frac{y}{3})\\
&=\frac{y}{3}\text{ for $y\in [0,3]$ }
\end{align}
Then 
\begin{equation}
f_Y(y)=\begin{cases}
  \frac{1}{3}& \text{ if $y\in [0,3]$ }\\
  0& \text{ if otherwise }
\end{cases}
\end{equation}
And
\begin{equation}
g^{-1}(f_X(g^{-1}(y)))=g^{-1}f_X(\frac{y}{3}))=g^{-1}(1)=\frac{1}{3}
\end{equation}
\end{proof}
\section{Law of Unconscious Statistician}
\begin{theorem}
\label{2.2.1}
\textbf{(Law of Unconscious Statistician)} Let $Y=g(X)$. If $X$ is discrete, we have
\begin{equation}
E[Y]=\sum_{x\in X[\Omega]} g(x)p_X(x)
\end{equation}
If $X$ is continuous, we have
\begin{equation}
E[Y]=\int_{-\infty}^\infty g(x)f_X(x)dx
\end{equation}
\end{theorem}
\begin{proof}
We only prove when $X$ is continuous and $g$ is differentiable and monotone and $g^{-1}$ exists and is differentiable and monotone.\\

First notice
\begin{equation}
x=g^{-1}(y)\implies \frac{dx}{dy}=\frac{d}{dy}g^{-1}(y)\implies dx=\frac{d}{dy}g^{-1}(y)dy
\end{equation}
Notice
\begin{align}
F_Y(y)&=P(Y\leq y)\\
&=P(g(X)\leq y)\\
&=P(X\leq g^{-1}(y))\\
&=F_X(g^{-1}(y))
\end{align}
Then 
\begin{align}
f_Y(y)&= \frac{d}{dy}F_Y(y)\\
&=\frac{d}{dy}F_X(g^{-1}(y))\\
&=\frac{d}{dg^{-1}(y)}F_X(g^{-1}(y)) \frac{dg^{-1}(y)}{dy}\\
&=f_X(g^{-1}(y)) \frac{dg^{-1}(y)}{dy}
\end{align}
We now have
\begin{align}
\int_{-\infty}^\infty yf_Y(y)dy &=\int_{-\infty}^\infty yf_X(g^{-1}(y)) dg^{-1}(y)\\
&=\int_{-\infty}^\infty g(x)f_X(x)dx
\end{align}

\end{proof}
\section{Uniform, Exponential and Normal}
\fbox{\begin{minipage}{39em}
We now go into each continuous random variable. Start with the easiest, uniform random variable.
\end{minipage}}
\begin{definition}
\label{2.3.1}
\textbf{(Definition of Uniform Random Variable)} We say an absolutely continuous random variable $X\sim \text{Uni}(\alpha ,\beta )$ is uniform if there exists $\alpha <\beta \inr$ such that
\begin{equation}
f_X(x)=\begin{cases}
\frac{1}{\beta -\alpha }& \text{ if $x\in [\alpha ,\beta ]$ }\\
0& \text{ if elsewhere }
\end{cases}
\end{equation}
\end{definition}
\begin{theorem}
\label{2.3.2}
\textbf{(Expected Value and Variance of Uniform Random Variable)} Let $X\sim \text{Uni}(\alpha ,\beta )$. We have
\begin{equation}
E[X]=\frac{\alpha +\beta }{2}\text{ and }\var [X]=\frac{(\beta -\alpha )^2}{12}
\end{equation}
\end{theorem}
\begin{proof}
First verify for
\begin{equation}
\int_{-\infty}^\infty f_X(x)dx=\int_{\alpha }^\beta \frac{1}{\beta -\alpha }dx=\frac{\beta -\alpha }{\beta -\alpha }=1
\end{equation}
Then 
\begin{align}
E[X]&=\int_{\alpha }^\beta \frac{x}{\beta -\alpha }dx\\
&=\frac{1}{\beta -\alpha }\times \frac{x^2}{2} \Big|_{x=\alpha }^\beta \\
&=\frac{\beta ^2-\alpha ^2}{2(\beta -\alpha )}\\
&=\frac{\beta +\alpha }{2}
\end{align}
Then 
\begin{align}
E[X^2]&=\int_{\alpha }^\beta \frac{x^2}{\beta -\alpha }dx\\
      &=\frac{1}{\beta -\alpha }\times \frac{x^3}{3} \at_{x=\alpha }^\beta\\
      &=\frac{\beta ^3-\alpha ^3}{3(\beta -\alpha )}\\
      &=\frac{(\beta -\alpha )(\beta ^2+\beta \alpha +\alpha ^2)}{3(\beta -\alpha )}\\
      &=\frac{\beta ^2+\beta \alpha +\alpha ^2}{3}
\end{align}
Then
\begin{align}
\var [X]&=E[X^2]-E^2[X]\\
&=\frac{\beta ^2+\beta \alpha +\alpha ^2}{3}-\frac{\beta ^2+2\beta \alpha +\alpha ^2}{4}\\
&=\frac{\beta^2-2\beta \alpha +\alpha ^2}{12}\\
&=\frac{(\beta -\alpha )^2}{12}
\end{align}
\end{proof}
\fbox{\begin{minipage}{39em}
Second, exponential.
\end{minipage}}
\begin{definition}
\label{2.3.3}
\textbf{(Definition of Exponential Random Variable)} We say an absolute continuous random variable $X\sim \text{Exp}(\ld )$ is exponential if there exists $\ld\inr^+$ such that
\begin{equation}
f_X(x)=\begin{cases}
  \ld e^{-\ld x}& \text{ if $x\geq 0$ }\\
  0& \text{ if elsewhere }
\end{cases}
\end{equation}
\end{definition}
\begin{theorem}
\label{2.3.4}
\textbf{(Expected Value and Variance of Exponential Random Variable)} Let $X\sim \text{Exp}(\ld )$. We have 
\end{theorem}
\begin{proof}
Fist, verify for
\begin{align}
\int_{-\infty}^\infty f_X(x)dx&=\int_{0}^\infty \ld e^{-\ld x}dx\\
                              &=-e^{-\ld x}\at_{x=0}^\infty\\
                              &=-0-(-1)\\
                              &=1
\end{align}
We have
\begin{align}
E[X]&=\int_0^\infty x\ld e^{-\ld x}dx\\
&=-xe^{-\ld x}\at_{x=0}^\infty -\int_0^\infty -e^{-\ld x}dx\\
&=\int_0^\infty e^{-\ld x}dx\\
&=\frac{e^{-\ld x}}{-\ld } \at_{x=0}^\infty\\
&=0-(\frac{1}{-\ld })\\
&=\frac{1}{\ld }
\end{align}
And have
\begin{align}
E[X^2]&=\int_0^\infty x^2\ld e^{-\ld x}dx\\
&=-x^2e^{-\ld x}\at_{x=0}^\infty-\int_0^\infty -2xe^{-\ld x}dx\\
&=2\int_0^\infty xe^{-\ld x}dx\\
&=2(\frac{-1}{\ld })xe^{-\ld x}\at_{x=0}^\infty -2\int_0^\infty (\frac{-1}{\ld })e^{-\ld x}dx\\
&=\frac{2}{\ld }\int_{0}^\infty e^{-\ld x}dx\\
&=\frac{2}{\ld }(\frac{-1}{\ld })e^{-\ld x}\at_{x=0}^\infty\\
&=(\frac{-2}{\ld ^2})(0-1)=\frac{2}{\ld ^2}
\end{align}
Then we have
\begin{equation}
\var [X]=E[X^2]-E^2[X]=\frac{2}{\ld^2}-(\frac{1}{\ld })^2=\frac{1}{\ld ^2}
\end{equation}
\end{proof}
\begin{theorem}
\label{2.3.5}
\textbf{(Property of Exponential Random Variable)} Let $X\sim \text{Exp}(\ld )$. We see 
\begin{equation}
P(X\geq a )=e^{-a\ld }
\end{equation}
\end{theorem}
\begin{proof}
\begin{align}
P(X\geq a)&=\int_a^\infty \ld e^{-\ld x}dx\\
&=-e^{-\ld x}\at_{x=a}^\infty\\
&=e^{-a\ld }
\end{align}
\end{proof}
\begin{theorem}
\label{2.3.6}
\textbf{(Memory-less Property of Exponential Random Variable)}  Let $X\sim \text{Exp}(\ld )$. We see
\begin{equation}
P(X\geq  a+b \at  X\geq  b)=\frac{e^{-(a+b)\ld }}{e^{-b\ld }}=e^{-a\ld }
\end{equation}
\end{theorem}
\fbox{\begin{minipage}{39em}
If $X\sim \text{Exp}(\ld )$, it means the measurement of waiting time of an occurrence of event that is expected to happen after $\ld $ time period.\\

Before we start to talk about normal distribution. We first have to talk about Gauss Integral.
\end{minipage}}
\begin{theorem}
\label{2.3.7}
\textbf{(Gauss Integral)}
\begin{equation}
\int_{-\infty}^\infty e^{-x^2}dx=\sqrt{\pi} 
\end{equation}
\end{theorem}
\begin{proof}
Let $I=\int_{-\infty}^\infty e^{-x^2}dx$. We have
\begin{align}
I^2&=\int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(x^2+y^2)}dxdy\\
&=\int_0^\infty \int_0^{2\pi} re^{-r^2}d\theta dr\\
&=2\pi \int_0^{\infty}re^{-r^2}dr\\
&=2\pi \frac{1}{-2}e^{-r^2}\at_{r=0}^\infty\\
&=\pi
\end{align}
Then
\begin{equation}
I^2=\pi \implies I=\sqrt{\pi} 
\end{equation}
\end{proof}
\begin{theorem}
\label{2.3.8}
\textbf{(Gauss Integral)} 
\begin{equation}
\int_{-\infty}^\infty e^{\frac{-(x-a)^2}{b}}dx=\sqrt{b\pi} 
\end{equation}
\end{theorem}
\begin{proof}
Let $x=a+\sqrt{b}y$. We have 
\begin{equation}
\frac{dx}{dy}=\sqrt{b} 
\end{equation}
Then we have
\begin{align}
\int_{-\infty}^\infty e^{\frac{-(x-a)^2}{b}}dx&= \int_{-\infty}^\infty e^{-y^2}\sqrt{b} dy\\
&=\sqrt{b\pi} 
\end{align}
\end{proof}
\begin{theorem}
\label{2.3.9}
\textbf{(Gamma Function)} For $z\inr$, let 
\begin{equation}
\Gamma (z)=\int_0^\infty t^z e^{-t}dt
\end{equation}
\end{theorem}
\fbox{\begin{minipage}{39em}
Now we start to discuss normal distribution. 
\end{minipage}}
\begin{definition}
\label{2.3.10}
\textbf{(Definition of Normal Distribution)} A normal random variable $X\sim N(\mu,\sigma^2)$ is defined by
\begin{equation}
f_X(x)=\frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(x-\mu)^2}{2\sigma^2}}
\end{equation}
\end{definition}
\begin{theorem}
\label{2.3.11}
\textbf{(Expected Value and Variance of Normal Random Variable)} Let $X\sim N(\mu,\sigma^2)$. We have
\begin{equation}
E[X]=\mu \text{ and }\var [X]=\sigma^2
\end{equation}
\end{theorem}
\begin{proof}
We first verify 
\begin{align}
\int_{-\infty}^\infty f_X(x)dx&=\int_{-\infty}^\infty \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx\\
&=\frac{1}{\sigma \sqrt{2\pi} }\int_{-\infty}^\infty e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx\\
&=\frac{1}{\sigma \sqrt{2\pi} } \sqrt{2\sigma^2 \pi}=1
\end{align}
Observe
\begin{align}
E[X]&=\int_{-\infty}^\infty x \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx\\
&=\frac{1}{\sigma \sqrt{2\pi} }\int_{-\infty}^\infty (x-\mu +\mu)e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx\\
&=\frac{1}{\sigma \sqrt{2\pi} }\int_{-\infty}^\infty (x-\mu) e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx + \mu\int_{-\infty}^\infty \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx\\
&=\frac{1}{\sigma \sqrt{2\pi} }\int_{-\infty}^\infty ze^{\frac{-z^2}{2\sigma^2}}dz+\mu\\
&=\mu
\end{align}
Where 
\begin{equation}
\frac{1}{\sigma \sqrt{2\pi} }\int_{-\infty}^\infty ze^{\frac{-z^2}{2\sigma^2}}dz=0\text{ because }ze^{\frac{-z^2}{2\sigma^2}}\text{ is odd function }
\end{equation}
Observe
\begin{align}
E[X^2]&=\int_{-\infty}^\infty x^2 \frac{1}{\sigma \sqrt{2\pi} } e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx\\
      &=\int_{-\infty}^\infty (x-\mu)^2 \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx+2\mu \int_{-\infty}^\infty x \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx-\mu^2 \int_{-\infty}^\infty \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx\\
      &=\int_{-\infty}^\infty z^2 \frac{1}{\sigma\sqrt{2\pi} }e^{\frac{-z^2}{2\sigma^2}}dz+\mu^2\\
      &=2 \frac{\sigma^2}{\sqrt{\pi} }\int_{-\infty}^\infty s^2e^{-s^2}ds+\mu^2\text{ where }s=\frac{z}{\sqrt{2} \sigma}\\
      &=4 \frac{\sigma^2}{\sqrt{\pi} }\int_0^\infty s^2e^{-s^2}ds +\mu^2\\
      &=2 \frac{\sigma^2}{\sqrt{\pi} }\int_0^\infty t^{\frac{1}{2}} e^{-t}dt +\mu^2\text{ where }t=s^2\\
      &=2 \frac{\sigma^2}{\sqrt{\pi} }\Gamma(\frac{3}{2})+\mu^2\\
      &=2 \frac{\sigma^2}{\sqrt{\pi} } (\frac{\sqrt{\pi} }{2})+\mu^2\\
      &=\sigma^2+\mu^2
\end{align}
Then we have
\begin{equation}
\var [X]=E[X^2]-E^[X]=\sigma^2+\mu^2-\mu^2=\sigma^2
\end{equation}
\end{proof}
\fbox{\begin{minipage}{39em}
Now, we introduce how to do linear change of variable for normal distribution.
\end{minipage}}
\begin{theorem}
\label{2.3.12}
\textbf{(Linear Change of Normal Random Variable)} Let 
\begin{equation}
X\sim N(\mu,\sigma^2)\text{ and }Y=\alpha X+\beta \text{ where  }\alpha >0
\end{equation}
We have
\begin{equation}
Y\sim N(\alpha \mu+\beta , (\alpha \sigma)^2)
\end{equation}
\end{theorem}
\begin{proof}
Observe
\begin{align}
F_Y(y)&=P(Y\leq y)\\
&=P(\alpha X+\beta \leq y)\\
&=P(X\leq \frac{y-\beta }{\alpha })\\
&=\int_{-\infty}^{\frac{y-\beta }{\alpha }} \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(t-\mu)^2}{2\sigma^2}}dt
\end{align}
This give us
\begin{align}
f_Y(y)&=\frac{d}{dy}F_Y(y)\\
&=\frac{d \frac{y-\beta }{\alpha }}{dy}\frac{d}{d\frac{y-\beta }{\alpha }}\int_{-\infty}^{\frac{y-\beta }{\alpha }}\frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(t-\mu)^2}{2\sigma^2}}dt\\
&=\frac{1}{\alpha }\times \frac{1}{\sigma \sqrt{2\pi} }e^{\frac{-(\frac{y-\beta }{\alpha }-\mu)^2}{2\sigma^2}}\\
&=\frac{1}{\alpha \sigma \sqrt{2\pi} }e^{\frac{-(y-\beta -\alpha \mu)^2}{2\alpha ^2\sigma^2}}
\end{align}
This give us
\begin{equation}
Y\sim N(\alpha \mu+\beta , (\alpha \sigma)^2)
\end{equation}
\end{proof}
\begin{corollary}
\label{2.3.13}
\begin{equation}
X\sim N(\mu,\sigma^2)\text{ and }Z=\frac{X-\mu}{\sigma}\implies Z\sim N(0,1)
\end{equation}
\end{corollary}
\begin{definition}
\label{2.3.14}
\textbf{(Definition of Standard Normal Random Variable)} Let $Z\sim N(0,1)$. We denote
\begin{equation}
\Phi(z):=F_Z(z)
\end{equation}
Thus
\begin{equation}
\Phi(z)=F_Z(z)=\int_{-\infty}^z \frac{1}{1\sqrt{2\pi} }e^{\frac{-(s-0)^2}{2 1^2}}= \frac{1}{\sqrt{2\pi} }\int_{-\infty}^z e^{\frac{-s^2}{2}}ds
\end{equation}
\end{definition}
\begin{theorem}
\label{2.3.15}
\textbf{(Probability of Normal Variable within Confidence Level)} Let $X\sim N(\mu, \sigma^2)$. We have
\begin{gather}
P(X\in [\mu-\sigma,\mu+\sigma])=0.3173\\
P(X\in [\mu-2\sigma, \mu+2\sigma])=0.0455\\
P(X\in [\mu-3\sigma,\mu+3\sigma])=0.0027
\end{gather}
\end{theorem}
\begin{proof}
Let $Z=\frac{X-\mu}{\sigma}$. We have $Z\sim N(0,1)$. This give us
\begin{gather}
P(X\in [\mu-\sigma,\mu+\sigma])=P(Z\in [-1,1])=0.3173\\
P(X\in [\mu-2\sigma,\mu+2\sigma])=P(Z\in [-2,2])=0.0455\\
P(X\in [\mu-\sigma,\mu+\sigma])=P(Z\in [-3,3])=0.0027
\end{gather}
\end{proof}
\begin{theorem}
\label{2.3.16}
\textbf{(Example)} Let
\begin{equation}
X\sim N(2,25)
\end{equation}
Compute
\begin{equation}
P(1\leq X\leq 4)
\end{equation}
\end{theorem}
\begin{proof}
\begin{equation}
\mu=2\text{ and }\sigma=5
\end{equation}
\end{proof}
\section{Exercises}
\begin{question}{}{}
A ``dyadic rational number'' is a real number of the form $\frac{m}{2^k}$ where $k$ and $m$ are integers. Suppose we set $p = q = \frac{1}{2}$ in the construction for a probability measure on $\Omega_{\infty}$ and $X(\omega) = \sum_{i=1}^{\infty} \frac{Y_i(\omega)}{2^i}$ is a random variable on $\Omega_{\infty}$.

\begin{itemize}
  \item Show that, the induced measure $\mathcal{L}_X$ by the random variable $X$ on $\Omega$ satisfies that, for any positive integers $k$ and $m$ such that $0 \leq \frac{m-1}{2^k} < \frac{m}{2^k} \leq 1$, we have
  \[
  \mathcal{L}_X\left[\frac{m-1}{2^k}, \frac{m}{2^k}\right] = \frac{1}{2^k}.
  \]
  
  In other words, the induced measure $\mathcal{L}_X$ on all intervals in $[0, 1]$ whose endpoints are dyadic rational numbers is the same as the Lebesgue measure of these intervals. The only possible way is that $\mathcal{L}_X$ is indeed the Lebesgue measure.
  
  \item Show that, in this case ($p = \frac{1}{2}$), the random variable $X(\omega) = \sum_{k=1}^{\infty} \frac{Y_k(\omega)}{2^k}$ is uniformly distributed on $[0, 1]$.
\end{itemize}
\end{question}
\begin{proof}
\textbf{(Part 1)}
First notice
\begin{align*}
\mathcal{L}_X\left[\frac{m-1}{2^k},\frac{m}{2^k} \right]&=P\left(X\in \left[\frac{m-1}{2^k},\frac{m}{2^k} \right] \right)\\
&=P\left(\left(2^k \right) X \in \left[m-1,m \right] \right)
\end{align*}
Then we express $X$ in the form
 \begin{equation*}
X\left(\omega \right)=\sum_{i=1}^\infty \frac{Y_i\left(\omega \right)}{2^i}=\sum_{i=1}^k \frac{Y_i\left(\omega \right)}{2^i}+\sum_{i=k+1}^\infty \frac{Y_i\left(\omega \right)}{2^i}
\end{equation*}
Notice that because $Y_i\left(\omega \right)\in \set{0,1}$, we can compute 
\begin{equation*}
\sum_{i=k+1}^\infty \frac{Y_i\left(\omega \right)}{2^i}\in \left[0,\frac{1}{2^k} \right]
\end{equation*}
Then 
\begin{align*}
  2^k X\left(\omega \right)=\sum_{i=1}^k \left(2^{k-i} \right)Y_i\left(\omega \right)+ 2^k\sum_{i=k+1}^\infty \frac{Y_i\left(\omega \right)}{2^i}\text{ where }&2^k\sum_{i=k+1}^\infty \frac{Y_i\left(\omega \right)}{2^i}\in \left[0,1 \right]\\
  \text{ and }&\sum_{i=1}^k \left(2^{k-i}Y_i\left(\omega \right) \right)\inz
\end{align*}
Notice that we have 
\begin{equation*}
P\left( \sum_{i=k+1}^{\infty} \frac{Y_i\left(\omega \right)}{2^i}=0 \right)=0
\end{equation*}
Because of such, we can deduce
\begin{align*}
P\left(\left(2^k \right)X\in \left[m-1,m \right] \right)&=P\left( \sum_{i=1}^k \left(2^{k-i} \right)Y_i\left(\omega \right)+ 2^k\sum_{i=k+1}^\infty \frac{Y_i\left(\omega \right)}{2^i} \in \left[m-1,m \right]\right)\\
&=P\left( \sum_{i=1}^k \left(2^{k-i} \right)Y_i\left(\omega \right) =m-1\right)
\end{align*}
Let $N=m-1$. Express $N$ by binary
 \begin{equation*}
N=\sum_{j=0}^n a_j 2^j\text{ where $a_j\in \set{0,1}$ and $a_n=1$}
\end{equation*}
That is 
\begin{equation*}
N=a_n2^n+a_{n-1}2^{n-1}+\cdots +a_{1}2^1+a_{0}2^0
\end{equation*}
The premise of the problem include $\frac{m}{2^k}\leq 1$, so we have $n<k$. Then we can express  $N$ by
\begin{equation*}
N=a_{k-1}2^{k-1}+a_{k-2}2^{k-2}+\cdots + a_{0}2^0
\end{equation*}
Then we see 
\begin{align*}
P\left( \sum_{i=1}^k \left(2^{k-i} \right)Y_i\left(\omega \right) =m-1\right)&=P\left(  \sum_{i=1}^k \left(2^{k-i} \right)Y_i\left(\omega \right)=N \right)\\
&=P\left(\forall i:1\leq i\leq k, Y_i\left(\omega \right)=a_{k-i} \right)\\
&=\prod_{i=1}^k P\left(Y_i\left(\omega \right)=a_{k-i} \right)\\
&=\prod_{i=1}^k \frac{1}{2}=\frac{1}{2^k}
\end{align*}
\textbf{(Part 2)} Arbitrarily select an closed interval  $\left[a,b \right]$ contained by $\left[0,1 \right]$, we only wish to prove 
\begin{equation*}
\mathcal{L}_X \left[a,b \right]=b-a
\end{equation*}
Express $a,b$ by 
 \begin{equation*}
a=\sum_{n=1}^\infty \frac{a_n}{2^n}\text{ and }b=\sum_{n=1}^\infty \frac{b_n}{2^n}
\end{equation*}
Define
\begin{equation*}
A_k=\sum_{n=1}^k \frac{a_n}{2^n}\text{ and }B_k=\sum_{n=1}^k \frac{b_n}{2^n}\text{ where }\set{a_n:n\inn}\cup \set{b_n:n\inn}\subseteq \set{0,1}
\end{equation*}
We have 
\begin{equation*}
\lim_{k\to\infty} A_k=a\text{ and }\lim_{k\to\infty}B_k=b
\end{equation*}
Express $A_k,B_k$ by 
\begin{equation*}
  A_k=\frac{\sum_{n=1}^k \left( 2^{k-n}\right)a_n}{2^k}\text{ and }B_k=\frac{\sum_{n=1}^k \left( 2^{k-n}\right)b_n}{2^k}
\end{equation*}
Define $A'_k:=\sum_{n=1}^k \left(2^{k-n} \right)a_n$ and $B'_k=\sum_{n=1}^k \left(2^{k-n} \right)b_n$. It is easy to check $A'_k,B'_k \in \Z \cap \left[0,2^k \right]$. Then by the result in part 1, we have
\begin{align*}
\forall k,P\left(X\in \left[A_k,B_k \right] \right)&=P\left(X \in \left[\frac{A'_k}{2^k},\frac{B'_k}{2^k} \right] \right)\\
&=B_k-A_k
\end{align*}
Then we see 
\begin{align*}
\mathcal{L}_X \left[a,b \right]&=P\left(X\in \left[a,b \right]\right)\\
&=\lim_{k\to\infty} P\left(X\in \left[A_k,B_k \right] \right)\\
&=\lim_{k\to\infty} B_k-A_k\\
&=b-a
\end{align*}




\end{proof}
\section{Central Limit Theorem}
\begin{theorem}
\label{2.5.1}
\textbf{(Central Limit Theorem)} Let $X$ be an absolutely continuous random variable with expectation value  $\mu$ and variance $\sigma^2$. Define 
\begin{equation*}
Z_n:=\frac{X_1+\cdots +X_n-\mu n}{\sigma \sqrt{n} }
\end{equation*}
We have 
\begin{equation*}
\forall x, \lim_{n\to\infty} f_{Z_n}\left(x \right)=f_X\left(x \right)
\end{equation*}
\end{theorem}
\begin{corollary}
\label{2.5.2}
\textbf{(De Moivre-Laplace Central Limit Theorem)} Fix $p$. Suppose $S_n\sim \text{Bin}\left(n,p \right)$. Define $\mu_n:=E\left[S_n \right]=np$ and $\sigma_n:=\sqrt{\var \left[S_n \right]}=\sqrt{np\left(1-p \right)}  $. Define $Z_n:=\frac{S_n-\mu_n}{\sigma_n}$. We have 
\begin{equation*}
\forall k\inn,\lim_{n\to\infty} \frac{f_{S_n}\left(k \right)}{f_{Z_n}\left(k \right)}=1
\end{equation*}
In other words 
\begin{equation*}
\binom{n}{k}p^kq^{n-k}\approx \frac{1}{ \sqrt{2\pi npq} }e^{\frac{-\left(k-np \right)^2}{2npq \right)}}
\end{equation*}

\end{corollary}
\begin{corollary}
\label{2.5.3}
\textbf{(De Moivre-Laplace Central Limit Theorem)} 
Recall
\begin{equation*}
\Phi \left(x \right)=\int_{-\infty}^x \frac{1}{\sqrt{2\pi} }e^{\frac{-t^2}{2}}dt
\end{equation*}
Let
\begin{equation}
S_n\sim \text{Bin}(n,p)
\end{equation}
Fix $p$, we have
\begin{equation}
\forall x\inr,\lim_{n\to\infty} P(\frac{S_n-\mu_n}{\sigma_n}\leq x)=\Phi(x)
\end{equation}
where 
\begin{equation*}
\mu_n=E\left[S_n \right]=np\text{ and }\sigma_n=\sqrt{\var \left[S_n \right]}=\sqrt{np\left(1-p \right)}  
\end{equation*}
Then very weakly, we have
\begin{equation*}
  \lim_{n\to\infty}\frac{S_n-\mu_n}{\sigma_n} \sim N\left(0,1 \right)
\end{equation*}
\end{corollary}
\begin{corollary}
\label{2.5.4}
\textbf{(Approximation of Large Binomial R.V.)} Let $S_n\sim \text{Bin}\left(n,p \right)$. If $n$ is large enough, we can approximate
 \begin{align*}
P\left(m_1\leq S_n\leq m_2 \right)&=P\left(\frac{m_1-np}{\sqrt{npq} }\leq \frac{S_n-np}{\sqrt{npq} }\leq \frac{m_2-np}{\sqrt{npq} } \right)\\
&\approx \Phi\left(\frac{m_2-np}{\sqrt{npq} } \right)-\Phi\left( \frac{m_1-np}{\sqrt{npq} } \right)\\
&=\int_{\frac{m_1-np}{\sqrt{npq} }}^{\frac{m_2-np}{\sqrt{npq} } }  \frac{1}{\sqrt{2\pi} }e^{\frac{-x^2}{2}}dx
\end{align*}
And 
\begin{equation*}
P\left(S_n=m \right)\approx\frac{1}{\sqrt{2\pi npq} }e^{\frac{-\left(m-np \right)^2}{2npq}}
\end{equation*}
\end{corollary}
\begin{question}{}{}
A die is threw $12000$ times. What is the probability that there will be exactly $1800$ rolls of $6$? What is the probability that the number of $6$'s lies in the interval  $\left[1950,2100 \right]$
\end{question}
\begin{proof}
We have $n=12000$ and  $p=\frac{1}{6}$. Let $S_n\sim\text{Bin}\left(n,p \right)$. We have 
\begin{equation*}
P\left(S_n=1800 \right)\approx \frac{1}{\sqrt{2\pi \times 12000\times \frac{5}{36}}}e^{\frac{-\left(1800-2000 \right)^2}{2\times 12000\times \frac{5}{36}}}\sim 6.004\times 10^{-8}
\end{equation*}
Notice that the direct computation of $P\left(S_n=1800 \right)$ is $\binom{12000}{1800}\left(\frac{1}{6} \right)^{1800}\left(\frac{5}{6} \right)^{10200}$, in which the computation of $\binom{12000}{1800}$ is difficult even for computer.\\

Again, notice that the Poisson approximation $V\sim \text{Poisson}\left(2000 \right)$ of $P\left(V=1800 \right)$ is $\frac{e^{-2000}2000^{1800}}{1800!}$, in which the computation of $1800!$ is difficult even for computer.\\

Let $\sigma=\sqrt{npq} $ and $\mu=np$. We have 
\begin{align*}
  P\left(1950\leq S_n\leq 2100 \right)&\approx \int^{\frac{2100-\mu}{\sigma}}_{\frac{1950-\mu}{\sigma}} \frac{1}{\sqrt{2\pi} }e^{\frac{-x^2}{2}}dx\\
&=\Phi\left(\frac{2100-\mu}{\sigma} \right)-\Phi\left(\frac{1950-\mu}{\sigma} \right)\\
&\approx 0.8825
\end{align*}
\end{proof}
\chapter{Independence}
\section{Independence of Event}
\begin{definition}
\label{3.1.1}
\textbf{(Definition of Independence of Events)} Let $A,B$ be two events in  $\mathcal{F}$. We define
\begin{equation*}
P\left(A|B \right):=\frac{P\left(A\cap B \right)}{P\left(B \right)}
\end{equation*}
We say $A$ is independent of $B$ if 
\begin{equation*}
P\left( A|B\right)=P\left(A \right)
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.1.2}
\textbf{(Independence)}
\begin{align*}
  A\text{ is independent of $B$ }&\iff  P\left(A\cap B \right)=P\left(A \right)P\left(B \right)\\
  &\iff B\text{ is independent of $A$ }
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
  P\left( A|B\right)=P\left(A \right)&\iff \frac{P\left(A\cap B \right)}{P\left(B \right)}=P\left(A \right)\\
  &\iff P\left(A\cap B \right)=P\left(A \right)P\left(B \right)\\
  &\iff \frac{P\left(A\cap B \right)}{P\left(A \right)}=P\left(B \right)\\
  &\iff P\left(B|A\right)=P\left(B \right)
\end{align*}
\end{proof}
\begin{corollary}
\label{3.1.3}
\begin{equation*}
A,A^c\text{ are independent }\iff P\left(A \right)=0\text{ or }1
\end{equation*}
\begin{equation*}
\text{ Every event is independent of  }\Omega\text{ and }\varnothing
\end{equation*}
\end{corollary}
\begin{corollary}
\label{3.1.4}
\textbf{(Independence)} 
\begin{align}
  A\text{ is independent of $B$ }&\iff  A^c\text{ is independent of $B$ }\\
  &\iff A\text{ is independent of }B^c\\
  &\iff A^c\text{ is independent of $B^c$ }
\end{align}
\end{corollary}
\begin{proof}
We have 
\begin{align*}
P\left(A^c \cap B \right)&=P\left(B\setminus A \right)\\
&=P\left(B \right)-P\left(A\cap B \right)
\end{align*}
and
\begin{equation*}
P\left(A^c \right)=1-P\left(A \right)
\end{equation*}
Then we have 
\begin{equation*}
P\left(A^c \right)P\left(B \right)=P\left(B \right)-P\left(A \right)P\left(B \right)
\end{equation*}
This tell us 
\begin{equation*}
P\left(A^c\cap B \right)=P\left(A^c \right)P\left(B \right)\iff A,B\text{ are independent }
\end{equation*}
\end{proof}
\begin{corollary}
\label{3.1.5}
\begin{equation*}
A,B\text{ are independent }\iff  \forall A'\in \sigma\left(A \right),\forall B'\in \sigma\left(B \right), A',B'\text{ are independent }
\end{equation*}
\end{corollary}
\begin{definition}
\label{3.1.6}
\textbf{(Independence of Sub-$\sigma$-Algebra)} We say two sub-$\sigma$-algebras $\mathcal{G},\mathcal{H}\subseteq \mathcal{F}$ is independent if 
\begin{equation*}
\forall A\in \mathcal{G},\forall B\in \mathcal{H}, A,B\text{ are independent }
\end{equation*}
\end{definition}
\begin{corollary}
\label{3.1.7}
Given two event $A,B$
\begin{equation*}
\text{ $A,B$ are independent }\iff  \sigma\left(A \right)\text{ and }\sigma\left(B \right)\text{ are independent }
\end{equation*}
\end{corollary}
\begin{question}{}{}
Toss a coin twice with $\Omega_2=\set{HH,HT,TH,TT}$ and $\mathcal{F}_2\left(\text{with }\abso{\mathcal{F}_2}=16 \right)$ is the $\sigma$-algebra consisting of all information up to time $2$.\\

Let the probability of tossing a  $H$ in the first trial be $p_1\in \left(0,1 \right)$  and that for the second trial be $p_2\in \left(0,1 \right)$.\\

Define the probability $P$ as 
\begin{equation*}
P\left(HH \right)=p_1p_2,P\left(HT \right)=p_1\left(1-p_2 \right)\text{ vice versa }
\end{equation*}
Then, 
\begin{equation*}
  \left(\Omega_2,\mathcal{F}_2,P \right)\text{ is a probability space }
\end{equation*}
Let 
\begin{equation*}
\mathcal{G}=\set{\varnothing,\Omega_2,\set{HH,HT},\set{TH,TT}}\subset \mathcal{F}_2
\end{equation*}
be the sub-$\sigma$-algebra consisting of information of the first toss; while 
\begin{equation*}
\mathcal{H}=\set{\varnothing,\Omega_2,\set{HH,TH},\set{HT,TT}}\subset \mathcal{F}_2
\end{equation*}
be the sub-$\sigma$-algebra consisting of information of the second toss.\\

Show that 
\begin{equation*}
p_1=\frac{1}{2}=p_2\implies \mathcal{G},\mathcal{H}\text{ are independent }
\end{equation*}
and that 
\begin{equation*}
P\left(HH \right)=\frac{1}{9},P\left(HT \right)=\frac{2}{9},P\left(TH \right)=\frac{1}{3},P\left(TT \right)=\frac{1}{3}\implies \mathcal{G},\mathcal{H}\text{ are dependent }
\end{equation*}
\end{question}
\begin{proof}
Let $A=\set{HH,HT}$ and $B=\set{HH,TH}$. We see 
\begin{equation*}
\mathcal{G}=\set{\varnothing,\Omega_2, A,A^c}\text{ and }\mathcal{H}=\set{\varnothing,\Omega_2,B,B^c}
\end{equation*}
And 
\begin{equation*}
A\cap B=\set{HH}
\end{equation*}
Then if $p_1=\frac{1}{2}=p_2$, we see 
\begin{equation*}
P\left(A\cap B \right)=\frac{1}{4}=\frac{1}{2}\times \frac{1}{2}=P\left(A \right)P\left(B \right)
\end{equation*}
If $P\left(HH \right)=\frac{1}{9},P\left(HT \right)=\frac{2}{9},P\left(TH \right)=\frac{1}{3},P\left(TT \right)=\frac{1}{3}$, we see 
\begin{equation*}
P\left(A\cap B \right)=\frac{1}{9}\neq \frac{1}{3}\times \frac{4}{9}=P\left(A \right)P\left(B \right)
\end{equation*}
\end{proof}
\section{Independence of Random Variable}
\begin{definition}
\label{3.2.1}
\textbf{($\sigma$-Algebra Generated by Random Variable)} The $\sigma$-algebra $\sigma\left(X \right)$ generated by random variable $X$ is defined by
 \begin{equation*}
\sigma\left(X \right):=\set{X^{-1}\left[B \right]:B\in \mathcal{B}\left(\R \right)}
\end{equation*}
\end{definition}
\begin{definition}
\label{3.2.2}
\textbf{(Definition of Independence of Random Variable)} Given two random variable $X,Y$ on the same probability space  $(\Omega,\mathcal{F})$, we say $X,Y$ are independent if 
 \begin{align*}
\sigma (X)\text{ and }\sigma (Y)\text{ are independent }
\end{align*}
\end{definition}
\begin{lemma}
\label{3.2.3}
Suppose 
\begin{equation*}
A\text{ is independent of }C\text{ and }B\text{ is independent of  }C\text{ and }A\cap B=\varnothing
\end{equation*}
Then
\begin{equation*}
A\cup B\text{ is independent of $C$ }
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
P\left(\left(A\cup B \right)\cap C \right)&=P\left(\left(A\cap C \right)\cup \left(B\cap C \right) \right)\\
&=P\left(A\cap C \right)+P\left(B\cap C \right)\\
&=P\left(A \right)P\left(C \right)+P\left(B \right)P\left(C \right)\\
&=\left(P\left(A \right)+P\left(B \right) \right)P\left(C \right)\\
&=P\left(A\cup B \right)P\left(C \right)
\end{align*}
\end{proof}
\begin{question}{}{}
Show that in the probability space of the binomial asset pricing model with three independent tosses of the same coin, the two random variable  $S_2$ and $\frac{S_3}{S_2}$ are independent.
\end{question}
\begin{proof}
The $\sigma$-algebra $\sigma\left(S_2 \right)$ generated by $S_2$ is $\mathcal{F}_2$
\begin{equation*}
\sigma\left(S_2 \right)=\set{\bigcup B:B\subseteq \set{A_{HH},A_{TT},A_{HT}\cup A_{TH}}}
\end{equation*}
The $\sigma$-algebra $\sigma\left(\frac{S_3}{S_2} \right)$ generated by $\frac{S_3}{S_2}$ is 
\begin{equation*}
\sigma\left(\frac{S_3}{S_2} \right)=\set{\varnothing,\Omega, A_{\cdot \cdot H},A_{\cdot \cdot T}}
\end{equation*}
Observe 
\begin{equation*}
P\left(A_{HH}\cap  A_{\cdot \cdot H} \right)=P\left(\set{HHH} \right)=\frac{1}{p^3}=\frac{1}{p^2}\times \frac{1}{p}=P\left(A_{HH} \right)P\left(A_{\cdot \cdot H} \right)
\end{equation*}
and 
\begin{equation*}
P\left(A_{TT}\cap A_{\cdot \cdot H} \right)=P\left(\set{TTH} \right)=\frac{1}{q^2p}=\frac{1}{q^2}\times \frac{1}{p}=P\left(A_{TT} \right)P\left(A_{\cdot \cdot H} \right)
\end{equation*}
Then by \myref{corollary}{3.1.3} and \myref{lemma}{3.2.4}, we see 
\begin{equation*}
\sigma \left(S_2 \right),\sigma \left(\frac{S_3}{S_2} \right)\text{ are independent }
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.2.4}
\textbf{(Change of Independent Random Variable)} Given 
\begin{enumerate}[label=(\alph*)]
  \item $X,Y$ are independent
  \item $g,h$ are measurable.  
\end{enumerate}
Then
\begin{equation*}
g\left(X \right)\text{ and }h\left(Y \right)\text{ are independent }
\end{equation*}
\end{theorem}
\begin{proof}
This is an immediate consequence of 
\begin{equation*}
\sigma \left(g\left(X \right) \right)\subseteq \sigma \left(X \right)\text{ and }\sigma \left(h\left(Y \right) \right)\subseteq \sigma \left(Y \right)
\end{equation*}
\end{proof}
\fbox{\begin{minipage}{39em}
An example for \myref{Theorem}{3.2.4} is $X\left(H \right)=1$ and $X\left(T \right)=-1$. We see 
\begin{equation*}
\sigma \left(X \right)=\set{\varnothing,\set{H},\set{T},\Omega} 
\end{equation*}
and 
\begin{equation*}
\sigma\left(X^2 \right)=\set{\varnothing ,\Omega}\subset \sigma \left(X \right)
\end{equation*}
\end{minipage}}
\begin{theorem}
\label{3.2.5}
\textbf{(Joint Distribution of Independent Random Variable)} 
\begin{align*}
&X\text{ and }Y\text{ are independent }\\
\iff & \forall A,B \in \mathcal{B}(\R), P(X\in A\text{ and }Y\in B)=P(X\in A)P(Y\in B)
\end{align*}
\end{theorem}
\begin{proof}
$(\longleftarrow)$\\

Let $V\in \sigma (X)$ and $W\in \sigma (Y)$. We wish to prove 
\begin{align*}
  \vi{P(V\cap W)=P(V)P(W)}
\end{align*}
Because $V\in \sigma (X)$ and $W\in \sigma (Y)$, we know there exists $A,B\in \mathcal{B}(\R)$ such that 
\begin{align*}
V=X^{-1}[A]\text{ and }W=Y^{-1}[B]
\end{align*}
Then 
\begin{align*}
P(V\cap W)&=P(X^{-1}[A]\cap Y^{-1}[B])\\
&=P(\set{\omega \in \Omega: X\in A\text{ and }Y\in B})\\
&=P(X\in A\text{ and }Y\in B)\\
&=P(X\in A)P(Y\in B)\\
&=P(X^{-1}[A])P(Y^{-1}[B])\\
&=P(V)P(W)\vdone
\end{align*}
$(\longrightarrow)$\\

Let $A,B\in \mathcal{B}(\R)$. We wish to prove 
\begin{align*}
\blue{P(X\in A\text{ and }Y\in B)=P(X\in A)P(Y\in B)}
\end{align*}
In other words, we wish to prove 
\begin{align*}
P(\set{\omega\in\Omega:X\in A\text{ and }Y\in B})=P(\set{\omega\in\Omega:X\in A})P(\set{\omega\in\Omega:Y\in B})
\end{align*}
Notice 
\begin{align*}
\set{\omega \in \Omega:X\in A\text{ and }Y\in B}=X^{-1}[A]\cap Y^{-1}[B]
\end{align*}
and notice 
\begin{align*}
\set{\omega\in\Omega:X\in A}=X^{-1}[A]\in \sigma(X)\text{ and }\set{\omega\in\Omega:Y\in B}=Y^{-1}[B]\in \sigma (Y)
\end{align*}
Then because $X,Y$ are independent, we now have 
 \begin{align*}
P(\set{\omega\in \Omega:X\in A\text{ and }Y\in B})&=P(X^{-1}[A]\cap Y^{-1}[B])\\
&=P(X^{-1}[A])P(Y^{-1}[B])\\
&=P(\set{\omega\in \Omega:X\in A})P(\set{\omega\in\Omega:Y\in B}) \bdone
\end{align*}



\end{proof}
\begin{corollary}
\label{3.2.6}
\textbf{(Joint Distribution of Independent Random Variable)} Given two discrete random variable $X,Y$, we have
\begin{align*}
X\text{ and }Y\text{ are independent }\iff \forall (x,y)\in R(X)\times R(Y), f_{X,Y}(x,y)=f_X(x)f_Y(y)
\end{align*}
\end{corollary}
\begin{proof}
$(\longrightarrow)$\\

Notice that 
\begin{align*}
&f_{X,Y}(x,y)=P(X\in \set{x}\text{ and }Y\in \set{y})\\
\text{ and }&f_{X}(x)=P(X\in \set{x})\text{ and }f_Y(y)=P(Y\in \set{y})
\end{align*}
and we are done.\\

$(\longleftarrow)$\\

Let $A,B\in \mathcal{B}(\R)$. We wish to prove 
\begin{align*}
\vi{P(X\in A\text{ and }Y\in B)=P(X\in A)P(Y\in B)}
\end{align*}

\end{proof}
\begin{corollary}
\label{3.2.7}
\textbf{(Joint Distribution of Independent Random Variable)} Given two continuous random 
\end{corollary}
\section{Bayes' Theorem}
\begin{question}{}{}
We have the following knowledge 
\begin{equation*}
P\left(\text{ tested }H|\text{ being }H\right)=0.9
\end{equation*}
\begin{equation*}
P\left(\text{ tested }T|\text{ being }T\right)=0.8
\end{equation*}
\begin{equation*}
P\left(\text{ being }H \right)=0.05
\end{equation*}
Compute
\begin{equation*}
P\left(\text{ being }H|\text{ tested }H\right)
\end{equation*}
\end{question}
\begin{proof}
We have 
\begin{equation*}
\begin{cases}
  \frac{P\left(\text{ tested }H\cap \text{ being }H \right)}{P\left(\text{ being }H \right)}=0.9\\
  P\left(\text{ being }H \right)=0.05
\end{cases}
\end{equation*}
So we have 
\begin{equation*}
P\left(\text{ tested }H\cap \text{ being }H \right)=0.045
\end{equation*}
Also, the fact $P\left(\text{ being }H \right)=0.05$ tell us $P\left(\text{ being }T \right)=0.95$. Then from 
\begin{equation*}
\begin{cases}
  \frac{P\left(\text{ tested }T\cap \text{ being }T \right)}{P\left(\text{ being }T \right)}=0.8\\
  P\left(\text{ being }T \right)=0.95
\end{cases}
\end{equation*}
We have
\begin{equation*}
P\left(\text{ tested }T\cap \text{ being }T \right)=0.76
\end{equation*}
This give us 
\begin{equation*}
P\left(\text{ tested }H\cap \text{ being }T \right)=P\left(\text{ being }T \right)-P\left(\text{ tested }T\cap \text{ being }T \right)=0.19
\end{equation*}
Then we have 
\begin{equation*}
P\left(\text{ tested }H \right)=P\left(\text{ tested }H\cap \text{ being }H \right)+P\left(\text{ tested }H\cap \text{ being }T \right)=0.235
\end{equation*}
Then
\begin{equation*}
\frac{P\left(\text{ being }H\cap \text{ tested }H \right)}{P\left(\text{ tested }H \right)}=\frac{0.045}{0.235}
\end{equation*}
\end{proof}
\begin{theorem}
\label{3.3.1}
\textbf{(Bayes's Theorem)}
\begin{equation*}
P\left(B|A\right)=\frac{P\left(B\cap A \right)}{P\left(A \right)}=\frac{P\left(A|B \right)P\left(B \right)}{P\left(A \right)}
\end{equation*}
\end{theorem}
\begin{question}{}{}
We have the following knowledge 
\begin{equation*}
P\left(\text{ tested }H|\text{ being }H\right)=0.9
\end{equation*}
\begin{equation*}
P\left(\text{ tested }T|\text{ being }T\right)=0.8
\end{equation*}
\begin{equation*}
P\left(\text{ being }H \right)=0.05
\end{equation*}
Compute
\begin{equation*}
P\left(\text{ being }H|\text{ tested }H\right)
\end{equation*}
\end{question}
\begin{proof}
Let $B=\left(\text{being } H \right)$  and $A=\left(\text{tested } H \right)$. By Bayes' Theorem, we have 
\begin{equation*}
P\left(B|A \right)=\frac{P\left(A\cap B \right)}{P\left(A \right)}=\frac{P\left(A|B \right)P\left(B \right)}{P\left(A \right)}=\frac{0.9\times 0.05}{0.235}
\end{equation*}
where $P\left(A \right)$ is computed as above.
\end{proof}
\begin{theorem}
\label{3.3.2}
\textbf{(First Bayes' Theorem)} Let $\set{F_1,\dots ,F_n}\subseteq \mathcal{F}$ be a partition of $\Omega$. We have 
\begin{align*}
P(A)=\sum_{k=1}^n P(A|F_k)P(F_k)
\end{align*}
\end{theorem}
\begin{question}{}{}
Consider the situation: We try to put $u$ balls into distinct $n$ barrels. Let $p_{r,k}$ be the probability that there are exactly $r$ barrels filled with balls, when the first  $k\leq u$ balls is putted.\\

Find $p_{n,u}$ by giving a recurrence relation. 
\end{question}
\begin{proof}
We have 
\begin{align*}
p_{r,k}=p_{r,k-1} \frac{r}{n}+p_{r-1,k-1} \frac{n-(r-1)}{n}
\end{align*}
In fact, we have 
\begin{align*}
  p_{r,k}&=\\
&=\sum_{i=0}^n p_{i,{k-1}}P(\text{$r$ barrels is filled with the first $k$ balls}|\text{$i$ barrels are filled with the first  $k-1$ balls})
\end{align*}
where we define $P(A|B)=0$ if $P(B)=0$.\\

The reason we have the recurrence relation is that the conditional probability is $0$ when  $i<r-1$ or $i>r$.    
\end{proof}
\begin{theorem}
\label{3.3.3}
\textbf{(Second Bayes' Theorem)} Let $\set{F_1,F_2,\dots , F_n}\subseteq \mathcal{F}$ be a partition of $\Omega$. We have 
\begin{equation*}
P\left(F_j|A \right)=\frac{P\left(A|F_j \right)P\left(F_j \right)}{\sum_{i=1}^n P\left(A|F_i \right)P\left(F_i \right)}
\end{equation*}
\end{theorem}
\begin{proof}
\begin{align*}
P\left(F_j|A \right)&=\frac{P\left(F_j\cap A \right)}{P\left(A \right)}\\
&=\frac{P\left(A|F_j \right)P\left(F_j \right)}{P\left(A \right)}\\
&=\frac{P\left(A|F_j \right)P\left(F_j \right)}{P\left(A\cap F_1 \right)+P\left(A\cap F_2 \right)+\cdots P\left(A\cap F_n \right)}\\
&=\frac{P\left(A|F_j \right)P\left(F_j \right)}{\sum_{i=1}^n P\left(A|F_i \right)P\left(F_i \right)}
\end{align*}
\end{proof}
\section{Independent Disjoint Grouping Theorem}
\begin{definition}
\label{3.4.1}
\textbf{(Definition of Sub-$\sigma$-Algebra Being Independent)} In $\left(\Omega,\mathcal{F},P \right)$, we say a finite amount of sub-$\sigma$-algebra $\set{\mathcal{G}_1,\mathcal{G}_2,\dots ,\mathcal{G}_n}$ is independent if we arbitrarily choose one event $G_i$ from each sub-$\sigma$-algebra $\mathcal{G}_i$, we have
\begin{equation*}
P\left(\bigcap_{i=1}^n G_i \right)=\prod_{i=1}^n P\left(G_i \right)
\end{equation*}
\end{definition}
\begin{definition}
\label{3.4.2}
\textbf{(Definition of Set of Random Variable Being Independent)} We say a set of random variable $\set{x_1,\dots ,x_n}$ is independent if 
\begin{equation*}
\set{\sigma \left(X_1 \right),\sigma \left(X_2 \right),\dots ,\sigma\left(X_n \right)}\text{ are independent }
\end{equation*}
\end{definition}
\begin{theorem}
\label{3.4.3}
\textbf{(Example)}  
\begin{equation*}
\set{X_1,X_2,X_3,X_4}\text{ are independent }\implies X_2X_3\text{ is independent of }X_1+\cos \left(2X_4 \right)
\end{equation*}
\end{theorem}
\chapter{Multiple Random Variable}
\section{Joint Distribution}
\begin{definition}
\label{4.1.1}
\textbf{(Definition of Random Vector)} Given $(\Omega,\mathcal{F},P)$ and two random variables $X,Y$ on $(\Omega,\mathcal{F})$. Consider the random vector
\begin{equation*}
  (X,Y):(\Omega,\mathcal{F},P)\rightarrow (\R \times \R,\sigma(\mathcal{B}\times \mathcal{B}))
\end{equation*}
Notice that $\sigma (\mathcal{B}\times \mathcal{B})$ is the smallest sigma algebra that contain $\mathcal{B}\times \mathcal{B}$. This shows that not all element element in $\sigma (\mathcal{B},\mathcal{B})$ is rectangular. That is, not all element is of the form 
\begin{equation*}
A\times B
\end{equation*}
We see 
\begin{equation*}
[0,2]\times [1,2]\text{ and }[1,2]\times [0,2]\text{ are both element of $\sigma(\mathcal{B},\mathcal{B})$ }
\end{equation*}
and their union is not rectangle. The induced measure by the pair of random vector is defined to be
\begin{equation*}
\mathcal{L}_{X,Y}(C)=P(\set{\omega\in \Omega:(X,Y)\in C})
\end{equation*}
Notice that given rectangular $C=A\times B$ where $A,B \in \mathcal{B}$. We have
\begin{equation*}
\set{\omega \in \Omega :(X,Y)\in C}=\set{\omega\in \Omega :X \in A}\cap \set{\omega\in \Omega :X\in B}
\end{equation*}
This kind of expression can only be given if $C$ is rectangular.
\end{definition}
\begin{definition}
\label{4.1.2}
\textbf{(Definition of Discrete Joint Distribution)} Given $(\Omega,\mathcal{F},P)$ and two discrete random variable $X,Y$. The random vector 
 \begin{equation*}
   (X,Y):(\Omega,\mathcal{F},P)\rightarrow (\R\times \R, \sigma (\mathcal{B}\times \mathcal{B}))
\end{equation*}
has joint probability $p_{(X,Y)}$ defined on $R(X)\times R(Y)$ by
\begin{align*}
  p_{(X,Y)}(x,y)&:=P(X=x,Y=y)\\
  &=P(\omega \in \Omega: (X,Y)=(x,y))\\
  &=\mathcal{L}_{(X,Y)}(\set{x},\set{y})
\end{align*}
\end{definition}
\begin{theorem}
\label{4.1.3}
\textbf{(Computation of Probability)} Let $A\in \sigma (\mathcal{B}\times \mathcal{B})$  be a joint Borel set. We have 
\begin{align*}
  P((X,Y)\in A)&=P(\omega\in \Omega:(X,Y)\in A)\\
  &=\sum_{(x,y)\in A}P(\set{\omega\in \Omega: (X,Y)=(x,y)})\\
  &=\sum_{(x,y)\in A}p_{(X,Y)}(x,y)
\end{align*}
\end{theorem}
\begin{corollary}
\label{4.1.4}
We have 
\begin{align*}
P(X=x)&=P((X,Y)\in \set{x}\times R(Y))\\
&=\sum_{y \in R(Y)} p_{(X,Y)}(x,y)\\
&=\sum_{y \in R(Y)} P(X=x,Y=y)
\end{align*}
\end{corollary}
\begin{question}{}{}
An urn has 2 red, 5 white and 3 green balls. Select 3 balls at random and let 
\begin{enumerate}[label=(\alph*)]
  \item $X$ be the number of red balls
   \item  $Y$ be the number of white balls 
\end{enumerate}
We have the following table

\renewcommand{\arraystretch}{1.2} % Slightly increase the padding between rows

\newcolumntype{P}[1]{>{\hspace{2pt}}p{#1}<{\hspace{2pt}}}
\begin{center}
   \begin{minipage}{0.9\linewidth}  
       \centering
\begin{tabular}{c|P{2cm}P{2cm}P{2cm}|c}
\hline
$y \backslash x$ & 0 & 1 & 2 & $P(Y = y)$ \\
\hline
0 & $\frac{1}{120}$ & $\frac{2 \cdot 3}{120}$ & $\frac{3}{120}$ & $\frac{10}{120}$ \\
1 & $\frac{5 \cdot 3}{120}$ & $\frac{2 \cdot 5 \cdot 3}{120}$ & $\frac{5}{120}$ & $\frac{50}{120}$ \\
2 & $\frac{10 \cdot 3}{120}$ & $\frac{10 \cdot 2}{120}$ & 0 & $\frac{50}{120}$ \\
3 & $\frac{10}{120}$ & 0 & 0 & $\frac{10}{120}$ \\
\hline
$P(X = x)$ & $\frac{56}{120}$ & $\frac{56}{120}$ & $\frac{8}{120}$ & 1\\
\hline
\end{tabular}
   \end{minipage}
\end{center}
One should notice that 
\begin{align*}
P(X=x,Y=y)&= \frac{\binom{2}{x}\binom{5}{y}\binom{ 3}{3-x-y}}{\binom{10}{3}}\text{    where }\binom{n}{m}=0\text{ if } m<0
\end{align*}
With this table, we can compute 
\begin{align*}
P(X=2|X\geq Y)=\frac{8}{45}
\end{align*}
\end{question}
\begin{definition}
\label{4.1.5}
\textbf{(Definition of Jointly Continuous Random Variable)} Let $X,Y$ be two continuous random variable. There exists  $f:R(X)\times R(Y)\rightarrow \R$ such that 
\begin{align*}
P((X,Y)\in S)=\iint_S f(x,y) dxdy
\end{align*}
\end{definition}
\section{Conditional Probability and Expectation}
\begin{theorem}
\label{4.2.1}
\textbf{(Conditional Probability of Discrete Random Variable)} Given two discrete random variable  $X,Y:(\Omega,\mathcal{F},P)\rightarrow \R$, we have 
\begin{align*}
P(X=x|Y=y)=\frac{p_{X,Y}(x,y)}{p_{Y}(y)}
\end{align*}
\end{theorem}
\begin{theorem}
\label{4.2.2}
\textbf{(Conditional Probability of Continuous Random Variable)} Given two continuous random variable $X,Y:(\Omega,\mathcal{F},P)\rightarrow \R$, we have 
\begin{align*}
P(X\in [a,b]|Y=y)=\frac{\int_a^b f_{X,Y}(x,y)dx}{f_{Y}(y)}
\end{align*}
\end{theorem}
\begin{definition}
\label{4.2.3}
\textbf{(Conditional Distribution)} Given two discrete random variables, we write 
\begin{align*}
p_X(x|Y=y)=P(X=x|Y=y)
\end{align*}
and two continuous random variables, we write 
\begin{align*}
f_X(x|Y=y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}
\end{align*}
\end{definition}
\begin{definition}
\label{4.2.4}
\textbf{(Definition of Conditional Expectation)} Conditional Expectation of $Y$ given  $X=x$ is defined by 
 \begin{align*}
E[Y|X=x]&=\sum_{y \in R(Y)}y P(Y=y|X=x)\\
&=\int_\R y f_{Y}(y|X=x)dy
\end{align*}
\end{definition}
\begin{theorem}
\textbf{(Tower Property)} We have 
\begin{align*}
E[Y]=E[E[Y|X=x]]
\end{align*}
\end{theorem}
\begin{proof}
This is clear as one can deduce 
\begin{align*}
E[E[Y|X=x]]&= \sum_{x\in R(X)} E[Y|X=x] p_X(x)\\
&=\sum_{x\in R(X)} \sum_{y \in R(Y)} y p_Y(y |X=x)p_X(x)\\
&=\sum_{y\in R(Y)}y\sum_{x\in R(X)} p_{X,Y}(x,y)\\
&=\sum_{y \in R(Y)} y p_Y(y)\\
&=E[Y]
\end{align*}
\end{proof}
\fbox{\begin{minipage}{39em}
Conjecture: Provided $\forall \omega, P(\set{\omega})\inr$, is it true
\begin{align*}
\forall A \in \sigma (E[X|Y]), \sum_{\omega \in A} E[X|Y]P(\set{\omega}) = \sum_{\omega \in A} X P(\set{\omega})
\end{align*}
?
\end{minipage}}
\section{On Expectation}
\begin{theorem}
\label{4.3.1}
\textbf{(Expectation of Real Valued Function of Random Variable)} Given $g:\R\rightarrow \R$ and $X:\Omega\rightarrow \R$, we have 
\begin{align*}
E[g(X)]&=\sum_{x \in R(X)} g(x)p_X(x)\\
\text{ or if continuous }&=\int_\R g(x)f_X(x)dx
\end{align*}
\end{theorem}
\begin{corollary}
\label{4.3.2}
\textbf{(Expectation of Linear Change of Random Variable)} Given $X:\Omega\rightarrow \R$, we have 
\begin{align*}
E[aX+b]=aE[X]+b
\end{align*}
\end{corollary}


\begin{theorem}
\label{4.3.3}
\textbf{(Expectation of Addition of Random Variables)} Given random variables $X_1,\dots, X_n$
\begin{align*}
E[\sum_{k=1}^n X_k]=\sum_{k=1}^n E[X_k]
\end{align*}
\end{theorem}
\begin{theorem}
\label{4.3.4}
\textbf{(Expectation of Real Valued Function of Multiple Random Variables)} Given $g:\R^2\rightarrow \R$ and $X,Y:\Omega \rightarrow \R$, we have 
\begin{align*}
E[g(X,Y)]&=\sum_{(x,y)\in R(X)\times R(Y)} g(x,y)p_{X,Y}(x,y)\\
\text{ or if continuous }&=\iint_{\R^2} g(x,y)f_{X,Y}(x,y)dxdy
\end{align*}
\end{theorem}
\begin{corollary}
\label{4.3.5}
\textbf{(Expectation of Product of Independent Random Variables)} Given $X,Y:\Omega \rightarrow \R$, we have 
\begin{align*}
E[g(X)h(Y)]=E[g(X)]\times E[h(Y)]
\end{align*}
\end{corollary}
\begin{proof}
If $X,Y$ are discrete 
 \begin{align*}
E[g(X)h(Y)]&=\sum_{(x,y)\in R(X)\times R(Y)} g(x)h(y)p_{X,Y}(x,y)\\
&=\sum_{(x,y)\in R(X)\times R(Y)}g(x)p_X(x)h(y)p_Y(y)\\
&=\sum_{x\in R(X)}\sum_{y \in R(Y)}g(x)p_X(x)h(y)p_Y(y)\\
&=\sum_{x\in R(X)}g(x)p_X(x) \sum_{y \in R(Y)}h(y)p_Y(y)\\
&=E[g(X)]\times E[h(Y)]
\end{align*}
If $X,Y$ are continuous 
 \begin{align*}
E[g(X)h(Y)]&= \iint_{\R^2} g(x)h(y)f_{X,Y}(x,y)dxdy\\
&=\iint_{\R^2}g(x)h(y)f_X(x)f_Y(y)dxdy\\
&=\int_\R g(x)f_X(x)dx\int_\R h(y)f_Y(y)dy\\
&=E[g(X)]\times E[h(Y)]
\end{align*}
\end{proof}
\begin{question}{}{}
Let $(X,Y)$ be a random point in the triangle $\set{(x,y):x,y\geq 0,x+y\leq 1}$. Show that $X,Y$ are not independent and  
\begin{align*}
E[XY]\neq E[X]\times E[Y]
\end{align*}
\end{question}
\begin{proof}
Notice the area of the triangle is $\frac{1}{2}$, so we have 
\begin{align*}
f_{X,Y}(x,y)=\begin{cases}
  2& \text{ if $(x,y)$ in the triangle }\\
  0& \text{ if else where }
\end{cases}
\end{align*}
Compute the marginal probability 
\begin{align*}
f_X(x)&=\int_0^{1-x} f_{X,Y}(x,y)dy\\
&=\int_0^{1-x} 2dy=2-2x
\end{align*}
Notice the symmetry so we have 
\begin{align*}
f_Y(y)=2-2y
\end{align*}
Then it is quite clear that $X,Y$ are not independent, as we can observe  $f_{X,Y}(0,0)=2\neq 4=f_X(0)f_Y(0)$
\end{proof}
Compute 
\begin{align*}
E[X]&=\int_0^1 x f_X(x)dx\\
&=\int_0^1 2x-2x^2 dx\\
&=x^2-\frac{2x^3}{3}\at_{x=0}^1\\
&=1-\frac{2}{3}=\frac{1}{3}
\end{align*}
and compute 
\begin{align*}
E[XY]&=\int_0^1\int_0^{1-x} xy f_{X,Y}(x,y)dydx\\
&=2\int_0^1\int_0^{1-x}xydydx\\
&=2\int_0^1 \frac{xy^2}{2}\at_{y=0}^{1-x}dx\\
&=\int_0^1 x(1-x)^2 dx\\
&=\int_0^1 x^3-2x^2+xdx\\
&=\frac{x^4}{4}-\frac{2x^3}{3}+\frac{x^2}{2}\at_{x=0}^1\\
&=\frac{1}{4}-\frac{2}{3}+\frac{1}{2}=\frac{1}{12}\neq \frac{1}{9}=E[X]\times E[Y]
\end{align*}
\begin{question}{}{}
Let $(X,Y)$ be a random point in the diamond $\set{(x,y):
\abso{x}\leq 1\text{ and }\abso{y}\leq 1}$. Show that $X,Y$ are not independent, yet 
 \begin{align*}
E[XY]=E[X]\times E[Y]
\end{align*}
\end{question}
\begin{proof}
The area is $2$. Then the density function is 
 \begin{align*}
f_{X,Y}(x,y)=\begin{cases}
  \frac{1}{2}& \text{ if $(x,y)$ in the diamond }\\
  0& \text{ if not }
\end{cases}
\end{align*}
Compute the marginal density 
\begin{align*}
f_X(x)&=\int_{-\abso{1-x}}^{\abso{1-x}} f_{X,Y}(x,y)dy\\
&=\abso{1-x}
\end{align*}
We see $X,Y$ are not independent as 
\begin{align*}
f_{X,Y}(0,0)=\frac{1}{2}\neq 1=f_X(0)\times f_Y(0)
\end{align*}
Compute 
\begin{align*}
E[XY]&=\int_0^1 \int_{-\abso{1-x}}^{\abso{1-x}}\frac{xy}{2}dydx\\
&=\int_0^1 \frac{xy^2}{4}\at_{y=-\abso{1-x}}^{\abso{1-x}}dx=0
\end{align*}
and also  
\begin{align*}
E[Y]=E[X]=\int_{-1}^1 x\abso{1-x}dx=0(\because \text{ odd function })
\end{align*}
\end{proof}
\begin{question}{}{}
Assumethat an urn contains 10 black, 7 red, and 5 white balls. Select 5 balls 
\begin{enumerate}[label=(\alph*)]
  \item with and 
  \item without replacement
\end{enumerate}
and let $X$ be the number of red balls selected. Compute $E[X]$
\end{question}
\begin{proof}
Let $X=\sum_{i=1}^5 X_i$ where 
\begin{align*}
p_{X_i}(1)=\frac{7}{22}\text{ and }p_{X_i}(0)=\frac{15}{22}
\end{align*}
We have 
\begin{align*}
E[X]&=E[\sum_{i=1}^5 X_i]\\
&=\sum_{i=1}^5 E[X_i]\\
&=\sum_{i=1}^5 \frac{7}{22}=\frac{35}{22}
\end{align*}
For $(b)$, we again have  $E[X_i]=\frac{7}{22}$. The fact they are dependent doesn't matter, we still have
\begin{align*}
E[X]=\sum_{i=1}^5 E[X_i]=\frac{35}{22}
\end{align*}
\end{proof}
\begin{question}{}{}
$n$ people bought $n$ gifts. The gifts then are distributed to the $n$ people at random. Let $X$ be the number of people who receive their own gift. Compute  $E[X]$
\end{question}
\begin{proof}
The question is ambiguous, but the ambiguity doesn't make our question have multiple answer. The question doesn't specified that whether every one get one gift or if one can get multiple. Yet, if we let 
\begin{align*}
I_i=I_{\text{person $i$ receive own gift}}
\end{align*}
Former situation leads to the dependence of variables while the latter leads to the independence. However, dependent or not, because 
\begin{align*}
X=\sum_{i=1}^{n}I_i
\end{align*}
we have 
\begin{align*}
E[X]=\sum_{i=1}^{n}E[I_i]=n (\frac{1}{n})=1
\end{align*}
\end{proof}
\begin{question}{}{}
Five married couples are seated around a table at random. Let $X$ be the number of wives who sit next to their husbands. What is  $E[X]$? 
\end{question}
\begin{proof}
Let 
\begin{align*}
X=\sum_{i=1}^5 I_i
\end{align*}
We have $E[I_i]=\frac{2}{9}$, then 
\begin{align*}
  E[X]=\sum_{i=1}^5 E[I_i]=\frac{10}{9}
\end{align*}
\end{proof}
\begin{question}{}{}
Sample from $n$ cards, with replacement, indefinitely. Let $N$ be the number of draws you need to get a collection of all $n$ cards. Compute $E[N]$
\end{question}
\begin{proof}
Let $\Omega$ be the set of sequences of the card drawn. Let $N_1(\omega)$ be the number of card you need to draw to get the first new card if $\omega$ is given. Let $N_2(\omega)$ be the number of additional cards you need to draw after the first new card to get the second new card. Random Variables $N_{i}(\omega)$ is defined similarly. Then we see 
\begin{align*}
N=\sum_{i=1}^n N_i
\end{align*}
It is quite clear that 
\begin{align*}
N_i \sim \text{Geo}(\frac{n-i+1}{n})
\end{align*}
Then we have 
\begin{align*}
E[N]=\sum_{i=1}^n E[N_i]=\sum_{i=1}^n \frac{n}{n-i+1}=n\sum_{i=1}^n \frac{1}{i} 
\end{align*}
\end{proof}
\begin{question}{}{}
Assume that an urn contain $10$ black,  $7$ red, and  $5$ white balls. Select  $5$ balls 
 \begin{enumerate}[label=(\alph*)]
  \item with replacement 
  \item without replacement
\end{enumerate}
Let $W$ be the number of whit balls selected, and $Y$ the number of different colors. Compute  $E[W],E[Y]$
\end{question}
\begin{proof}
In both $(a),(b)$, 
\begin{align*}
E[W]=\sum_{i=1}^5 E[I_i]=\frac{25}{22}
\end{align*}
Let 
\begin{align*}
Y=I_B+I_R+I_W
\end{align*}
where $I_B$ is the indicator that if black ball is selected in the 5 balls.\\


In $(a)$, we have 
\begin{align*}
E[I_B]&=P(I_B=1)=1-(\frac{12}{22})^5\\
E[I_R]&=1-(\frac{15}{22})^5\\
E[I_W]&=1-(\frac{17}{22})^5
\end{align*}
Then 
\begin{align*}
E[Y]=3-\frac{12^5+15^5+17^5}{22^5}
\end{align*}
In $(b)$, we have 
\begin{align*}
E[I_B]&=1-\frac{\binom{12}{5}}{\binom{22}{5}}\\
E[I_R]&=1-\frac{\binom{15}{5}}{\binom{22}{5}}\\
E[I_W]&=1-\frac{\binom{17}{5}}{\binom{22}{5}}
\end{align*}
Then 
\begin{align*}
E[Y]=3-\frac{\binom{12}{5}+\binom{15}{5}+\binom{17}{5}}{\binom{22}{5}}
\end{align*}
\end{proof}
\section{Covariance}
\begin{definition}
\label{4.4.1}
\textbf{(Definition of Covariance)} The covariance of two random variable $X,Y$ is defined by 
\begin{align*}
\text{Cov}[X,Y]=E[(X-E[X])(Y-E[Y])]
\end{align*}
\end{definition}
\begin{theorem}
\label{4.4.2}
\textbf{(Formula for Covariance)} We have 
\begin{align*}
\text{Cov}[X,Y]=E[XY]-E[X]\times E[Y]
\end{align*}
\end{theorem}
\begin{proof}
\myref{Theorem}{4.3.3} let us deduce
\begin{align*}
\text{Cov}[X,Y]&=E[(X-E[X])(Y-E[Y])]\\
&=E[XY-Y\times E[X]-X \times E[Y]+E[X]\times E[Y]]\\
&=E[XY]-E[X]\times E[Y]-E[Y]\times E[X]+E[X]\times E[Y]\\
&=E[XY]-E[X]\times E[Y]
\end{align*}
\end{proof}
\begin{theorem}
\label{4.4.3}
\textbf{(Independent Implies Zero Covariance)}
\begin{align*}
X,Y\text{ are independent }\implies \text{Cov}[X,Y]=0
\end{align*}
\end{theorem}
\begin{proof}
See \myref{Theorem}{4.3.4}
\end{proof}
\fbox{\begin{minipage}{39em}
An example for $X,Y$ be dependent, yet $\text{Cov}[X,Y]=0$ is question 10 in last section.
\end{minipage}}
\section{Variance of Sum of Random Variables}
\fbox{\begin{minipage}{39em}
At now, one should notice that expectation of sum of random variables is just the sum of expectations of random variables with or without independence.\\

Also, the expectation of product of independent random variables is the product of expectation of independent variables. Notice that this won't necessarily hold true if variables are independent. 
\end{minipage}}
\begin{theorem}
\label{4.5.1}
\textbf{(Variance-Covariance Formula)} We have
\begin{align*}
  E[(\sum_{i=1}^n X_i)^2]&=\sum_{i=1}^n E[X_i^2]+\sum_{i\neq j} E[X_iX_j]\\
  \var[\sum_{i=1}^n X_i]&=\sum_{i=1}^n \var [X_i]+\sum_{i\neq j} \text{Cov}[X_i,X_j]
\end{align*}
\end{theorem}
\begin{proof}
Notice 
\begin{align*}
  (\sum_{i=1}^n X_i)^2 = \sum_{i=1}^n X_i^2 + \sum_{i\neq j} X_iX_j
\end{align*}
Then the proof follows from the fact expectation is a homomorphism from set of random variables on $(\Omega,\mathcal{F},P)$ to $(\R,+)$.\\

Compute 
\begin{align*}
  \var [\sum_{i=1}^n X_i]&= E[(\sum_{i=1}^m X_i)^2]-(E[\sum_{i=1}^n X_i])^2\\
  &=\sum_{i=1}^n E[X_i^2]+\sum_{i\neq j}E[X_iX_j]-(\sum_{i=1}^n E[X_i])^2\\
  &=\sum_{i=1}^n E[X_i^2]+\sum_{i\neq j}E[X_iX_j]-(\sum_{i=1}^n E^2[X_i] + \sum_{i\neq j}E[X_i]E[X_j])\\
  &=\sum_{i=1}^n E[X_i^2]-E^2[X_i]+\sum_{i\neq j}E[X_iX_j]-E[X_i]E[X_j]\\
  &=\sum_{i=1}^n \var [X_i]+\sum_{i \neq j} \text{Cov}[X_i,X_j]
\end{align*}
\end{proof}
\begin{corollary}
\label{4.5.2}
\textbf{(Variance of Sum of Mutually Independent Random Variables Is the Sum of Variances of Mutually Independent Random Variables)} If $X_1,\dots ,X_n$ are mutually independent, then 
\begin{align*}
  \var [\sum_{i=1}^n X_i]=\sum_{i=1}^n \var [X_i]
\end{align*}
\end{corollary}
\begin{proof}
Because $X_1,\dots ,X_n$ are mutually independent, we can deduce 
\begin{align*}
\var [\sum_{i=1}^n X_i]&=\sum_{i=1}^n \var [X_i]+\sum_{i\neq j}\text{Cov}[X_i,X_j]\\
&=\sum_{i=1}^n \var [X_i]
\end{align*}
\end{proof}
\end{document}
